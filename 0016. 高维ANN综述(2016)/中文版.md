## Approximate Nearest Neighbor Search on High Dimensional Data — Experiments, Analyses, and Improvement  



# 0. Abstract

最近邻搜索是许多领域（如数据库、机器学习、多媒体和计算机视觉）中的基本且重要的操作。由于在高维空间中精确搜索的效率不高，许多研究者将重点转向近似最近邻搜索。尽管每年文献中不断提出许多算法，但对它们性能的全面评估和分析仍然缺乏。在本文中，我们对多种最先进的近似最近邻搜索方法进行了全面的实验评估。我们的研究 (1) 是跨学科的（即包括来自不同领域的19种算法以及实践者的贡献），并且 (2) 评估了多种设置，包括20个数据集、多个评估指标和不同的查询负载。实验结果经过仔细报告和分析，以理解性能表现。此外，我们提出了一种新方法，实证显示在广泛的设置下，在大多数数据集上同时实现了高查询效率和高召回率。

# 1. INTRODUCTION  

最近邻搜索（NNS）旨在从参考数据库中找到与查询对象距离最小的对象。这是许多领域（包括数据库、计算机视觉、多媒体、机器学习和推荐系统）中的基本且重要的操作。

尽管对此问题进行了大量研究，但普遍认为在高维欧几里得空间中找到精确最近邻是非常昂贵的，这被称为维度灾难[1]。实验表明，在高维（例如，维度超过20）时，精确方法很少能超过暴力线性扫描方法的性能[2]。然而，返回足够接近的对象，即近似最近邻搜索（ANNS），可以高效地执行，并且对许多实际问题非常有用，因此吸引了大量的研究工作。

## 1.1. Motivation  

There are hundreds of papers published on algorithms for (approximate) nearest neighbor search, but there has been few systematic and comprehensive comparisons among these algorithms. In this paper, we conduct a comprehensive experimental evaluation on the state-of-the-art approximate nearest neighbor search algorithms in the literature, due to the following needs:

---

**1. Coverage of Competitor Algorithms and Datasets from Different Areas.** As the need for performing ANNS arises naturally in so many diverse domains, researchers have come up with many methods while unaware of alternative methods proposed in another area. In addition, there are practical methods proposed by practitioners and deployed in large-scale projects such as the music recommendation system at spotify. com [3]. As a result, it is not uncommon that important algorithms from different areas are overlooked and not compared with. For example, there is no evaluation among Rank Cover Tree [4] (from Machine Learning), Product Quantization [5], [6] (from Multimedia), SRS [7] (from Databases), and KGraph [8] (from practitioners). Moreover, each domain typically has a small set of commonly used datasets to evaluate ANNS algorithms; there are very few datasets used by all these domains.

---

In contrast, we conduct comprehensive experiments using carefully selected representative or latest algorithms from different domains, and test all of them on 20 datasets including those frequently used in prior studies in different domains. Our study confirms that there are substantial variability of the performance of all the algorithms across these datasets.

---

**2. Overlooked Evaluation Measures/Settings.** An ANNS algorithm can be measured from various aspects, including (i) search time complexity, (ii) search quality, (iii) index size, (iv) scalability with respect to the number of objects and the number of dimensions, (v)robustness against datasets, query workloads, and parameter settings, (vi) updatability, and (vii) efforts required in tuning its parameters.

---

Unfortunately, none of the prior studies evaluates these measures completely and thoroughly.

For example, most existing studies use a query workload that is essentially the same as the distribution of the data. Measuring algorithms under different query workloads is an important issue, but little result is known. In this paper, we evaluate the performance of the algorithms under a wide variety of settings and measures, to gain a complete understanding of each algorithm (c.f., Table 3).

---

**3. Discrepancies in Existing Results.** There are discrepancies in the experimental results reported in some of the notable papers on this topic.

For example, AGH was shown to perform better than SpectralHashing in the literature [9], while the study in [10] indicates otherwise. This situation also exists between SpectralHashing and DSH [10], [11]. While much of the discrepancies can be explained by the different settings, datasets and tuning methods used, as well as implementation differences, it is always desirable to have a maximally consistent result to reach an up-to-date rule-of-the-thumb recommendation in different scenarios for researchers and practitioners.

---

In this paper, we try our best to make a fair comparison of several algorithms, and test them on all 20 datasets. Finally, we will also publish the source code, datasets, and other documents so that the results can be easily reproduced.

We classify popular ANNS algorithms into three categories: Hashing-based, Partition-based and Graph-based. The key idea of each category of the method will be introduced in Section 3.

## 1.2. Contributions  

Our principle contributions are summarized as follows.  

Comprehensive experimental study of state-of-the-art ANNS methods across several different research areas. Our comprehensive experimental study extends beyond past studies by: (i) comparing all the methods without adding any implementation tricks, which makes the comparison more fair; (ii) evaluating all the methods using multiple measures; and (iii) we provide rule-of-the-thumb recommendations about how to select the method under different settings. We believe such a comprehensive experimental evaluation will be beneficial to both the scientific community and practitioners, and similar studies have been performed in other areas (e.g., classification algorithms [12]).

---

We group algorithms into several categories (Section 3), and then perform detailed analysis on both intra- and inter-category evaluations (Sections 5 and 6). Our data-basedanalyses provide confirmation of useful principles to solve the problem, the strength and weakness of some of the best methods, and some initial explanation and understanding of why some datasets are harder than others. The experience and insights we gained throughout the study enable us to engineer a new empirical algorithm, DPG (Section 4), that achieves both high query efficiency and high recall empirically on majority of the datasets under a wide range of settings.

---

The rest of the paper is organised as follows. Section 2 introduces the problem definition and some constraints in this paper. Section 3 presents the related research for ANNS problem and Section 4 presents our improved ANNS approach. In Sections 5 and 6, the comprehensive experiments and the analyses are reported. Finally, we conclude this paper in Section 7.



# 2. BACKGROUND  

## 2.1. Problem Definition  

In this paper, we focus on the case where data points are $d$-dimensional vectors in $\mathbb{R}^d$ and the distance metric is the euclidean distance. Exact nearest neighbor search (NNS) for a given query point is defined as returning the data point with the smallest distance from the query. A generalization of the nearest neighbor search is called k-nearest-neighbor search (k-NNS), which aims to the $k$ closest vectors for the query.

---

Due to the curse of dimensionality, much research efforts focus on the approximate solution for the problem of NNS and k-NNS on high dimensional data, which means we hope the algorithms (defined as approximate NNS (ANNS) and approximate k-NNS (k-ANNS) respectively) returns as many true neighbors as possible.

## 2.2. Scope  

The problem of ANNS on high dimensional data has been extensively studied in various literature. Over hundreds of algorithms have been proposed to solve the problem from different perspectives, and this line of research remains very active in the above domains due to its importance and the huge challenges. To make a comprehensive yet focused comparison of ANNS algorithms, in this paper, we restrict the scope of the study by imposing the following constraints.

---

**Representative and Competitive ANNS Algorithms.** We consider the state-of-the-art algorithms in several domains, and omit other algorithms that have been dominated by them unless there is strong evidence against the previous findings.

**No Hardware Specific Optimizations.** Not all the implementations we obtained or implemented have the same level of sophistication in utilizing the hardware specific features to speed up the query processing. We pay more attention on the query technology itself, and weaken the implementation skills. Therefore, we modified several implementations so that no algorithm uses multiple threads, multiple CPUs, SIMD instructions, hardware pre-fetching, or GPUs.

**Dense Vectors.** We treat the input data as dense vectors, taking no account of the specific processing for sparse data.

**Support the Euclidian Distance.** The euclidean distance is one of the most widely used measure on high-dimensional datasets. It is also supported by most of the ANNS algorithms.

**Exact $k$-NN as the Ground Truth.** In several existing works, each data point has a label (typically in classification or clustering applications) and the labels are regarded as the ground truth when evaluating the recall of approximate k-NN algorithms. However, The labels mostly do not exist for large data sets. In this paper, we use the exact $k$-NN points as the ground truth as this works for all datasets and majority of the applications.

---

Prior Benchmark Studies: There are two recent ANNS benchmark studies: [13] and ann - benchmark [14]. The former considers a large number of other distance measure in addition to the euclidean distance, and the latter does not disable general implementation tricks. In both cases, their studies are less comprehensive than ours, e.g., with respect to the number of algorithms and datasets evaluated.



# 3. RELATED WORK  

We classify the state-of-the-art ANNS algorithms into three main categories: **Hashing-based**, **Partition-based** and **Graph-based**.

## 3.1. Hashing-based Methods  

The algorithms belonging to this class transform data point to a low-dimensional representation, so each point could be represented by a short code (called hash code). There are two main sub-categories in this class: locality sensitive hashing (LSH) and learning to Hash (L2H).

### 3.1.1. Locality Sensitive Hashing  

Locality sensitive hashing (LSH) is data-independent hashing approach. The LSH methods rely on a family of locality sensitive hash functions that map similar input data points (distance $<r$ ) to the same hash codes with higher probability than dissimilar points (distance $>c r$ ), so LSH methods are initial designed to solve ( $\mathrm{r}, \mathrm{c}$ )-ANN problem. The designing of good locality sensitive hash functions is vital for LSH-related methods. For Euclidian distance measure, a great number of hash functions are proposed [15], [16], [17], [18], [19]. Random linear projections [15], [20], [21], [22] are the most commonly used hash function to generate hash code, in which the random projection parameters are chosen from a 2-stable distribution (e.g., Gaussian distribution).

---

In order to achieve good search precision, several hash functions are concatenated to form a hash table, thus decreasing the collision probability for dissimilar points. While it also reduces the collision probability of nearby points, so one usually requires to construct multiple hash tables, leading to large memory cost and long query time. Hence, some heuristic methods [23], [24], [25] are presented to check more hash buckets which may contain the nearest neighbor or the candidates near the query point, so as to increase the search quality without increasing the number of hash tables.

---

Because the hash tables are constructed before searching, the points collilded with the query in a part of hash functions in one hash table are neglected although they are likely near. Hence, instead of using "static" compound hash functions to construct hash table before searching, some recent LSHbased methods (e.g., C2LSH [26], LazyLSH [27], QALSH [25] ) employ dynamic collision counting scheme for more efficient searching.

---

LSH-based methods are widely studied by the theory community and enjoy the sound probabilistic theoretical guarantees on query result quality (based on distance ratios), efficiency, and index size even in the worst case. Note that the soundness of theoretical guarantees of LSH algorithms relies on the assumption that: given two data points, the hash functions are selected randomly and independently [28].

### 3.1.2. Learning to Hash (L2H)  

Learning to Hash methods fully make use of the data distribution to generate specific hash functions, leading to higher efficiency at the cost of relinquishing the theoretical guarantees. The main methodology of Learning to Hash methods is similarity-preserving, so that the ANN relationships between the data points in the original space could be maximally preserved in the hash coding space.

---

According to the difference of the optimization objective design to preserve similarity, the learning to hash algorithms could be grouped into the following classes: pairwise-similarity persevering class [9], [29], [30], [31], [32], multiwise-similarity persevering class [33], [34], implicitlysimilarity persevering class [35], [36] and quantization class [5], [37], [38]. More related references could be found in [39], [40], [41]. Besides similarity persevering criterion, most of the hashing methods require the codes to be balance and uncorrelated.

---

Many literature indicates that the quantization algorithms are more efficient than other learning to hash methods. The quantization-based methods seek to minimize the quantization distortion (equal to the sum difference of each data point and its approximation). Product Quantization (PQ) [5] is a popular methods for ANNS, which decomposes the original vector space into the Cartesian product of $M$ lower dimensional subspaces, and performs vector quantization [42] in each subspace separately. A vector is then represented by a short code composed of its subspace quantization indices. Recently, there are many extensions are proposed to improve the performance of PQ for indexing step [6], [43], [44], [45], [46] and searching step [46], [47], [48], [49]. For example, Optimized Product Quantization (OPQ) [6] use pre-rotation to further minimize the quantization distortion. Additive Quantization (AQ) [50] and Composite Quantization (CQ) [51] are the generalization of PQ and represent a vector as the sum of $M D$-dimensional vectors where $D$ is equal to the dimension of input data.

---

Benefiting from the development of deep neural network, deep hashing methods that employ deep learning are widely studied in recent years. As we doesn't use label information, we only introduce unsupervised deep hashing methods in this paper. More evaluation of supervised deep hashing algorithms could be found in [52]. Semantic hashing [53] is the first work on using deep learning techniques for hashing, which builds a multi-layers Restricted Boltzmann Machines (RBM) to learn compact binary codes for text and documents. In order to learn the binary codes, most of deep hashing methods design a sign activation layer to produce binary codes and minimize the loss between the compact real-valued code and the learned binary vector [54], [55], [56], [57]. Another solution is to reconstruct the original data. For example, [58], [59] use autoencoder as hidden layers. Thanh-Toan etc. [60] propose to constrain the penultimate layer to directly output the binary.

---

Because hashing methods must obtain the binary code from the output of hash functions, the binary constraint optimization problem is an NP-hard problem. To ease the optimization, most of the hashing methods adopt the following "relaxation + rounding" approach, which makes the binary codes are suboptimal. For handling this problem, some discrete optimization methods are developed [30], [61], [62].

## 3.2. Partition-based Methods  

Methods in this category can be deemed as dividing the entire high dimensional space into multiple disjoint regions. Let the query $q$ be located in a region $r_q$, then its nearest neighbors should reside in $r_q$ or regions near $r_q$.

---

The partition process often carry out in a recursive way, so partition-based methods are best represented by a tree or a forest. Based on the way to partition, there are mainly three kinds: pivoting, hyperplane and compact partitioning schemes. Pivoting methods divide the points relying on the distances from the point to some pivots (usually randomly chosen). Algorithms in this class contain VP-Tree [63], Ball Tree [64] etc. Hyperplane partitioning methods recursively divide the space by the hyperplane with random direction (e.g., Annoy [3], Random-Projection Tree [65]) or axis-aligned separating hyperplane (e.g., Randomized KDtrees) [66], [67]. Compact partitioning algorithms either divide the data into clusters [68] or create possibly approximate Voronoi partitions [69], [70] to exploit locality.

## 3.3. Graph-based Methods  

Graph-based methods construct a proximity graph where each data point corresponds to a node and edges connecting some nodes define the neighbor-relationship. The main idea of these methods is a neighbor's neighbor is likely to also be a neighbor. The search could be efficiently performed by iteratively expanding neighbors' neighbors in a best-first search strategy following the edges.

---

According to the difference of the graph structures, graphbased methods are divided into several classes. The first one tries to build an exact or approximate k-nearest neighbor graph that records the top- $k$ nearest neighbors for each node. Especially for high dimensional space, the approximate knearest neighbor graph construction methods were widely studied recently [71], [72], [73], [74], [75], [76]. With the support of kNN graph, nearest neighbor search is conducted by hill-climbing strategy [77] and usually assigns some random data points as initial enter points, which is easy to get trapped in local optimal. In order to obtain better starting points, some schemes are proposed to locate some initial entries quickly. For example, Wei Dong uses LSH to generate the initial points in his thesis [78]. IEH [79] and Efanna [80] employ hashing based methods and Randomized KD-tree for initialization.

---

The second class is a proximity graph called navigable Small World graph (SW-graph) [81]. The SW-graph is an undirected graph, which contains an approximation of the Delaunay graph and has long-range links together with the small-world navigation property. Thus, it is more efficient for the nearest neighbor search. Yury et al. [82] proposed NSW method to build an SW-graph by iteratively inserted the points where each point is linked to some nodes selected by a greedy search algorithm from the graph in building. But the degree of NSW is too high to be efficient and there also exist connectivity problems in it. HNSW [83] is an Extension of NSW. It generates a multi-layer proximity graph with different scales and uses a heuristic to preferentially select the neighbors in diverse directions. HNSW is one of the most efficient ANNS algorithms so far.



# 4. DIVERSIFIED PROXIMITY GRAPH  

The experience and insights we gained from this study enable us to engineer a new method, Diversified Proximity Graph (DPG ), which constructs a different proximity graph to achieve better and more robust search performance.

## 4.1. Motivation  

The construction of K-NN graph mainly consider the distances of neighbors for each data point, But intuitively we should also consider the coverage of the neighbors. As shown in Fig. 1, the two closest neighbors of the point $p$ are $a_3$ and $a_4$, and hence in the $2-\mathrm{NN}$ graph $p$ cannot lead the search to the NN of $q$ (i.e., the node $b$ ) although it is close to $b$. Since $a_1, \ldots, a_4$ are clustered, it is not cost-effective to retain both $a_3$ and $a_4$ in the K-NN list of $p$. This motivates us to consider the direction diversity (i.e., angular dissimilarity) of the K-NN list of $p$ in addition to the distance, leading to the diversified K-NN graph. Regarding the example, including $a_3$ and $b$ is a better choice for the K-NN list of $p$.

---

Now assume we have replaced edge $\left(p, a_4\right)$ with the edge $(p, b)$ (i.e., the dashed line in Fig. 1), but there is still another problem. As we can see that there is no incoming edge for $p$ because it is relatively far from two clusters of points (i.e., $p$ is not 2-NN of these data points). This implies that $p$ is isolated, and two clusters are disconnected in the example. This is not uncommon in high dimensional data due to the phenomena of "hubness"[84] where a large portion of data points rarely serve as K-NN of other data points, and thus have no or only a few incoming edges in the K-NN graph. This motivates us to also use the reverse edges in the diversified K-NN graph; that is, we keep an bidirected diversified K-NN graph as the index, and we name it Diversified Proximity Graph (DPG).

## 4.2. Diversified Proximity Graph  

The construction of DPG is a diversification of an existing KNN graph, followed by adding reverse edges. Given a reference data point $p$, the similarity of two points $x$ and $y$ in $p^{\prime}$ s KNN list $\mathcal{L}$ is defined as the angle of $\angle x p y$, denoted by $\theta(x, y)$. We aim to choose a subset of $\kappa$ data points, denoted by $\mathcal{S}$, from $\mathcal{L}$ so that the average angle between two points in $\mathcal{S}$ is maximized; or equivalently,  $\displaystyle{}\mathcal{S}=\arg \min _{\mathcal{S} \subseteq \mathcal{N},|\mathcal{S}|=\kappa} \sum_{o_i, o_j \in \mathcal{S}} \theta\left(o_i, o_j\right)$.

---

The above problem is NP-hard [85]. Hence, we design a simple greedy heuristic. Initially, $\mathcal{S}$ is set to the closest point of $p$ in $\mathcal{L}$. In each of the following $\kappa-1$ iterations, a point is moved from $\mathcal{L} \backslash \mathcal{S}$ to $\mathcal{S}$ so that the average pairwise angular similarity of the points in $\mathcal{S}$ is minimized. Then for each data point $u$ in $\mathcal{S}$, we include both edges $(p, u)$ and $(u, p)$ in the diversified proximity graph. The time complexity of the diversification process is $O\left(\kappa^2 K n\right)$ where $n$ is the number of data points, and there are totally at most $2 \kappa n$ edges in the diversified proximity graph.

---

It is critical to find a proper $K$ value for a desired $\kappa$ in the diversified proximity graph as we need to find a good trade-off between diversity and proximity. In our empirical study, the DPG algorithm usually achieves the best performance when $K=2 \kappa$. Thus, we set $K=2 \kappa$ for the diversified proximity graph construction. This greedy algorithm has the time complexity of $O\left(\kappa^2 K n\right)$. We actually implemented a simplified version whose complexity is $O\left(K^2 n\right)$, which has only slightly worse performance than the full greedy version, but significantly fewer diversification time (with full details in Appendix D, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/TKDE.2019.2909204).

Note that the search process of the DPG is the same as that of KGraph.



# 5. EXPERIMENTS  

## 5.1. Experimental Setting  

### 5.1.1. Compared Algorithms  

We test 19 representative existing ANNS algorithms from three categories and our proposed diversified proximity graph (DPG ) method. The detailed description of the test algorithm can be found in A. All of the modified source code used in this paper are public available on GitHub [86].

---

(1) **LSH-based Methods**. We evaluate Query-Aware LSH [87] (QALSH, PVLDB’15), SRS [7] (SRS, PVLDB’14) and FALCONN [19] (FALCONN,NIPS’15) in this class.

---

**(2) L2H-based Methods.** We evaluate Scalable Graph Hashing [88] (SGH, IJCAI’15), Anchor Graph Hashing [9] (AGH, ICML’11) and Neighbor-Sensitive Hashing [10] (NSH, PVLDB’15) from binary-encoded learning to hash methods. In order to do non-exhaustive search, We organize the hash codes with the hierarchical clustering tree [67]. The comparisons with linear scan search are given in Appendix E, available in the online supplemental material.

Selective Hashing [89] (SH, KDD’15) is an extension of LSH, which considers the local density of each point. Selective Hashing performs hash table lookup to search. We classify Permutation-based methods into L2H-based Methods. Permutation-based methods encode each point based on the order to some random selected pivots. Neighborhood APProximation index [13] (NAPP PVLDB’15) is one of the most efficient Permutation-based methods, which relies on inverted index for searching.  

We also evaluate Optimal Product Quantization[6] (OPQ, TPAMI’14) with non-exhaustive search, which is implemented by the inverted multi-indexing [47].  

Composite Quantization [51] (CQ, ICML’11) conducts linear scan search and a lookup table is used to pre-compute the distances from the query to the codewords of each subquantizer.  

---

**(3) Partition-based Methods.** We evaluate FLANN[67] (TPAMI’14), FLANN-HKM, FLANN-KD, Annoy and an advanced Vantage-Point tree [90] (VP  tree, NIPS’13) in this class.  

---

**(4) Graph-based Methods.** We evaluate Small World Graph [82] (SW, IS’14), Hierarchical Navigable Small World [83] (HNSW, CoRR’16), K-NN graph [8], [74] (KGraph, WWW’11), Rank Cover Tree [4] (RCT, TPAMI’15), and our Diversified Proximity Graph (DPG). Note that though the tree structure is employed by RCT, the key idea of RCT belongs to connect the nearest nodes between each adjacent layer. Thus, we divide RCT into Graph-based Methods.  

---

We use the implementation of NAPP, VP - tree, SW and HNSW from the Non-Metric Space Library (NMSLIB). We carefully tuned the hyper-parameters for each algorithm and each dataset. More detailed could be found in Appendix C , available in the online supplemental material. Considering of the comparison fairness, we would like to focus on the algorithm perspective of the existing methods. we turned off all hardware-specific optimizations in the implementations of the methods. Specifically, we disabled distance computation using SIMD and multi-threading in KGraph, -ffast-math compiler option in Annoy, multithreading in FLANN, and distance computation using SIMD, multi-threading, prefetching technique implemented in the NMSLIB, i.e., SW, NAPP, VP - tree and HNSW ). In addition, we disabled the optimized search implementation used in HNSW.

---

**Computing Environment.** All C++ source codes are complied by g++ 4.7, and MATLAB source codes (only for index construction of some learning to hash algorithms) are compiled by MATLAB 8.5. All experiments are conducted on a Linux server with Intel Xeon 8 core CPU at 2.9 GHz , and 32 G memory.

### 5.1.2. Datasets and Query Workload  

We deploy 18 real datasets used by existing works which cover a wide range of applications including image, audio, video and text. We also use two synthetic datasets. Table 1 summarizes the characteristics of the datasets including the number of data points, dimensionality, Relative Contrast (RC [91]), local intrinsic dimensionality (LID [92]), and data type. RC indicates the ratio of the mean distance and NN distance for the data points, and smaller RC value implies harder dataset. LID calculates the local intrinsic dimensionality and a higher LID value implies harder dataset. We mark the first four datasets in Table 1 with asterisks to indicate that they are "hard" datasets compared with others according to their RC and LID values.

---

**Query Workload.** Following the convention, we randomly remove 200 data points as the query points for each dataset. The average performance of the $k-\mathrm{NN}$ searches is reported. In this paper, $k$ is equal to 20 by default. We evaluate the impact of $k$ in Appendix H, available in the online supplemental material.

## 5.2. Evaluation Measures  

For each algorithm, we retrieve $N$ points based on its searching process and then rerank these candidates using original features. The search quality is measured using recall, precision, accuracy and mAP. The recall is the ratio of the true nearest items in the retrieved $N$ items to $k$. The precision is defined as the ratio of the number of retrieved true items to $N$. F-score (F1 score) is the harmonic mean of precision and recall: $F 1=2 *$ precision $*$ recall $/($ precision + recall $)$. Then mean average precision $(m A P)$ is computed as the mean of average precisions over all the queries. Accuracy is equal to $\displaystyle\sum_{i=0}^{i=k} \frac{\operatorname{dist}(q, k A N N(q)[i])}{\operatorname{dist}(q, k N N(q)[i])}$, where $q$ is a query, $k N N(q)[i]$ is $q$ ‘s $i$th true nearest neighbor, and $k A N N(q)[i]$ is $i$ th nearest neighbor estimated by one ANNS algorithm.

---

The search efficiency is usually measured as the time taken to return the search results for a query. For most of the algorithms (expect for Graph-based methods), we could vary the number of the retrieved points $N$ to get different pair of recall/ precision/accuracy and its corresponding search time. Since exact $k$-NN can be found by a brute-force linear scan algorithm, we use its query time as the baseline and define the speedup as $\frac{\bar{t}}{t^{\prime}}$, where $\bar{t}$ is query time for linear scan and $t^{\prime}$ is the search time at a specific recall or $N$. For instance, we come up with a speedup 10 if an algorithm takes 1 second while the linear scan takes 10 seconds.

---

In addition to evaluating the search performance, we also evaluate other aspects such as index construction time, index size, index memory cost and scalability.

## 5.3. Comparison with Each Category  

In this subsection, we evaluate the trade-offs between speedup and recall of all the algorithms on Sift and Notre data in each category. Given the large number of algorithms in the hashing-based category, we evaluate them in LSHbased and learning to hash-based subcategories separately. The goal of this round of evaluation is to select several algorithms from each category as the representatives in the second round evaluation (Section 5.4).

### 5.3.1. LSH-based Methods  

Figs. 2 a and 2 e plot the trade-offs between the speedup and recall of two most recent data-independent algorithms SRS and QALSH on Sift and Notre . Note that, as FALCONN doesn't provide theoretical guarantee on L2 distance, we evaluate it in the second round.

---

As both algorithms are originally external memory based approaches, we evaluate the speedup by means of the total number of pages of the dataset divided by the number of pages accessed during the search. It shows that SRS consistently outperforms QALSH, and the similar trend is observed on other datasets. Thus, SRS is chosen as the representative in the second round evaluation where a covertree based in-memory implementation will be used.

### 5.3.2. Learning to Hash-based Methods  

We evaluate seven learning to hash based algorithms including OPQ, NAPP, SGH, AGH, NSH, SH and CQ. Figs. $2 b$ and 2 f demonstrate that, of all methods, the search performance of OPQ beats other algorithms by a big margin. Due to the linear scan search employed in the CQ, it shows a poor performance in terms of speedup@recall. In fact, CQ is very competitive in recall@N, but the indexing time of CQ is extremely high. So its hard to apply in practice.

---

For most of datasets, SelectiveHashing has the largest index size because it requires multiple long hash tables to achieve high recall. The index time value of OPQ has a strong association with the length of the sub-codeword and dimension of the data point. Nevertheless, the index construction time of OPQ still turns out to be very competitive compared with other algorithms in the second round evaluation. Therefore, we choose OPQ as the representative of the learning to hash based methods.

### 5.3.3. Partition-based Methods  

We evaluate three algorithms in this category: FLANN, Annoy and VP - tree. To better illustrate the performance of FLANN, we report the performance of both randomized kdtrees and hierarchical $k$-means tree, namely FLANN - KD and FLANN - HKM, respectively. Note that among 20 datasets, the randomized kd-trees method (FLANN - KD ) is chosen by FLANN in five datasets: Enron, Trevi, UQ - V, BANN and Gauss. The linear scan is used in the hardest dataset: Rand, and the hierarchical $k$-means tree (FLANN $-H K M$ ) is employed in the remaining 14 datasets.

---

Figs. 2 c and $2 g$ show that Annoy and FLANN - HKM have good performance on both datasets. For all datasets, Annoy, FLANN - HKM and FLANN - KD can obtain the highest performance on different datasets.

---

As the search performance of VP - tree is not competitive compared to FLANN and Annoy under all settings, it is excluded from the next round evaluation.

### 5.3.4. Graph-based Methods  

In the category of Graph-based methods, we evaluate four existing techniques: KGraph, SW, HNSW and RCT . Figs. 2d and 2 h show that the search performance of KGraph, SW and HNSW substantially outperforms that of RCT on Sift and Notre. Because HNSW is an improvement version of SW and can achieve better performance on all datasets, we discard SW from the next round evaluation.

---

Although the construction time of KGraph and HNSW are relatively large, due to the outstanding search performance, we choose them as the representatives of the graphbased methods. Note that we delay the comparison of DPG to the second round.

## 5.4. Second Round Evaluation  

In the second round evaluation, we conduct comprehensive experiments on eight representative algorithms: SRS, FALCONN, OPQ, FLANN, Annoy, HNSW, KGraph, and DPG .

### 5.4.1. Comparison of Search Quality and Time  

In the first set of experiments, Fig. 3 reports the speedup of eight algorithms when they reach the recall around 0.8 on 20 datasets. Note that the speedup is set to 1.0 if an algorithm cannot outperform the linear scan algorithm. Among eight algorithms, DPG and HNSW have the best overall search performance and KGraph follows. It is shown that DPG enhances the performance of KGraph, especially on hard datasets: Nusw, Gist, Glove and Rand . As reported thereafter, the improvement is also more significant on higher recall. For instance, DPG is ranked after KGraph on four datasets under this setting (recall 0.8), but it eventually surpasses KGraph on higher recall. Overall, DPG and HNSW have the best performance for different datasets. Not surprisingly, SRS is slower than other competitors with a huge margin as it does not exploit the data distribution. Similar observations are reported in Fig. 4, which depicts the recalls achieved by the algorithms with speedup around 50.

---

Fig. 5 illustrates speedup of the algorithms on eight datasets with recall varying from 0 to 1 . It further demonstrates the superior search performance of DPG on high recall. The overall performance of HNSW, KGraph and Annoy are also very competitive, followed by FLANN. It is shown that the performance of both DPG and KGraph are ranked lower than that of HNSW, Annoy, FLANN and OPQ in Fig. 5h where the data points are clustered. As thereafter discussed in Section 6, Annoy, FLANN and OPQ essentially use the variants of $k$-means approach, and hence can well handle the clustered data. HNSW uses a heuristic to increase the probability of building the links between the clusters. FALCONN significantly outperforms SRS on all datasets, and it also surpasses the tree-based methods and learning to hash methods on some hard datsets, such as Glove, Nusw and random. It is worth noting that FALCONN even outperforms graph-based methods for Gauss dataset.

---

In Fig. 6, we evaluate the recalls of the algorithms against the percentage of data points accessed. As the search of most Graph-based methods starts from random entrance points and then gradually approaches the results while other algorithms initiate their search from the most promising candidates, the search quality of Graph-based methods is outperformed by Annoy, FLANN and even OPQ at the beginning stage. While, benefiting from HNSW's hierarchical structure, it could continue the search from the element which is the local optimum in the previous layer. The entries of each layer are carefully selected to ensure it could locate the closer points of the query quickly.

---

Fig. 7 shows the range search quality for a specific recall. Smaller accuracy indicates the closer of the results and the query, so the search quality is better. SRS and FALCONN which are designed for c-ANN search outperform all other algorithms. 

---

In Figs. 8 and 9, we evaluate the precision@recall and F1@recall for each algorithm and report the mAP in Table 2. The results further verify our observations in Fig. 6. Tree-based methods and HNSW could find the closer neighbors after retrieving small points.

### 5.4.2. Comparison of Indexing Cost  

In addition to search performance, we also evaluate the index size, memory cost and construction time. Fig. 10 reports ratio of the index size (exclude the data points) and the data size. Except Annoy, the index sizes of all algorithms are smaller than the corresponding data sizes. The index sizes of DPG, KGraph, HNSW, SRS and FALCONN are irrelevant to the dimensionality because a fixed number of neighbor IDs and projections are kept for each data point. Consequently, they have a relatively small ratio on data with high dimensionality (e.g., Trevi ). Overall, OPQ and SRS have the smallest index sizes, less than 5 percent among most of the datasets, followed by FALCONN, HNSW, DPG, KGraph and FLANN . It is shown that the rank of the index size of FLANN varies dramatically over 20 datasets because it may choose three possible index structures. Annoy needs to maintain a considerable number of trees for a good search quality, and hence has the largest index size.

---

Fig. 11 reports the index construction time on 20 datasets. FALCONN has the smallest index construction time among all the test algorithms. SRS ranks the second. The construction time of OPQ is related to the dimensionality because of the calculation of the sub-codewords (e.g., Trevi ). HNSW, KGraph and DPG have similar construction time. Compared with KGraph, DPG doesn't spend large extra time for the graph diversification. Nevertheless, they can still build the indexes within one hour for 16 out of 20 datasets.

---

Fig. 12 reports the index memory cost of the algorithms on 20 datasets. OPQ needs less memory resource to build the index, so it is very efficient for large-scale datasets.

## 5.5. Summary  

Table 3 ranks the performance of the eight algorithms from various perspectives including search performance, index size, index construction time, index memory cost, and scalability. We also indicate that SRS is the only one with theoretical guarantee of searching quality, and it is very easy to tune the parameters for search quality and search time.

Below are some recommendations for users according to our comprehensive evaluations.

---

- When there are sufficient computing resources (both main memory and CPU) for the off-line index construction, and sufficient main memory to hold the resulting index, DPG and HNSW are the best choices for ANNS on high dimensional data due to their outstanding search performance in terms of robustness to the datasets, result quality, search time and search scalability.

---

- Except DPG and HNSW, we also recommend Annoy, due to its excellent search performance, construction cost, and robustness to the datasets. Moreover, it can provide a good trade-off between search performance and index size/ construction time.

---

- If the construction time is a big concern, FALCONN would be a good choice because of its small construction time and good search performance.

---

- To deal with large scale datasets (e.g., 1 billion of data points) with moderate computing resources, $O P Q$ and $S R S$ are good candidates due to their small memory cost and construction time. It is worthwhile to mention that, SRS can easily handle the data points updates and have theoretical guarantee, which distinguish itself from other seven algorithms.

# 6. FURTHER ANALYSES  

In this section, we analyze the most competitive algorithms in our evaluations, grouped by category, in order to understand their strength and weakness.

## 6.1. Space Partition-based Approach  

Our comprehensive experiments show that Annoy and FLANN have the best performance among the space partitionbased methods. Note that FLANN chooses FLANN-HKM in most of the datasets. In addition, $O P Q$ divides the space by utilizing the production of k - means on $M$ disjoint subspaces. Therefore, all three algorithms are based on $k$-means space partitioning. We define the algorithms who borrow the idea of $k$-means as $k$-means-like algorithms.

---

We identify that a key factor for the effectiveness of $k$ means-like algorithms is that the large number of clusters, typically $\Theta(n)$. Note that we cannot directly apply k - means with $k=\Theta(n)$ because (i) the index construction time complexity of k - means is linear to $k$, and (ii) the time complexity to identify the partition where the query is located takes $\Theta(n)$ time. Both OPQ and FLANN - HKM / Annoy achieve this goal indirectly by using the ideas of subspace partitioning and recursion, respectively.

---

We perform experiments to understand which idea is more effective. We consider the goal of achieving $k$-means-like space partitioning with approximately the same number of non-empty partitions. Specifically, for Audio dataset, we consider the following choices: (i) Use $O P Q$ with 2 subspaces and each has 256 clusters. The number of effective partitions $k$ (i.e., non-empty partitions) is counted. For Audio dataset, $k=18,611$. (ii) Use original FLANN-HKM to build a tree with roughly $k$ leaf nodes. After carefully tuning the value of branching factor $L$, we found the number of total leaf nodes is 18000 when $L=42$. (iii) Use FLANN-HKM with $L=2$ and modify the stopping condition as the points in each leaf node is no larger than $m$. using this approach, the number of all leaf nodes $k$ could be decided. After tuning the value of $m$, we get $k=17898$ for $L=2$. (iv) Use $k$-means directly with $k=18,611$ to generate the partition.

---

Fig. 13a reports the recalls of the above choices on Audio against the percentage of data points accessed. Partitions are accessed ordered by the distances of their centers to the query. We can see that OPQ -based partition has the worst performance, followed by (modified) FLANN - HKM with $L=42$, and then $L=2 . \mathrm{k}$ - means has the best performance, although the performance differences between the latter three are not significant. Therefore, our analysis suggests that hierarchical k - means -based partitioning is the most promising direction so far.

---

Our second analysis is to investigate whether we can further boost the search performance by using multiple hierarchical k-means trees. Note that Annoy already uses multiple trees and it significantly outperforms a single hierarchical $k$ - means tree in FLANN - HKM on most of the datasets. It is natural to try to enhance the performance of FLANN - HKM in a similar way. We set up an experiment to construct multiple FLANN - HKM trees. In order to build different trees, we perform $k$ - means clustering on a set of random samples of the input data points. Fig. 13b shows the resulting speedup versus recall where we use up to 50 trees. We can see that it is not cost-effective to apply multiple trees for FLANN - HKM on Audio, mainly because the trees obtained are still similar to each other, and hence the advantage of multiple trees cannot offset the extra indexing and searching overheads.

## 6.2. Graph-based Approach  

Our first analysis is to understand why KGraph, DPG and HNSW work very well (esp. attaining a very high recall) in most of the datasets. Our preliminary analysis indicates that this is because (i) the $k$-NN points of a query are typically closely connected in the proximity graph, and (ii) most points are well connected to at least one of the $k$-NN points of a query. (ii) means there is a high empirical probability that one of the $p$ entry points selected randomly by the search algorithm can reach one of the $k$-NN points, and (i) ensures that most of the $k$-NN points can be returned. By well connected, we mean there are many paths from an entry point to one of the $k$-NN point, hence there is a large probability that the "hills" on one of the path is low enough so that the search algorithm won't stuck in the local minima.

---

We also investigate why KGraph does not work well on some datasets and why DPG and HNSW works much better. KGraph does not work on Yout and Gauss mainly because both datasets have many well-separated clusters. Hence, the index of KGraph has many disconnected components. Thus, unless one of the entrance points used by its search algorithm is located in the same cluster as the query results, there is no or little chance for KGraph to find any near point. On the other hand, mainly due to the diversification step and the use of the reverse edges in DPG, there are edges linking points from different clusters, hence resulting in much improved recalls. Similarly, the neighbors selection strategy used in HNSW could be seen as another form of neighborhood diversification. The neighbors selection strategy and bidirectional connections also makes the edges in HNSW well linked.

---

For example, we perform the experiment where we use the NN of the query as the entrance point of the search on Yout. KGraph then achieves 100 percent recall. In addition, we plot the distribution of the minimum number of hops (i.e., the length of the shortest path, denoted as minHops) between a data point and any of the kNN points of a query for the indexes of KGraph and DPG on Yout and Gist in Fig. 14. We can observe that

---

- For KGraph, there are a large percentage of data points that cannot reach any $k$-NN points (i.e., those corresponding to $\infty$ hops) on Yout (60.38 percent), while the percentage is low on Gist ( 0.04 percent).
- The percentages of the $\infty$ hops are much lower for DPG (1.28 percent on Yout and 0.005 percent on Gist ).
- There is no $\infty$ hops for HNSW on both datasets.
- DPG and HNSW have much more points with small minHops than KGraph, which contributes to making it easier to reach one of the $k$-NN points. Moreover, on Yout, HNSW has the most points with small minHops over three algorithms, which results in a better performance as shown in Fig. 5g.



# 7. CONCLUSION AND FUTURE WORK  

NNS is an fundamental problem with both significant theoretical values and empowering a diverse range of applications. It is widely believed that there is no practically competitive algorithm to answer exact NN queries in sublinear time with linear sized index. A natural question is whether we can have an algorithm that empirically returns most of the $k$-NN points in a robust fashion by building an index of size $O(n)$ and by accessing at most $\alpha n$ data points, where $\alpha$ is a small constant (such as 1 percent).

---

In this paper, we evaluate many state-of-the-art algorithms proposed in different research areas and by practitioners in a comprehensive manner. We analyze their performance and give practical recommendations.

---

Due to various constraints, the study in this paper is inevitably limited. In our future work, we will (i) consider high dimensional sparse data; (ii) use more complete, including exhaustive method, to tune the algorithms; (iii) consider other distance metrics.

---

Finally, our understanding of high dimensional real data is still vastly inadequate. This is manifested in many heuristics with no reasonable justification, yet working very well in real datasets. We hope that this study opens up more questions that call for innovative solutions by the entire community.





<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928155307724.png" alt="image-20240928155307724" style="zoom: 15%;" /> 

Fig. 1. Speedup with recall of 0.8.  



$\small\begin{array}{lccccc}
\hline \text { Name } & n\left(\times 10^3\right) & d & \text { RC } & \text { LID } & \text { Type } \\
\hline \text { Nus }^* & 269 & 500 & 1.67 & 24.5 & \text { Image } \\
\text { Gist }^* & 983 & 960 & 1.94 & 18.9 & \text { Image } \\
\text { Rand }^* & 1,000 & 100 & 3.05 & 58.7 & \text { Synthetic } \\
\text { Glove }^* & 1,192 & 100 & 1.82 & 20.0 & \text { Text } \\
\text { Cifa } & 50 & 512 & 1.97 & 9.0 & \text { Image } \\
\text { Audio } & 53 & 192 & 2.97 & 5.6 & \text { Audio } \\
\text { Mnist } & 69 & 784 & 2.38 & 6.5 & \text { Image } \\
\text { Sun } & 79 & 512 & 1.94 & 9.9 & \text { Image } \\
\text { Enron } & 95 & 1,369 & 6.39 & 11.7 & \text { Text } \\
\text { Trevi } & 100 & 4,096 & 2.95 & 9.2 & \text { Image } \\
\text { Notre } & 333 & 128 & 3.22 & 9.0 & \text { Image } \\
\text { Yout } & 346 & 1,770 & 2.29 & 12.6 & \text { Video } \\
\text { Msong } & 922 & 420 & 3.81 & 9.5 & \text { Audio } \\
\text { Sift } & 994 & 128 & 3.50 & 9.3 & \text { Image } \\
\text { Deep } & 1,000 & 128 & 1.96 & 12.1 & \text { Image } \\
\text { Ben } & 1,098 & 128 & 1.96 & 8.3 & \text { Image } \\
\text { Gauss } & 2,000 & 512 & 3.36 & 19.6 & \text { Synthetic } \\
\text { Imag } & 2,340 & 150 & 2.54 & 11.6 & \text { Image } \\
\text { UQ-V } & 3,038 & 256 & 8.39 & 7.2 & \text { Video } \\
\text { BANN } & 10,000 & 128 & 2.60 & 10.3 & \text { Image } \\
\hline
\end{array}
$ 

TABLE 1. Dataset Summary





$\small\begin{array}{lcccc}
\hline \text { Name } & \text { Nusw } & \text { Gist } & \text { Msong } & \text { Glove } \\
\hline \text { DPG } & 0.008386 & 0.008586 & 0.01876 & 0.004906 \\
\text { HNSW } & 0.013122 & 0.012627 & 0.024439 & 0.012414 \\
\text { KGraph } & 0.008147 & 0.007161 & 0.017455 & 0.005867 \\
\text { Annoy } & 0.010082 & 0.006304 & 0.025659 & 0.019304 \\
\text { Flann } & 0.012074 & 0.026154 & 0.069939 & 0.044106 \\
\text { OPQ } & 0.013121 & 0.004076 & 0.072857 & 0.003116 \\
\text { FALCONN } & 0.001661 & 0.002457 & 0.002444 & 0.002584 \\
\text { SRS } & 0.010264 & 0.005244 & 0.022778 & 0.002837 \\
\hline
\end{array}$ 

TABLE 2. mAP for Each Algorithm

































<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171652969.png" alt="image-20240928171652969" style="zoom:38%;" /> 

TABLE 3. Ranking of the Algorithms Under Different Criteria













<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160353049.png" alt="image-20240928160353049" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (a) Sift LSH-based



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160425131.png" alt="image-20240928160425131" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (b) Sift L2H-based



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160500641.png" alt="image-20240928160500641" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (c) Sift Paratition-based



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160553159.png" alt="image-20240928160553159" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (d) Sift Graph-based



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160624885.png" alt="image-20240928160624885" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (e) Notre LSH-based



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160648153.png" alt="image-20240928160648153" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (f) Notre L2H-based



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160741061.png" alt="image-20240928160741061" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (g) Notre Partition-based



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160814641.png" alt="image-20240928160814641" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (h) Notre Graph-based



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928165814355.png" alt="image-20240928165814355" width=400 /> 

Fig. 5. Speedup versus recall on different datasets.  (a) Nusw



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928165845842.png" alt="image-20240928165845842" width=400 /> 

Fig. 5. Speedup versus recall on different datasets.  (b) Gist



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928165916060.png" alt="image-20240928165916060" width=400 /> 

Fig. 5. Speedup versus recall on different datasets.  (c) Glove



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928165951520.png" alt="image-20240928165951520" width=400 /> 

Fig. 5. Speedup versus recall on different datasets.  (d) Rand



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170028006.png" alt="image-20240928170028006" width=400 /> 

Fig. 5. Speedup versus recall on different datasets.  (e) Sift



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170116229.png" alt="image-20240928170116229" width=400 /> 

Fig. 5. Speedup versus recall on different datasets.  (f) Msong



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170138709.png" alt="image-20240928170138709" width=400 /> 

Fig. 5. Speedup versus recall on different datasets.  (g) Yout



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170204042.png" alt="image-20240928170204042" width=400 /> 

Fig. 5. Speedup versus recall on different datasets.  (h) Gauss



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170504025.png" alt="image-20240928170504025" width=400 /> 

Fig. 6. Recall vs % of data accessed  (a) Nusw



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170524202.png" alt="image-20240928170524202" width=400 /> 

Fig. 6. Recall vs % of data accessed  (b) Gist



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170545611.png" alt="image-20240928170545611" width=400 /> 

Fig. 6. Recall vs % of data accessed  (c) Msong



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170611073.png" alt="image-20240928170611073" width=400 />  

Fig. 6. Recall vs % of data accessed  (d) Glove





<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170838491.png" alt="image-20240928170838491" width=400 /> 

Fig. 7. Accuracy versus recall.  (a) Nusw



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170906818.png" alt="image-20240928170906818" width=400 /> 

Fig. 7. Accuracy versus recall.  (b) Gist



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170935471.png" alt="image-20240928170935471" width=400 /> 

Fig. 7. Accuracy versus recall.  (c) Msong



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171001898.png" alt="image-20240928171001898" width=400 /> 

Fig. 7. Accuracy versus recall.  (d) Glove



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171105788.png" alt="image-20240928171105788" width=400 /> 

Fig. 11. Index construction time (seconds).  (a) Nusw



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171134515.png" alt="image-20240928171134515" width=400 /> 

Fig. 11. Index construction time (seconds).  (b) Gist





<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171242255.png" alt="image-20240928171242255" width=400 /> 

Fig. 12. Index memory cost (MB).  (a) Nusw



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171300086.png" alt="image-20240928171300086" width=400 /> 

Fig. 12. Index memory cost (MB).  (b) Gist





<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171854841.png" alt="image-20240928171748672" width=400 />  

Fig. 13. Analyses of space partitioning-based methods.  (a) Partition Quality



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171949136.png" alt="image-20240928171949136" width=400 /> 

Fig. 13. Analyses of space partitioning-based methods.  (b) Multiple HKM Trees





<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928172131791.png" alt="image-20240928172131791" width=600 /> 

Fig. 14. minHops distributions of KGraph and DPG .  (a) Min \# of hops to any 20-NNs (Yout)



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928172142737.png" alt="image-20240928172142737" width=600 />  

Fig. 14. minHops distributions of KGraph and DPG .  (b) Min \# of hops to any 20-NNs (Gist)











<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/图片1xxxxxxxxxxxxxxxxxxx.png" alt="图片1xxxxxxxxxxxxxxxxxxx" style="zoom:80%;" />

Fig. 3. Speedup with recall of 0.8.  
Fig. 4. Recall with Speedup of 50  
Fig. 8. Precision versus recall  
Fig. 9. F1 score versus recall.  
Fig. 10. index size / data size(%)  







