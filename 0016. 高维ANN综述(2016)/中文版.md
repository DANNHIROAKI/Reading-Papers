## Approximate Nearest Neighbor Search on High Dimensional Data — Experiments, Analyses, and Improvement  



# 0. Abstract

最近邻搜索是许多领域（如数据库、机器学习、多媒体和计算机视觉）中的基本且重要的操作。由于在高维空间中精确搜索的效率不高，许多研究者将重点转向近似最近邻搜索。尽管每年文献中不断提出许多算法，但对它们性能的全面评估和分析仍然缺乏。在本文中，我们对多种最先进的近似最近邻搜索方法进行了全面的实验评估。我们的研究 (1) 是跨学科的（即包括来自不同领域的19种算法以及实践者的贡献），并且 (2) 评估了多种设置，包括20个数据集、多个评估指标和不同的查询负载。实验结果经过仔细报告和分析，以理解性能表现。此外，我们提出了一种新方法，实证显示在广泛的设置下，在大多数数据集上同时实现了高查询效率和高召回率。

# 1. INTRODUCTION  

最近邻搜索（NNS）旨在从参考数据库中找到与查询对象距离最小的对象。这是许多领域（包括数据库、计算机视觉、多媒体、机器学习和推荐系统）中的基本且重要的操作。

尽管对此问题进行了大量研究，但普遍认为在高维欧几里得空间中找到精确最近邻是非常昂贵的，这被称为维度灾难[1]。实验表明，在高维（例如，维度超过20）时，精确方法很少能超过暴力线性扫描方法的性能[2]。然而，返回足够接近的对象，即近似最近邻搜索（ANNS），可以高效地执行，并且对许多实际问题非常有用，因此吸引了大量的研究工作。

## 1.1. Motivation  

在（近似）最近邻搜索算法领域，已有数百篇论文发表，但关于这些算法的系统和全面比较却相对较少。本文对文献中的最先进的近似最近邻搜索算法进行了全面的实验评估，原因如下：

---

**1. 竞争算法和来自不同领域的数据集的覆盖。** 随着近似最近邻搜索需求在众多不同领域的自然产生，研究人员提出了许多方法，却往往对其他领域提出的替代方法缺乏了解。此外，一些实践者提出的实用方法在大型项目中得到了应用，例如在 spotify.com 的音乐推荐系统中。因此，来自不同领域的重要算法常常被忽视，没有进行比较。例如，目前没有对 Rank Cover Tree（来自机器学习）、Product Quantization（来自多媒体）、SRS（来自数据库）和 KGraph（来自实践者）之间进行评估。此外，每个领域通常都有一小组常用的数据集来评估近似最近邻搜索算法，而在这些领域之间，共用的数据集极少。

---

相较之下，我们进行全面实验，使用来自不同领域的经过仔细选择的代表性或最新算法，并在包括不同领域中先前研究中常用的 20 个数据集上进行测试。我们的研究确认，在这些数据集上，所有算法的性能存在显著的变异性。

---

**2. 被忽视的评估指标/设置。** 近似最近邻搜索算法可以从多个方面进行测量，包括 (i) 搜索时间复杂度，(ii) 搜索质量，(iii) 索引大小，(iv) 随着对象数量和维度数量的可扩展性，(v) 对数据集、查询工作负载和参数设置的鲁棒性，(vi) 可更新性，以及 (vii) 调整其参数所需的工作量。

---

不幸的是，之前的研究中没有任何一项能够全面、彻底地评估这些指标。

例如，大多数现有研究使用的查询工作负载实际上与数据的分布基本相同。在不同查询工作负载下测量算法的性能是一个重要问题，但目前已知的结果很少。本文评估了算法在多种设置和指标下的性能，以获得对每个算法的全面理解（参见表 3）。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171652969.png" alt="image-20240928171652969" style="zoom:38%;" /> 

TABLE 3. Ranking of the Algorithms Under Different Criteria

---

**3. 现有结果的差异。** 在一些关于这一主题的重要论文中，实验结果存在差异。

例如，文献[9]显示 AGH 的表现优于 SpectralHashing，而[10]中的研究则表明情况正好相反。这种情况在 SpectralHashing 和 DSH [10] [11] 之间也存在。虽然许多差异可以通过使用不同的设置、数据集和调优方法以及实现差异来解释，但研究人员和实践者总希望在不同场景下获得最大程度一致的结果，以便形成最新的经验法则推荐。

---

在本文中，我们尽力对几种算法进行公平比较，并在所有 20 个数据集上进行测试。最后，我们还将发布源代码、数据集和其他文档，以便轻松复现结果。

我们将流行的近似最近邻搜索算法分为三类：基于哈希的、基于划分的和基于图的。每类方法的关键思想将在第 3 节中介绍。

## 1.2. Contributions  

我们的主要贡献总结如下：

对多个不同研究领域的最先进近似最近邻搜索方法进行全面实验研究。我们的综合实验研究超越了以往的研究，具体体现在： (i) 在比较所有方法时没有添加任何实现技巧，从而使比较更为公平； (ii) 使用多种指标对所有方法进行评估；以及 (iii) 提供了在不同设置下选择方法的经验法则推荐。我们相信，这样的全面实验评估将对科学界和实践者都有所裨益，类似的研究也在其他领域进行过（例如，分类算法[12]）。

---

我们将算法分为几类（第 3 节），然后对类别内部和类别之间的评估进行详细分析（第 5 和第 6 节）。我们的数据驱动分析确认了一些有用的原则来解决问题，展示了一些最佳方法的优缺点，并初步解释和理解了为什么某些数据集比其他数据集更难处理。在整个研究过程中获得的经验和见解使我们能够设计出一种新的实证算法 DPG（第 4 节），该算法在广泛设置下在大多数数据集上实现了高查询效率和高召回率。

---

本文的其余部分组织如下。第 2 节介绍了问题定义及本文的一些限制。第 3 节介绍了与近似最近邻搜索问题相关的研究，第 4 节介绍了我们改进的近似最近邻搜索方法。在第 5 和第 6 节中，报告了全面的实验和分析。最后，在第 7 节中总结了本文。



# 2. BACKGROUND  

## 2.1. Problem Definition  

在本文中，我们关注的数据点是 $\mathbb{R}^d$ 中的 $d$ 维向量，距离度量为欧几里得距离。给定查询点的精确最近邻搜索（NNS）被定义为返回与查询点距离最小的数据点。最近邻搜索的一个推广称为 k-最近邻搜索（k-NNS），其目标是找到与查询点最近的 $k$ 个向量。

---

由于维度诅咒，许多研究工作集中在高维数据上最近邻搜索（NNS）和 k-最近邻搜索（k-NNS）问题的近似解决方案上，这意味着我们希望这些算法（分别定义为近似 NNS（ANNS）和近似 k-NNS（k-ANNS））能够返回尽可能多的真实邻居。

## 2.2. Scope  

高维数据上的近似最近邻搜索（ANNS）问题在各种文献中得到了广泛研究。已经提出了数百种算法，从不同角度解决该问题，由于其重要性和巨大的挑战，这一研究领域仍然非常活跃。为了对 ANNS 算法进行全面而有针对性的比较，本文通过施加以下限制来缩小研究范围。

---

**代表性和竞争性 ANNS 算法。** 我们考虑多个领域的最先进算法，并省略那些被其主导的其他算法，除非有强有力的证据反驳之前的发现。

**不使用硬件特定优化。** 我们获得或实现的所有实现并不具备同等程度的复杂性来利用硬件特定特性加速查询处理。我们更关注查询技术本身，而减弱实现技巧。因此，我们修改了几种实现，以确保没有算法使用多线程、多 CPU、SIMD 指令、硬件预取或 GPU。

**密集向量。** 我们将输入数据视为密集向量，不考虑稀疏数据的特定处理。

**支持欧几里得距离。** 欧几里得距离是高维数据集中最广泛使用的度量之一，大多数 ANNS 算法也支持这种距离。

**精确 $k$-NN 作为真实标签。** 在一些现有研究中，每个数据点都有标签（通常在分类或聚类应用中），这些标签在评估近似 k-NN 算法的召回率时被视为真实标签。然而，对于大型数据集，标签大多并不存在。本文使用精确的 $k$-NN 点作为真实标签，因为这适用于所有数据集和大多数应用。

---

**先前的基准研究：** 最近有两项 ANNS 基准研究：[13] 和 ann-benchmark [14]。前者除了欧几里得距离外，还考虑了大量其他距离度量，而后者则没有禁用一般实现技巧。在这两种情况下，他们的研究都不如我们的全面，例如，在评估的算法和数据集数量方面。



# 3. RELATED WORK  

我们将最先进的近似最近邻搜索（ANNS）算法分为三大类：**基于哈希的**、**基于划分的**和**基于图的**。

## 3.1. Hashing-based Methods  

属于这一类别的算法将数据点转换为低维表示，从而使每个点可以用一个短码（称为哈希码）表示。该类别主要分为两个子类别：局部敏感哈希（LSH）和学习哈希（L2H）。

### 3.1.1. Locality Sensitive Hashing  

局部敏感哈希（LSH）是一种与数据无关的哈希方法。LSH方法依赖于一类局部敏感哈希函数，这些函数以较高的概率将相似的输入数据点（距离小于$r$）映射到相同的哈希码上，而将不相似的点（距离大于$cr$）映射到不同的哈希码上，因此LSH方法最初是为了解决（$\mathrm{r}, \mathrm{c}$）近似最近邻（ANN）问题。设计良好的局部敏感哈希函数对于LSH相关方法至关重要。对于欧几里得距离度量，已经提出了大量的哈希函数[15]，[16]，[17]，[18]，[19]。随机线性投影[15]，[20]，[21]，[22]是生成哈希码最常用的哈希函数，其中随机投影参数是从2-稳定分布（例如，高斯分布）中选择的。

---

为了实现良好的搜索精度，多个哈希函数被连接以形成哈希表，从而降低不相似点的碰撞概率。然而，这也减少了相近点的碰撞概率，因此通常需要构建多个哈希表，这会导致较大的内存开销和较长的查询时间。因此，提出了一些启发式方法[23]，[24]，[25]，以检查可能包含最近邻或接近查询点的更多哈希桶，从而提高搜索质量，而不增加哈希表的数量。

---

由于哈希表在搜索之前构建，与查询碰撞的点在一个哈希表中的某些哈希函数中被忽略，尽管它们可能很接近。因此，最近一些基于LSH的方法（例如，C2LSH [26]，LazyLSH [27]，QALSH [25]）采用动态碰撞计数方案，以便在搜索时更加高效，而不是在搜索之前使用“静态”复合哈希函数构建哈希表。

---

基于LSH的方法在理论界得到了广泛研究，并在查询结果质量（基于距离比）、效率和索引大小方面，即使在最坏情况下也享有良好的概率理论保证。值得注意的是，LSH算法理论保证的有效性依赖于以下假设：给定两个数据点，哈希函数是随机和独立选择的[28]。

### 3.1.2. Learning to Hash (L2H)  

学习哈希方法充分利用数据分布生成特定的哈希函数，从而提高效率，但以放弃理论保证为代价。学习哈希方法的主要方法论是保持相似性，使得原始空间中数据点之间的近似最近邻关系能够在哈希编码空间中得到最大程度的保留。

---

根据优化目标设计的不同，学习哈希算法可分为以下几类：成对相似性保持类[9]，[29]，[30]，[31]，[32]，多重相似性保持类[33]，[34]，隐式相似性保持类[35]，[36]和量化类[5]，[37]，[38]。更多相关参考文献可以在[39]，[40]，[41]中找到。除了相似性保持标准外，大多数哈希方法还要求哈希码平衡且无相关性。

---

许多文献表明，量化算法比其他学习哈希方法更高效。基于量化的方法旨在最小化量化失真（等于每个数据点与其近似值之间的差的总和）。产品量化（Product Quantization, PQ）是一种用于近似最近邻搜索（ANNS）的流行方法，它将原始向量空间分解为 $M$ 个低维子空间的笛卡尔积，并在每个子空间中单独执行向量量化。一条向量随后由一个由其子空间量化索引组成的短代码表示。最近，提出了许多扩展方法，以提高 PQ 在索引步骤和搜索步骤的性能。例如，优化产品量化（Optimized Product Quantization, OPQ）通过预旋转进一步最小化量化失真。加性量化（Additive Quantization, AQ）和复合量化（Composite Quantization, CQ）是 PQ 的推广，将一条向量表示为 $M$ 个 $D$ 维向量的和，其中 $D$ 等于输入数据的维度。

---

得益于深度神经网络的发展，近年来采用深度学习的深度哈希方法受到广泛研究。由于我们不使用标签信息，本文仅介绍无监督的深度哈希方法。有关监督深度哈希算法的更多评估可参见文献 [52]。语义哈希（Semantic Hashing）是使用深度学习技术进行哈希的第一项工作，它构建了多层限制玻尔兹曼机（Restricted Boltzmann Machines, RBM）来为文本和文档学习紧凑的二进制代码。为了学习二进制代码，大多数深度哈希方法设计了一个符号激活层，以生成二进制代码，并最小化紧凑的实值代码与学习到的二进制向量之间的损失 [54], [55], [56], [57]。另一种解决方案是重构原始数据。例如，[58], [59] 使用自编码器作为隐藏层。Thanh-Toan 等 [60] 提出了约束倒数第二层直接输出二进制代码。

---

由于哈希方法必须从哈希函数的输出中获得二进制代码，因此二进制约束优化问题是一个 NP-hard 问题。为了简化优化，大多数哈希方法采用以下“松弛 + 四舍五入”的方法，这使得二进制代码是次优的。为了解决这个问题，开发了一些离散优化方法 [30], [61], [62]。

## 3.2. Partition-based Methods  

该类别的方法可以视为将整个高维空间划分为多个不相交的区域。假设查询 $q$ 位于区域 $r_q$，那么它的最近邻应位于 $r_q$ 或靠近 $r_q$ 的区域内。

---

划分过程通常以递归方式进行，因此基于划分的方法最好用树或森林表示。根据划分方式，主要有三种类型：枢轴法、超平面法和紧凑划分方案。枢轴法根据点与一些枢轴（通常是随机选择的）之间的距离来划分点。该类算法包括 VP-Tree [63]、Ball Tree [64] 等。超平面划分方法通过随机方向的超平面（例如 Annoy [3]、随机投影树 Random-Projection Tree [65]）或轴对齐的分离超平面（例如随机 KD 树 Randomized KDtrees）[66], [67] 递归地划分空间。紧凑划分算法要么将数据划分为簇 [68]，要么创建可能的近似 Voronoi 划分 [69], [70] 以利用局部性。

## 3.3. Graph-based Methods  

基于图的方法构建一个邻近图，其中每个数据点对应一个节点，连接某些节点的边定义了邻居关系。这些方法的主要思想是一个邻居的邻居也可能是邻居。通过迭代扩展邻居的邻居，遵循边的最佳优先搜索策略，可以高效地进行搜索。

---

根据图结构的不同，基于图的方法分为几类。第一类试图构建一个精确或近似的 k 最近邻图，记录每个节点的前 $k$ 个最近邻。特别是在高维空间中，近似 k 最近邻图的构建方法最近得到了广泛研究 [71], [72], [73], [74], [75], [76]。在 kNN 图的支持下，最近邻搜索通过爬山策略进行 [77]，通常将一些随机数据点作为初始入口，这容易陷入局部最优。为获得更好的起始点，提出了一些方案以快速定位初始条目。例如，Wei Dong 在其论文中使用 LSH 生成初始点 [78]。IEH [79] 和 Efanna [80] 则采用基于哈希的方法和随机 KD 树进行初始化。

---

第二类是称为可导航的小世界图（SW-graph） [81] 的邻近图。SW-graph 是一个无向图，包含德劳内图的近似，并具有长程链接及小世界导航特性。因此，它在最近邻搜索中更高效。Yury 等人 [82] 提出了 NSW 方法，通过迭代插入点来构建 SW-graph，每个点通过贪婪搜索算法从构建中的图中链接到一些节点。但 NSW 的度数过高，效率不佳，并且存在连通性问题。HNSW [83] 是 NSW 的扩展。它生成一个多层邻近图，具有不同的尺度，并使用启发式方法优先选择各个方向上的邻居。HNSW 是迄今为止最有效的 ANNS 算法之一。



# 4. DIVERSIFIED PROXIMITY GRAPH  

我们从这项研究中获得的经验和见解使我们能够设计出一种新方法——多样化邻近图（Diversified Proximity Graph, DPG），该方法构建了一个不同的邻近图，以实现更好且更稳健的搜索性能。

## 4.1. Motivation  

K-NN 图的构建主要考虑每个数据点邻居之间的距离，但直观上我们也应该考虑邻居的覆盖度。如图 1 所示，点 $p$ 的两个最近邻是 $a_3$ 和 $a_4$，因此在 $2-\mathrm{NN}$ 图中，虽然 $p$ 离节点 $b$ 很近，但无法引导搜索到 $q$ 的最近邻（即节点 $b$）。由于 $a_1, \ldots, a_4$ 聚集在一起，保留 $p$ 的 K-NN 列表中的 $a_3$ 和 $a_4$ 并不具有成本效益。这促使我们考虑 $p$ 的 K-NN 列表的方向多样性（即角度不相似性），除了距离之外，从而形成多样化 K-NN 图。在这个例子中，将 $a_3$ 和 $b$ 包含在 $p$ 的 K-NN 列表中是更好的选择。

---

现在假设我们用边 $\left(p, a_4\right)$ 替换为边 $(p, b)$（即**图 1** 中的虚线），但仍然存在另一个问题。如我们所见，$p$ 没有任何入边，因为它相对远离两个点簇（即 $p$ 不是这些数据点的 2-NN）。这意味着 $p$ 是孤立的，并且在这个例子中两个簇是断开的。在高维数据中，由于“中心性”现象 [84]，许多数据点很少作为其他数据点的 K-NN，因此在 K-NN 图中没有或只有少数入边，这种情况并不少见。这促使我们在多样化 K-NN 图中使用反向边；即我们保持一个双向的多样化 K-NN 图作为索引，并将其命名为多样化邻近图（Diversified Proximity Graph, DPG）。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928155307724.png" alt="image-20240928155307724" style="zoom: 15%;" /> 

Fig. 1. Speedup with recall of 0.8.   

## 4.2. Diversified Proximity Graph  

DPG 的构建是对现有 KNN 图的多样化，随后添加反向边。给定一个参考数据点 $p$，点 $x$ 和 $y$ 在 $p$ 的 KNN 列表 $\mathcal{L}$ 中的相似度定义为 $\angle x p y$ 的角度，用 $\theta(x, y)$ 表示。我们的目标是从 $\mathcal{L}$ 中选择一个大小为 $\kappa$ 的数据点子集，记为 $\mathcal{S}$，以最大化 $\mathcal{S}$ 中两点之间的平均角度；或者等价地，$\mathcal{S} = \arg \min_{\mathcal{S} \subseteq \mathcal{N}, |\mathcal{S}| = \kappa} \sum_{o_i, o_j \in \mathcal{S}} \theta(o_i, o_j)$。

---

上述问题是 NP-难的 [85]。因此，我们设计了一个简单的贪婪启发式算法。最初，$\mathcal{S}$ 设置为在 $\mathcal{L}$ 中离 $p$ 最近的点。在接下来的 $\kappa-1$ 次迭代中，从 $\mathcal{L} \backslash \mathcal{S}$ 中移动一个点到 $\mathcal{S}$，以最小化 $\mathcal{S}$ 中点的平均成对角度相似度。然后，对于 $\mathcal{S}$ 中的每个数据点 $u$，我们在多样化邻近图中包括边 $(p, u)$ 和 $(u, p)$。多样化过程的时间复杂度为 $O\left(\kappa^2 K n\right)$，其中 $n$ 是数据点的数量，而在多样化邻近图中最多有 $2 \kappa n$ 条边。

---

在多样化邻近图中，为所需的 $\kappa$ 找到合适的 $K$ 值至关重要，因为我们需要在多样性和邻近性之间找到良好的权衡。在我们的实证研究中，DPG 算法通常在 $K=2 \kappa$ 时获得最佳性能。因此，我们将 $K$ 设置为 $2 \kappa$ 以构建多样化邻近图。该贪婪算法的时间复杂度为 $O\left(\kappa^2 K n\right)$。我们实际上实现了一个简化版本，其复杂度为 $O\left(K^2 n\right)$，该版本的性能仅比完整贪婪版本稍差，但多样化时间显著减少（具体细节见附录 D，可在计算机协会数字图书馆 http://doi.ieeecomputersociety.org/TKDE.2019.2909204 查阅）。

请注意，DPG 的搜索过程与 KGraph 的搜索过程相同。

# 5. EXPERIMENTS  

## 5.1. Experimental Setting  

### 5.1.1. Compared Algorithms  

我们测试了 19 种具有代表性的现有 ANNS 算法，涵盖三大类，以及我们提出的多样化邻近图 (DPG) 方法。测试算法的详细描述可以在附录 A 中找到。本文中使用的所有修改源代码均可在 GitHub 上公开获取 [86]。

---

**(1) 基于 LSH 的方法**。我们评估了查询感知 LSH [87] (QALSH, PVLDB’15)、SRS [7] (SRS, PVLDB’14) 和 FALCONN [19] (FALCONN, NIPS’15) 这一类的算法。

---

**(2) 基于 L2H 的方法**。我们评估了可扩展图哈希 [88] (SGH, IJCAI’15)、锚点图哈希 [9] (AGH, ICML’11) 和邻域敏感哈希 [10] (NSH, PVLDB’15) 这些基于二进制编码的学习哈希方法。为了进行非穷举搜索，我们使用层次聚类树来组织哈希码 [67]。与线性扫描搜索的比较在附录 E 中给出，在线补充材料中可查阅。

选择性哈希 [89] (SH, KDD’15) 是 LSH 的扩展，它考虑了每个点的局部密度。选择性哈希通过哈希表查找进行搜索。我们将基于排列的方法归类为基于 L2H 的方法。基于排列的方法根据某些随机选择的中心点的顺序对每个点进行编码。邻域逼近索引 [13] (NAPP, PVLDB’15) 是最有效的基于排列的方法之一，它依赖于倒排索引进行搜索。

我们还评估了最优产品量化 [6] (OPQ, TPAMI’14) 的非穷举搜索实现，该方法通过倒排多重索引 [47] 进行。

复合量化 [51] (CQ, ICML’11) 进行线性扫描搜索，并使用查找表预先计算查询与每个子量化器的码字之间的距离。

---

**(3) 基于划分的方法**。我们评估了 FLANN [67] (TPAMI’14)、FLANN-HKM、FLANN-KD、Annoy 和一种先进的 Vantage-Point 树 [90] (VP 树, NIPS’13) 这一类的算法。

---

**(4) 基于图的方法**。我们评估了小世界图 [82] (SW, IS’14)、层次可导航小世界 [83] (HNSW, CoRR’16)、K-NN 图 [8], [74] (KGraph, WWW’11)、Rank Cover Tree [4] (RCT, TPAMI’15) 和我们的多样化邻近图 (DPG)。需要注意的是，尽管 RCT 使用了树结构，但其关键思想是连接每个相邻层之间的最近节点。因此，我们将 RCT 划分为基于图的方法。

---

我们使用来自非度量空间库 (NMSLIB) 的 NAPP、VP 树、SW 和 HNSW 的实现。我们为每个算法和每个数据集仔细调整了超参数。更详细的信息可以在附录 C 中找到，在线补充材料中可查阅。为了比较的公平性，我们专注于现有方法的算法视角。在各方法的实现中，我们关闭了所有特定于硬件的优化。具体而言，我们在 KGraph 中禁用了使用 SIMD 和多线程的距离计算，禁用了 Annoy 中的 -ffast-math 编译器选项，禁用了 FLANN 中的多线程，以及在 NMSLIB（即 SW、NAPP、VP 树和 HNSW）中实施的使用 SIMD、多线程和预取技术的距离计算。此外，我们禁用了 HNSW 中使用的优化搜索实现。

---

**计算环境**。所有 C++ 源代码由 g++ 4.7 编译，MATLAB 源代码（仅用于某些学习哈希算法的索引构建）由 MATLAB 8.5 编译。所有实验均在配备 2.9 GHz Intel Xeon 8 核 CPU 和 32G 内存的 Linux 服务器上进行。

### 5.1.2. Datasets and Query Workload  

我们使用了18个真实数据集，这些数据集覆盖了图像、音频、视频和文本等广泛应用领域，并且还使用了两个合成数据集。表1总结了数据集的特征，包括数据点数量、维度、相对对比度 (RC [91])、局部内在维度 (LID [92]) 和数据类型。RC 表示数据点之间均值距离与最近邻距离的比率，较小的 RC 值意味着数据集更难。LID 计算局部内在维度，较高的 LID 值同样意味着数据集更难。我们在表1中用星号标记前四个数据集，表明它们与其他数据集相比是“难”数据集，依据是它们的 RC 和 LID 值。

$\small\begin{array}{lccccc}
\hline \text { Name } & n\left(\times 10^3\right) & d & \text { RC } & \text { LID } & \text { Type } \\
\hline \text { Nus }^* & 269 & 500 & 1.67 & 24.5 & \text { Image } \\
\text { Gist }^* & 983 & 960 & 1.94 & 18.9 & \text { Image } \\
\text { Rand }^* & 1,000 & 100 & 3.05 & 58.7 & \text { Synthetic } \\
\text { Glove }^* & 1,192 & 100 & 1.82 & 20.0 & \text { Text } \\
\text { Cifa } & 50 & 512 & 1.97 & 9.0 & \text { Image } \\
\text { Audio } & 53 & 192 & 2.97 & 5.6 & \text { Audio } \\
\text { Mnist } & 69 & 784 & 2.38 & 6.5 & \text { Image } \\
\text { Sun } & 79 & 512 & 1.94 & 9.9 & \text { Image } \\
\text { Enron } & 95 & 1,369 & 6.39 & 11.7 & \text { Text } \\
\text { Trevi } & 100 & 4,096 & 2.95 & 9.2 & \text { Image } \\
\text { Notre } & 333 & 128 & 3.22 & 9.0 & \text { Image } \\
\text { Yout } & 346 & 1,770 & 2.29 & 12.6 & \text { Video } \\
\text { Msong } & 922 & 420 & 3.81 & 9.5 & \text { Audio } \\
\text { Sift } & 994 & 128 & 3.50 & 9.3 & \text { Image } \\
\text { Deep } & 1,000 & 128 & 1.96 & 12.1 & \text { Image } \\
\text { Ben } & 1,098 & 128 & 1.96 & 8.3 & \text { Image } \\
\text { Gauss } & 2,000 & 512 & 3.36 & 19.6 & \text { Synthetic } \\
\text { Imag } & 2,340 & 150 & 2.54 & 11.6 & \text { Image } \\
\text { UQ-V } & 3,038 & 256 & 8.39 & 7.2 & \text { Video } \\
\text { BANN } & 10,000 & 128 & 2.60 & 10.3 & \text { Image } \\
\hline
\end{array}
$ 

TABLE 1. Dataset Summary

---

**查询工作负载。** 根据惯例，我们随机移除每个数据集中的200个数据点作为查询点。报告了 $k-\mathrm{NN}$ 搜索的平均性能。在本文中，默认情况下 $k$ 等于20。我们在附录 H 中评估了 $k$ 的影响，相关内容可在在线补充材料中查阅。

## 5.2. Evaluation Measures  

对于每个算法，我们根据其搜索过程检索 $N$ 个点，然后使用原始特征对这些候选项进行重新排序。搜索质量通过召回率、精确度、准确率和平均精确度（mAP）来衡量。召回率是检索到的 $N$ 个项目中真实最近邻项目与 $k$ 的比率。精确度定义为检索到的真实项目数量与 $N$ 的比率。F-score（F1分数）是精确度和召回率的调和平均数：$F_1=2 \times \text{精确度} \times \text{召回率} / (\text{精确度} + \text{召回率})$。然后，平均精确度（mAP）计算为所有查询的平均精确度的均值。准确率等于 $\displaystyle\sum_{i=0}^{i=k} \frac{\operatorname{dist}(q, k \text{NN}(q)[i])}{\operatorname{dist}(q, k \text{NN}(q)[i])}$，其中 $q$ 是查询，$k \text{NN}(q)[i]$ 是 $q$ 的第 $i$ 个真实最近邻，而 $k A \text{NN}(q)[i]$ 是通过一个 ANNS 算法估计的第 $i$ 个最近邻。

---

搜索效率通常通过返回查询结果所需的时间来衡量。对于大多数算法（除了基于图的方法），我们可以改变检索点的数量 $N$，以获得不同的召回率/精确度/准确率及其对应的搜索时间。由于可以通过暴力线性扫描算法找到精确的 $k$-NN，我们使用其查询时间作为基准，并定义加速比为 $\frac{\bar{t}}{t^{\prime}}$，其中 $\bar{t}$ 是线性扫描的查询时间，$t^{\prime}$ 是在特定召回率或 $N$ 下的搜索时间。例如，如果一个算法花费1秒，而线性扫描需要10秒，我们就得到了10倍的加速比。

---

除了评估搜索性能外，我们还评估其他方面，如索引构建时间、索引大小、索引内存成本和可扩展性。

## 5.3. Comparison with Each Category  

在本小节中，我们评估所有算法在 Sift 和 Notre 数据集上速度提升与召回率之间的权衡。由于哈希算法类别中的算法数量较多，我们将其分别在基于 LSH 和基于学习的哈希子类别中进行评估。本轮评估的目标是从每个类别中选择几个算法作为第二轮评估（第 5.4 节）的代表。

### 5.3.1. LSH-based Methods  

图 2 a 和 2 e 显示了两种最新的数据无关算法 SRS 和 QALSH 在 Sift 和 Notre 数据集上速度提升与召回率之间的权衡。需要注意的是，由于 FALCONN 对 L2 距离没有理论保证，我们将在第二轮评估中对其进行评估。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160353049.png" alt="image-20240928160353049" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (a) Sift LSH-based

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160624885.png" alt="image-20240928160624885" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (e) Notre LSH-based

---

由于这两种算法最初都是基于外部存储的方法，我们通过数据集的总页面数与搜索过程中访问的页面数之比来评估速度提升。结果显示，SRS 始终优于 QALSH，并且在其他数据集上也观察到了类似的趋势。因此，SRS 被选为第二轮评估中的代表，其中将使用基于覆盖树的内存实现。

### 5.3.2. Learning to Hash-based Methods  

我们评估了七种基于学习的哈希算法，包括 OPQ、NAPP、SGH、AGH、NSH、SH 和 CQ。图 2 b 和 2 f 显示，在所有方法中，OPQ 的搜索性能远远超过其他算法。由于 CQ 采用线性扫描搜索，因此在速度提升与召回率的权衡上表现较差。实际上，CQ 在召回率 @N 上非常具有竞争力，但其索引时间极高，因此在实际应用中很难使用。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160425131.png" alt="image-20240928160425131" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (b) Sift L2H-based

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160648153.png" alt="image-20240928160648153" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (f) Notre L2H-based

---

对于大多数数据集，Selective Hashing 的索引大小最大，因为它需要多个长哈希表以实现高召回率。OPQ 的索引时间值与子码字的长度和数据点的维度有很强的关联。不过，OPQ 的索引构建时间与其他算法相比仍然非常具有竞争力。因此，我们选择 OPQ 作为基于学习的哈希方法的代表。

### 5.3.3. Partition-based Methods  

我们评估了该类别中的三种算法：FLANN、Annoy 和 VP-tree。为了更好地展示 FLANN 的性能，我们分别报告了随机 kd-tree 和层次 k-means 树的性能，即 FLANN-KD 和 FLANN-HKM。注意，在 20 个数据集中，FLANN 在五个数据集中选择了随机 kd-tree 方法（FLANN-KD）：Enron、Trevi、UQ-V、BANN 和 Gauss。对于最难的数据集 Rand，则使用了线性扫描，而在其余 14 个数据集中采用了层次 k-means 树（FLANN-HKM）。

---

图 2 c 和 2 g 显示，Annoy 和 FLANN-HKM 在这两个数据集上表现良好。在所有数据集中，Annoy、FLANN-HKM 和 FLANN-KD 在不同的数据集上都能获得最高的性能。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160500641.png" alt="image-20240928160500641" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (c) Sift Paratition-based

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160741061.png" alt="image-20240928160741061" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (g) Notre Partition-based

---

由于 VP-tree 的搜索性能在所有设置下与 FLANN 和 Annoy 相比并不具有竞争力，因此被排除在下一轮评估之外。

### 5.3.4. Graph-based Methods  

在图基方法类别中，我们评估了四种现有技术：KGraph、SW、HNSW 和 RCT。图 2d 和 2h 显示，KGraph、SW 和 HNSW 的搜索性能在 Sift 和 Notre 数据集上明显优于 RCT。由于 HNSW 是 SW 的改进版本，并且在所有数据集上都能实现更好的性能，因此我们将 SW 排除在下一轮评估之外。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160553159.png" alt="image-20240928160553159" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (d) Sift Graph-based

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928160814641.png" alt="image-20240928160814641" width=400 /> 

Fig. 2. Speedup versus recall on Sift and Notre for each category.  (h) Notre Graph-based

---

尽管 KGraph 和 HNSW 的构建时间相对较长，但由于其出色的搜索性能，我们选择它们作为图基方法的代表。需要注意的是，我们将 DPG 的比较延迟到第二轮。

## 5.4. Second Round Evaluation  

在第二轮评估中，我们对八种代表性算法进行了全面实验：SRS、FALCONN、OPQ、FLANN、Annoy、HNSW、KGraph 和 DPG。

### 5.4.1. Comparison of Search Quality and Time  

在第一组实验中，图 3 报告了八种算法在 20 个数据集上达到约 0.8 的召回率时的加速比。注意，如果某个算法无法超越线性扫描算法，则加速比设置为 1.0。在这八种算法中，DPG 和 HNSW 的整体搜索性能最佳，其次是 KGraph。结果显示，DPG 提升了 KGraph 的性能，特别是在困难数据集上，如 Nusw、Gist、Glove 和 Rand。随后报告的结果表明，在更高召回率下，改进更为显著。例如，在这一设置下（召回率 0.8），DPG 在四个数据集中的排名位于 KGraph 之后，但在更高召回率下最终超越 KGraph。总体而言，DPG 和 HNSW 在不同数据集上表现最佳。毫不奇怪，SRS 的速度比其他竞争者慢得多，因为它没有利用数据分布。图 4 显示了在加速比约为 50 时各算法所达到的召回率，类似的观察结果也得到了验证。

![image-20240928183400515](https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928183400515.png)

Fig. 3. Speedup with recall of 0.8.  

![image-20240928183419792](https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928183419792.png) 

Fig. 4. Recall with Speedup of 50  

---

**图 5** 说明了算法在八个数据集上召回率从 0 到 1 变化时的加速比。它进一步展示了 DPG 在高召回率下的卓越搜索性能。HNSW、KGraph 和 Annoy 的整体性能也非常有竞争力，其次是 FLANN。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928165814355.png" alt="image-20240928165814355" width=250 /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928165845842.png" alt="image-20240928165845842" width=250 /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928165916060.png" alt="image-20240928165916060" width=250 />  

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170116229.png" alt="image-20240928170116229" width=250 /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928165951520.png" alt="image-20240928165951520" width=250 /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170028006.png" alt="image-20240928170028006" width=250 />

Fig. 5. Speedup versus recall on different datasets.  (a) Nusw (b) Gist (c) Glove (d) Rand (e) Sift (f) Msong 

---

**图 5h** 显示，DPG 和 KGraph 的性能排名低于 HNSW、Annoy、FLANN 和 OPQ，因为数据点聚集在一起。随后在第 6 节中讨论到，Annoy、FLANN 和 OPQ 本质上使用了 $k$-均值方法的变体，因此能够很好地处理聚类数据。HNSW 使用启发式方法来增加构建聚类之间链接的概率。FALCONN 在所有数据集上显著优于 SRS，并且在一些困难数据集（如 Glove、Nusw 和随机数据集）上超越了基于树的方法和学习哈希方法。值得注意的是，FALCONN 甚至在 Gauss 数据集上超越了基于图的方法。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170204042.png" alt="image-20240928170204042" width=400 /> 

Fig. 5. Speedup versus recall on different datasets.  (h) Gauss

---

在**图 6** 中，我们评估了算法的召回率与访问的数据点百分比之间的关系。由于大多数基于图的方法从随机入口点开始搜索，然后逐渐接近结果，而其他算法则从最有希望的候选项开始搜索，因此基于图的方法在初始阶段的搜索质量不如 Annoy、FLANN 甚至 OPQ。然而，得益于 HNSW 的分层结构，它可以从前一层的局部最优元素继续搜索。每一层的入口点经过精心选择，以确保能够快速定位到离查询更近的点。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170504025.png" alt="image-20240928170504025" width=300 /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170524202.png" alt="image-20240928170524202" width=300 /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170545611.png" alt="image-20240928170545611" width=300 />  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170611073.png" alt="image-20240928170611073" width=300 /> 

Fig. 6. Recall versus percentage of data points accessed.   (a) Nusw (b) Gist (c) Msong (d) Glove

---

**图 7** 显示了特定召回率下的范围搜索质量。较小的准确度表示结果与查询更接近，因此搜索质量更好。专为 c-ANN 搜索设计的 SRS 和 FALCONN 超越了所有其他算法。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170838491.png" alt="image-20240928170838491" width=300 />  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170906818.png" alt="image-20240928170906818" width=300 />

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170935471.png" alt="image-20240928170935471" width=300 /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171001898.png" alt="image-20240928171001898" width=300 /> 

Fig. 7. Accuracy versus recall.  (a) Nusw (b) Gist (c) Msong (d) Glove 

---

在**图 8** 和**图 9** 中，我们评估了每个算法的精确度@召回率和 F1@召回率，并在**表 2** 中报告了 mAP。结果进一步验证了我们在图 6 中的观察。基于树的方法和 HNSW 能够在检索到少量点后找到更近的邻居。

![image-20240928183442081](https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928183442081.png)

Fig. 8. Precision versus recall  

![image-20240928183456724](https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928183456724.png)

Fig. 9. F1 score versus recall.   

$\small\begin{array}{lcccc}
\hline \text { Name } & \text { Nusw } & \text { Gist } & \text { Msong } & \text { Glove } \\
\hline \text { DPG } & 0.008386 & 0.008586 & 0.01876 & 0.004906 \\
\text { HNSW } & 0.013122 & 0.012627 & 0.024439 & 0.012414 \\
\text { KGraph } & 0.008147 & 0.007161 & 0.017455 & 0.005867 \\
\text { Annoy } & 0.010082 & 0.006304 & 0.025659 & 0.019304 \\
\text { Flann } & 0.012074 & 0.026154 & 0.069939 & 0.044106 \\
\text { OPQ } & 0.013121 & 0.004076 & 0.072857 & 0.003116 \\
\text { FALCONN } & 0.001661 & 0.002457 & 0.002444 & 0.002584 \\
\text { SRS } & 0.010264 & 0.005244 & 0.022778 & 0.002837 \\
\hline
\end{array}$ 

TABLE 2. mAP for Each Algorithm

### 5.4.2. Comparison of Indexing Cost  

除了搜索性能，我们还评估了索引大小、内存成本和构建时间。**图 10** 报告了索引大小（不包括数据点）与数据大小的比率。除了 Annoy，所有算法的索引大小都小于相应的数据大小。DPG、KGraph、HNSW、SRS 和 FALCONN 的索引大小与维度无关，因为每个数据点保持固定数量的邻居 ID 和投影。因此，它们在高维数据（例如 Trevi）上具有相对较小的比率。总体而言，OPQ 和 SRS 的索引大小最小，在大多数数据集中低于 5%，其次是 FALCONN、HNSW、DPG、KGraph 和 FLANN。FLANN 的索引大小排名在 20 个数据集中变化剧烈，因为它可能选择三种不同的索引结构。Annoy 需要维护相当数量的树以获得良好的搜索质量，因此具有最大的索引大小。

![image-20240928183519950](https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928183519950.png)

Fig. 10. index size / data size(%)  

---

**图 11** 报告了 20 个数据集上的索引构建时间。FALCONN 在所有测试算法中具有最小的索引构建时间，SRS 排名第二。OPQ 的构建时间与维度有关，因为涉及子码字的计算（例如 Trevi）。HNSW、KGraph 和 DPG 的构建时间相似。与 KGraph 相比，DPG 在图的多样化上没有花费大量额外时间。然而，它们在 20 个数据集中有 16 个的索引构建时间仍然在一小时内完成。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171105788.png" alt="image-20240928171105788" width=400 /> 

Fig. 11. Index construction time (seconds).  (a) Nusw

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171134515.png" alt="image-20240928171134515" width=400 /> 

Fig. 11. Index construction time (seconds).  (b) Gist

---

**图 12** 报告了 20 个数据集上算法的索引内存成本。OPQ 在构建索引时需要较少的内存资源，因此在大规模数据集上非常高效。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171242255.png" alt="image-20240928171242255" width=400 /> 

Fig. 12. Index memory cost (MB).  (a) Nusw

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171300086.png" alt="image-20240928171300086" width=400 /> 

Fig. 12. Index memory cost (MB).  (b) Gist 

## 5.5. Summary  

表 3 从搜索性能、索引大小、索引构建时间、索引内存成本和可扩展性等多个角度对八种算法的性能进行了排名。我们还指出，SRS 是唯一具有搜索质量理论保证的算法，并且在调整搜索质量和搜索时间的参数时非常简单。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171652969.png" alt="image-20240928171652969" style="zoom:38%;" /> 

TABLE 3. Ranking of the Algorithms Under Different Criteria

以下是根据我们的综合评估对用户的一些推荐：

---

- 当有足够的计算资源（包括主内存和CPU）进行离线索引构建，并且有足够的主内存来存放生成的索引时，DPG 和 HNSW 是高维数据中 ANNS 的最佳选择，因为它们在数据集的鲁棒性、结果质量、搜索时间和搜索可扩展性方面表现出色。

---

- 除了 DPG 和 HNSW，我们还推荐 Annoy，因为它在搜索性能、构建成本和对数据集的鲁棒性方面表现优异。此外，它在搜索性能与索引大小/构建时间之间提供了良好的折衷。

---

- 如果构建时间是一个主要考虑因素，FALCONN 将是一个不错的选择，因为它的构建时间较短且搜索性能良好。

---

- 对于处理大规模数据集（例如，10亿个数据点）且计算资源适中的情况，OPQ 和 SRS 是很好的候选算法，因为它们的内存成本和构建时间较小。值得一提的是，SRS 能够轻松处理数据点的更新，并且具有理论保证，这使其区别于其他七种算法。

# 6. FURTHER ANALYSES  

在本节中，我们分析了评估中最具竞争力的算法，按类别分组，以便理解它们的优缺点。

## 6.1. Space Partition-based Approach  

我们的综合实验表明，在基于空间划分的方法中，Annoy 和 FLANN 的性能最佳。值得注意的是，FLANN 在大多数数据集中选择了 FLANN-HKM。此外，OPQ 通过利用在 $M$ 个不相交子空间上进行 $k$-均值的结果来划分空间。因此，这三种算法都基于 $k$-均值的空间划分。我们将借鉴 $k$-均值思想的算法定义为 $k$-均值类算法。

---

我们确定 $k$-均值类算法有效性的一个关键因素是大量聚类，通常为 $\Theta(n)$。需要注意的是，我们不能直接将 $k$-均值应用于 $k=\Theta(n)$，因为 (i) $k$-均值的索引构建时间复杂度与 $k$ 成线性关系，(ii) 确定查询所在划分的时间复杂度需要 $\Theta(n)$ 时间。OPQ 和 FLANN-HKM/Annoy 分别通过子空间划分和递归思想间接实现了这一目标。

---

我们进行实验以了解哪种思想更为有效。我们考虑在达到大致相同数量的非空划分的情况下实现 $k$-均值类空间划分的目标。具体来说，对于 Audio 数据集，我们考虑以下选择：(i) 使用 OPQ 进行 2 个子空间，每个子空间有 256 个聚类。计算有效划分 $k$（即非空划分）的数量。在 Audio 数据集中，$k=18,611$。(ii) 使用原始的 FLANN-HKM 构建一个树，叶节点数量大致为 $k$。经过仔细调整分支因子 $L$ 的值，我们发现当 $L=42$ 时，总叶节点数量为 18000。(iii) 使用 FLANN-HKM 和 $L=2$，并将停止条件修改为每个叶节点中的点不超过 $m$。通过这种方法，可以决定所有叶节点的数量 $k$。在调整 $m$ 的值后，当 $L=2$ 时我们得到了 $k=17898$。(iv) 直接使用 $k$-均值生成 $k=18,611$ 的划分。

---

**图 13a** 报告了上述选择在 Audio 数据集上的召回率与访问的数据点百分比的关系。划分的访问顺序是按照它们中心到查询的距离。我们可以看到，基于 OPQ 的划分性能最差，其次是（修改后的）FLANN-HKM，$L=42$，然后是 $L=2$。尽管后面三个算法之间的性能差异不显著，但 $k$-均值表现最好。因此，我们的分析表明，基于层次 $k$-均值的划分是目前最有前景的方向。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171854841.png" alt="image-20240928171748672" width=400 />  

Fig. 13. Analyses of space partitioning-based methods.  (a) Partition Quality

---

我们的第二个分析是探讨是否可以通过使用多个层次 $k$-均值树进一步提升搜索性能。需要注意的是，Annoy 已经使用了多个树，在大多数数据集中，其性能明显优于 FLANN-HKM 中的单个层次 $k$-均值树。因此，尝试以类似方式增强 FLANN-HKM 的性能是合乎逻辑的。我们设立实验来构建多个 FLANN-HKM 树。为了构建不同的树，我们对输入数据点的一组随机样本进行了 $k$-均值聚类。**图 13b** 显示了使用多达 50 棵树时，速度提升与召回率的关系。我们可以看到，在 Audio 数据集上应用多个树对 FLANN-HKM 来说并不划算，主要是因为获得的树彼此相似，因此多个树的优势无法抵消额外的索引和搜索开销。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928171949136.png" alt="image-20240928171949136" width=400 /> 

Fig. 13. Analyses of space partitioning-based methods.  (b) Multiple HKM Trees

## 6.2. Graph-based Approach  

我们的第一次分析是为了理解为什么 KGraph、DPG 和 HNSW 在大多数数据集上表现得非常好（尤其是在达到很高的召回率方面）。我们的初步分析表明，这主要是因为 (i) 查询的 $k$-NN 点在邻近图中通常是紧密连接的，(ii) 大多数点至少与查询的一个 $k$-NN 点良好连接。第二点意味着搜索算法随机选择的 $p$ 个入口点中，有很高的经验概率可以到达其中一个 $k$-NN 点，而第一点确保大多数 $k$-NN 点能够被返回。我们所说的“良好连接”是指从一个入口点到某个 $k$-NN 点有多条路径，因此存在很大的概率使得路径上的“山丘”足够低，以便搜索算法不会陷入局部最小值。

---

我们还研究了为什么 KGraph 在某些数据集上表现不佳，以及为什么 DPG 和 HNSW 的表现更好。KGraph 在 Yout 和 Gauss 数据集上的表现不佳，主要是因为这两个数据集有许多相互分离的聚类。因此，KGraph 的索引有很多不连通的组件。因此，除非搜索算法使用的入口点位于与查询结果相同的聚类中，否则 KGraph 找到任何近似点的机会几乎没有或很小。另一方面，主要由于 DPG 中的多样化步骤和反向边的使用，不同聚类之间存在连接边缘，从而显著提高了召回率。同样，HNSW 中使用的邻居选择策略可以视为另一种形式的邻域多样化。邻居选择策略和双向连接也使得 HNSW 中的边缘良好连接。

---

例如，我们进行了一项实验，在 Yout 数据集上使用查询的最近邻作为搜索的入口点。这样，KGraph 实现了 100% 的召回率。此外，我们绘制了 KGraph 和 DPG 在 Yout 和 Gist 数据集上的索引中，数据点与查询的任何 $k$-NN 点之间的最小跳数（即最短路径的长度，记为 minHops）的分布，如图 14 所示。我们可以观察到：

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928172131791.png" alt="image-20240928172131791" width=600 /> 

Fig. 14. minHops distributions of KGraph and DPG .  (a) Min \# of hops to any 20-NNs (Yout) 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928172142737.png" alt="image-20240928172142737" width=600 />  

Fig. 14. minHops distributions of KGraph and DPG .  (b) Min \# of hops to any 20-NNs (Gist)

---

- 对于 KGraph，Yout 上有很大比例的数据点无法到达任何 $k$-NN 点（即对应于 $\infty$ 跳数的点）为 60.38%，而在 Gist 上这个比例很低（0.04%）。

- DPG 的 $\infty$ 跳数比例要低得多（Yout 上为 1.28%，Gist 上为 0.005%）。

- HNSW 在这两个数据集上均没有 $\infty$ 跳数。

- DPG 和 HNSW 拥有的最小 minHops 较小的数据点数量远高于 KGraph，这使得到达某个 $k$-NN 点变得更加容易。此外，在 Yout 上，HNSW 拥有三种算法中最小 minHops 较小的数据点数量最多，这导致其性能更好，如图 5g 所示。

  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240928170138709.png" alt="image-20240928170138709" width=400 /> 

  Fig. 5. Speedup versus recall on different datasets.  (g) Yout 



# 7. CONCLUSION AND FUTURE WORK  

NNS 是一个具有重要理论价值的基本问题，并支持多种应用。人们普遍认为，目前没有实用的竞争算法能够在子线性时间内使用线性大小的索引来回答精确的最近邻查询。一个自然的问题是，我们是否能够通过构建大小为 $O(n)$ 的索引，并访问最多 $\alpha n$ 个数据点（其中 $\alpha$ 是一个小常数，如 1%），来实证性地返回大多数 $k$-NN 点。

---

在本文中，我们全面评估了不同研究领域和实践者提出的许多最先进的算法。我们分析了它们的性能，并提供了实用的建议。

---

由于各种限制，本文的研究不可避免地存在局限性。在未来的工作中，我们将 (i) 考虑高维稀疏数据；(ii) 使用更完整的方法，包括穷举法，来调整算法；(iii) 考虑其他距离度量。

---

最后，我们对高维真实数据的理解仍然非常不足。这在许多缺乏合理依据但在真实数据集中表现良好的启发式方法中得到了体现。我们希望本研究能够引发更多问题，从而促使整个社区寻求创新解决方案。
