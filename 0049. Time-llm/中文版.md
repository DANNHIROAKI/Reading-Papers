### Time-LLM：通过重新编程大型语言模型进行时间序列预测

## 摘要

时间序列预测在许多现实世界的动态系统中具有重要意义，并已得到广泛研究。与自然语言处理（NLP）和计算机视觉（CV）不同，在这些领域中，单一的大型模型可以处理多个任务，而时间序列预测模型通常是专门化的，需要针对不同任务和应用进行独特的设计。尽管预训练的基础模型在 NLP 和 CV 领域取得了显著进展，但它们在时间序列领域的发展却因数据稀疏性而受到限制。最近的研究表明，大型语言模型（LLMs）在复杂标记序列上具有强大的模式识别和推理能力。然而，如何有效地对齐时间序列数据和自然语言的模态以利用这些能力仍然是一个挑战。在本研究中，我们提出了 Time-LLM，这是一个重新编程框架，旨在利用 LLMs 进行通用时间序列预测，同时保持主干语言模型的完整性。我们首先通过文本原型重新编程输入时间序列，然后将其输入冻结的 LLM 以对齐这两种模态。为了增强 LLM 处理时间序列数据的推理能力，我们提出了 Prompt-as-Prefix（PaP），它丰富了输入上下文并指导重新编程的输入补丁的转换。来自 LLM 的转换后的时间序列补丁最终被投影以获得预测。我们的全面评估表明，Time-LLM 是一个强大的时间序列学习器，其性能优于最先进的专门预测模型。此外，Time-LLM 在少样本和零样本学习场景中表现出色。代码已在 https://github.com/KimMeen/Time-LLM 上公开。

# 1 引言

时间序列预测是许多现实世界动态系统中的关键能力（Jin 等，2023a），其应用范围从需求规划（Leonard 2001）和库存优化（Li 等，2022）到能源负荷预测（Liu 等，2023a）和气候建模（Schneider & Dickinson，1974）。每个时间序列预测任务通常需要广泛的领域专业知识和任务特定的模型设计。这与 GPT-3（Brown 等，2020）、GPT-4（OpenAI，2023）、Llama（Touvron 等，2023）等基础语言模型形成鲜明对比，这些模型可以在少样本甚至零样本设置下在多种 NLP 任务中表现良好。

---

预训练的基础模型，如大型语言模型（LLMs），推动了计算机视觉（CV）和自然语言处理（NLP）的快速发展。尽管时间序列建模尚未从同样的重大突破中受益，但 LLMs 的显著能力激发了其在时间序列预测中的应用（Jin 等，2023b）。利用 LLMs 推进预测技术存在几个需求：**泛化性**。LLMs 已展示了少样本和零样本迁移学习的显著能力（Brown 等，2020）。这表明它们具有跨领域通用预测的潜力，而无需从头开始进行每个任务的重新训练。相比之下，当前的预测方法通常严格按领域专门化。**数据效率**。通过利用预训练知识，LLMs 已展示了仅用少量示例执行新任务的能力。这种数据效率可以支持历史数据有限的环境中的预测。相比之下，当前方法通常需要丰富的领域内数据。**推理**。LLMs 展示了复杂的推理和模式识别能力（Mirchandani 等，2023；Wang 等，2023；Chu 等，2023）。利用这些技能可以通过学习到的更高层次概念进行高度精确的预测。现有的非 LLM 方法主要是统计性的，没有太多内在推理能力。**多模态知识**。随着 LLM 架构和训练技术的改进，它们获得了跨视觉、语音和文本等模态的多样化知识（Ma 等，2023）。利用这些知识可以实现融合不同类型数据的协同预测。传统工具缺乏联合利用多个知识库的方式。**易于优化**。LLMs 在大规模计算上训练一次，然后可以应用于预测任务而无需从头学习。优化现有预测模型通常需要大量的架构搜索和超参数调优（Zhou 等，2023b）。总之，与当前专门化的建模范式相比，LLMs 提供了一条有前景的路径，使时间序列预测更加通用、高效、协同和易于访问。因此，将这些强大的模型适应于时间序列数据可以释放出巨大的未开发潜力。

---

上述优势的实现依赖于时间序列数据和自然语言模态的有效对齐。然而，这是一项具有挑战性的任务，因为 LLMs 操作的是离散的标记，而时间序列数据本质上是连续的。此外，解释时间序列模式的知识和推理能力并不自然存在于 LLMs 的预训练中。因此，如何以准确、数据高效且任务无关的方式解锁 LLMs 中的知识，激活其通用时间序列预测能力，仍然是一个开放的挑战。

---

在本研究中，我们提出了 TIME-LLM，这是一个重新编程框架，旨在适应大型语言模型进行时间序列预测，同时保持主干模型的完整性。核心思想是将输入时间序列重新编程为更适合语言模型能力的文本原型表示。为了进一步增强模型对时间序列概念的推理能力，我们引入了 Prompt-as-Prefix (PaP)，这是一个新颖的想法，通过自然语言模态丰富输入时间序列的上下文并提供任务指令。这为应用于重新编程输入的期望转换提供了声明性指导。然后，语言模型的输出被投影以生成时间序列预测。我们的全面评估表明，通过这种重新编程方法，大型语言模型可以作为有效的少样本和零样本时间序列学习器，其性能优于专门的预测模型。通过利用 LLMs 的推理能力同时保持模型的完整性，我们的工作为在多模态基础模型上同时擅长语言和序列数据任务指明了方向。我们提出的重新编程框架为赋予大型模型超越其原始预训练的新能力提供了一个可扩展的范式。我们在这项工作中的主要贡献可以总结如下：

- 我们引入了一个新颖的概念，即在不改变预训练主干模型的情况下，重新编程大型语言模型进行时间序列预测。通过这样做，我们展示了预测可以被视为另一种“语言”任务，可以由现成的 LLM 有效解决。

- 我们提出了一个新的框架 Time-LLM，它包括将输入时间序列重新编程为更适合 LLM 的文本原型表示，并通过声明性提示（例如，领域专家知识和任务指令）丰富输入上下文以指导 LLM 推理。我们的技术指向在多模态基础模型上同时擅长语言和时间序列。

- Time-LLM 在主流预测任务中始终超越最先进的性能，尤其是在少样本和零样本场景中。此外，这种卓越的性能是在保持出色的模型重新编程效率的同时实现的。因此，我们的研究是释放 LLMs 在时间序列乃至其他序列数据中未开发潜力的具体一步。

# 2 相关工作

**任务特定学习**。大多数时间序列预测模型是为特定任务和领域（例如交通预测）量身定制的，并在小规模数据上进行端到端训练。图 1(a) 展示了一个示例。例如，ARIMA 模型设计用于单变量时间序列预测（Box 等，2015），LSTM 网络专为序列建模设计（Hochreiter & Schmidhuber 1997），而时间卷积网络（Bai 等，2018）和 Transformer（Wen 等，2023）则开发用于处理更长的时间依赖性。尽管这些模型在狭窄任务上表现出色，但它们缺乏对多样化时间序列数据的通用性和泛化能力。

- 图 1：重新编程大型语言模型（LLMs）的示意图，与 (a) 任务特定学习和 (b) 模型微调进行比较。我们的研究探索并展示了 (c) 如何有效地重新编程开源 LLMs 作为强大的时间序列学习器，尤其是在缺乏完善的时间序列预训练模型的情况下。

---

**模态内适应**。CV 和 NLP 领域的相关研究证明了预训练模型的有效性，这些模型可以针对各种下游任务进行微调，而无需从头开始进行昂贵的训练（Devlin 等，2018；Brown 等，2020；Touvron 等，2023）。受这些成功的启发，最近的研究集中在时间序列预训练模型（TSPTMs）的开发上。其中的第一步涉及使用不同策略（如监督学习（Fawaz 等，2018）或自监督学习（Zhang 等，2022b；Deldari 等，2022；Zhang 等，2023））进行时间序列预训练。这使得模型能够学习表示各种输入时间序列。一旦预训练完成，它可以在类似领域上进行微调，以学习如何执行特定任务（Tang 等，2022）。图 1(b) 展示了一个示例。TSPTMs 的开发借鉴了 NLP 和 CV 中预训练和微调的成功，但由于数据稀疏性，其规模仍然有限。

---

**跨模态适应**。在模态内适应的基础上，最近的研究进一步探索了通过多模态微调（Yin 等，2023）和模型重新编程（Chen，2022）等技术，将 NLP 和 CV 中强大的预训练基础模型的知识转移到时间序列建模中。我们的方法属于这一类；然而，关于时间序列的相关研究有限。一个示例是 Voice2Series（Yang 等，2021），它通过将时间序列编辑为适合声学模型（AM）的格式，将语音识别中的声学模型适应于时间序列分类。最近，Chang 等（2023）提出了 LLM4TS，用于使用 LLMs 进行时间序列预测。它设计了一个两阶段微调过程，首先在时间序列上进行监督预训练，然后进行任务特定微调。Zhou 等（2023a）利用预训练的语言模型而不改变其自注意力和前馈层。该模型在各种时间序列分析任务上进行微调和评估，并通过从自然语言预训练中转移知识，展示了可比或最先进的性能。与这些方法不同，我们既不直接编辑输入时间序列，也不微调主干 LLM。相反，如图 1(c) 所示，我们提出通过源数据模态重新编程时间序列并结合提示，以释放 LLMs 作为有效时间序列机器的潜力。

# 3 方法论

我们的模型架构如图 2 所示。我们专注于重新编程一个嵌入可见的语言基础模型，例如 Llama（Touvron 等，2023）和 GPT-2（Radford 等，2019），以进行通用时间序列预测，而无需对主干模型进行任何微调。具体来说，我们考虑以下问题：给定一个由 $N$ 个不同的 1 维变量在 $T$ 个时间步长上组成的历史观测序列 $\mathbf{X} \in \mathbb{R}^{N \times T}$，我们的目标是重新编程一个大型语言模型 $f(\cdot)$，以理解输入时间序列并准确预测未来 $H$ 个时间步长的读数，记为 $\hat{\mathbf{Y}} \in \mathbb{R}^{N \times H}$，总体目标是最小化真实值 $\mathbf{Y}$ 和预测值之间的均方误差，即 $\frac{1}{H} \sum_{h=1}^{H}\left\|\hat{\mathbf{Y}}_{h}-\mathbf{Y}_{h}\right\|_{F}^{2}$。

- 图 2：Time-LLM 的模型框架。给定一个输入时间序列，我们首先通过 (1) 分块和 (2) 自定义嵌入层对其进行标记化和嵌入。(3) 然后，这些分块嵌入通过压缩的文本原型重新编程，以对齐两种模态。为了增强 LLM 的推理能力，(4) 额外的提示前缀被添加到输入中，以指导输入分块的转换。(5) 来自 LLM 的输出分块被投影以生成预测。

---

我们的方法包括三个主要组件：(1) 输入转换，(2) 预训练并冻结的 LLM，以及 (3) 输出投影。首先，一个多变量时间序列被划分为 $N$ 个单变量时间序列，随后独立处理（Nie 等，2023）。第 $i$ 个序列记为 $\mathbf{X}^{(i)} \in \mathbb{R}^{1 \times T}$，在通过学习的文本原型重新编程以对齐源和目标模态之前，它经历了归一化、分块和嵌入。然后，我们通过提示 LLM 与重新编程的分块一起生成输出表示，以增强其时间序列推理能力，这些表示被投影到最终的预测 $\hat{\mathbf{Y}}^{(i)} \in \mathbb{R}^{1 \times H}$。

---

我们注意到，只有轻量级输入转换和输出投影的参数被更新，而主干语言模型被冻结。与视觉语言和其他多模态语言模型通常使用成对的跨模态数据进行微调不同，Time-LLM 直接优化，并且仅需少量时间序列和少量训练周期即可使用，与从头构建大型领域特定模型或对其进行微调相比，保持了高效率并施加了更少的资源限制。为了进一步减少内存占用，各种现成技术（例如量化）可以无缝集成以精简 Time-LLM。

## 3.1 模型结构

**输入嵌入**。每个输入通道 $\mathbf{X}^{(i)}$ 首先通过可逆实例归一化（RevIN）进行独立归一化，以缓解时间序列分布漂移（Kim 等，2021）。然后，我们将 $\mathbf{X}^{(i)}$ 划分为若干个连续的重叠或非重叠的分块（Nie 等，2023），每个分块长度为 $L_{p}$；因此，输入分块的总数为 $P=\left\lfloor\frac{\left(T-L_{p}\right)}{S}\right\rfloor+2$，其中 $S$ 表示水平滑动步长。其背后的动机有两个：(1) 通过将局部信息聚合到每个分块中，更好地保留局部语义信息；(2) 作为标记化，形成紧凑的输入标记序列，减少计算负担。给定这些分块 $\mathbf{X}_{P}^{(i)} \in \mathbb{R}^{P \times L_{p}}$，我们将它们嵌入为 $\hat{\mathbf{X}}_{P}^{(i)} \in \mathbb{R}^{P \times d_{m}}$，采用简单的线性层作为分块嵌入器，以创建维度 $d_{m}$。

---

**分块重新编程**。在这里，我们将分块嵌入重新编程到源数据表示空间中，以对齐时间序列和自然语言的模态，从而激活主干模型的时间序列理解和推理能力。常见的做法是学习一种“噪声”，当应用于目标输入样本时，允许预训练的源模型生成所需的目标输出，而无需更新参数。这在技术上对于桥接相同或相似的数据模态是可行的。例如，重新编程视觉模型以处理跨领域图像（Misra 等，2023），或重新编程声学模型以处理时间序列数据（Yang 等，2021）。在这两种情况下，源数据和目标数据之间存在明确的可学习转换，允许直接编辑输入样本。然而，时间序列既不能直接编辑，也不能用自然语言无损描述，这给直接引导 LLM 理解时间序列带来了重大挑战，而无需资源密集型的微调。

---

为了弥补这一差距，我们提出使用主干中的预训练词嵌入 $\mathbf{E} \in \mathbb{R}^{V \times D}$ 重新编程 $\hat{\mathbf{X}}_{P}^{(i)}$，其中 $V$ 是词汇量。然而，没有先验知识表明哪些源标记是直接相关的。因此，简单地利用 $\mathbf{E}$ 将导致庞大且可能密集的重新编程空间。一个简单的解决方案是通过线性探测 $\mathbf{E}$ 来维护一小部分文本原型，记为 $\mathbf{E}^{\prime} \in \mathbb{R}^{V^{\prime} \times D}$，其中 $V^{\prime} \ll V$。图 3(a) 展示了一个示例。文本原型学习连接语言线索，例如“短期上升”（红线）和“稳步下降”（蓝线），然后将它们组合起来表示局部分块信息（例如，“短期上升然后稳步下降”用于描述分块 5），而不会离开语言模型预训练的空间。这种方法效率高，并且允许自适应地选择相关的源信息。为了实现这一点，我们采用了一个多头交叉注意力层。具体来说，对于每个头 $k=\{1, \cdots, K\}$，我们定义查询矩阵 $\mathbf{Q}_{k}^{(i)}=\hat{\mathbf{X}}_{P}^{(i)} \mathbf{W}_{k}^{Q}$，键矩阵 $\mathbf{K}_{k}^{(i)}=\mathbf{E}^{\prime} \mathbf{W}_{k}^{K}$，和值矩阵 $\mathbf{V}_{k}^{(i)}=\mathbf{E}^{\prime} \mathbf{W}_{k}^{V}$，其中 $\mathbf{W}_{k}^{Q} \in \mathbb{R}^{d_{m} \times d}$ 和 $\mathbf{W}_{k}^{K}, \mathbf{W}_{k}^{V} \in \mathbb{R}^{D \times d}$。具体来说，$D$ 是主干模型的隐藏维度，$d=\left\lfloor\frac{d_{m}}{K}\right\rfloor$。然后，我们定义了在每个注意力头中重新编程时间序列分块的操作：
$$
\begin{equation*}
\mathbf{Z}_{k}^{(i)}=\operatorname{ATtention}\left(\mathbf{Q}_{k}^{(i)}, \mathbf{K}_{k}^{(i)}, \mathbf{V}_{k}^{(i)}\right)=\operatorname{Softmax}\left(\frac{\mathbf{Q}_{k}^{(i)} \mathbf{K}_{k}^{(i) \top}}{\sqrt{d_{k}}}\right) \mathbf{V}_{k}^{(i)} . \tag{1}
\end{equation*}
$$

通过聚合每个头中的 $\mathbf{Z}_{k}^{(i)} \in \mathbb{R}^{P \times d}$，我们得到 $\mathbf{Z}^{(i)} \in \mathbb{R}^{P \times d_{m}}$。然后将其线性投影以对齐主干模型的隐藏维度，得到 $\mathbf{O}^{(i)} \in \mathbb{R}^{P \times D}$。

---

**Prompt-as-Prefix（提示作为前缀）**。提示是一种直接而有效的 LLMs 任务特定激活方法（Yin 等，2023）。然而，将时间序列直接翻译为自然语言存在相当大的挑战，阻碍了指令跟随数据集的创建以及在性能不受影响的情况下有效利用即时提示（Xue & Salim，2022）。最近的进展表明，其他数据模态（如图像）可以无缝集成到提示的前缀中，从而促进基于这些输入的有效推理（Tsimpoukelli 等，2021）。受这些发现的启发，为了使我们的方法直接适用于现实世界的时间序列，我们提出了一个替代问题：提示是否可以作为前缀来丰富输入上下文并指导重新编程的时间序列分块的转换？我们将这一概念称为 Prompt-as-Prefix（PaP），并观察到它显著增强了 LLM 对下游任务的适应性，同时补充了分块重新编程（参见后面的第 4.5 节）。

---

图 3(b) 展示了两种提示方法的示意图。在 Patch-as-Prefix 中，语言模型被提示以自然语言表达的时间序列预测后续值。这种方法遇到了一些限制：(1) 语言模型通常在处理高精度数字时表现出较低的敏感性，在没有外部工具辅助的情况下，准确处理长期预测任务面临重大挑战；(2) 由于不同语言模型在生成高精度数字时可能使用不同的标记化类型，因此需要复杂的定制后处理。这导致预测结果以不同的自然语言格式表示，例如 [' 0 ', '., ' 6 ', ' 1 '] 和 [' 0 ', ' $\because$, ' 61 '] 来表示小数 0.61 。

---

另一方面，Prompt-as-Prefix 巧妙地避免了这些限制。在实践中，我们确定了构建有效提示的三个关键组成部分：(1) 数据集上下文，(2) 任务指令，和 (3) 输入统计信息。图 4 展示了一个提示示例。数据集上下文为 LLM 提供了有关输入时间序列的基本背景信息，这些信息通常在不同领域中表现出不同的特征。任务指令作为 LLM 在特定任务中转换分块嵌入的关键指导。我们还通过添加趋势和滞后等关键统计信息来丰富输入时间序列，以促进模式识别和推理。

---

**输出投影**。如图 2 所示，在通过冻结的 LLM 打包和前馈提示和分块嵌入 $\mathbf{O}^{(i)}$ 后，我们丢弃前缀部分并获得输出表示。随后，我们将其展平并线性投影以得出最终预测 $\hat{\mathbf{Y}}^{(i)}$。

# 4 主要结果

TIME-LLM 在多个基准和设置中始终以较大优势超越最先进的预测方法，尤其是在少样本和零样本场景中。我们将我们的方法与一系列最新的模型进行了比较，包括最近一项微调语言模型用于时间序列分析的研究（Zhou 等，2023a）。为了确保公平比较，我们在所有基线中遵循（Wu 等，2023）的实验配置，并使用统一的评估管道 ${ }^{11}$。除非另有说明，我们默认使用 Llama-7B（Touvron 等，2023）作为主干模型。

---

**基线**。我们与最先进的时间序列模型进行比较，如果适用，我们从（Zhou 等，2023a）中引用它们的性能。我们的基线包括一系列基于 Transformer 的方法：PatchTST（2023）、ESTformer（2022）、Non-Stationary Transformer（2022）、FEDformer（2022）、Autoformer（2021）、Informer（2021）和 Reformer（2020）。我们还选择了一组最近具有竞争力的模型，包括 GPT4TS（2023a）、LLMTime（2023）、DLinear（2023）、TimesNet（2023）和 LightTS（2022a）。在短期预测中，我们进一步将我们的模型与 N-HiTS（2023b）和 N-BEATS（2020）进行比较。更多细节见附录 A。

## 4.1 长期预测

**设置**。我们在 ETTh1、ETTh2、ETTm1、ETTm2、Weather、Electricity (ECL)、Traffic 和 ILI 上进行了评估，这些数据集已被广泛用于长期预测模型的基准测试（Wu 等，2023）。实现和数据集的详细信息见附录 B。输入时间序列长度 $T$ 设置为 512，我们使用四个不同的预测范围 $H \in\{96,192,336,720\}$。评估指标包括均方误差（MSE）和平均绝对误差（MAE）。

---

**结果**。我们的简要结果如表 1 所示，TIME-LLM 在大多数情况下优于所有基线，并且对其中大多数模型显著优于。与 GPT4TS（Zhou 等，2023a）的比较尤其值得注意。GPT4TS 是一项非常新的工作，涉及对主干语言模型进行微调。我们注意到，与 GPT4TS 和 TimesNet 相比，平均性能分别提高了 $\mathbf{12\%}$ 和 $\mathbf{20\%}$。与最先进的任务特定 Transformer 模型 PatchTST 相比，通过重新编程最小的 Llama，Time-LLM 实现了平均 MSE 减少 $1.4\%$。相对于其他模型，例如 DLinear，我们的改进也很显著，超过 $\mathbf{12\%}$。

- 表 1：长期预测结果。所有结果均来自四个不同预测范围的平均值：$H \in$ $\{24,36,48,60\}$ 用于 ILI，$\{96,192,336,720\}$ 用于其他数据集。较低的值表示更好的性能。红色：最佳，蓝色：次佳。我们的完整结果见附录 D。

## 4.2 短期预测

**设置**。我们选择 M4 基准（Makridakis 等，2018）作为测试平台，其中包含不同采样频率的营销数据集合。更多详细信息见附录 B。在这种情况下，预测范围相对较小，且在 $[6,48]$ 之间。输入长度是预测范围的两倍。评估指标包括对称平均绝对百分比误差（SMAPE）、平均绝对缩放误差（MSAE）和总体加权平均值（OWA）。

---

**结果**。我们在所有方法中使用统一种子的简要结果如表 2 所示。Time-LLM 始终超越所有基线，比 GPT4TS 高出 $\mathbf{8.7\%}$。即使与最先进的模型 N-HiTS（Challu 等，2023b）相比，TIME-LLM 在 MASE 和 OWA 方面仍然具有竞争力。

- 表 2：M4 上的短期时间序列预测结果。预测范围在 $[6,48]$ 之间，提供的三行是不同采样间隔下所有数据集的加权平均值。较低的值表示更好的性能。红色：最佳，蓝色：次佳。更多结果见附录 D。

## 4.3 少样本预测

**设置**。LLMs 最近展示了显著的少样本学习能力（Liu 等，2023b）。在本节中，我们评估我们重新编程的 LLM 是否在预测任务中保留了这种能力。我们遵循（Zhou 等，2023a）中的设置以确保公平比较，并在训练数据有限的情况下进行评估（即 $\leq$ 前 $10\%$ 的训练时间步长）。

---

**结果**。我们的简要 $10\%$ 和 5\% 少样本学习结果分别如表 3 和表 4 所示。TimeLLM 显著优于所有基线方法，我们将其归因于我们重新编程的 LLM 中成功的知识激活。有趣的是，我们的方法和 GPT4TS 都始终超越其他竞争基线，进一步强调了语言模型作为熟练时间序列机器的潜在能力。

- 表 3：在 $10\%$ 训练数据上的少样本学习。我们使用表 1 中的相同协议。所有结果均来自四个不同预测范围的平均值：$H \in\{96,192,336,720\}$。我们的完整结果见附录 E。

- 表 4：在 $5\%$ 训练数据上的少样本学习。我们使用表 1 中的相同协议。所有结果均来自四个不同预测范围的平均值：$H \in\{96,192,336,720\}$。我们的完整结果见附录 E。

---

在 $10\%$ 少样本学习领域，我们的方法实现了与 GPT4TS 相比 $\mathbf{5\%}$ 的 MSE 减少，而无需对 LLM 进行任何微调。与最近的 SOTA 模型（如 PatchTST、DLinear 和 TimesNet）相比，我们在 MSE 方面的平均改进分别超过 $\mathbf{8\%}$、$\mathbf{12\%}$ 和 $\mathbf{33\%}$。类似的趋势在 $5\%$ 少样本学习场景中也可见，我们相对于 GPT4TS 的平均进步超过 5\%。与 PatchTST、DLinear 和 TimesNet 相比，TIME-LLM 显示出超过 $\mathbf{20\%}$ 的显著平均改进。

## 4.4 零样本预测

**设置**。除了少样本学习外，LLMs 还具有作为有效零样本推理者的潜力（Kojima 等，2022）。在本节中，我们评估了重新编程的 LLM 在跨领域适应框架内的零样本学习能力。具体来说，我们考察了一个模型在数据集 \& 上的表现，而该模型在另一个数据集 $\boldsymbol{\phi}$ 上进行了优化，其中模型从未遇到过数据集 \& 的任何数据样本。与少样本学习类似，我们使用长期预测协议，并利用 ETT 数据集在各种跨领域场景中进行评估。

---

**结果**。我们的简要结果如表 5 所示。TIME-LLM 始终以较大优势超越最具竞争力的基线，在 MSE 减少方面比第二名高出 $\mathbf{14.2\%}$。考虑到少样本结果，我们观察到在数据稀缺场景中，重新编程 LLM 往往能产生显著更好的结果。例如，我们在 $10\%$ 少样本预测、$5\%$ 少样本预测和零样本预测中相对于 GPT4TS 的整体误差减少逐渐增加：7.7\%，$\mathbf{8.4\%}$，和 $\mathbf{22\%}$。即使与 LLMTime（该领域的最新方法，主干 LLM 规模相当，为 7B）进行基准测试，TIME-LLM 也显示出超过 $\mathbf{75\%}$ 的显著改进。我们将其归因于我们的方法在执行时间序列任务时，能够以资源高效的方式更好地激活 LLM 的知识转移和推理能力。

## 4.5 模型分析

**语言模型变体**。我们比较了两个具有不同容量的代表性主干（表 6 中的 A.1-4）。我们的结果表明，在 LLM 重新编程后，缩放定律仍然适用。我们默认采用 Llama-7B 的全部容量，其明显优于其 $1/4$ 容量变体（A.2；包括前 8 个 Transformer 层），性能高出 $\mathbf{14.5\%}$。相对于 GPT-2（A.3），我们观察到平均 MSE 减少 $\mathbf{14.7\%}$，其略优于其变体 GPT-2 (6)（A.4），性能高出 $2.7\%$。

---

**跨模态对齐**。我们在表 6 中的结果表明，在重新编程 LLM 以进行有效时间序列预测时，取消分块重新编程或 Prompt-as-Prefix 都会损害知识转移。在没有表示对齐的情况下（B.1），我们观察到平均性能显著下降 $\mathbf{9.2\%}$，在少样本任务中更为明显（超过 $\mathbf{17\%}$）。在 Time-LLM 中，提示行为是发挥 LLM 理解输入和任务能力的关键要素。取消该组件（B.2）会导致标准和少样本预测任务中的性能分别下降超过 $\mathbf{8\%}$ 和 $\mathbf{19\%}$。我们发现，移除输入统计信息（C.1）对性能影响最大，导致平均 MSE 增加 $\mathbf{10.2\%}$。这是预料之中的，因为外部知识可以通过提示自然融入，以促进学习和推理。此外，为 LLM 提供明确的任务指令和输入上下文（例如数据集描述）也有益（即 C.2 和 C.1；分别引出超过 $\mathbf{7.7\%}$ 和 $\mathbf{9.6\%}$）。

---

**重新编程解释**。我们在图 5 中提供了一个关于 ETTh1 的案例研究，使用 100 个文本原型重新编程 48 个时间序列分块。前 4 个子图展示了从随机初始化（a）到优化良好（d）的重新编程空间的优化过程。我们发现只有一小部分原型（列）参与了输入分块（行）的重新编程，如子图（e）所示。此外，分块通过不同的原型组合经历了不同的表示。这表明：(1) 文本原型学习总结语言线索，其中少数几个与局部时间序列分块中的信息表示高度相关，我们在子图（f）中随机选择了 10 个进行可视化。我们的结果表明，与描述时间序列属性的单词（即单词集 1 和 2）高度相关；(2) 分块通常具有不同的底层语义，因此需要不同的原型来表示。

---

**重新编程效率**。表 7 提供了带有和不带有主干 LLM 的 TiME-LLM 的整体效率分析。我们提出的重新编程网络本身（D.3）在激活 LLM 的时间序列预测能力方面是轻量级的（即少于 660 万个可训练参数；仅占 Llama-7B 总参数的约 $\mathbf{0.2\%}$），而 TIME-LLM 的整体效率实际上受限于所利用的主干（例如 D.1 和 D.2）。即使在平衡任务性能和效率方面，与参数高效微调方法（例如 QLoRA（Dettmers 等，2023））相比，这也是有利的。

# 5 结论与未来工作

TIME-LLM 展示了通过将时间序列数据重新编程为更适合 LLMs 的文本原型，并通过 Prompt-as-Prefix 提供自然语言指导以增强推理能力，来适应冻结的大型语言模型进行时间序列预测的潜力。评估表明，经过适应的 LLMs 可以超越专门的专家模型，表明它们作为有效时间序列机器的潜力。我们的结果还提供了一个新颖的见解，即时间序列预测可以被视为另一种“语言”任务，可以通过现成的 LLM 通过我们的 Time-LLM 框架实现最先进的性能。进一步的研究应探索最佳的重新编程表示，通过持续预训练丰富 LLMs 的显式时间序列知识，并构建跨时间序列、自然语言和其他模态的联合推理的多模态模型。此外，还应考虑应用重新编程框架为 LLMs 配备更广泛的时间序列分析能力或其他新能力。

# A. 更多相关工作

**任务特定学习**。我们提供了任务特定学习相关工作的扩展，特别关注我们进行比较的最相关模型。最近的工作通过结合信号处理原理（如分块、指数平滑、分解和频率分析）改进了 Transformer（Vaswani 等，2017）用于时间序列预测。例如，PatchTST（Nie 等，2023）将时间序列分割为分块作为 Transformer 的输入标记。这保留了局部语义，减少了注意力的计算/内存需求，并允许更长的历史记录。它在长期预测准确性上优于其他 Transformer 模型。它还在自监督预训练和迁移学习方面表现出色。ETSformer（Woo 等，2022）将指数平滑原理融入 Transformer 注意力中，以提高准确性和效率。它使用指数平滑注意力和频率注意力来替代标准的自注意力。FEDformer（Zhou 等，2022）将 Transformer 与季节性趋势分解相结合。分解捕捉全局轮廓，而 Transformer 捕捉详细结构。它还使用频率增强进行长期预测。这提供了比标准 Transformer 更好的性能和效率。Autoformer（Wu 等，2021）使用具有自相关的分解架构，以实现对复杂序列的渐进分解能力。自相关基于序列周期性设计，以进行依赖发现和表示聚合。它在效率和准确性上优于自注意力。

---

尽管这些方法在效率和准确性上相比原始 Transformer 有所提升，但它们大多是为特定领域内的狭窄预测任务设计和优化的。这些模型通常在小规模、特定领域的数据集上进行端到端训练。虽然在其目标任务上表现出色，但这些专门化模型牺牲了在现实世界中遇到的多样化时间序列数据的通用性和泛化能力。狭窄的焦点限制了它们在新数据集和任务上的适用性。为了推进时间序列预测，需要更灵活、广泛适用的模型，能够适应新的数据分布和任务而无需大量重新训练。理想的模型将学习跨领域转移知识的鲁棒时间序列表示。开发这种广泛适用的预测模型仍然是一个开放的挑战。根据我们对相关先前工作的讨论，最近的研究已经开始通过预训练和架构创新探索模型的通用性。然而，需要进一步努力以实现我们在本研究中推进的真正通用预测系统。

---

**跨模态适应**。我们提供了跨模态适应相关工作的扩展概述，特别关注时间序列和其他数据模态的模型重新编程的最新进展。模型重新编程是一种资源高效的跨领域学习方法，涉及将从一个领域（源）开发良好的预训练模型适应于解决不同领域（目标）的任务，而无需模型微调，即使这些领域显著不同，如 Chen（2022）所述。在时间序列数据的背景下，Voice2Series（Yang 等，2021）将语音识别中的声学模型适应于时间序列分类，通过转换时间序列以适应模型并将输出重新映射到新标签。类似地，LLMTime（Gruver 等，2023）将 LLMs 适应于零样本时间序列预测，专注于输入时间序列的有效标记化，以便主干 LLM 能够自回归地生成预测。与这些方法不同，TIME-LLM 不直接编辑输入时间序列。相反，它提出通过源数据模态重新编程时间序列并结合提示，以释放 LLMs 作为标准、少样本和零样本场景中多功能预测器的全部潜力。该领域的其他重要工作，主要在生物学领域，包括 R2DL（Vinod 等，2020）和 ReproBert（Melnyk 等，2023），它们使用词嵌入重新编程氨基酸。与我们的分块重新编程方法的一个关键区别是，与完整的氨基酸集合不同，时间序列分块不形成完整集合。因此，我们提出优化一小部分文本原型及其到时间序列分块的映射，而不是直接优化两个完整集合（如词汇和氨基酸）之间的庞大转换矩阵。

# B 实验细节

## B.1 实现

我们主要在 https://github.com/thuml/Time-Series-Library 的统一评估管道中遵循（Wu 等，2023）的实验配置，以确保公平比较。除非另有说明，我们默认使用 Llama-7B（Touvron 等，2023）作为主干模型。所有实验均重复三次，我们报告平均结果。我们的模型实现基于 PyTorch（Paszke 等，2019），所有实验均在 NVIDIA A100-80G GPU 上进行。我们的详细模型配置见附录 B.4，代码可在 https://github.com/KimMeen/Time-LLM 获取。

---

**技术细节**。我们从三个方面提供 Time-LLM 的额外技术细节：(1) 文本原型的学习，(2) 时间序列中趋势和滞后的计算以用于提示，以及 (3) 输出投影的实现。为了从 $\mathbf{E} \in \mathbb{R}^{V \times D}$ 中识别出一小部分文本原型 $\mathbf{E}^{\prime} \in \mathbb{R}^{V^{\prime} \times D}$，我们学习一个矩阵 $\mathbf{W} \in \mathbb{R}^{V^{\prime} \times V}$ 作为中介。为了用自然语言描述整体时间序列趋势，我们计算连续时间步长之间差异的总和。总和大于 0 表示上升趋势，而较小的总和表示下降趋势。此外，我们计算时间序列的前 5 个滞后，通过使用快速傅里叶变换计算自相关并选择具有最高相关值的五个滞后来确定。在我们通过冻结的 LLM 打包和前馈提示和分块嵌入 $\mathbf{O}^{(i)} \in \mathbb{R}^{P \times D}$ 后，我们丢弃前缀部分并获得输出表示，记为 $\tilde{\mathbf{O}}^{i} \in \mathbb{R}^{P \times D}$。随后，我们遵循 PatchTST（Nie 等，2023）将 $\tilde{\mathbf{O}}^{i}$ 展平为长度为 $P \times D$ 的一维张量，然后线性投影为 $\hat{\mathbf{Y}}^{i} \in \mathbb{R}^{H}$。

## B.2 数据集细节

数据集统计信息总结在表 8 中。我们在八个不同的基准上评估长期预测性能，包括四个 ETT 数据集（Zhou 等，2021）（即 ETTh1、ETTh2、ETTm1 和 ETTm2）、Weather、Electricity、Traffic 和 ILI（来自 Wu 等，2023）。此外，我们在 M4 基准（Makridakis 等，2018）和 M3 基准（Makridakis & Hibon，2000）中的季度数据集上评估短期预测性能。

---

**电力变压器温度（ETT；反映长期电力部署的指标）**基准包括来自中国两个县的两年数据，并细分为四个不同的数据集，每个数据集具有不同的采样率：ETTh1 和 ETTh2，采样率为 1 小时，ETTm1 和 ETTm2，采样率为 15 分钟。ETT 数据集中的每个条目包括六个电力负荷特征和一个目标变量，称为“油温”。**Electricity** 数据集包括 321 名客户的电力消耗记录，采样率为 1 小时。**Weather** 数据集包括来自德国 21 个气象站的一年记录，采样率为 10 分钟。**Traffic** 数据集包括加州高速公路系统的占用率数据，采样率为 1 小时。**流感样疾病（ILI）** 数据集包括患有严重流感并伴有并发症的患者的记录。

---

**M4** 基准包括 100 K 个时间序列，收集自商业、金融和经济预测中常见的各个领域。这些时间序列已划分为六个不同的数据集，每个数据集具有从年到小时的采样频率。**M3-Quarterly** 数据集包括 M3 基准中的 756 个季度采样时间序列。这些序列分为五个不同领域：人口统计、微观、宏观、工业和金融。

# B.3 评估指标

对于评估指标，我们使用均方误差（MSE）和平均绝对误差（MAE）来评估长期预测。对于 M4 基准的短期预测，我们采用对称平均绝对百分比误差（SMAPE）、平均绝对缩放误差（MASE）和整体加权平均值（OWA），如 N-BEATS（Oreshkin 等，2020）中所述。需要注意的是，OWA 是 M4 竞赛中使用的特定指标。这些指标的计算方法如下：
$$
\begin{aligned}
& \operatorname{MSE}=\frac{1}{H} \sum_{h=1}^{T}\left(\mathbf{Y}_{h}-\hat{\mathbf{Y}}_{h}\right)^{2}, \\
& \mathrm{MAE}=\frac{1}{H} \sum_{h=1}^{H}\left|\mathbf{Y}_{h}-\hat{\mathbf{Y}}_{h}\right|, \\
& \text { SMAPE }=\frac{200}{H} \sum_{h=1}^{H} \frac{\left|\mathbf{Y}_{h}-\hat{\mathbf{Y}}_{h}\right|}{\left|\mathbf{Y}_{h}\right|+\left|\hat{\mathbf{Y}}_{h}\right|}, \\
& \text { MAPE }=\frac{100}{H} \sum_{h=1}^{H} \frac{\left|\mathbf{Y}_{h}-\hat{\mathbf{Y}}_{h}\right|}{\left|\mathbf{Y}_{h}\right|}, \\
& \operatorname{MASE}=\frac{1}{H} \sum_{h=1}^{H} \frac{\left|\mathbf{Y}_{h}-\hat{\mathbf{Y}}_{h}\right|}{\frac{1}{H-s} \sum_{j=s+1}^{H}\left|\mathbf{Y}_{j}-\mathbf{Y}_{j-s}\right|}, \\
& \mathrm{OWA}=\frac{1}{2}\left[\frac{\mathrm{SMAPE}}{\mathrm{SMAPE}_{\text {Naïve } 2}}+\frac{\text { MASE }}{\mathrm{MASE}_{\text {Naïve2 }}}\right],
\end{aligned}
$$

其中 $s$ 是时间序列数据的周期性。$H$ 表示数据点的数量（即我们的案例中的预测范围）。$\mathbf{Y}_{h}$ 和 $\hat{\mathbf{Y}}_{h}$ 分别是第 $h$ 个真实值和预测值，其中 $h \in\{1, \cdots, H\}$。

## B.4 模型配置

我们模型的配置，相对于不同的任务和数据集，总结在表 9 中。默认情况下，所有实验均采用 Adam 优化器（Kingma & Ba，2015）。具体来说，文本原型的数量 $V^{\prime}$ 在短期和长期预测任务中分别固定为 100 和 1000。我们使用 Llama-7B 模型的全容量，在所有任务中保持主干模型层数为 32 作为标准。输入长度 $T$ 表示原始输入时间序列数据中的时间步数。分块维度 $d_{m}$ 表示重新编程前嵌入时间序列分块的隐藏维度。最后，头数 $K$ 与用于分块重新编程的多头交叉注意力相关。在表 9 的最右侧四列中，我们详细说明了与模型训练相关的配置。

# C 超参数敏感性

我们对 TIME-LLM 中的四个重要超参数进行了敏感性分析：即主干模型的层数、文本原型的数量 $V^{\prime}$、时间序列输入长度 $T$ 和分块重新编程交叉注意力头数 $K$。相关结果见图 6。从我们的分析中，我们得出以下观察结果：(1) 主干 LLM 中 Transformer 层的数量与 TIME-LLM 的性能呈正相关，证实了在 LLM 重新编程后缩放定律仍然适用；(2) 通常，获取更多文本原型可以提高性能。我们假设，有限的原型数量 $V^{\prime}$ 可能在聚合语言线索时引入噪声，从而阻碍高效学习对表征输入时间序列分块至关重要的高度代表性原型；(3) 输入时间长度 $T$ 与预测准确性直接相关，特别是在预测较长范围时尤为明显。这一观察是合乎逻辑的，并且与传统时间序列模型一致；(4) 在重新编程输入分块时增加注意力头的数量是有益的。

# D 长期和短期预测

## D.1 长期预测

通过仅重新编程最小的 Llama 模型并保持其完整性，Time-LLM 在八个时间序列基准中的 **36** 个实例中取得了 SOTA 性能。这突显了 LLMs 作为强大且可靠的时间序列预测器的巨大潜力。此外，我们在表 11 中将所提出的方法与其他成熟的基线进行了基准测试。这一比较包括三种著名的统计方法（AutoARIMA、AutoTheta 和 AutoETS）（Herzen 等，2022）以及两种最近的时间序列模型 N-HiTS（Challu 等，2023b）和 N-BEATS（Oreshkin 等，2020）。值得注意的是，Time-LLM 在所有案例中均取得了 SOTA 性能，在 MSE 和 MAE 方面分别以超过 **22%** 和 **16%** 的显著优势超越了第二佳结果。

- 表 11：长期预测任务中与其他基线的额外比较。我们为 ILI 设置了预测范围 $H \in\{24,36,48,60\}$，为其他数据集设置了 $\{96,192,336,720\}$。较低的值表示更好的性能。红色：最佳，蓝色：次佳。

## D.2 短期预测

我们在短期预测中的完整结果展示在表 12 中。Time-LLM 在大多数情况下始终优于大多数基线模型。值得注意的是，我们以较大优势超越了 GPT4TS（例如，总体 **8.7%**，M4-Yearly **13.4%**，M4-Hourly、M4-Daily 和 M4-Weekly 平均 **21.5%**），以及 TimesNet（例如，总体 **10%**，M4-Yearly **14.1%**，M4-Hourly、M4-Daily 和 M4-Weekly 平均 **30.1%**）。与最近的最先进预测模型 N-HiTS 和 PatchTST 相比，Time-LLM 在不更新主干 LLM 参数的情况下表现出相当或更优的性能。

- 表 12：完整的短期时间序列预测结果。预测范围在 $[6,48]$ 之间，最后三行是不同采样间隔下所有数据集的加权平均值。较低的值表示更好的性能。红色：最佳，蓝色：次佳。

---

此外，我们在 M3-Quarterly 数据集上对 Time-LLM 和表现最佳的模型进行了比较分析，结果展示在表 13 中。我们提供了额外的指标，即 MRAE 和 MAPE，以及 M3 竞赛中默认使用的 SMAPE。在该数据集上，Time-LLM 与 TimesNet 和 PatchTST 表现相当，以显著优势超越 GPT4TS，在 SMAPE、MRAE 和 MAPE 方面分别实现了超过 **23%**、**35%** 和 **26%** 的减少。

- 表 13：M3（Quarterly）上的额外短期时间序列预测结果。预测范围为 8。较低的值表示更好的性能。红色：最佳，蓝色：次佳。

# E 少样本和零样本预测

## E.1 少样本预测

我们在少样本预测任务中的完整结果详见表 14 和表 15。在 $10\%$ 少样本学习的范围内，Time-LLM 在七个不同时间序列基准的 35 个案例中的 32 个取得了 SOTA 性能。在 5\% 少样本场景中，我们的方法的优势更加明显，在 32 个案例中的 21 个取得了 SOTA 结果。我们将其归因于我们重新编程的 LLM 中成功的知识激活。

- 表 14：在 $10\%$ 训练数据上的完整少样本学习结果。我们使用与表 1 相同的协议。

- 表 15：在 $5\%$ 训练数据上的完整少样本学习结果。我们使用与表 1 相同的协议。“-”表示 5\% 的时间序列不足以构成训练集。

## E.2 零样本预测

零样本预测的完整结果总结在表 16 中。Time-LLM 显著超越了六个最具竞争力的时间序列模型在零样本适应中的表现。总体而言，我们观察到在所有基线中平均 MSE 和 MAE 分别减少了 **23.5\%** 和 **12.4\%**。在典型的跨领域场景中（例如 ETTh2 $\rightarrow$ ETTh1 和 ETTm $2 \rightarrow$ ETTm1），我们的改进始终显著，平均 MSE 和 MAE 分别超过 **20.8\%** 和 **11.3\%**。值得注意的是，与 LLMTime（Gruver 等，2023）相比，Time-LLM 表现出更优越的性能提升，LLMTime 采用了类似大小的主干 LLM（7B），并且是最近利用 LLMs 进行零样本时间序列预测的努力。我们将这一成功归因于我们的重新编程框架在执行时间序列任务时，能够以资源高效的方式更好地激活 LLM 的知识转移和推理能力。

- 表 16：在 ETT 数据集上的完整零样本学习结果。较低的值表示更好的性能。红色：最佳，蓝色：次佳。

# F 消融研究

完整的消融结果在表 17 中。我们进一步比较了在重新编程和微调（使用 QLoRA Dettmers 等，2023）协议下的模型性能。我们的结果表明，与 QLoRA 变体（A.5）相比，我们的方法平均性能提升了 **19\%**。

- 表 17：在 ETTh1 和 ETTm1 上预测 96 和 192 步的完整消融结果（报告 MSE）。

# G 与模型微调的效率比较

**设置**。我们在本节中比较了模型微调（使用 QLoRA Dettmers 等，2023）和我们提出的模型重新编程的效率，使用两种不同的主干，即 $1/4$ 容量的 Llama（前 8 个 Transformer 层）和全容量。在这里，我们遵循 ETTh1 上的长期预测协议，预测两个不同的步长（即本例中的 96 和 336）。对于评估指标，我们报告了可训练参数的总数（以百万计）、GPU 内存（以 mebibyte 计）和运行时间（每次迭代的秒数）。

---

**结果**。我们的结果在表 18 中给出。我们看到，与使用 QLoRA 的参数高效微调（PEFT）相比，模型重新编程在长期预测任务中显著提高了效率，无论是在可训练参数总数、GPU 内存开销还是训练速度方面。定量来看，在四种场景中，平均可训练参数减少了 **71.2\%**，内存消耗减少了 **23.1\%**，训练速度加快了 **25.3\%**。

- 表 18：在 ETTh1 数据集上预测两个不同步长时，模型重新编程与使用 QLoRA（Dettmers 等，2023）的参数高效微调（PEFT）的效率比较。

# H 误差条

所有实验均进行了三次，我们在此展示了我们的模型和亚军模型的标准偏差。我们在长期预测任务中，将我们的方法与第二佳方法 PatchTST（Nie 等，2023）的比较详见表 19。在该表中，报告了四个 ETT 数据集上的平均 MSE 和 MAE，并附有标准偏差。此外，表 20 对比了我们的方法与第二佳方法 N-HiTS（Challu 等，2023a）的有效性，使用不同的 M4 数据集进行比较。

- 表 19：我们的方法和第二佳方法（PatchTST）在所有时间序列数据集上的长期预测标准偏差。

- 表 20：我们的 Time-LLM 和第二佳方法（N-HiTS）在 M4 数据集上的短期预测标准偏差。

# I 可视化

在本部分中，我们可视化了 Time-LLM 的预测结果，并与最先进和代表性的方法（例如 GPT4TS（Zhou 等，2023a）、PatchTST（Nie 等，2023）和 Autoformer（Wu 等，2021））在各种场景中的表现进行了比较，以展示 Time-LLM 的卓越性能。

---

在图 7 和图 8 中，比较了各种方法的长期（输入-96-预测-96）和短期（输入-36-预测-36）预测结果与真实值。在这里，Time-LLM 展示了比 GPT4TS、PatchTST 和经典的基于 Transformer 的方法 Autoformer 显著更高的预测准确性。

- 图 7：不同模型在 ETTh1 上的长期预测案例，采用输入-96-预测-96 设置。蓝线为真实值，橙线为模型预测值。

- 图 8：不同模型在 M4 数据集上的短期预测案例，采用输入-36-预测-18 设置。

---

我们还提供了少样本和零样本场景中预测结果的可视化比较，如图 9 和图 10 所示。在这两种情况下，我们均遵循长期（输入-96-预测-96）预测设置。Time-LLM 在数据有限的情况下表现出显著的预测优势——这一事实在与 GPT4TS 的比较中尤为突出。

- 图 9：不同模型在 ETTm1 上的少样本预测案例，采用输入-96-预测-96 设置。蓝线为真实值，橙线为模型预测值。

- 图 10：不同模型在 ETTh $1 \rightarrow$ ETTh2 上的零样本预测案例，采用输入-96-预测-96 设置。蓝线为真实值，橙线为模型预测值。
