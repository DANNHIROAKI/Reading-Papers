# TIME-LLM: TIME SERIES FORECASTING BY REPROGRAMMING LARGE LANGUAGE MODELS

# TIME-LLM：通过重新编程大语言模型进行时间序列预测

Ming Jin ${}^{1}$ ; Shiyu Wang ${}^{2}$ ; Lintao ${\mathrm{{Ma}}}^{2}$ ,Zhixuan ${\mathrm{{Chu}}}^{2}$ ,James Y. Zhang ${}^{2}$ ,Xiaoming ${\mathrm{{Shi}}}^{2}$ , Pin-Yu Chen ${}^{3}$ ,Yuxuan Liang ${}^{6}$ ,Yuan-Fang Li ${}^{1}$ ,Shirui Pan ${}^{4}$ ; Qingsong Wen ${}^{5 \dagger  }$

金明 ${}^{1}$；王诗雨 ${}^{2}$；林涛 ${\mathrm{{Ma}}}^{2}$，智轩 ${\mathrm{{Chu}}}^{2}$，詹姆斯·Y·张 ${}^{2}$，小明 ${\mathrm{{Shi}}}^{2}$，陈品宇 ${}^{3}$，梁宇轩 ${}^{6}$，李元芳 ${}^{1}$，潘世瑞 ${}^{4}$；文青松 ${}^{5 \dagger  }$

${}^{1}$ Monash University ${}^{2}$ Ant Group ${}^{3}$ IBM Research ${}^{4}$ Griffith University ${}^{5}$ Alibaba Group ${}^{6}$ The Hong Kong University of Science and Technology (Guangzhou)

${}^{1}$ 莫纳什大学（Monash University） ${}^{2}$ 蚂蚁集团（Ant Group） ${}^{3}$ IBM研究院（IBM Research） ${}^{4}$ 格里菲斯大学（Griffith University） ${}^{5}$ 阿里巴巴集团（Alibaba Group） ${}^{6}$ 香港科技大学（广州）（The Hong Kong University of Science and Technology (Guangzhou)）

\{ming.jin, yuanfang.li\}@monash.edu, pin-yu.chen@ibm.com

\{ming.jin, yuanfang.li\}@monash.edu, pin-yu.chen@ibm.com

yuxliang@outlook.com, s.pan@griffith.edu.au, qingsongedu@gmail.com

于亮（yuxliang）@outlook.com，潘先生（s.pan）@格里菲斯大学（griffith.edu.au），青松教育（qingsongedu）@gmail.com

\{weiming.wsy, lintao.mlt, chuzhixuan.czx, james.z, peter.sxm\}@antgroup.com

{魏明（weiming.wsy），林涛（lintao.mlt），褚志轩（chuzhixuan.czx），詹姆斯（james.z），彼得（peter.sxm）}@蚂蚁集团（antgroup.com）

## Abstract

## 摘要

Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present TIME-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that TIME-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, TIME-LLM excels in both few-shot and zero-shot learning scenarios. The code is made available at https://github.com/KimMeen/Time-LLM

时间序列预测在许多现实世界的动态系统中具有重要意义，并且已经得到了广泛的研究。与自然语言处理（NLP）和计算机视觉（CV）不同，在自然语言处理和计算机视觉领域，一个单一的大模型可以处理多个任务，而时间序列预测模型通常是专门化的，需要针对不同的任务和应用进行独特的设计。虽然预训练基础模型在自然语言处理和计算机视觉领域取得了令人瞩目的进展，但它们在时间序列领域的发展受到了数据稀疏性的限制。最近的研究表明，大语言模型（LLM）在处理复杂的标记序列时具有强大的模式识别和推理能力。然而，如何有效地对齐时间序列数据和自然语言的模态以利用这些能力仍然是一个挑战。在这项工作中，我们提出了TIME - LLM，这是一个重新编程框架，用于在保持基础语言模型不变的情况下，将大语言模型重新用于通用时间序列预测。我们首先用文本原型对输入的时间序列进行重新编程，然后将其输入到冻结的大语言模型中，以对齐这两种模态。为了增强大语言模型对时间序列数据进行推理的能力，我们提出了“提示即前缀”（PaP）方法，该方法丰富了输入上下文，并指导重新编程后的输入块的转换。最后，对大语言模型输出的转换后的时间序列块进行投影以获得预测结果。我们的综合评估表明，TIME - LLM是一个强大的时间序列学习器，其性能优于最先进的专门预测模型。此外，TIME - LLM在少样本和零样本学习场景中也表现出色。代码可在https://github.com/KimMeen/Time - LLM获取

## 1 INTRODUCTION

## 1 引言

Time series forecasting is a critical capability across many real-world dynamic systems (Jin et al., 2023a), with applications ranging from demand planning (Leonard, 2001) and inventory optimization (Li et al., 2022) to energy load forecasting (Liu et al., 2023a) and climate modeling (Schneider & Dickinson, 1974). Each time series forecasting task typically requires extensive domain expertise and task-specific model designs. This stands in stark contrast to foundation language models like GPT-3 (Brown et al. 2020), GPT-4 (OpenAI, 2023), Llama (Touvron et al., 2023), inter alia, which can perform well on a diverse range of NLP tasks in a few-shot or even zero-shot setting.

时间序列预测是许多现实世界动态系统中的一项关键能力（Jin等人，2023a），其应用范围广泛，涵盖从需求规划（Leonard，2001）和库存优化（Li等人，2022）到能源负荷预测（Liu等人，2023a）和气候建模（Schneider & Dickinson，1974）等领域。每个时间序列预测任务通常都需要大量的领域专业知识和特定于任务的模型设计。这与像GPT - 3（Brown等人，2020）、GPT - 4（OpenAI，2023）、Llama（Touvron等人，2023）等基础语言模型形成了鲜明对比，这些基础语言模型能够在少样本甚至零样本的情况下，在各种自然语言处理（NLP）任务中表现出色。

Pre-trained foundation models, such as large language models (LLMs), have driven rapid progress in computer vision (CV) and natural language processing (NLP). While time series modeling has not benefited from the same significant breakthroughs, LLMs' impressive capabilities have inspired their application to time series forecasting (Jin et al. 2023b). Several desiderata exist for leveraging LLMs to advance forecasting techniques: Generalizability. LLMs have demonstrated a remarkable capability for few-shot and zero-shot transfer learning (Brown et al. 2020). This suggests their potential for generalizable forecasting across domains without requiring per-task retraining from scratch. In contrast, current forecasting methods are often rigidly specialized by domain. Data efficiency. By leveraging pre-trained knowledge, LLMs have shown the ability to perform new tasks with only a few examples. This data efficiency could enable forecasting for settings where historical data is limited. In contrast, current methods typically require abundant in-domain data. Reasoning. LLMs exhibit sophisticated reasoning and pattern recognition capabilities (Mirchandani et al., 2023; Wang et al., 2023; Chu et al., 2023). Harnessing these skills could allow making highly precise forecasts by leveraging learned higher-level concepts. Existing non-LLM methods are largely statistical without much innate reasoning. Multimodal knowledge. As LLM architectures and training techniques improve, they gain more diverse knowledge across modalities like vision, speech, and text (Ma et al. 2023). Tapping into this knowledge could enable synergistic forecasting that fuses different data types. Conventional tools lack ways to jointly leverage multiple knowledge bases. Easy optimization. LLMs are trained once on massive computing and then can be applied to forecasting tasks without learning from scratch. Optimizing existing forecasting models often requires significant architecture search and hyperparameter tuning (Zhou et al. 2023b). In summary, LLMs offer a promising path to make time series forecasting more general, efficient, synergistic, and accessible compared to current specialized modeling paradigms. Thus, adapting these powerful models for time series data can unlock significant untapped potential.

预训练基础模型，如大语言模型（LLMs），推动了计算机视觉（CV）和自然语言处理（NLP）的快速发展。虽然时间序列建模尚未从同样重大的突破中受益，但大语言模型令人印象深刻的能力激发了其在时间序列预测中的应用（Jin等人，2023b）。利用大语言模型推进预测技术存在几个理想目标：通用性。大语言模型已展现出强大的少样本和零样本迁移学习能力（Brown等人，2020）。这表明它们有潜力在无需针对每个任务从头重新训练的情况下，实现跨领域的通用预测。相比之下，当前的预测方法通常按领域进行严格的专门化。数据效率。通过利用预训练知识，大语言模型已显示出仅用少量示例就能执行新任务的能力。这种数据效率可以在历史数据有限的情况下进行预测。相比之下，当前的方法通常需要大量的领域内数据。推理能力。大语言模型展现出复杂的推理和模式识别能力（Mirchandani等人，2023；Wang等人，2023；Chu等人，2023）。利用这些技能可以通过利用所学的更高级概念进行高精度的预测。现有的非大语言模型方法大多是统计性的，缺乏内在的推理能力。多模态知识。随着大语言模型架构和训练技术的改进，它们在视觉、语音和文本等多种模态上获得了更丰富多样的知识（Ma等人，2023）。利用这些知识可以实现融合不同数据类型的协同预测。传统工具缺乏联合利用多个知识库的方法。易于优化。大语言模型在大规模计算上进行一次训练后，就可以应用于预测任务，而无需从头学习。优化现有的预测模型通常需要大量的架构搜索和超参数调整（Zhou等人，2023b）。总之，与当前的专门化建模范式相比，大语言模型为使时间序列预测更具通用性、高效性、协同性和可及性提供了一条有前景的途径。因此，将这些强大的模型应用于时间序列数据可以释放巨大的未开发潜力。

---

<!-- Footnote -->

*Equal Contribution

*同等贡献

${}^{ \dagger  }$ Corresponding Authors

${}^{ \dagger  }$ 通讯作者

<!-- Footnote -->

---

The realization of the above benefits hinges on the effective alignment of the modalities of time series data and natural language. However, this is a challenging task as LLMs operate on discrete tokens, while time series data is inherently continuous. Furthermore, the knowledge and reasoning capabilities to interpret time series patterns are not naturally present within LLMs' pre-training. Therefore, it remains an open challenge to unlock the knowledge within LLMs in activating their ability for general time series forecasting in a way that is accurate, data-efficient, and task-agnostic.

上述优势的实现取决于时间序列数据和自然语言模态的有效对齐。然而，这是一项具有挑战性的任务，因为大语言模型（LLMs）处理的是离散的标记，而时间序列数据本质上是连续的。此外，大语言模型（LLMs）的预训练中并不天然具备解释时间序列模式的知识和推理能力。因此，以一种准确、数据高效且与任务无关的方式挖掘大语言模型（LLMs）中的知识，以激活其进行通用时间序列预测的能力，仍然是一个有待解决的挑战。

In this work, we propose TIME-LLM, a reprogramming framework to adapt large language models for time series forecasting while keeping the backbone model intact. The core idea is to reprogram the input time series into text prototype representations that are more naturally suited to language models' capabilities. To further augment the model's reasoning about time series concepts, we introduce Prompt-as-Prefix (PaP), a novel idea in enriching the input time series with additional context and providing task instructions in the modality of natural language. This provides declarative guidance about desired transformations to apply to the reprogrammed input. The output of the language model is then projected to generate time series forecasts. Our comprehensive evaluation demonstrates that large language models can act as effective few-shot and zero-shot time series learners when adopted through this reprogramming approach, outperforming specialized forecasting models. By leveraging LLMs' reasoning capability while keeping the models intact, our work points the way toward multimodal foundation models that can excel on both language and sequential data tasks. Our proposed reprogramming framework offers an extensible paradigm for imbuing large models with new capabilities beyond their original pre-training. Our main contributions in this work can be summarized as follows:

在这项工作中，我们提出了TIME - LLM，这是一个重新编程框架，用于在保持主干模型完整的同时，使大语言模型适用于时间序列预测。其核心思想是将输入的时间序列重新编程为更自然地适合语言模型能力的文本原型表示。为了进一步增强模型对时间序列概念的推理能力，我们引入了“提示前缀法”（Prompt - as - Prefix，PaP），这是一种新颖的方法，用于用额外的上下文丰富输入的时间序列，并以自然语言的形式提供任务指令。这为应用于重新编程输入的所需转换提供了声明性指导。然后对语言模型的输出进行投影以生成时间序列预测。我们的综合评估表明，当通过这种重新编程方法采用大语言模型时，它们可以作为有效的少样本和零样本时间序列学习者，性能优于专门的预测模型。通过在保持模型完整的同时利用大语言模型的推理能力，我们的工作为能够在语言和序列数据任务上都表现出色的多模态基础模型指明了方向。我们提出的重新编程框架为赋予大模型超出其原始预训练的新能力提供了一个可扩展的范式。我们在这项工作中的主要贡献可总结如下：

- We introduce a novel concept of reprogramming large language models for time series forecasting without altering the pre-trained backbone model. In doing so, we show that forecasting can be cast as yet another "language" task that can be effectively tackled by an off-the-shelf LLM.

- 我们引入了一种在不改变预训练主干模型的情况下对大语言模型进行重新编程以用于时间序列预测的全新概念。通过这样做，我们表明预测可以被视为另一种能够由现成的大语言模型（LLM）有效处理的“语言”任务。

- We propose a new framework, TIME-LLM, which encompasses reprogramming the input time series into text prototype representations that are more natural for the LLM, and augmenting the input context with declarative prompts (e.g., domain expert knowledge and task instructions) to guide LLM reasoning. Our technique points towards multimodal foundation models excelling in both language and time series.

- 我们提出了一个新的框架，即TIME - LLM，该框架包括将输入的时间序列重新编程为对大语言模型来说更自然的文本原型表示，并使用声明性提示（例如，领域专家知识和任务指令）来增强输入上下文，以引导大语言模型进行推理。我们的技术指向了在语言和时间序列方面都表现出色的多模态基础模型。

- TIME-LLM consistently exceeds state-of-the-art performance in mainstream forecasting tasks, especially in few-shot and zero-shot scenarios. Moreover, this superior performance is achieved while maintaining excellent model reprogramming efficiency. Thus, our research is a concrete step in unleashing LLMs' untapped potential for time series and perhaps other sequential data.

- TIME - LLM在主流预测任务中始终超越了现有最先进的性能，尤其是在少样本和零样本场景中。此外，在实现这种卓越性能的同时，还保持了出色的模型重新编程效率。因此，我们的研究是挖掘大语言模型在时间序列以及可能的其他序列数据方面未开发潜力的具体一步。

## 2 RELATED WORK

## 2 相关工作

Task-specific Learning. Most time series forecasting models are crafted for specific tasks and domains (e.g., traffic prediction), and trained end-to-end on small-scale data. An illustration is in

特定任务学习。大多数时间序列预测模型是为特定任务和领域（例如，交通预测）而设计的，并在小规模数据上进行端到端训练。一个例子是

<!-- Media -->

<!-- figureText: (a) Task-Specific (b) Model Fine-Tuning (c) Model Reprogramming Source Modality Learning A Model 0000000 O Source Data Sample O Target Data Sample ☐ Source Task 例 Target Task 2018 -->

<img src="https://cdn.noedgeai.com/01957f5d-9de5-7380-ae8c-08021ee888f7_2.jpg?x=312&y=223&w=1173&h=313&r=0"/>

Figure 1: Schematic illustration of reprogramming large language models (LLMs) in comparison of (a) task-specific learning and (b) model fine-tuning. Our proposal investigates and demonstrates (c) how to effectively reprogram open-sourced LLMs as powerful time series learners where well-developed time series pre-trained models are not readily available.

图1：与（a）特定任务学习和（b）模型微调相比，对大语言模型（LLMs）进行重新编程的示意图。我们的提议研究并展示了（c）在缺乏完善的时间序列预训练模型的情况下，如何有效地将开源大语言模型重新编程为强大的时间序列学习器。

<!-- Media -->

Fig. 1(a). For example, ARIMA models are designed for univariate time series forecasting (Box et al. 2015), LSTM networks are tailored for sequence modeling (Hochreiter & Schmidhuber, 1997), and temporal convolutional networks (Bai et al. 2018) and transformers (Wen et al. 2023) are developed for handling longer temporal dependencies. While achieving good performance on narrow tasks, these models lack versatility and generalizability to diverse time series data.

图1（a）。例如，自回归积分滑动平均模型（ARIMA）是为单变量时间序列预测而设计的（Box等人，2015年），长短期记忆网络（LSTM）是为序列建模量身定制的（Hochreiter和Schmidhuber，1997年），时间卷积网络（Bai等人，2018年）和Transformer模型（Wen等人，2023年）则是为处理更长的时间依赖关系而开发的。虽然这些模型在特定任务上取得了良好的性能，但它们缺乏对不同时间序列数据的通用性和泛化能力。

In-modality Adaptation. Relevant research in CV and NLP has demonstrated the effectiveness of pre-trained models that can be fine-tuned for various downstream tasks without the need for costly training from scratch (Devlin et al. 2018; Brown et al. 2020; Touvron et al. 2023). Inspired by these successes, recent studies have focused on the development of time series pre-trained models (TSPTMs). The first step among them involves time series pre-training using different strategies like supervised (Fawaz et al., 2018) or self-supervised learning (Zhang et al., 2022b; Deldari et al., 2022) Zhang et al. 2023). This allows the model to learn representing various input time series. Once pre-trained, it can be fine-tuned on similar domains to learn how to perform specific tasks (Tang et al., 2022). An example is in Fig. 1(b). The development of TSPTMs leverages the success of pretraining and fine-tuning in NLP and CV but remains limited on smaller scales due to data sparsity.

模态内自适应。计算机视觉（CV）和自然语言处理（NLP）领域的相关研究表明，预训练模型具有有效性，这些模型可以针对各种下游任务进行微调，而无需从头进行代价高昂的训练（德夫林等人，2018年；布朗等人，2020年；图弗龙等人，2023年）。受这些成功案例的启发，近期的研究聚焦于时间序列预训练模型（TSPTMs）的开发。其中的第一步是使用不同的策略进行时间序列预训练，如监督学习（法瓦兹等人，2018年）或自监督学习（张等人，2022b；德尔达里等人，2022年；张等人，2023年）。这使模型能够学习表示各种输入时间序列。一旦完成预训练，就可以在相似领域对其进行微调，以学习如何执行特定任务（唐等人，2022年）。图1（b）给出了一个示例。时间序列预训练模型的开发借鉴了自然语言处理和计算机视觉领域中预训练和微调的成功经验，但由于数据稀疏性，在较小规模上的应用仍受到限制。

Cross-modality Adaptation. Building on in-modality adaptation, recent work has further explored transferring knowledge from powerful pre-trained foundations models in NLP and CV to time series modeling, through techniques such as multimodal fine-tuning (Yin et al. 2023) and model reprogramming (Chen, 2022). Our approach aligns with this category; however, there is limited pertinent research available on time series. An example is Voice2Series (Yang et al. 2021), which adapts an acoustic model (AM) from speech recognition to time series classification by editing a time series into a format suitable for the AM. Recently, Chang et al. (2023) proposes LLM4TS for time series forecasting using LLMs. It designs a two-stage fine-tuning process on the LLM-first supervised pre-training on time series, then task-specific fine-tuning. Zhou et al. (2023a) leverages pre-trained language models without altering their self-attention and feedforward layers. This model is fine-tuned and evaluated on various time series analysis tasks and demonstrates comparable or state-of-the-art performance by transferring knowledge from natural language pre-training. Distinct from these approach, we neither edit the input time series directly nor fine-tune the backbone LLM. Instead, as illustrated in Fig. 1(c), we propose reprogramming time series with the source data modality along with prompting to unleash the potential of LLMs as effective time series machines.

跨模态适应。在模态内适应的基础上，近期的研究进一步探索了通过多模态微调（Yin等人，2023年）和模型重编程（Chen，2022年）等技术，将自然语言处理（NLP）和计算机视觉（CV）领域强大的预训练基础模型的知识迁移到时间序列建模中。我们的方法属于这一类别；然而，关于时间序列的相关研究有限。例如Voice2Series（Yang等人，2021年），它通过将时间序列编辑成适合声学模型（AM）的格式，将语音识别中的声学模型（AM）应用于时间序列分类。最近，Chang等人（2023年）提出了使用大语言模型（LLM）进行时间序列预测的LLM4TS。它设计了一个在大语言模型上的两阶段微调过程——首先在时间序列上进行有监督的预训练，然后进行特定任务的微调。Zhou等人（2023a）利用预训练语言模型，而不改变其自注意力和前馈层。该模型在各种时间序列分析任务上进行了微调并评估，通过迁移自然语言预训练的知识，展现出了相当或最先进的性能。与这些方法不同，我们既不直接编辑输入的时间序列，也不对骨干大语言模型进行微调。相反，如图1（c）所示，我们提出结合源数据模态对时间序列进行重编程，并使用提示技术，以释放大语言模型作为有效时间序列处理工具的潜力。

## 3 METHODOLOGY

## 3 研究方法

Our model architecture is depicted in Fig. 2. We focus on reprogramming an embedding-visible language foundation model, such as Llama (Touvron et al. 2023) and GPT-2 (Radford et al. 2019), for general time series forecasting without requiring any fine-tuning of the backbone model. Specifically,we consider the following problem: given a sequence of historical observations $\mathbf{X} \in  {\mathbb{R}}^{N \times  T}$ consisting of $N$ different 1-dimensional variables across $T$ time steps,we aim to reprogram a large language model $f\left( \cdot \right)$ to understand the input time series and accurately forecast the readings at $H$ future time steps,denoted by $\widehat{\mathbf{Y}} \in  {\mathbb{R}}^{N \times  H}$ ,with the overall objective to minimize the mean square errors between the ground truths $\mathbf{Y}$ and predictions,i.e., $\frac{1}{H}\mathop{\sum }\limits_{{h = 1}}^{H}{\begin{Vmatrix}{\widehat{\mathbf{Y}}}_{h} - {\mathbf{Y}}_{h}\end{Vmatrix}}_{F}^{2}$ .

我们的模型架构如图2所示。我们专注于对嵌入可见的语言基础模型（如Llama（图夫龙等人，2023年）和GPT - 2（拉德福德等人，2019年））进行重新编程，以用于通用时间序列预测，而无需对主干模型进行任何微调。具体而言，我们考虑以下问题：给定一个由跨越$T$个时间步长的$N$个不同一维变量组成的历史观测序列$\mathbf{X} \in  {\mathbb{R}}^{N \times  T}$，我们的目标是对大型语言模型$f\left( \cdot \right)$进行重新编程，使其理解输入的时间序列，并准确预测未来$H$个时间步长的读数（用$\widehat{\mathbf{Y}} \in  {\mathbb{R}}^{N \times  H}$表示），总体目标是最小化真实值$\mathbf{Y}$和预测值之间的均方误差，即$\frac{1}{H}\mathop{\sum }\limits_{{h = 1}}^{H}{\begin{Vmatrix}{\widehat{\mathbf{Y}}}_{h} - {\mathbf{Y}}_{h}\end{Vmatrix}}_{F}^{2}$。

Our method encompasses three main components: (1) input transformation, (2) a pre-trained and frozen LLM,and (3) output projection. Initially,a multivariate time series is partitioned into $N$

我们的方法包含三个主要组件：（1）输入转换，（2）预训练且固定参数的大语言模型（LLM），以及（3）输出投影。最初，一个多变量时间序列被划分为 $N$

<!-- Media -->

<!-- figureText: Forecasts Add & Layer Norm Feed Forward Add & Layer Norm Multi-Head Attention Input Embeddings Reprogrammed Patch Embeddings Patch Reprogram Patching ① ③ Multi-Head Attention ② Patch Text Prototypes Embedder ↑ Time Series Pre-trained Patches Word Embeddings Arr. A Output Projection Flatten & Linear ↑↓ Output Patch Embeddings Pre-trained LLM (Body) Output Token Embeddings ↑ Token Embedder ↑ (Embedder) Tokenization ↑ Input Text ###Instruction: <task information> <time series statistic 1> -->

<img src="https://cdn.noedgeai.com/01957f5d-9de5-7380-ae8c-08021ee888f7_3.jpg?x=334&y=231&w=1129&h=685&r=0"/>

Figure 2: The model framework of TIME-LLM. Given an input time series, we first tokenize and embed it via ① patching along with a ② customized embedding layer. ③ These patch embeddings are then reprogrammed with condensed text prototypes to align two modalities. To augment the LLM's reasoning ability, ④ additional prompt prefixes are added to the input to direct the transformation of input patches. ⑤ The output patches from the LLM are projected to generate the forecasts.

图2：TIME - LLM的模型框架。给定一个输入时间序列，我们首先通过①分块操作以及②定制的嵌入层对其进行分词和嵌入。③然后，这些分块嵌入会用浓缩的文本原型进行重新编程，以使两种模态对齐。为了增强大语言模型的推理能力，④会在输入中添加额外的提示前缀，以引导输入分块的转换。⑤大语言模型的输出分块会经过投影以生成预测结果。

<!-- Media -->

univariate time series,which are subsequently processed independently (Nie et al. 2023). The $i$ -th series is denoted as ${\mathbf{X}}^{\left( i\right) } \in  {\mathbb{R}}^{1 \times  T}$ ,which undergoes normalization,patching,and embedding prior to being reprogrammed with learned text prototypes to align the source and target modalities. Then, we augment the LLM's time series reasoning ability by prompting it together with reprogrammed patches to generate output representations,which are projected to the final forecasts ${\widehat{\mathbf{Y}}}^{\left( i\right) } \in  {\mathbb{R}}^{1 \times  H}$ .

单变量时间序列，随后对其进行独立处理（聂等人，2023年）。第$i$个序列表示为${\mathbf{X}}^{\left( i\right) } \in  {\mathbb{R}}^{1 \times  T}$，在使用学习到的文本原型进行重新编程以对齐源模态和目标模态之前，该序列要经过归一化、分块和嵌入处理。然后，我们通过将重新编程后的分块与大语言模型（LLM）一起进行提示，以增强其时间序列推理能力，从而生成输出表示，这些表示会被映射到最终预测结果${\widehat{\mathbf{Y}}}^{\left( i\right) } \in  {\mathbb{R}}^{1 \times  H}$。

We note that only the parameters of the lightweight input transformation and output projection are updated, while the backbone language model is frozen. In contrast to vision-language and other multimodal language models, which usually fine-tune with paired cross-modality data, TIME-LLM is directly optimized and becomes readily available with only a small set of time series and a few training epochs, maintaining high efficiency and imposing fewer resource constraints compared to building large domain-specific models from scratch or fine-tuning them. To further reduce memory footprints, various off-the-shelf techniques (e.g., quantization) can be seamlessly integrated for slimming TIME-LLM.

我们注意到，仅更新轻量级输入变换和输出投影的参数，而主干语言模型保持冻结。与通常使用配对跨模态数据进行微调的视觉语言和其他多模态语言模型不同，TIME-LLM（时间大语言模型）仅需少量时间序列和几个训练周期即可直接优化并投入使用，与从头构建大型特定领域模型或对其进行微调相比，它保持了较高的效率，并且对资源的限制更少。为了进一步减少内存占用，可以无缝集成各种现成的技术（例如，量化）来精简TIME-LLM。

### 3.1 MODEL STRUCTURE

### 3.1 模型结构

Input Embedding. Each input channel ${\mathbf{X}}^{\left( i\right) }$ is first individually normalized to have zero mean and unit standard deviation via reversible instance normalization (RevIN) in mitigating the time series distribution shift (Kim et al. 2021). Then,we divide ${\mathbf{X}}^{\left( i\right) }$ into several consecutive overlapped or non-overlapped patches (Nie et al. 2023) with length ${L}_{p}$ ; thus the total number of input patches is $P = \left\lfloor  \frac{\left( T - {L}_{p}\right) }{S}\right\rfloor   + 2$ ,where $S$ denotes the horizontal sliding stride. The underlying motivations are two-fold: (1) better preserving local semantic information by aggregating local information into each patch and (2) serving as tokenization to form a compact sequence of input tokens, reducing computational burdens. Given these patches ${\mathbf{X}}_{P}^{\left( i\right) } \in  {\mathbb{R}}^{P \times  {L}_{p}}$ ,we embed them as ${\widehat{\mathbf{X}}}_{P}^{\left( i\right) } \in  {\mathbb{R}}^{P \times  {d}_{m}}$ , adopting a simple linear layer as the patch embedder to create dimensions ${d}_{m}$ .

输入嵌入。每个输入通道${\mathbf{X}}^{\left( i\right) }$首先通过可逆实例归一化（RevIN）单独归一化为均值为零、标准差为1，以缓解时间序列分布偏移问题（Kim等人，2021年）。然后，我们将${\mathbf{X}}^{\left( i\right) }$划分为几个连续的重叠或非重叠的块（Nie等人，2023年），块的长度为${L}_{p}$；因此，输入块的总数为$P = \left\lfloor  \frac{\left( T - {L}_{p}\right) }{S}\right\rfloor   + 2$，其中$S$表示水平滑动步长。这样做的潜在动机有两个方面：（1）通过将局部信息聚合到每个块中，更好地保留局部语义信息；（2）作为分词操作，形成一个紧凑的输入标记序列，减轻计算负担。给定这些块${\mathbf{X}}_{P}^{\left( i\right) } \in  {\mathbb{R}}^{P \times  {L}_{p}}$，我们将它们嵌入为${\widehat{\mathbf{X}}}_{P}^{\left( i\right) } \in  {\mathbb{R}}^{P \times  {d}_{m}}$，采用一个简单的线性层作为块嵌入器来创建维度${d}_{m}$。

Patch Reprogramming. Here we reprogram patch embeddings into the source data representation space to align the modalities of time series and natural language to activate the backbone's time series understanding and reasoning capabilities. A common practice is learning a form of "noise" that, when applied to target input samples, allows the pre-trained source model to produce the desired target outputs without requiring parameter updates. This is technically feasible for bridging data

补丁重编程。在此，我们将补丁嵌入重编程到源数据表示空间，以对齐时间序列和自然语言的模态，从而激活主干网络对时间序列的理解和推理能力。一种常见的做法是学习一种“噪声”形式，当将其应用于目标输入样本时，可使预训练的源模型在无需更新参数的情况下产生所需的目标输出。从技术上讲，这对于弥合相同或相似的数据模态是可行的。

<!-- Media -->

<!-- figureText: Source Target 0.6 Projection Pre-trained LLM Pre-trained LLM 台阶小 Pre-trained LLM Patch (Text Embedder) (Text Embedder) Reprogram (b) Patch-as-Prefix and Prompt-as-Prefix Vocab. Prototypes Reprogrammed Patch Embeddings $O$ Patch 2 Patch Patch 5 Reprogram (a) Patch Reprogramming -->

<img src="https://cdn.noedgeai.com/01957f5d-9de5-7380-ae8c-08021ee888f7_4.jpg?x=319&y=228&w=1163&h=395&r=0"/>

Figure 3: Illustration of (a) patch reprogramming and (b) Patch-as-Prefix versus Prompt-as-Prefix.

图3：（a）补丁重编程和（b）“补丁作为前缀”与“提示作为前缀”的示意图。

<!-- Media -->

modalities that are identical or similar. Examples include repurposing a vision model to work with cross-domain images (Misra et al. 2023) or reprogramming an acoustic model to handle time series data (Yang et al. 2021). In both cases, there are explicit, learnable transformations between the source and target data, allowing for the direct editing of input samples. However, time series can neither be directly edited nor described losslessly in natural language, posing significant challenges to directly bootstrap the LLM for understanding time series without resource-intensive fine-tuning.

对于相同或相似的数据模态，这种方法是可行的。例如，将视觉模型重新用于跨领域图像（米斯拉等人，2023年），或者对声学模型进行重编程以处理时间序列数据（杨等人，2021年）。在这两种情况下，源数据和目标数据之间存在明确的、可学习的转换，从而可以直接编辑输入样本。然而，时间序列既不能直接编辑，也不能用自然语言进行无损描述，这给在不进行资源密集型微调的情况下直接引导大语言模型（LLM）理解时间序列带来了重大挑战。

To close this gap,we propose reprogramming ${\widehat{\mathbf{X}}}_{P}^{\left( i\right) }$ using pre-trained word embeddings $\mathbf{E} \in  {\mathbb{R}}^{V \times  D}$ in the backbone,where $V$ is the vocabulary size. Nevertheless,there is no prior knowledge indicating which source tokens are directly relevant. Thus,simply leveraging $\mathbf{E}$ will result in large and potentially dense reprogramming space. A simple solution is to maintain a small collection of text prototypes by linearly probing $\mathbf{E}$ ,denoted as ${\mathbf{E}}^{\prime } \in  {\mathbb{R}}^{{V}^{\prime } \times  D}$ ,where ${V}^{\prime } \ll  V$ . An illustration is in Fig. 3(a). Text prototypes learn connecting language cues, e.g., "short up" (red lines) and "steady down" (blue lines), which are then combined to represent the local patch information (e.g., "short up then down steadily" for characterizing patch 5) without leaving the space where the language model is pre-trained. This approach is efficient and allows for the adaptive selection of relevant source information. To realize this, we employ a multi-head cross-attention layer. Specifically, for each head $k = \{ 1,\cdots ,K\}$ ,we define query matrices ${\mathbf{Q}}_{k}^{\left( i\right) } = {\widehat{\mathbf{X}}}_{P}^{\left( i\right) }{\mathbf{W}}_{k}^{Q}$ ,key matrices ${\mathbf{K}}_{k}^{\left( i\right) } = {\mathbf{E}}^{\prime }{\mathbf{W}}_{k}^{K}$ , and value matrices ${\mathbf{V}}_{k}^{\left( i\right) } = {\mathbf{E}}^{\prime }{\mathbf{W}}_{k}^{V}$ ,where ${\mathbf{W}}_{k}^{Q} \in  {\mathbb{R}}^{{d}_{m} \times  d}$ and ${\mathbf{W}}_{k}^{K},{\mathbf{W}}_{k}^{V} \in  {\mathbb{R}}^{D \times  d}$ . Specifically, $D$ is the hidden dimension of the backbone model,and $d = \left\lfloor  \frac{{d}_{m}}{K}\right\rfloor$ . Then,we have the operation to reprogram time series patches in each attention head defined as:

为了弥合这一差距，我们提议在主干网络中使用预训练词嵌入 $\mathbf{E} \in  {\mathbb{R}}^{V \times  D}$ 对 ${\widehat{\mathbf{X}}}_{P}^{\left( i\right) }$ 进行重新编程，其中 $V$ 是词汇量大小。然而，没有先验知识表明哪些源标记是直接相关的。因此，简单地利用 $\mathbf{E}$ 会导致一个庞大且可能密集的重新编程空间。一个简单的解决方案是通过对 $\mathbf{E}$ 进行线性探测来维护一小部分文本原型，记为 ${\mathbf{E}}^{\prime } \in  {\mathbb{R}}^{{V}^{\prime } \times  D}$，其中 ${V}^{\prime } \ll  V$。图 3(a) 给出了一个示例。文本原型学习连接语言线索，例如“短期上升”（红线）和“持续下降”（蓝线），然后将这些线索组合起来表示局部块信息（例如，用“短期上升然后持续下降”来描述块 5），而不脱离语言模型的预训练空间。这种方法效率高，并且允许自适应地选择相关的源信息。为了实现这一点，我们采用了多头交叉注意力层。具体来说，对于每个头 $k = \{ 1,\cdots ,K\}$，我们定义查询矩阵 ${\mathbf{Q}}_{k}^{\left( i\right) } = {\widehat{\mathbf{X}}}_{P}^{\left( i\right) }{\mathbf{W}}_{k}^{Q}$、键矩阵 ${\mathbf{K}}_{k}^{\left( i\right) } = {\mathbf{E}}^{\prime }{\mathbf{W}}_{k}^{K}$ 和值矩阵 ${\mathbf{V}}_{k}^{\left( i\right) } = {\mathbf{E}}^{\prime }{\mathbf{W}}_{k}^{V}$，其中 ${\mathbf{W}}_{k}^{Q} \in  {\mathbb{R}}^{{d}_{m} \times  d}$ 和 ${\mathbf{W}}_{k}^{K},{\mathbf{W}}_{k}^{V} \in  {\mathbb{R}}^{D \times  d}$。具体而言，$D$ 是主干模型的隐藏维度，且 $d = \left\lfloor  \frac{{d}_{m}}{K}\right\rfloor$。然后，我们定义了在每个注意力头中对时间序列块进行重新编程的操作如下：

$$
{\mathbf{Z}}_{k}^{\left( i\right) } = \operatorname{ATTENTION}\left( {{\mathbf{Q}}_{k}^{\left( i\right) },{\mathbf{K}}_{k}^{\left( i\right) },{\mathbf{V}}_{k}^{\left( i\right) }}\right)  = \operatorname{SOFTMAX}\left( \frac{{\mathbf{Q}}_{k}^{\left( i\right) }{\mathbf{K}}_{k}^{\left( i\right) \top }}{\sqrt{{d}_{k}}}\right) {\mathbf{V}}_{k}^{\left( i\right) }. \tag{1}
$$

By aggregating each ${\mathbf{Z}}_{k}^{\left( i\right) } \in  {\mathbb{R}}^{P \times  d}$ in every head,we obtain ${\mathbf{Z}}^{\left( i\right) } \in  {\mathbb{R}}^{P \times  {d}_{m}}$ . This is then linearly projected to align the hidden dimensions with the backbone model,yielding ${\mathbf{O}}^{\left( i\right) } \in  {\mathbb{R}}^{P \times  D}$ .

通过聚合每个头中的每个${\mathbf{Z}}_{k}^{\left( i\right) } \in  {\mathbb{R}}^{P \times  d}$，我们得到${\mathbf{Z}}^{\left( i\right) } \in  {\mathbb{R}}^{P \times  {d}_{m}}$。然后对其进行线性投影，以使隐藏维度与主干模型对齐，从而得到${\mathbf{O}}^{\left( i\right) } \in  {\mathbb{R}}^{P \times  D}$。

Prompt-as-Prefix. Prompting serves as a straightforward yet effective approach task-specific activation of LLMs (Yin et al., 2023). However, the direct translation of time series into natural language presents considerable challenges, hindering both the creation of instruction-following datasets and the effective utilization of on-the-fly prompting without performance compromise (Xue & Salim, 2022). Recent advancements indicate that other data modalities, such as images, can be seamlessly integrated as the prefixes of prompts, thereby facilitating effective reasoning based on these inputs (Tsimpoukelli et al., 2021). Motivated by these findings, and to render our approach directly applicable to real-world time series, we pose an alternative question: can prompts act as prefixes to enrich the input context and guide the transformation of reprogrammed time series patches? We term this concept as Prompt-as-Prefix (PaP) and observe that it significantly enhances the LLM's adaptability to downstream tasks while complementing patch reprogramming (See Sec. 4.5 later). An illustration of the two prompting approaches is in Fig. 3(b). In Patch-as-Prefix, a language model is prompted to predict subsequent values in a time series, articulated in natural language. This approach encounters certain constraints: (1) language models typically exhibit reduced sensitivity in processing high-precision numerals without the aid of external tools, thereby presenting substantial challenges in accurately addressing practical forecasting tasks over long horizons; (2) intricate, customized post-processing is required for different language models, given that they are pre-trained on diverse corpora and may employ different tokenization types in generating high-precision numerals with precision and efficiency. This results in forecasts being represented in disparate natural language formats,such as $\left\lbrack  {{}^{6}{}^{\prime },{}^{\prime }{}^{\prime },{}^{6}{}^{\prime },{}^{4}{}^{\prime }}\right\rbrack$ and $\left\lbrack  {{}^{6}{}^{\prime }{}^{\prime },{}^{2}{}^{\prime },{}^{6}{}^{\prime }}\right\rbrack$ ,to denote the decimal 0.61.

提示作为前缀。提示是一种直接而有效的方法，用于特定任务激活大语言模型（LLMs）（尹等人，2023）。然而，将时间序列直接转换为自然语言存在相当大的挑战，这既阻碍了遵循指令的数据集的创建，也阻碍了在不降低性能的情况下即时提示的有效利用（薛和萨利姆，2022）。最近的进展表明，其他数据模态，如图像，可以无缝集成作为提示的前缀，从而促进基于这些输入的有效推理（钦普凯利等人，2021）。受这些发现的启发，为了使我们的方法直接适用于现实世界的时间序列，我们提出了一个替代问题：提示能否作为前缀来丰富输入上下文并引导重新编程的时间序列片段的转换？我们将这一概念称为提示作为前缀（Prompt-as-Prefix，PaP），并观察到它显著增强了大语言模型对下游任务的适应性，同时补充了片段重新编程（详见后文第4.5节）。图3（b）展示了这两种提示方法。在片段作为前缀方法中，语言模型被提示以自然语言表述来预测时间序列中的后续值。这种方法存在一定的局限性：（1）语言模型在没有外部工具辅助的情况下，处理高精度数字时通常灵敏度较低，因此在准确解决长周期实际预测任务方面面临重大挑战；（2）不同的语言模型需要复杂的定制后处理，因为它们是在不同的语料库上进行预训练的，并且在高效精确地生成高精度数字时可能采用不同的分词类型。这导致预测结果以不同的自然语言格式表示，例如用$\left\lbrack  {{}^{6}{}^{\prime },{}^{\prime }{}^{\prime },{}^{6}{}^{\prime },{}^{4}{}^{\prime }}\right\rbrack$和$\left\lbrack  {{}^{6}{}^{\prime }{}^{\prime },{}^{2}{}^{\prime },{}^{6}{}^{\prime }}\right\rbrack$来表示小数0.61。

<!-- Media -->

The Electricity Transformer Temperature (ETT) indicates the electric power long-term deployment. Each data point consists of the target oil temperature and 6 power load features ... Below is the information about the input time series: [BEGIN DATA]

电力变压器温度（ETT）反映了电力的长期部署情况。每个数据点包含目标油温以及6个电力负荷特征……以下是关于输入时间序列的信息：[数据开始]

*** [Domain]: We usually observe that electricity consumption peaks at noon, with a significant increase in transformer load [Instruction]: Predict the next $< H >$ steps given the previous - steps information attached [Statistics]: The input has a minimum of <min_val>, a maximum of <max_val>, and a median of <median_val>. The overall trend is <upward or downward>. The top five lags are <lag_val>, [END DATA]

*** [领域]：我们通常观察到，中午时段用电量达到峰值，变压器负载显著增加。[指令]：根据所附的前几步信息，预测接下来的$< H >$步。[统计信息]：输入的最小值为<min_val>，最大值为<max_val>，中位数为<median_val>。总体趋势为<上升或下降>。前五个滞后值为<lag_val>。[数据结束]

Figure 4: Prompt example. $<  >$ and $<  >$ are task-specific configurations and calculated input statistics.

图4：提示示例。$<  >$和$<  >$是特定任务的配置和计算得出的输入统计数据。

<!-- Media -->

Prompt-as-Prefix, on the other hand, tactfully avoids these constraints. In practice, we identify three pivotal components for constructing effective prompts: (1) dataset context, (2) task instruction, and (3) input statistics. A prompt example is in Fig. 4. The dataset context furnishes the LLM with essential background information concerning the input time series, which often exhibits distinct characteristics across various domains. Task instruction serves as a crucial guide for the LLM in the transformation of patch embeddings for specific tasks. We also enrich the input time series with additional crucial statistics, such as trends and lags, to facilitate pattern recognition and reasoning.

另一方面，提示前缀法（Prompt-as-Prefix）巧妙地避免了这些限制。在实践中，我们确定了构建有效提示的三个关键要素：（1）数据集上下文；（2）任务指令；（3）输入统计信息。图4给出了一个提示示例。数据集上下文为大语言模型（LLM）提供了有关输入时间序列的必要背景信息，这些时间序列在不同领域往往具有不同的特征。任务指令是大语言模型（LLM）针对特定任务转换图像块嵌入（patch embeddings）的关键指南。我们还为输入时间序列添加了其他重要统计信息，如趋势和滞后，以促进模式识别和推理。

Output Projection. Upon packing and feedforwarding the prompt and patch embeddings ${\mathbf{O}}^{\left( i\right) }$ through the frozen LLM as shown in Fig. 2, we discard the prefixal part and obtain the output representations. Following this,we flatten and linear project them to derive the final forecasts ${\widehat{\mathbf{Y}}}^{\left( i\right) }$ .

输出投影。如图2所示，将提示和图像块嵌入 ${\mathbf{O}}^{\left( i\right) }$ 打包并通过冻结的大语言模型（LLM）进行前馈传播后，我们舍弃前缀部分并获得输出表示。随后，我们将这些表示展平并进行线性投影，以得到最终预测结果 ${\widehat{\mathbf{Y}}}^{\left( i\right) }$。

## 4 MAIN RESULTS

## 4 主要结果

TIME-LLM consistently outperforms state-of-the-art forecasting methods by large margins across multiple benchmarks and settings, especially in few-shot and zero-shot scenarios. We compared our approach against a broad collection of up-to-date models, including a recent study that fine-tunes language model for time series analysis (Zhou et al. 2023a). To ensure a fair comparison, we adhere to the experimental configurations in (Wu et al. 2023) across all baselines with a unified evaluation pipeline ${}^{1}$ . We use Llama-7B (Touvron et al. 2023) as the default backbone unless stated otherwise.

TIME - 大语言模型（TIME - LLM）在多个基准测试和设置中始终大幅优于最先进的预测方法，尤其是在小样本和零样本场景中。我们将我们的方法与大量最新模型进行了比较，其中包括一项最近针对时间序列分析微调语言模型的研究（周等人，2023a）。为确保公平比较，我们在所有基线模型中遵循（吴等人，2023）的实验配置，并采用统一的评估流程${}^{1}$。除非另有说明，我们默认使用羊驼70亿参数模型（Llama - 7B，图夫龙等人，2023）作为主干模型。

Baselines. We compare with the SOTA time series models, and we cite their performance from (Zhou et al., 2023a) if applicable. Our baselines include a series of Transformer-based methods: PatchTST (2023), ESTformer (2022), Non-Stationary Transformer (2022), FEDformer (2022), Aut-oformer (2021), Informer (2021), and Reformer (2020). We also select a set of recent competitive models, including GPT4TS (2023a), LLMTime (2023), DLinear (2023), TimesNet (2023), and LightTS (2022a). In short-term forecasting, we further compare our model with N-HiTS (2023b) and N-BEATS (2020). More details are in Appendix A

基线模型。我们将与最先进的时间序列模型进行比较，并在适用的情况下引用（Zhou等人，2023a）中的模型性能。我们的基线模型包括一系列基于Transformer的方法：PatchTST（2023年）、ESTformer（2022年）、非平稳Transformer（Non-Stationary Transformer，2022年）、FEDformer（2022年）、Autoformer（2021年）、Informer（2021年）和Reformer（2020年）。我们还选择了一组近期有竞争力的模型，包括GPT4TS（2023a）、LLMTime（2023年）、DLinear（2023年）、TimesNet（2023年）和LightTS（2022a）。在短期预测中，我们进一步将我们的模型与N-HiTS（2023b）和N-BEATS（2020年）进行比较。更多细节见附录A

### 4.1 LONG-TERM FORECASTING

### 4.1 长期预测

Setups. We evaluate on ETTh1, ETTh2, ETTm1, ETTm2, Weather, Electricity (ECL), Traffic, and ILI, which have been extensively adopted for benchmarking long-term forecasting models (Wu et al. 2023). Details of the implementation and datasets can be found in Appendix B. The input time series length $T$ is set as 512,and we use four different prediction horizons $H \in  \{ {96},{192},{336},{720}\}$ . The evaluation metrics include mean square error (MSE) and mean absolute error (MAE).

实验设置。我们在ETTh1、ETTh2、ETTm1、ETTm2、气象（Weather）、电力（ECL）、交通（Traffic）和流感样疾病（ILI）数据集上进行评估，这些数据集已被广泛用于长期预测模型的基准测试（Wu等人，2023年）。实现细节和数据集详情可在附录B中找到。输入时间序列长度$T$设置为512，我们使用四种不同的预测时长$H \in  \{ {96},{192},{336},{720}\}$。评估指标包括均方误差（MSE）和平均绝对误差（MAE）。

Results. Our brief results are shown in Tab. 1, where TIME-LLM outperforms all baselines in most cases and significantly so to the majority of them. The comparison with GPT4TS (Zhou et al. 2023a) is particularly noteworthy. GPT4TS is a very recent work that involves fine-tuning on the backbone language model. We note average performance gains of $\mathbf{{12}\% }$ and $\mathbf{{20}\% }$ over GPT4TS and TimesNet, respectively. When compared with the SOTA task-specific Transformer model PatchTST, by reprogramming the smallest Llama, TIME-LLM realizes an average MSE reduction of 1.4%. Relative to the other models, e.g., DLinear, our improvements are also pronounced, exceeding 12%.

结果。我们的简要结果如表1所示，在大多数情况下，TIME-LLM的表现优于所有基线模型，并且在大多数情况下优势显著。与GPT4TS（Zhou等人，2023a）的比较尤其值得关注。GPT4TS是一项近期的研究，涉及对骨干语言模型进行微调。我们注意到，与GPT4TS和TimesNet相比，TIME-LLM的平均性能提升分别为$\mathbf{{12}\% }$和$\mathbf{{20}\% }$。与最先进的特定任务Transformer模型PatchTST相比，通过对最小的Llama模型进行重新编程，TIME-LLM实现了平均均方误差（MSE）降低1.4%。相对于其他模型，如DLinear，我们的改进也很显著，超过了12%。

### 4.2 SHORT-TERM FORECASTING

### 4.2 短期预测

Setups. We choose the M4 benchmark (Makridakis et al. 2018) as the testbed, which contains a collection of marketing data in different sampling frequencies. More details are provided in Appendix B. The prediction horizons in this case are relatively small and in $\left\lbrack  {6,{48}}\right\rbrack$ . The input lengths are twice as prediction horizons. The evaluation metrics are symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MSAE), and overall weighted average (OWA).

设置。我们选择M4基准测试（Makridakis等人，2018年）作为测试平台，它包含了不同采样频率的营销数据集合。更多细节见附录B。在这种情况下，预测期相对较短，范围在$\left\lbrack  {6,{48}}\right\rbrack$内。输入长度是预测期的两倍。评估指标包括对称平均绝对百分比误差（SMAPE）、平均绝对缩放误差（MSAE）和总体加权平均值（OWA）。

---

<!-- Footnote -->

${}^{1}$ https://github.com/thuml/Time-Series-Library

${}^{1}$ https://github.com/thuml/Time-Series-Library

<!-- Footnote -->

---

<!-- Media -->

Table 1: Long-term forecasting results. All results are averaged from four different forecasting horizons: $H \in$ $\{ {24},{36},{48},{60}\}$ for ILI and $\{ {96},{192},{336},{720}\}$ for the others. A lower value indicates better performance. Red: the best, Blue: the second best. Our full results are in Appendix D

表1：长期预测结果。所有结果均为四个不同预测期的平均值，流感样病例（ILI）的预测期为[ [latex0 [ $\{ {24},{36},{48},{60}\}$，其他的预测期为$\{ {96},{192},{336},{720}\}$。数值越低表示性能越好。红色：最优；蓝色：最佳中的最佳。我们的完整结果见附录D。

<table><tr><td>Methods</td><td colspan="2">TIME-LLM (Ours)</td><td colspan="2">GPT4TS 2023a)</td><td>DLinear 2023)</td><td colspan="2">PatchTST 2023)</td><td colspan="2">TimesNet 2023)</td><td colspan="2">FEDformer (2022)</td><td colspan="2">Autoformer 2021)</td><td colspan="2">Stationary 2022)</td><td colspan="2">ETSformer 2022)</td><td colspan="2">LightTS 2022a)</td><td colspan="2">Informer 2021)</td><td colspan="2">Reformer 2020)</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAEMSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>ETTh1</td><td>0.408</td><td>0.423</td><td>0.465</td><td>0.4550.422</td><td>0.437</td><td>$\underline{0.413}$</td><td>$\underline{0.430}$</td><td>0.458</td><td>0.450</td><td>0.440</td><td>0.460</td><td>0.496</td><td>0.487</td><td>0.570</td><td>0.537</td><td>0.542</td><td>0.510</td><td>0.491</td><td>0.479</td><td>1.040</td><td>0.795</td><td>1.029</td><td>0.805</td></tr><tr><td>${ETTh2}$</td><td>$\underline{0.334}$</td><td>$\underline{0.383}$</td><td>0.381</td><td>0.4120.431</td><td>0.446</td><td>0.330</td><td>0.379</td><td>0.414</td><td>0.427</td><td>0.437</td><td>0.449</td><td>0.450</td><td>0.459</td><td>0.526</td><td>0.516</td><td>0.439</td><td>0.452</td><td>0.602</td><td>0.543</td><td>4.431</td><td>1.729</td><td>6.736</td><td>2.191</td></tr><tr><td>${ETTm1}$</td><td>0.329</td><td>0.372</td><td>0.388</td><td>0.4030.357</td><td>$\underline{0.378}$</td><td>$\underline{0.351}$</td><td>0.380</td><td>0.400</td><td>0.406</td><td>0.448</td><td>0.452</td><td>0.588</td><td>0.517</td><td>0.481</td><td>0.456</td><td>0.429</td><td>0.425</td><td>0.435</td><td>0.437</td><td>0.961</td><td>0.734</td><td>0.799</td><td>0.671</td></tr><tr><td>${ETTm2}$</td><td>0.251</td><td>0.313</td><td>0.284</td><td>0.3390.267</td><td>0.333</td><td>$\underline{0.255}$</td><td>$\underline{0.315}$</td><td>0.291</td><td>0.333</td><td>0.305</td><td>0.349</td><td>0.327</td><td>0.371</td><td>0.306</td><td>0.347</td><td>0.293</td><td>0.342</td><td>0.409</td><td>0.436</td><td>1.410</td><td>0.810</td><td>1.479</td><td>0.915</td></tr><tr><td>${Weather}$</td><td>0.225</td><td>0.257</td><td>0.237</td><td>0.2700.248</td><td>0.300</td><td>0.225</td><td>$\underline{0.264}$</td><td>0.259</td><td>0.287</td><td>0.309</td><td>0.360</td><td>0.338</td><td>0.382</td><td>0.288</td><td>0.314</td><td>0.271</td><td>0.334</td><td>0.261</td><td>0.312</td><td>0.634</td><td>0.548</td><td>0.803</td><td>0.656</td></tr><tr><td>${ECL}$</td><td>0.158</td><td>0.252</td><td>0.167</td><td>0.2630.166</td><td>0.263</td><td>$\underline{0.161}$</td><td>0.252</td><td>0.192</td><td>0.295</td><td>0.214</td><td>0.327</td><td>0.227</td><td>0.338</td><td>0.193</td><td>0.296</td><td>0.208</td><td>0.323</td><td>0.229</td><td>0.329</td><td>0.311</td><td>0.397</td><td>0.338</td><td>0.422</td></tr><tr><td>Traffic</td><td>0.388</td><td>0.264</td><td>0.414</td><td>0.2940.433</td><td>0.295</td><td>$\underline{0.390}$</td><td>0.263</td><td>0.620</td><td>0.336</td><td>0.610</td><td>0.376</td><td>0.628</td><td>0.379</td><td>0.624</td><td>0.340</td><td>0.621</td><td>0.396</td><td>0.622</td><td>0.392</td><td>0.764</td><td>0.416</td><td>0.741</td><td>0.422</td></tr><tr><td>${ILI}$</td><td>1.435</td><td>$\underline{0.801}$</td><td>1.925</td><td>0.9032.169</td><td>1.041</td><td>1.443</td><td>0.797</td><td>2.139</td><td>0.931</td><td>2.847</td><td>1.144</td><td>3.006</td><td>1.161</td><td>2.077</td><td>0.914</td><td>2.497</td><td>1.004</td><td>7.382</td><td>2.003</td><td>5.137</td><td>1.544</td><td>4.724</td><td>1.445</td></tr><tr><td>${1}^{\text{st }}$ Count</td><td/><td>7</td><td/><td>0</td><td>0</td><td/><td>$\underline{5}$</td><td/><td>0</td><td/><td>0</td><td/><td>0</td><td/><td>0</td><td/><td>0</td><td>0</td><td/><td>0</td><td/><td>0</td><td/></tr></table>

<table><tbody><tr><td>方法</td><td colspan="2">TIME-大语言模型（我们的方法）</td><td colspan="2">GPT4TS 2023a)</td><td>线性差分模型（DLinear 2023)</td><td colspan="2">PatchTST 2023)</td><td colspan="2">时间网络（TimesNet 2023)</td><td colspan="2">FEDformer（2022年）</td><td colspan="2">Autoformer（2021年）</td><td colspan="2">Stationary（2022年）</td><td colspan="2">ETSformer（2022年）</td><td colspan="2">LightTS（2022a年）</td><td colspan="2">Informer（2021年）</td><td colspan="2">改革者2020（Reformer 2020）</td></tr><tr><td>指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差均方误差（MAEMSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td>ETTh1</td><td>0.408</td><td>0.423</td><td>0.465</td><td>0.4550.422</td><td>0.437</td><td>$\underline{0.413}$</td><td>$\underline{0.430}$</td><td>0.458</td><td>0.450</td><td>0.440</td><td>0.460</td><td>0.496</td><td>0.487</td><td>0.570</td><td>0.537</td><td>0.542</td><td>0.510</td><td>0.491</td><td>0.479</td><td>1.040</td><td>0.795</td><td>1.029</td><td>0.805</td></tr><tr><td>${ETTh2}$</td><td>$\underline{0.334}$</td><td>$\underline{0.383}$</td><td>0.381</td><td>0.4120.431</td><td>0.446</td><td>0.330</td><td>0.379</td><td>0.414</td><td>0.427</td><td>0.437</td><td>0.449</td><td>0.450</td><td>0.459</td><td>0.526</td><td>0.516</td><td>0.439</td><td>0.452</td><td>0.602</td><td>0.543</td><td>4.431</td><td>1.729</td><td>6.736</td><td>2.191</td></tr><tr><td>${ETTm1}$</td><td>0.329</td><td>0.372</td><td>0.388</td><td>0.4030.357</td><td>$\underline{0.378}$</td><td>$\underline{0.351}$</td><td>0.380</td><td>0.400</td><td>0.406</td><td>0.448</td><td>0.452</td><td>0.588</td><td>0.517</td><td>0.481</td><td>0.456</td><td>0.429</td><td>0.425</td><td>0.435</td><td>0.437</td><td>0.961</td><td>0.734</td><td>0.799</td><td>0.671</td></tr><tr><td>${ETTm2}$</td><td>0.251</td><td>0.313</td><td>0.284</td><td>0.3390.267</td><td>0.333</td><td>$\underline{0.255}$</td><td>$\underline{0.315}$</td><td>0.291</td><td>0.333</td><td>0.305</td><td>0.349</td><td>0.327</td><td>0.371</td><td>0.306</td><td>0.347</td><td>0.293</td><td>0.342</td><td>0.409</td><td>0.436</td><td>1.410</td><td>0.810</td><td>1.479</td><td>0.915</td></tr><tr><td>${Weather}$</td><td>0.225</td><td>0.257</td><td>0.237</td><td>0.2700.248</td><td>0.300</td><td>0.225</td><td>$\underline{0.264}$</td><td>0.259</td><td>0.287</td><td>0.309</td><td>0.360</td><td>0.338</td><td>0.382</td><td>0.288</td><td>0.314</td><td>0.271</td><td>0.334</td><td>0.261</td><td>0.312</td><td>0.634</td><td>0.548</td><td>0.803</td><td>0.656</td></tr><tr><td>${ECL}$</td><td>0.158</td><td>0.252</td><td>0.167</td><td>0.2630.166</td><td>0.263</td><td>$\underline{0.161}$</td><td>0.252</td><td>0.192</td><td>0.295</td><td>0.214</td><td>0.327</td><td>0.227</td><td>0.338</td><td>0.193</td><td>0.296</td><td>0.208</td><td>0.323</td><td>0.229</td><td>0.329</td><td>0.311</td><td>0.397</td><td>0.338</td><td>0.422</td></tr><tr><td>交通流量</td><td>0.388</td><td>0.264</td><td>0.414</td><td>0.2940.433</td><td>0.295</td><td>$\underline{0.390}$</td><td>0.263</td><td>0.620</td><td>0.336</td><td>0.610</td><td>0.376</td><td>0.628</td><td>0.379</td><td>0.624</td><td>0.340</td><td>0.621</td><td>0.396</td><td>0.622</td><td>0.392</td><td>0.764</td><td>0.416</td><td>0.741</td><td>0.422</td></tr><tr><td>${ILI}$</td><td>1.435</td><td>$\underline{0.801}$</td><td>1.925</td><td>0.9032.169</td><td>1.041</td><td>1.443</td><td>0.797</td><td>2.139</td><td>0.931</td><td>2.847</td><td>1.144</td><td>3.006</td><td>1.161</td><td>2.077</td><td>0.914</td><td>2.497</td><td>1.004</td><td>7.382</td><td>2.003</td><td>5.137</td><td>1.544</td><td>4.724</td><td>1.445</td></tr><tr><td>${1}^{\text{st }}$ 计数</td><td></td><td>7</td><td></td><td>0</td><td>0</td><td></td><td>$\underline{5}$</td><td></td><td>0</td><td></td><td>0</td><td></td><td>0</td><td></td><td>0</td><td></td><td>0</td><td>0</td><td></td><td>0</td><td></td><td>0</td><td></td></tr></tbody></table>

Table 2: Short-term time series forecasting results on M4. The forecasting horizons are in [6, 48] and the three rows provided are weighted averaged from all datasets under different sampling intervals. A lower value indicates better performance. Red: the best, Blue: the second best. More results are in Appendix D

表2：M4数据集上的短期时间序列预测结果。预测期在[6, 48]范围内，所提供的三行数据是不同采样间隔下所有数据集的加权平均值。数值越低表示性能越好。红色：最佳，蓝色：次佳。更多结果见附录D

<table><tr><td rowspan="2">Methods</td><td>TIME-LLM</td><td>GPT4TS</td><td>TimesNet</td><td>PatchTST</td><td>N-HiTS</td><td>N-BEATS</td><td>ETSformer</td><td>LightTS</td><td>DLinear</td><td>FEDformer</td><td>Stationary</td><td>Autoformer</td><td>Informer</td><td>Reformer</td></tr><tr><td>(Ours)</td><td>2023a)</td><td>2023)</td><td>2023)</td><td>2023b)</td><td>2020)</td><td>2022)</td><td>2022a)</td><td>2023)</td><td>2022)</td><td>2022)</td><td>2021)</td><td>(2021)</td><td>2020)</td></tr><tr><td>SMAPE</td><td>11.983</td><td>12.69</td><td>12.88</td><td>12.059</td><td>12.035</td><td>12.25</td><td>14.718</td><td>13.525</td><td>13.639</td><td>13.16</td><td>12.780</td><td>12.909</td><td>14.086</td><td>18.200</td></tr><tr><td>AverageMASE</td><td>1.595</td><td>1.808</td><td>1.836</td><td>1.623</td><td>1.625</td><td>1.698</td><td>2.408</td><td>2.111</td><td>2.095</td><td>1.775</td><td>1.756</td><td>1.771</td><td>2.718</td><td>4.223</td></tr><tr><td>OWA</td><td>0.859</td><td>0.94</td><td>0.955</td><td>$\underline{0.869}$</td><td>0.869</td><td>0.896</td><td>1.172</td><td>1.051</td><td>1.051</td><td>0.949</td><td>0.930</td><td>0.939</td><td>1.230</td><td>1.775</td></tr></table>

<table><tbody><tr><td rowspan="2">方法</td><td>TIME大语言模型（TIME-LLM）</td><td>GPT4时间序列模型（GPT4TS）</td><td>时间网络（TimesNet）</td><td>补丁时间序列变换器（PatchTST）</td><td>神经层次时间序列预测模型（N-HiTS）</td><td>N-BEATS（N-节拍）</td><td>ETSformer（ETS变换器）</td><td>LightTS（轻量级时间序列）</td><td>DLinear（深度线性）</td><td>FEDformer（联邦变换器）</td><td>平稳的</td><td>自动转换器（Autoformer）</td><td>信息转换器（Informer）</td><td>重构转换器（Reformer）</td></tr><tr><td>（我们的方法）</td><td>2023a)</td><td>2023)</td><td>2023)</td><td>2023b)</td><td>2020)</td><td>2022)</td><td>2022a)</td><td>2023)</td><td>2022)</td><td>2022)</td><td>2021)</td><td>(2021)</td><td>2020)</td></tr><tr><td>对称平均绝对百分比误差（SMAPE）</td><td>11.983</td><td>12.69</td><td>12.88</td><td>12.059</td><td>12.035</td><td>12.25</td><td>14.718</td><td>13.525</td><td>13.639</td><td>13.16</td><td>12.780</td><td>12.909</td><td>14.086</td><td>18.200</td></tr><tr><td>平均平均绝对尺度误差（AverageMASE）</td><td>1.595</td><td>1.808</td><td>1.836</td><td>1.623</td><td>1.625</td><td>1.698</td><td>2.408</td><td>2.111</td><td>2.095</td><td>1.775</td><td>1.756</td><td>1.771</td><td>2.718</td><td>4.223</td></tr><tr><td>Outlook Web App（OWA）</td><td>0.859</td><td>0.94</td><td>0.955</td><td>$\underline{0.869}$</td><td>0.869</td><td>0.896</td><td>1.172</td><td>1.051</td><td>1.051</td><td>0.949</td><td>0.930</td><td>0.939</td><td>1.230</td><td>1.775</td></tr></tbody></table>

<!-- Media -->

Results. Our brief results with unified seeds across all methods are in Tab. 2 TIME-LLM consistently surpasses all baselines, outperforming GPT4TS by 8.7%. TIME-LLM remains competitive even when compared with the SOTA model, N-HiTS (Challu et al. 2023b), w.r.t. MASE and OWA.

结果。我们在所有方法中使用统一种子得到的简要结果见表2。TIME - 大语言模型（TIME - LLM）始终超越所有基线模型，比GPT4TS的性能高出8.7%。在平均绝对尺度误差（MASE）和总体加权平均（OWA）方面，即使与最先进的模型N - HiTS（查卢等人，2023b）相比，TIME - 大语言模型（TIME - LLM）仍具有竞争力。

### 4.3 FEW-SHOT FORECASTING

### 4.3 小样本预测

Setups. LLMs have recently demonstrated remarkable few-shot learning capabilities (Liu et al., 2023b). In this section, we assess whether our reprogrammed LLM retains this ability in forecasting tasks. We adhere to the setups in (Zhou et al. 2023a) for fair comparisons, and we evaluate on scenarios with limited training data (i.e., $\leq$ first ${10}\%$ training time steps).

设置。大语言模型（LLMs）最近展现出了卓越的小样本学习能力（刘等人，2023b）。在本节中，我们评估经过重新编程的大语言模型（LLM）在预测任务中是否仍保留这种能力。为了进行公平比较，我们遵循（周等人，2023a）中的设置，并在训练数据有限的场景下进行评估（即，前$\leq$至${10}\%$个训练时间步）。

Results. Our brief 10% and 5% few-shot learning results are in Tab. 3 and Tab. 4 respectively. TIME-LLM remarkably excels over all baseline methods, and we attribute this to the successful knowledge activation in our reprogrammed LLM. Interestingly, both our approach and GPT4TS consistently surpass other competitive baselines, further underscoring the potential prowess of language models as proficient time series machines.

结果。我们在10%和5%数据量下的小样本学习结果分别列于表3和表4。TIME-LLM显著优于所有基线方法，我们将此归因于我们重新编程的大语言模型中知识的成功激活。有趣的是，我们的方法和GPT4TS都始终超越其他有竞争力的基线方法，这进一步凸显了大语言模型作为高效时间序列处理工具的潜在优势。

In the realm of ${10}\%$ few-shot learning,our methodology realizes a 5% MSE reduction in comparison to GPT4TS, without necessitating any fine-tuning on the LLM. In relation to recent SOTA models

在${10}\%$小样本学习领域，我们的方法与GPT4TS相比，均方误差（MSE）降低了5%，且无需对大语言模型进行任何微调。与近期的最优模型相比

<!-- Media -->

Table 3: Few-shot learning on 10% training data. We use the same protocol in Tab. 1 All results are averaged from four different forecasting horizons: $H \in  \{ {96},{192},{336},{720}\}$ . Our full results are in Appendix E

表3：在10%训练数据上的小样本学习。我们采用与表1相同的协议。所有结果均为四个不同预测时域$H \in  \{ {96},{192},{336},{720}\}$的平均值。完整结果见附录E

<table><tr><td>Methods</td><td colspan="2">TIME-LLM (Ours)</td><td colspan="2">GPT4TS $|{2023}\mathrm{a}\rangle$</td><td colspan="2">DLinear 2023)</td><td colspan="2">PatchTST 2023)</td><td colspan="2">TimesNet 2023)</td><td colspan="2">FEDformer 2022)</td><td colspan="2">Autoformer 2021)</td><td>Stationary 2022)</td><td>ETSformer 2022)</td><td colspan="2">LightTS 2022a)</td><td colspan="2">Informer 2021)</td><td colspan="2">Reformer 2020)</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAEMSE</td><td>MAEMSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>${ETTh1}$</td><td>0.556</td><td>0.522</td><td>$\underline{0.590}$</td><td>$\underline{0.525}$</td><td>0.691</td><td>0.600</td><td>0.633</td><td>0.542</td><td>0.869</td><td>0.628</td><td>0.639</td><td>0.561</td><td>0.702</td><td>0.5960.915</td><td>0.6391.180</td><td>0.834</td><td>1.375</td><td>0.877</td><td>1.199</td><td>0.809</td><td>1.249</td><td>0.833</td></tr><tr><td>${ETTh2}$</td><td>0.370</td><td>0.394</td><td>$\underline{0.397}$</td><td>$\underline{0.421}$</td><td>0.605</td><td>0.538</td><td>0.415</td><td>0.431</td><td>0.479</td><td>0.465</td><td>0.466</td><td>0.475</td><td>0.488</td><td>0.4990.462</td><td>0.4550.894</td><td>0.713</td><td>2.655</td><td>1.160</td><td>3.872</td><td>1.513</td><td>3.485</td><td>1.486</td></tr><tr><td>${ETTm1}$</td><td>0.404</td><td>0.427</td><td>0.464</td><td>0.441</td><td>$\underline{0.411}$</td><td>$\underline{0.429}$</td><td>0.501</td><td>0.466</td><td>0.677</td><td>0.537</td><td>0.722</td><td>0.605</td><td>0.802</td><td>0.6280.797</td><td>0.5780.980</td><td>0.714</td><td>0.971</td><td>0.705</td><td>1.192</td><td>0.821</td><td>1.426</td><td>0.856</td></tr><tr><td>${ETTm2}$</td><td>0.277</td><td>0.323</td><td>0.293</td><td>$\underline{0.335}$</td><td>0.316</td><td>0.368</td><td>0.296</td><td>0.343</td><td>0.320</td><td>0.353</td><td>0.463</td><td>0.488</td><td>1.342</td><td>0.9300.332</td><td>0.3660.447</td><td>0.487</td><td>0.987</td><td>0.756</td><td>3.370</td><td>1.440</td><td>3.978</td><td>1.587</td></tr><tr><td>${Weather}$</td><td>0.234</td><td>0.273</td><td>$\underline{0.238}$</td><td>$\underline{0.275}$</td><td>0.241</td><td>0.283</td><td>0.242</td><td>0.279</td><td>0.279</td><td>0.301</td><td>0.284</td><td>0.324</td><td>0.300</td><td>0.3420.318</td><td>0.3230.318</td><td>0.360</td><td>0.289</td><td>0.322</td><td>0.597</td><td>0.495</td><td>0.546</td><td>0.469</td></tr><tr><td>${ECL}$</td><td>0.175</td><td>$\underline{0.270}$</td><td>$\underline{0.176}$</td><td>0.269</td><td>0.180</td><td>0.280</td><td>0.180</td><td>0.273</td><td>0.323</td><td>0.392</td><td>0.346</td><td>0.427</td><td>0.431</td><td>0.4780.444</td><td>0.4800.660</td><td>0.617</td><td>0.441</td><td>0.489</td><td>1.195</td><td>0.891</td><td>0.965</td><td>0.768</td></tr><tr><td>Traffic</td><td>0.429</td><td>$\underline{0.306}$</td><td>0.440</td><td>0.310</td><td>0.447</td><td>0.313</td><td>$\underline{0.430}$</td><td>0.305</td><td>0.951</td><td>0.535</td><td>0.663</td><td>0.425</td><td>0.749</td><td>0.4461.453</td><td>0.8151.914</td><td>0.936</td><td>1.248</td><td>0.684</td><td>1.534</td><td>0.811</td><td>1.551</td><td>0.821</td></tr><tr><td>${1}^{\text{st }}$ Count</td><td/><td>7</td><td>1</td><td/><td/><td>0</td><td/><td>$\underline{1}$</td><td/><td>0</td><td/><td>0</td><td>0</td><td/><td>0</td><td>0</td><td>0</td><td/><td>0</td><td/><td>0</td><td/></tr></table>

<table><tbody><tr><td>方法</td><td colspan="2">TIME-大语言模型（我们的方法）</td><td colspan="2">GPT4TS $|{2023}\mathrm{a}\rangle$</td><td colspan="2">深度线性模型（2023年）</td><td colspan="2">补丁时间序列变换器（2023年）</td><td colspan="2">时间网络（2023年）</td><td colspan="2">FEDformer（2022年）</td><td colspan="2">Autoformer（2021年）</td><td>Stationary（2022年）</td><td>ETSformer（2022年）</td><td colspan="2">LightTS（2022a年）</td><td colspan="2">Informer（2021年）</td><td colspan="2">改革者2020（Reformer 2020）</td></tr><tr><td>指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差均方误差（MAEMSE）</td><td>平均绝对误差均方误差（MAEMSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td>${ETTh1}$</td><td>0.556</td><td>0.522</td><td>$\underline{0.590}$</td><td>$\underline{0.525}$</td><td>0.691</td><td>0.600</td><td>0.633</td><td>0.542</td><td>0.869</td><td>0.628</td><td>0.639</td><td>0.561</td><td>0.702</td><td>0.5960.915</td><td>0.6391.180</td><td>0.834</td><td>1.375</td><td>0.877</td><td>1.199</td><td>0.809</td><td>1.249</td><td>0.833</td></tr><tr><td>${ETTh2}$</td><td>0.370</td><td>0.394</td><td>$\underline{0.397}$</td><td>$\underline{0.421}$</td><td>0.605</td><td>0.538</td><td>0.415</td><td>0.431</td><td>0.479</td><td>0.465</td><td>0.466</td><td>0.475</td><td>0.488</td><td>0.4990.462</td><td>0.4550.894</td><td>0.713</td><td>2.655</td><td>1.160</td><td>3.872</td><td>1.513</td><td>3.485</td><td>1.486</td></tr><tr><td>${ETTm1}$</td><td>0.404</td><td>0.427</td><td>0.464</td><td>0.441</td><td>$\underline{0.411}$</td><td>$\underline{0.429}$</td><td>0.501</td><td>0.466</td><td>0.677</td><td>0.537</td><td>0.722</td><td>0.605</td><td>0.802</td><td>0.6280.797</td><td>0.5780.980</td><td>0.714</td><td>0.971</td><td>0.705</td><td>1.192</td><td>0.821</td><td>1.426</td><td>0.856</td></tr><tr><td>${ETTm2}$</td><td>0.277</td><td>0.323</td><td>0.293</td><td>$\underline{0.335}$</td><td>0.316</td><td>0.368</td><td>0.296</td><td>0.343</td><td>0.320</td><td>0.353</td><td>0.463</td><td>0.488</td><td>1.342</td><td>0.9300.332</td><td>0.3660.447</td><td>0.487</td><td>0.987</td><td>0.756</td><td>3.370</td><td>1.440</td><td>3.978</td><td>1.587</td></tr><tr><td>${Weather}$</td><td>0.234</td><td>0.273</td><td>$\underline{0.238}$</td><td>$\underline{0.275}$</td><td>0.241</td><td>0.283</td><td>0.242</td><td>0.279</td><td>0.279</td><td>0.301</td><td>0.284</td><td>0.324</td><td>0.300</td><td>0.3420.318</td><td>0.3230.318</td><td>0.360</td><td>0.289</td><td>0.322</td><td>0.597</td><td>0.495</td><td>0.546</td><td>0.469</td></tr><tr><td>${ECL}$</td><td>0.175</td><td>$\underline{0.270}$</td><td>$\underline{0.176}$</td><td>0.269</td><td>0.180</td><td>0.280</td><td>0.180</td><td>0.273</td><td>0.323</td><td>0.392</td><td>0.346</td><td>0.427</td><td>0.431</td><td>0.4780.444</td><td>0.4800.660</td><td>0.617</td><td>0.441</td><td>0.489</td><td>1.195</td><td>0.891</td><td>0.965</td><td>0.768</td></tr><tr><td>流量</td><td>0.429</td><td>$\underline{0.306}$</td><td>0.440</td><td>0.310</td><td>0.447</td><td>0.313</td><td>$\underline{0.430}$</td><td>0.305</td><td>0.951</td><td>0.535</td><td>0.663</td><td>0.425</td><td>0.749</td><td>0.4461.453</td><td>0.8151.914</td><td>0.936</td><td>1.248</td><td>0.684</td><td>1.534</td><td>0.811</td><td>1.551</td><td>0.821</td></tr><tr><td>${1}^{\text{st }}$ 计数</td><td></td><td>7</td><td>1</td><td></td><td></td><td>0</td><td></td><td>$\underline{1}$</td><td></td><td>0</td><td></td><td>0</td><td>0</td><td></td><td>0</td><td>0</td><td>0</td><td></td><td>0</td><td></td><td>0</td><td></td></tr></tbody></table>

Table 4: Few-shot learning on 5% training data. We use the same protocol in Tab. 1 All results are averaged from four different forecasting horizons: $H \in  \{ {96},{192},{336},{720}\}$ . Our full results are in Appendix E

表4：在5%训练数据上的小样本学习。我们采用与表1相同的协议。所有结果均为四个不同预测范围$H \in  \{ {96},{192},{336},{720}\}$的平均值。完整结果见附录E

<table><tr><td>Methods</td><td colspan="2">TIME-LLM (Ours)</td><td colspan="2">GPT4TS 2023a)</td><td colspan="2">DLinear 2023)</td><td colspan="2">PatchTST 2023)</td><td colspan="2">TimesNet 2023)</td><td colspan="2">FEDformer 2022)</td><td colspan="2">Autoformer 2021)</td><td colspan="2">Stationary 2022)</td><td colspan="2">ETSformer 2022)</td><td colspan="2">LightTS 2022a)</td><td colspan="2">Informer 2021)</td><td colspan="2">Reformer 2020)</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>${ETTh1}$</td><td>0.627</td><td>0.543</td><td>0.681</td><td>$\underline{0.560}$</td><td>0.750</td><td>0.611</td><td>0.694</td><td>0.569</td><td>0.925</td><td>0.647</td><td>$\underline{0.658}$</td><td>0.562</td><td>0.722</td><td>0.598</td><td>0.943</td><td>0.646</td><td>1.189</td><td>0.839</td><td>1.451</td><td>0.903</td><td>1.225</td><td>0.817</td><td>1.241</td><td>0.835</td></tr><tr><td>${ETTh2}$</td><td>0.382</td><td>0.418</td><td>$\underline{0.400}$</td><td>0.433</td><td>0.694</td><td>0.577</td><td>0.827</td><td>0.615</td><td>0.439</td><td>0.448</td><td>0.463</td><td>0.454</td><td>0.441</td><td>0.457</td><td>0.470</td><td>0.489</td><td>0.809</td><td>0.681</td><td>3.206</td><td>1.268</td><td>3.922</td><td>1.653</td><td>3.527</td><td>1.472</td></tr><tr><td>ETTm1</td><td>$\underline{0.425}$</td><td>$\underline{0.434}$</td><td>0.472</td><td>0.450</td><td>0.400</td><td>0.417</td><td>0.526</td><td>0.476</td><td>0.717</td><td>0.561</td><td>0.730</td><td>0.592</td><td>0.796</td><td>0.620</td><td>0.857</td><td>0.598</td><td>1.125</td><td>0.782</td><td>1.123</td><td>0.765</td><td>1.163</td><td>0.791</td><td>1.264</td><td>0.826</td></tr><tr><td>${ETTm2}$</td><td>0.274</td><td>0.323</td><td>$\underline{0.308}$</td><td>$\underline{0.346}$</td><td>0.399</td><td>0.426</td><td>0.314</td><td>0.352</td><td>0.344</td><td>0.372</td><td>0.381</td><td>0.404</td><td>0.388</td><td>0.433</td><td>0.341</td><td>0.372</td><td>0.534</td><td>0.547</td><td>1.415</td><td>0.871</td><td>3.658</td><td>1.489</td><td>3.581</td><td>1.487</td></tr><tr><td>Weather</td><td>0.260</td><td>0.309</td><td>0.263</td><td>0.301</td><td>0.263</td><td>0.308</td><td>0.269</td><td>$\underline{0.303}$</td><td>0.298</td><td>0.318</td><td>0.309</td><td>0.353</td><td>0.310</td><td>0.353</td><td>0.327</td><td>0.328</td><td>0.333</td><td>0.371</td><td>0.305</td><td>0.345</td><td>0.584</td><td>0.527</td><td>0.447</td><td>0.453</td></tr><tr><td>${ECL}$</td><td>$\underline{0.179}$</td><td>0.268</td><td>0.178</td><td>0.273</td><td>0.176</td><td>0.275</td><td>0.181</td><td>0.277</td><td>0.402</td><td>0.453</td><td>0.266</td><td>0.353</td><td>0.346</td><td>0.404</td><td>0.627</td><td>0.603</td><td>0.800</td><td>0.685</td><td>0.878</td><td>0.725</td><td>1.281</td><td>0.929</td><td>1.289</td><td>0.904</td></tr><tr><td>Traffic</td><td>$\underline{0.423}$</td><td>$\underline{0.298}$</td><td>0.434</td><td>0.305</td><td>0.450</td><td>0.317</td><td>0.418</td><td>0.296</td><td>0.867</td><td>0.493</td><td>0.676</td><td>0.423</td><td>0.833</td><td>0.502</td><td>1.526</td><td>0.839</td><td>1.859</td><td>0.927</td><td>1.557</td><td>0.795</td><td>1.591</td><td>0.832</td><td>1.618</td><td>0.851</td></tr><tr><td>${1}^{\text{st}}$ Count</td><td>5</td><td/><td/><td>2</td><td/><td>1</td><td/><td>1</td><td/><td>0</td><td/><td>0</td><td/><td>0</td><td/><td>0</td><td/><td>0</td><td>0</td><td/><td/><td>0</td><td/><td>0</td></tr></table>

<table><tbody><tr><td>方法</td><td colspan="2">TIME-大语言模型（我们的方法）</td><td colspan="2">GPT4TS 2023a)</td><td colspan="2">D线性模型（2023年）</td><td colspan="2">PatchTST（2023年）</td><td colspan="2">时间网络（2023年）</td><td colspan="2">FEDformer（2022年）</td><td colspan="2">Autoformer（2021年）</td><td colspan="2">Stationary（2022年）</td><td colspan="2">ETSformer（2022年）</td><td colspan="2">LightTS（2022a年）</td><td colspan="2">Informer（2021年）</td><td colspan="2">改革者2020（Reformer 2020）</td></tr><tr><td>指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td>${ETTh1}$</td><td>0.627</td><td>0.543</td><td>0.681</td><td>$\underline{0.560}$</td><td>0.750</td><td>0.611</td><td>0.694</td><td>0.569</td><td>0.925</td><td>0.647</td><td>$\underline{0.658}$</td><td>0.562</td><td>0.722</td><td>0.598</td><td>0.943</td><td>0.646</td><td>1.189</td><td>0.839</td><td>1.451</td><td>0.903</td><td>1.225</td><td>0.817</td><td>1.241</td><td>0.835</td></tr><tr><td>${ETTh2}$</td><td>0.382</td><td>0.418</td><td>$\underline{0.400}$</td><td>0.433</td><td>0.694</td><td>0.577</td><td>0.827</td><td>0.615</td><td>0.439</td><td>0.448</td><td>0.463</td><td>0.454</td><td>0.441</td><td>0.457</td><td>0.470</td><td>0.489</td><td>0.809</td><td>0.681</td><td>3.206</td><td>1.268</td><td>3.922</td><td>1.653</td><td>3.527</td><td>1.472</td></tr><tr><td>ETTm1</td><td>$\underline{0.425}$</td><td>$\underline{0.434}$</td><td>0.472</td><td>0.450</td><td>0.400</td><td>0.417</td><td>0.526</td><td>0.476</td><td>0.717</td><td>0.561</td><td>0.730</td><td>0.592</td><td>0.796</td><td>0.620</td><td>0.857</td><td>0.598</td><td>1.125</td><td>0.782</td><td>1.123</td><td>0.765</td><td>1.163</td><td>0.791</td><td>1.264</td><td>0.826</td></tr><tr><td>${ETTm2}$</td><td>0.274</td><td>0.323</td><td>$\underline{0.308}$</td><td>$\underline{0.346}$</td><td>0.399</td><td>0.426</td><td>0.314</td><td>0.352</td><td>0.344</td><td>0.372</td><td>0.381</td><td>0.404</td><td>0.388</td><td>0.433</td><td>0.341</td><td>0.372</td><td>0.534</td><td>0.547</td><td>1.415</td><td>0.871</td><td>3.658</td><td>1.489</td><td>3.581</td><td>1.487</td></tr><tr><td>天气</td><td>0.260</td><td>0.309</td><td>0.263</td><td>0.301</td><td>0.263</td><td>0.308</td><td>0.269</td><td>$\underline{0.303}$</td><td>0.298</td><td>0.318</td><td>0.309</td><td>0.353</td><td>0.310</td><td>0.353</td><td>0.327</td><td>0.328</td><td>0.333</td><td>0.371</td><td>0.305</td><td>0.345</td><td>0.584</td><td>0.527</td><td>0.447</td><td>0.453</td></tr><tr><td>${ECL}$</td><td>$\underline{0.179}$</td><td>0.268</td><td>0.178</td><td>0.273</td><td>0.176</td><td>0.275</td><td>0.181</td><td>0.277</td><td>0.402</td><td>0.453</td><td>0.266</td><td>0.353</td><td>0.346</td><td>0.404</td><td>0.627</td><td>0.603</td><td>0.800</td><td>0.685</td><td>0.878</td><td>0.725</td><td>1.281</td><td>0.929</td><td>1.289</td><td>0.904</td></tr><tr><td>交通流量</td><td>$\underline{0.423}$</td><td>$\underline{0.298}$</td><td>0.434</td><td>0.305</td><td>0.450</td><td>0.317</td><td>0.418</td><td>0.296</td><td>0.867</td><td>0.493</td><td>0.676</td><td>0.423</td><td>0.833</td><td>0.502</td><td>1.526</td><td>0.839</td><td>1.859</td><td>0.927</td><td>1.557</td><td>0.795</td><td>1.591</td><td>0.832</td><td>1.618</td><td>0.851</td></tr><tr><td>${1}^{\text{st}}$ 计数</td><td>5</td><td></td><td></td><td>2</td><td></td><td>1</td><td></td><td>1</td><td></td><td>0</td><td></td><td>0</td><td></td><td>0</td><td></td><td>0</td><td></td><td>0</td><td>0</td><td></td><td></td><td>0</td><td></td><td>0</td></tr></tbody></table>

<!-- Media -->

such as PatchTST, DLinear, and TimesNet, our average enhancements surpass 8%, 12%, and 33% w.r.t. MSE. Analogous trends are discernible in the $5\%$ few-shot learning scenarios,where our average advancement over GPT4TS exceeds 5%. When compared with PatchTST, DLinear, and TimesNet, TIME-LLM manifests a striking average improvement of over 20%.

例如PatchTST、DLinear和TimesNet，我们在均方误差（MSE）方面的平均提升幅度分别超过了8%、12%和33%。在$5\%$少样本学习场景中也可观察到类似趋势，我们相对于GPT4TS的平均提升幅度超过了5%。与PatchTST、DLinear和TimesNet相比，TIME-LLM展现出了显著的平均超过20%的提升。

### 4.4 Zero-shot Forecasting

### 4.4 零样本预测

<!-- Media -->

Table 5: Zero-shot learning results. Red: the best, Blue: the second best. Appendix E shows our detailed results.

表5：零样本学习结果。红色：最佳，蓝色：次佳。附录E展示了我们的详细结果。

<table><tr><td>Methods</td><td colspan="2">TIME-LLM (Ours)</td><td colspan="2">GPT4TS 2023a)</td><td colspan="2">LLMTime 2023)</td><td colspan="2">DLinear 2023)</td><td colspan="2">PatchTST 2023)</td><td colspan="2">TimesNet 2023)</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>${ETTh1} \rightarrow  {ETTh2}$</td><td>0.353</td><td>0.387</td><td>0.406</td><td>0.422</td><td>0.992</td><td>0.708</td><td>0.493</td><td>0.488</td><td>$\underline{0.380}$</td><td>0.405</td><td>0.421</td><td>0.431</td></tr><tr><td>${ETTh1} \rightarrow  {ETTm2}$</td><td>0.273</td><td>0.340</td><td>0.325</td><td>0.363</td><td>1.867</td><td>0.869</td><td>0.415</td><td>0.452</td><td>0.314</td><td>0.360</td><td>0.327</td><td>0.361</td></tr><tr><td>${ETTh2} \rightarrow  {ETTh1}$</td><td>0.479</td><td>0.474</td><td>0.757</td><td>0.578</td><td>1.961</td><td>0.981</td><td>0.703</td><td>0.574</td><td>0.565</td><td>$\underline{0.513}$</td><td>0.865</td><td>0.621</td></tr><tr><td>${ETTh2} \rightarrow  {ETTm2}$</td><td>0.272</td><td>0.341</td><td>0.335</td><td>0.370</td><td>1.867</td><td>0.869</td><td>0.328</td><td>0.386</td><td>0.325</td><td>$\underline{0.365}$</td><td>0.342</td><td>0.376</td></tr><tr><td>${ETTm1} \rightarrow  {ETTh2}$</td><td>0.381</td><td>0.412</td><td>$\underline{0.433}$</td><td>0.439</td><td>0.992</td><td>0.708</td><td>0.464</td><td>0.475</td><td>0.439</td><td>$\underline{0.438}$</td><td>0.457</td><td>0.454</td></tr><tr><td>${ETTm1} \rightarrow  {ETTm2}$</td><td>0.268</td><td>0.320</td><td>0.313</td><td>0.348</td><td>1.867</td><td>0.869</td><td>0.335</td><td>0.389</td><td>$\underline{0.296}$</td><td>0.334</td><td>0.322</td><td>0.354</td></tr><tr><td>${ETTm2} \rightarrow  {ETTh2}$</td><td>0.354</td><td>0.400</td><td>0.435</td><td>0.443</td><td>0.992</td><td>0.708</td><td>0.455</td><td>0.471</td><td>$\underline{0.409}$</td><td>$\underline{0.425}$</td><td>0.435</td><td>0.443</td></tr><tr><td>${ETTm2} \rightarrow  {ETTm1}$</td><td>0.414</td><td>0.438</td><td>0.769</td><td>0.567</td><td>1.933</td><td>0.984</td><td>0.649</td><td>0.537</td><td>0.568</td><td>0.492</td><td>0.769</td><td>0.567</td></tr></table>

<table><tbody><tr><td>方法</td><td colspan="2">TIME-大语言模型（我们的方法）</td><td colspan="2">GPT4TS（2023a）</td><td colspan="2">大语言模型时间（2023）</td><td colspan="2">线性差分模型（2023）</td><td colspan="2">补丁时间序列变换器（2023）</td><td colspan="2">时间网络（TimesNet 2023）</td></tr><tr><td>指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td>${ETTh1} \rightarrow  {ETTh2}$</td><td>0.353</td><td>0.387</td><td>0.406</td><td>0.422</td><td>0.992</td><td>0.708</td><td>0.493</td><td>0.488</td><td>$\underline{0.380}$</td><td>0.405</td><td>0.421</td><td>0.431</td></tr><tr><td>${ETTh1} \rightarrow  {ETTm2}$</td><td>0.273</td><td>0.340</td><td>0.325</td><td>0.363</td><td>1.867</td><td>0.869</td><td>0.415</td><td>0.452</td><td>0.314</td><td>0.360</td><td>0.327</td><td>0.361</td></tr><tr><td>${ETTh2} \rightarrow  {ETTh1}$</td><td>0.479</td><td>0.474</td><td>0.757</td><td>0.578</td><td>1.961</td><td>0.981</td><td>0.703</td><td>0.574</td><td>0.565</td><td>$\underline{0.513}$</td><td>0.865</td><td>0.621</td></tr><tr><td>${ETTh2} \rightarrow  {ETTm2}$</td><td>0.272</td><td>0.341</td><td>0.335</td><td>0.370</td><td>1.867</td><td>0.869</td><td>0.328</td><td>0.386</td><td>0.325</td><td>$\underline{0.365}$</td><td>0.342</td><td>0.376</td></tr><tr><td>${ETTm1} \rightarrow  {ETTh2}$</td><td>0.381</td><td>0.412</td><td>$\underline{0.433}$</td><td>0.439</td><td>0.992</td><td>0.708</td><td>0.464</td><td>0.475</td><td>0.439</td><td>$\underline{0.438}$</td><td>0.457</td><td>0.454</td></tr><tr><td>${ETTm1} \rightarrow  {ETTm2}$</td><td>0.268</td><td>0.320</td><td>0.313</td><td>0.348</td><td>1.867</td><td>0.869</td><td>0.335</td><td>0.389</td><td>$\underline{0.296}$</td><td>0.334</td><td>0.322</td><td>0.354</td></tr><tr><td>${ETTm2} \rightarrow  {ETTh2}$</td><td>0.354</td><td>0.400</td><td>0.435</td><td>0.443</td><td>0.992</td><td>0.708</td><td>0.455</td><td>0.471</td><td>$\underline{0.409}$</td><td>$\underline{0.425}$</td><td>0.435</td><td>0.443</td></tr><tr><td>${ETTm2} \rightarrow  {ETTm1}$</td><td>0.414</td><td>0.438</td><td>0.769</td><td>0.567</td><td>1.933</td><td>0.984</td><td>0.649</td><td>0.537</td><td>0.568</td><td>0.492</td><td>0.769</td><td>0.567</td></tr></tbody></table>

<!-- Media -->

Setups. Beyond few-shot learning, LLMs hold potential as effective zero-shot reasoners (Kojima et al., 2022). In this section, we evaluate the zero-shot learning capabilities of the reprogrammed LLM within the framework of cross-domain adaptation. Specifically, we examine how well a model performs on a dataset - when it is optimized on another dataset $\spadesuit$ ,where the model has not encountered any data samples from the dataset $\downarrow$ . Similar to few-shot learning,we use long-term forecasting protocol and evaluate on various cross-domain scenarios utilizing the ETT datasets.

设置。除了小样本学习之外，大语言模型（LLM）有潜力成为有效的零样本推理器（小岛等人，2022年）。在本节中，我们在跨领域适应的框架内评估重新编程的大语言模型的零样本学习能力。具体来说，我们研究当一个模型在另一个数据集$\spadesuit$上进行优化时，它在一个数据集上的表现如何，其中该模型从未遇到过来自数据集$\downarrow$的任何数据样本。与小样本学习类似，我们使用长期预测协议，并利用ETT数据集在各种跨领域场景下进行评估。

Results. Our brief results are in Tab. 5. TIME-LLM consistently outperforms the most competitive baselines by a large margin, over 14.2% w.r.t. the second-best in MSE reduction. Considering the few-shot results, we observe that reprogramming an LLM tends to yield significantly better results in data scarcity scenarios. For example, our overall error reductions w.r.t. GPT4TS in 10% few-shot forecasting, 5% few-shot forecasting, and zero-shot forecasting are increasing gradually: 7.7%, 8.4%, and 22%. Even when benchmarked against LLMTime, the most recent approach in this field, with the backbone LLM of comparable size (7B), TIME-LLM shows a substantial improvement exceeding 75%. We attribute this to our approach being better at activating the LLM's knowledge transfer and reasoning capabilities in a resource-efficient manner when performing time series tasks.

结果。我们的简要结果见表5。TIME - LLM始终大幅优于最具竞争力的基线模型，在均方误差（MSE）降低方面，相较于次优模型高出超过14.2%。考虑到小样本学习结果，我们发现对大语言模型（LLM）进行重新编程在数据稀缺的场景中往往能产生显著更好的结果。例如，在10%小样本预测、5%小样本预测和零样本预测中，我们的模型相对于GPT4TS的总体误差降低率逐渐增加，分别为7.7%、8.4%和22%。即使与该领域最新的方法LLMTime（其主干大语言模型规模相当，为70亿参数）进行对比，TIME - LLM也显示出超过75%的显著改进。我们将此归因于我们的方法在执行时间序列任务时，能够以资源高效的方式更好地激活大语言模型的知识迁移和推理能力。

### 4.5 MODEL ANALYSIS

### 4.5 模型分析

Language Model Variants. We compare two representative backbones with varying capacities (A.1-4 in Tab. 6). Our results indicate that the scaling law retain after the LLM reprogramming. We adopt Llama-7B by default in its full capacity, which manifestly outperforms its 1/4 capacity variant (A.2; inclusive of the first 8 Transformer layers) by 14.5%. An average MSE reduction of 14.7% is observed over GPT-2 (A.3), which slightly outperforms its variant GPT-2 (6) (A.4) by 2.7%.

语言模型变体。我们比较了两种具有不同能力的代表性主干模型（表6中的A.1 - 4）。我们的结果表明，在大语言模型（LLM）重新编程后，缩放定律仍然成立。我们默认采用全容量的Llama - 7B模型，它的表现明显优于其1/4容量的变体（A.2；包含前8个Transformer层），领先幅度为14.5%。与GPT - 2（A.3）相比，均方误差（MSE）平均降低了14.7%，GPT - 2（A.3）的表现略优于其变体GPT - 2 (6)（A.4），领先幅度为2.7%。

Cross-modality Alignment. Our results in Tab. 6 indicate that ablating either patch reprogramming or Prompt-as-Prefix hurts knowledge transfer in reprogramming the LLM for effective time series forecasting. In the absence of representation alignment (B.1), we observe a notable average performance degradation of ${9.2}\%$ ,which becomes more pronounced (exceeding ${17}\%$ ) in few-shot tasks. In TIME-LLM, the act of prompting stands as a pivotal element in harnessing the LLM's capacity for understanding the inputs and tasks. Ablation of this component (B.2) results in over $\mathbf{8}\%$ and 19% degradation in standard and few-shot forecasting tasks, respectively. We find that removing the input statistics (C.1) hurts the most, resulting in an average increase of 10.2% MSE. This is anticipated as external knowledge can be naturally incorporated via prompting to facilitate the learning and inference. Additionally, providing the LLM with clear task instructions and input context (e.g., dataset captioning) is also beneficial (i.e., C. 2 and C. 1; eliciting over 7.7% and 9.6%, respectively).

跨模态对齐。表6中的结果表明，在对大语言模型（LLM）进行重新编程以实现有效的时间序列预测时，去除补丁重新编程（patch reprogramming）或“提示即前缀”（Prompt-as-Prefix）都会损害知识迁移。在缺乏表征对齐（B.1）的情况下，我们观察到平均性能显著下降了${9.2}\%$，在少样本任务中这种下降更为明显（超过${17}\%$）。在TIME - LLM中，提示操作是利用大语言模型理解输入和任务能力的关键要素。去除这一组件（B.2）后，标准预测任务和少样本预测任务的性能分别下降了超过$\mathbf{8}\%$和19%。我们发现，去除输入统计信息（C.1）的影响最大，导致均方误差（MSE）平均增加了10.2%。这是可以预见的，因为外部知识可以通过提示自然地融入，以促进学习和推理。此外，为大语言模型提供清晰的任务指令和输入上下文（例如，数据集标注）也是有益的（即C. 2和C. 1；分别能提升超过7.7%和9.6%）。

<!-- Media -->

Table 6: Ablations on ETTh1 and ETTm1 in predicting 96 and 192 steps ahead (MSE reported). Red: the best.

表6：在ETTh1和ETTm1数据集上提前96步和192步预测的消融实验（报告均方误差）。红色：最优结果。

<table><tr><td rowspan="2">Variant</td><td colspan="4">Long-term Forecasting</td><td colspan="4">Few-shot Forecasting</td></tr><tr><td>ETTh1-96</td><td>ETTh1-192</td><td>ETTm1-96</td><td>ETThm1-192</td><td>ETTh1-96</td><td>ETTh1-192</td><td>ETTm1-96</td><td>ETThm1-192</td></tr><tr><td>A.1 Llama (Default; 32)</td><td>0.362</td><td>0.398</td><td>0.272</td><td>0.310</td><td>0.448</td><td>0.484</td><td>0.346</td><td>0.373</td></tr><tr><td>A.2 Llama (8)</td><td>0.389</td><td>0.412</td><td>0.297</td><td>0.329</td><td>0.567</td><td>0.632</td><td>0.451</td><td>0.490</td></tr><tr><td>A.3 GPT-2 (12)</td><td>0.385</td><td>0.419</td><td>0.306</td><td>0.332</td><td>0.548</td><td>0.617</td><td>0.447</td><td>0.509</td></tr><tr><td>A.4 GPT-2 (6)</td><td>0.394</td><td>0.427</td><td>0.311</td><td>0.342</td><td>0.571</td><td>0.640</td><td>0.468</td><td>0.512</td></tr><tr><td>B. 1 w/o Patch Reprogramming</td><td>0.410</td><td>0.412</td><td>0.310</td><td>0.342</td><td>0.498</td><td>0.570</td><td>0.445</td><td>0.487</td></tr><tr><td>B. $2\mathrm{\;w}/\mathrm{o}$ Prompt-as-Prefix</td><td>0.398</td><td>0.423</td><td>0.298</td><td>0.339</td><td>0.521</td><td>0.617</td><td>0.432</td><td>0.481</td></tr><tr><td>C.1 w/o Dataset Context</td><td>0.402</td><td>0.417</td><td>0.298</td><td>0.331</td><td>0.491</td><td>0.538</td><td>0.392</td><td>0.447</td></tr><tr><td>C. $2\mathrm{\;w}/\mathrm{o}$ Task Instruction</td><td>0.388</td><td>0.420</td><td>0.285</td><td>0.327</td><td>0.476</td><td>0.529</td><td>0.387</td><td>0.439</td></tr><tr><td>C.3w/o Statistical Context</td><td>0.391</td><td>0.419</td><td>0.279</td><td>0.347</td><td>0.483</td><td>0.547</td><td>0.421</td><td>0.461</td></tr></table>

<table><tbody><tr><td rowspan="2">变体</td><td colspan="4">长期预测</td><td colspan="4">小样本预测</td></tr><tr><td>ETTh1-96</td><td>ETTh1-192</td><td>ETTm1-96</td><td>ETThm1 - 192</td><td>ETTh1-96</td><td>ETTh1-192</td><td>ETTm1-96</td><td>ETThm1 - 192</td></tr><tr><td>A.1 大羊驼（默认；32）(Llama)</td><td>0.362</td><td>0.398</td><td>0.272</td><td>0.310</td><td>0.448</td><td>0.484</td><td>0.346</td><td>0.373</td></tr><tr><td>A.2 大羊驼（8）(Llama)</td><td>0.389</td><td>0.412</td><td>0.297</td><td>0.329</td><td>0.567</td><td>0.632</td><td>0.451</td><td>0.490</td></tr><tr><td>A.3 GPT - 2（12）</td><td>0.385</td><td>0.419</td><td>0.306</td><td>0.332</td><td>0.548</td><td>0.617</td><td>0.447</td><td>0.509</td></tr><tr><td>A.4 GPT - 2（6）</td><td>0.394</td><td>0.427</td><td>0.311</td><td>0.342</td><td>0.571</td><td>0.640</td><td>0.468</td><td>0.512</td></tr><tr><td>B. 1 无补丁重编程</td><td>0.410</td><td>0.412</td><td>0.310</td><td>0.342</td><td>0.498</td><td>0.570</td><td>0.445</td><td>0.487</td></tr><tr><td>B. $2\mathrm{\;w}/\mathrm{o}$ 提示作为前缀</td><td>0.398</td><td>0.423</td><td>0.298</td><td>0.339</td><td>0.521</td><td>0.617</td><td>0.432</td><td>0.481</td></tr><tr><td>C.1 无数据集上下文</td><td>0.402</td><td>0.417</td><td>0.298</td><td>0.331</td><td>0.491</td><td>0.538</td><td>0.392</td><td>0.447</td></tr><tr><td>C. $2\mathrm{\;w}/\mathrm{o}$ 任务指令</td><td>0.388</td><td>0.420</td><td>0.285</td><td>0.327</td><td>0.476</td><td>0.529</td><td>0.387</td><td>0.439</td></tr><tr><td>C.3 无统计上下文</td><td>0.391</td><td>0.419</td><td>0.279</td><td>0.347</td><td>0.483</td><td>0.547</td><td>0.421</td><td>0.461</td></tr></tbody></table>

Table 7: Efficiency analysis of TIME-LLM on ETTh1 in forecasting different steps ahead.

表7：TIME-LLM在ETTh1数据集上不同超前预测步长的效率分析。

<table><tr><td>Length$\alpha$</td><td colspan="3">ETTh1-96</td><td colspan="3">ETTh1-192</td><td colspan="3">ETTh1-336</td><td colspan="3">ETTh1-512</td></tr><tr><td>Metric</td><td>Param. (M)</td><td>Mem. (MiB)</td><td>Speed(s/iter)</td><td>Param. (M)</td><td>Mem. (MiB)</td><td>Speed(s/iter)</td><td>Param. (M)</td><td>Mem. (MiB)</td><td>Speed(s/iter)</td><td>Param. (M)</td><td>Mem.(MiB)</td><td>Speed(s/iter)</td></tr><tr><td>D.1 LLama (32)</td><td>3404.53</td><td>32136</td><td>0.517</td><td>3404.57</td><td>33762</td><td>0.582</td><td>3404.62</td><td>37988</td><td>0.632</td><td>3404.69</td><td>39004</td><td>0.697</td></tr><tr><td>D. 2 LLama (8)</td><td>975.83</td><td>11370</td><td>0.184</td><td>975.87</td><td>12392</td><td>0.192</td><td>975.92</td><td>13188</td><td>0.203</td><td>976.11</td><td>13616</td><td>0.217</td></tr><tr><td>D. 3 w/o LLM</td><td>6.39</td><td>3678</td><td>0.046</td><td>6.42</td><td>3812</td><td>0.087</td><td>6.48</td><td>3960</td><td>0.093</td><td>6.55</td><td>4176</td><td>0.129</td></tr></table>

<table><tbody><tr><td>长度$\alpha$</td><td colspan="3">ETTh1-96</td><td colspan="3">ETTh1-192</td><td colspan="3">ETTh1-336</td><td colspan="3">ETTh1-512</td></tr><tr><td>指标</td><td>参数 (M)</td><td>内存 (MiB)</td><td>速度 (秒/迭代)</td><td>参数 (M)</td><td>内存 (MiB)</td><td>速度 (秒/迭代)</td><td>参数 (M)</td><td>内存 (MiB)</td><td>速度 (秒/迭代)</td><td>参数 (M)</td><td>内存 (MiB)</td><td>速度 (秒/迭代)</td></tr><tr><td>D.1 羊驼模型 (32)</td><td>3404.53</td><td>32136</td><td>0.517</td><td>3404.57</td><td>33762</td><td>0.582</td><td>3404.62</td><td>37988</td><td>0.632</td><td>3404.69</td><td>39004</td><td>0.697</td></tr><tr><td>D. 2 羊驼模型 (8)</td><td>975.83</td><td>11370</td><td>0.184</td><td>975.87</td><td>12392</td><td>0.192</td><td>975.92</td><td>13188</td><td>0.203</td><td>976.11</td><td>13616</td><td>0.217</td></tr><tr><td>D. 无大语言模型（LLM）的3次</td><td>6.39</td><td>3678</td><td>0.046</td><td>6.42</td><td>3812</td><td>0.087</td><td>6.48</td><td>3960</td><td>0.093</td><td>6.55</td><td>4176</td><td>0.129</td></tr></tbody></table>

<!-- Media -->

Reprogramming Interpretation. We provide a case study on ETTh1 of reprogramming 48 time series patches with 100 text prototypes in Fig. 5. The top 4 subplots visualize the optimization of reprogramming space from randomly-initialized (a) to well-optimized (d). We find only a small set of prototypes (columns) participated in reprogramming the input patches (rows) in subplot (e). Also, patches undergo different representations through varying combinations of prototypes. This indicates: (1) text prototypes learn to summarize language cues, and a select few are highly relevant for representing information in local time series patches, which we visualize by randomly selecting 10 in subplot (f). Our results suggest a high relevance to the words that describe time series properties (i.e., word sets 1 and 2); (2) patches usually have different underlying semantics, necessitating different prototypes to represent.

重编程解读。我们在图5中提供了一个关于ETTh1的案例研究，该研究使用100个文本原型对48个时间序列片段进行重编程。前4个子图展示了重编程空间从随机初始化（a）到优化良好（d）的优化过程。我们发现，在子图（e）中，只有一小部分原型（列）参与了对输入片段（行）的重编程。此外，片段通过原型的不同组合呈现出不同的表示形式。这表明：（1）文本原型学会了总结语言线索，并且少数原型与表示局部时间序列片段中的信息高度相关，我们在子图（f）中随机选择了10个进行可视化展示。我们的结果表明，这些原型与描述时间序列属性的词汇（即词汇集1和2）高度相关；（2）片段通常具有不同的潜在语义，因此需要不同的原型来表示。

<!-- Media -->

<!-- figureText: (f) Visualization of 10 different learned text prototypes Word Set 1: \{'periodic', 'seasonal', ...\} Word Set 2: \{'quantile', 'average', ...\} -->

<img src="https://cdn.noedgeai.com/01957f5d-9de5-7380-ae8c-08021ee888f7_8.jpg?x=894&y=875&w=594&h=410&r=0"/>

Figure 5: A showcase of patch reprogramming.

图5：片段重编程展示。

<!-- Media -->

Reprogramming Efficiency. Tab. 7 provides an overall efficiency analysis of TIME-LLM with and without the backbone LLM. Our proposed reprogramming network itself (D.3) is lightweight in activating the LLM's ability for time series forecasting (i.e., fewer than 6.6 million trainable parameters; only around $\mathbf{{0.2}}\%$ of the total parameters in Llama-7B),and the overall efficiency of TIME-LLM is actually capped by the leveraged backbones (e.g., D.1 and D.2). This is favorable even compared to the parameter-efficient fine-tuning methods (e.g., QLoRA (Dettmers et al., 2023)) in balancing task performance and efficiency.

重编程效率。表7提供了使用和不使用主干大语言模型（LLM）时TIME - LLM的整体效率分析。我们提出的重编程网络本身（D.3）在激活大语言模型的时间序列预测能力方面是轻量级的（即可训练参数少于660万个；仅约为Llama - 7B总参数的$\mathbf{{0.2}}\%$），并且TIME - LLM的整体效率实际上受所利用的主干模型（例如D.1和D.2）的限制。即使与参数高效微调方法（例如QLoRA（德特默斯等人，2023年））相比，这在平衡任务性能和效率方面也是有利的。

## 5 CONCLUSION AND FUTURE WORK

## 5 结论与未来工作

TIME-LLM shows promise in adapting frozen large language models for time series forecasting by reprogramming time series data into text prototypes more natural for LLMs and providing natural language guidance via Prompt-as-Prefix to augment reasoning. Evaluations demonstrate the adapted LLMs can outperform specialized expert models, indicating their potential as effective time series machines. Our results also provide a novel insight that time series forecasting can be cast as yet another "language" task that can be tackled by an off-the-shelf LLM to achieve state-of-the-art performance through our Time-LLM framework. Further research should explore optimal reprogramming representations, enrich LLMs with explicit time series knowledge through continued pre-training, and build towards multimodal models with joint reasoning across time series, natural language, and other modalities. Furthermore, applying the reprogramming framework to equip LLMs with broader time series analytical abilities or other new capabilities should also be considered.

TIME - LLM在将冻结的大语言模型应用于时间序列预测方面展现出了潜力，它通过将时间序列数据重新编码为对大语言模型来说更自然的文本原型，并通过“提示前缀法”（Prompt - as - Prefix）提供自然语言指导来增强推理能力。评估表明，经过适配的大语言模型可以超越专门的专家模型，这表明它们有潜力成为有效的时间序列处理工具。我们的研究结果还提供了一个新颖的见解，即时间序列预测可以被视为另一种“语言”任务，通过我们的Time - LLM框架，现成的大语言模型可以处理该任务并达到最先进的性能。未来的研究应探索最优的重新编码表示方法，通过持续预训练为大语言模型注入明确的时间序列知识，并构建能够跨时间序列、自然语言和其他模态进行联合推理的多模态模型。此外，还应考虑应用重新编码框架，使大语言模型具备更广泛的时间序列分析能力或其他新能力。

## REFERENCES

## 参考文献

Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.

白少杰（Shaojie Bai）、J·齐科·科尔特（J Zico Kolter）和弗拉德连·科尔图恩（Vladlen Koltun）。通用卷积和循环网络用于序列建模的实证评估。预印本arXiv:1803.01271，2018年。

George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series analysis: forecasting and control. John Wiley & Sons, 2015.

乔治·E·P·博克斯（George EP Box）、格威利姆·M·詹金斯（Gwilym M Jenkins）、格雷戈里·C·赖因塞尔（Gregory C Reinsel）和格雷塔·M·永（Greta M Ljung）。《时间序列分析：预测与控制》。约翰·威利父子出版公司（John Wiley & Sons），2015年。

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877-1901, 2020.

汤姆·布朗（Tom Brown）、本杰明·曼（Benjamin Mann）、尼克·赖德（Nick Ryder）、梅兰妮·苏比亚（Melanie Subbiah）、贾里德·D·卡普兰（Jared D Kaplan）、普拉富拉·达里瓦尔（Prafulla Dhariwal）、阿温德·尼尔坎坦（Arvind Neelakantan）、普拉纳夫·沙姆（Pranav Shyam）、吉里什·萨斯特里（Girish Sastry）、阿曼达·阿斯凯尔（Amanda Askell）等。《语言模型是少样本学习者》。《神经信息处理系统进展》，第33卷：1877 - 1901页，2020年。

Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza, Max Mergenthaler, and Artur Dubrawski. N-hits: Neural hierarchical interpolation for time series forecasting. Proceedings of the AAAI Conference on Artificial Intelligence, 2023a.

克里斯蒂安·查卢（Cristian Challu）、金·G·奥利瓦雷斯（Kin G Olivares）、鲍里斯·N·奥列什金（Boris N Oreshkin）、费德里科·加尔萨（Federico Garza）、马克斯·默根塔勒（Max Mergenthaler）和阿图尔·杜布拉夫斯基（Artur Dubrawski）。《N - HITS：用于时间序列预测的神经分层插值》。《AAAI人工智能会议论文集》，2023a。

Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco, and Artur Dubrawski. Nhits: neural hierarchical interpolation for time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 6989-6997, 2023b.

克里斯蒂安·查卢（Cristian Challu）、金·G·奥利瓦雷斯（Kin G Olivares）、鲍里斯·N·奥列什金（Boris N Oreshkin）、费德里科·加尔萨·拉米雷斯（Federico Garza Ramirez）、马克斯·默根塔勒·坎塞科（Max Mergenthaler Canseco）和阿图尔·杜布拉夫斯基（Artur Dubrawski）。Nhits：用于时间序列预测的神经分层插值法。收录于《AAAI人工智能会议论文集》，第37卷，第6989 - 6997页，2023b。

Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms. arXiv preprint arXiv:2308.08469, 2023.

张晴（Ching Chang）、彭文智（Wen - Chih Peng）和陈天富（Tien - Fu Chen）。Llm4ts：使用预训练大语言模型进行时间序列预测的两阶段微调。预印本arXiv:2308.08469，2023。

Pin-Yu Chen. Model reprogramming: Resource-efficient cross-domain machine learning. arXiv preprint arXiv:2202.10629, 2022.

陈品宇（Pin - Yu Chen）。模型重编程：资源高效的跨领域机器学习。预印本arXiv:2202.10629，2022。

Zhixuan Chu, Hongyan Hao, Xin Ouyang, Simeng Wang, Yan Wang, Yue Shen, Jinjie Gu, Qing Cui, Longfei Li, Siqiao Xue, et al. Leveraging large language models for pre-trained recommender systems. arXiv preprint arXiv:2308.10837, 2023.

褚志轩（Zhixuan Chu）、郝红岩（Hongyan Hao）、欧阳鑫（Xin Ouyang）、王思萌（Simeng Wang）、王艳（Yan Wang）、沈悦（Yue Shen）、顾金杰（Jinjie Gu）、崔清（Qing Cui）、李龙飞（Longfei Li）、薛思乔（Siqiao Xue）等。利用大语言模型构建预训练推荐系统。预印本arXiv:2308.10837，2023。

Shohreh Deldari, Hao Xue, Aaqib Saeed, Jiayuan He, Daniel V Smith, and Flora D Salim. Beyond just vision: A review on self-supervised representation learning on multimodal and temporal data. arXiv preprint arXiv:2206.02353, 2022.

肖赫雷·德尔达里（Shohreh Deldari）、薛浩（Hao Xue）、阿基布·赛义德（Aaqib Saeed）、何家源（Jiayuan He）、丹尼尔·V·史密斯（Daniel V Smith）和弗洛拉·D·萨利姆（Flora D Salim）。超越视觉：多模态和时间序列数据的自监督表征学习综述。预印本arXiv:2206.02353，2022年。

Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 2023.

蒂姆·德特默斯（Tim Dettmers）、阿尔蒂多罗·帕尼奥尼（Artidoro Pagnoni）、阿里·霍尔茨曼（Ari Holtzman）和卢克·泽特尔莫耶（Luke Zettlemoyer）。QLoRA：量化大语言模型的高效微调。《神经信息处理系统进展》，2023年。

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2018.

雅各布·德夫林（Jacob Devlin）、张明伟（Ming-Wei Chang）、肯顿·李（Kenton Lee）和克里斯蒂娜·图托纳娃（Kristina Toutanova）。BERT：用于语言理解的深度双向Transformer预训练。《2019年北美计算语言学协会人类语言技术会议论文集》，2018年。

Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. Transfer learning for time series classification. In IEEE International Conference on Big Data, pp. 1367-1376. IEEE, 2018.

哈桑·伊斯梅尔·法瓦兹（Hassan Ismail Fawaz）、热尔曼·福雷斯捷（Germain Forestier）、乔纳森·韦伯（Jonathan Weber）、拉桑·伊杜姆加尔（Lhassane Idoumghar）和皮埃尔 - 阿兰·米勒（Pierre-Alain Muller）。时间序列分类的迁移学习。《IEEE国际大数据会议》，第1367 - 1376页。IEEE，2018年。

Nate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. Advances in Neural Information Processing Systems, 2023.

内特·格鲁弗（Nate Gruver）、马克·安东·芬齐（Marc Anton Finzi）、邱世凯（Shikai Qiu）和安德鲁·戈登·威尔逊（Andrew Gordon Wilson）。大语言模型是零样本时间序列预测器。《神经信息处理系统进展》，2023年。

Julien Herzen, Francesco Lassig, Samuele Giuliano Piazzetta, Thomas Neuer, Leo Tafti, Guillaume Raille, Tomas Van Pottelbergh, Marek Pasieka, Andrzej Skrodzki, Nicolas Huguenin, et al. Darts: User-friendly modern machine learning for time series. The Journal of Machine Learning Research, 23(1):5442-5447, 2022.

朱利安·赫尔岑（Julien Herzen）、弗朗西斯科·拉西格（Francesco Lassig）、萨穆埃莱·朱利亚诺·皮亚泽塔（Samuele Giuliano Piazzetta）、托马斯·诺伊（Thomas Neuer）、利奥·塔夫蒂（Leo Tafti）、纪尧姆·拉耶（Guillaume Raille）、托马斯·范·波特尔贝赫（Tomas Van Pottelbergh）、马雷克·帕西卡（Marek Pasieka）、安杰伊·斯克罗德茨基（Andrzej Skrodzki）、尼古拉斯·于格南（Nicolas Huguenin）等。《飞镖：面向时间序列的用户友好型现代机器学习》。《机器学习研究杂志》，23(1):5442 - 5447，2022年。

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.

塞普·霍赫赖特（Sepp Hochreiter）和尤尔根·施密德胡伯（Jürgen Schmidhuber）。长短期记忆网络。《神经计算》，9(8): 1735 - 1780，1997年。

Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Geoffrey I Webb, Irwin King, and Shirui Pan. A survey on graph neural networks for time series: Forecasting, classification, imputation, and anomaly detection. arXiv preprint arXiv:2307.03759, 2023a.

金明（Ming Jin）、许焕仪（Huan Yee Koh）、文青松（Qingsong Wen）、丹尼尔·赞邦（Daniele Zambon）、切萨雷·阿利皮（Cesare Alippi）、杰弗里·I·韦伯（Geoffrey I Webb）、金耀初（Irwin King）和潘仕锐（Shirui Pan）。图神经网络在时间序列中的应用综述：预测、分类、插补和异常检测。预印本arXiv:2307.03759，2023a。

Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, et al. Large models for time series and spatio-temporal data: A survey and outlook. arXiv preprint arXiv:2310.10196, 2023b.

金明（Ming Jin）、文青松（Qingsong Wen）、梁宇轩（Yuxuan Liang）、张朝利（Chaoli Zhang）、薛思乔（Siqiao Xue）、王雪（Xue Wang）、詹姆斯·张（James Zhang）、王毅（Yi Wang）、陈海峰（Haifeng Chen）、李小丽（Xiaoli Li）等。时间序列和时空数据的大模型：综述与展望。预印本arXiv:2310.10196，2023b。

Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2021.

金泰成（Taesung Kim）、金珍熙（Jinhee Kim）、太允元（Yunwon Tae）、朴천복（Cheonbok Park）、崔章镐（Jang - Ho Choi）和朱在杰（Jaegul Choo）。用于应对分布偏移的准确时间序列预测的可逆实例归一化。发表于国际学习表征会议，2021年。

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.

迪德里克·P·金马（Diederik P. Kingma）和吉米·巴（Jimmy Ba）。Adam：一种随机优化方法。国际学习表征会议，2015年。

Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020.

尼基塔·基塔耶夫（Nikita Kitaev）、卢卡斯·凯泽（Łukasz Kaiser）和安塞尔姆·列夫斯卡亚（Anselm Levskaya）。《改革者：高效的Transformer》。发表于2020年国际学习表征会议。

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199-22213, 2022.

小岛武（Takeshi Kojima）、顾世翔（Shixiang Shane Gu）、马舍尔·里德（Machel Reid）、松尾丰（Yutaka Matsuo）和岩泽雄介（Yusuke Iwasawa）。《大语言模型是零样本推理器》。《神经信息处理系统进展》，35:22199 - 22213，2022年。

Michael Leonard. Promotional analysis and forecasting for demand planning: a practical time series approach. with exhibits, 1, 2001.

迈克尔·伦纳德（Michael Leonard）。《需求规划的促销分析与预测：实用的时间序列方法》，附图表，2001年第1版。

Na Li, Donald M Arnold, Douglas G Down, Rebecca Barty, John Blake, Fei Chiang, Tom Courtney, Marianne Waito, Rick Trifunov, and Nancy M Heddle. From demand forecasting to inventory ordering decisions for red blood cells through integrating machine learning, statistical modeling, and inventory optimization. Transfusion, 62(1):87-99, 2022.

李娜（Na Li）、唐纳德·M·阿诺德（Donald M Arnold）、道格拉斯·G·唐（Douglas G Down）、丽贝卡·巴蒂（Rebecca Barty）、约翰·布莱克（John Blake）、蒋飞（Fei Chiang）、汤姆·考特尼（Tom Courtney）、玛丽安·韦托（Marianne Waito）、里克·特里富诺夫（Rick Trifunov）和南希·M·赫德尔（Nancy M Heddle）。《通过集成机器学习、统计建模和库存优化，从需求预测到红细胞库存订购决策》。《输血》，62(1):87 - 99，2022年。

Hengbo Liu, Ziqing Ma, Linxiao Yang, Tian Zhou, Rui Xia, Yi Wang, Qingsong Wen, and Liang Sun. Sadi: A self-adaptive decomposed interpretable framework for electric load forecasting under extreme events. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2023a.

刘恒博（Hengbo Liu）、马子清（Ziqing Ma）、杨琳霄（Linxiao Yang）、周天（Tian Zhou）、夏瑞（Rui Xia）、王毅（Yi Wang）、文青松（Qingsong Wen）和孙亮（Liang Sun）。Sadi：极端事件下电力负荷预测的自适应分解可解释框架。见《电气与电子工程师协会国际声学、语音和信号处理会议论文集》，2023a。

Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak Patel. Large language models are few-shot health learners. arXiv preprint arXiv:2305.15525, 2023b.

刘鑫（Xin Liu）、丹尼尔·麦克达夫（Daniel McDuff）、盖扎·科瓦奇（Geza Kovacs）、艾萨克·加拉策 - 利维（Isaac Galatzer-Levy）、雅各布·桑夏因（Jacob Sunshine）、詹杰宁（Jiening Zhan）、 Poh Ming - Zher、廖顺（Shun Liao）、保罗·迪阿基莱（Paolo Di Achille）和什韦塔克·帕特尔（Shwetak Patel）。大语言模型是少样本健康学习者。预印本 arXiv:2305.15525，2023b。

Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Exploring the stationarity in time series forecasting. Advances in Neural Information Processing Systems, 35:9881-9893, 2022.

刘永（Yong Liu）、吴海旭（Haixu Wu）、王建民（Jianmin Wang）和龙明盛（Mingsheng Long）。非平稳变压器：探索时间序列预测中的平稳性。《神经信息处理系统进展》，35:9881 - 9893，2022。

Ziyang Ma, Wen Wu, Zhisheng Zheng, Yiwei Guo, Qian Chen, Shiliang Zhang, and Xie Chen. Leveraging speech ptm, text llm, and emotional tts for speech emotion recognition. arXiv preprint arXiv:2309.10294, 2023.

马紫阳（Ziyang Ma）、吴文（Wen Wu）、郑志胜（Zhisheng Zheng）、郭依伟（Yiwei Guo）、陈倩（Qian Chen）、张世亮（Shiliang Zhang）和陈谢（Xie Chen）。利用语音预训练模型、文本大语言模型和情感文本转语音技术进行语音情感识别。预印本 arXiv:2309.10294，2023。

Spyros Makridakis and Michele Hibon. The m3-competition: results, conclusions and implications. International journal of forecasting, 16(4):451-476, 2000.

斯皮罗斯·马基拉基斯（Spyros Makridakis）和米歇尔·希邦（Michele Hibon）。M3 竞赛：结果、结论和启示。《国际预测期刊》，16(4):451 - 476，2000。

Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. The m4 competition: Results, findings, conclusion and way forward. International Journal of Forecasting, 34(4):802-808, 2018.

斯皮罗斯·马基拉基斯（Spyros Makridakis）、埃万耶洛斯·斯皮利奥蒂斯（Evangelos Spiliotis）和瓦西利奥斯·阿西马科波洛斯（Vassilios Assimakopoulos）。M4竞赛：结果、发现、结论与未来方向。《国际预测期刊》（International Journal of Forecasting），34(4):802 - 808，2018年。

Igor Melnyk, Vijil Chenthamarakshan, Pin-Yu Chen, Payel Das, Amit Dhurandhar, Inkit Padhi, and Devleena Das. Reprogramming pretrained language models for antibody sequence infilling. In International Conference on Machine Learning, 2023.

伊戈尔·梅尔尼克（Igor Melnyk）、维吉尔·陈塔马拉克山（Vijil Chenthamarakshan）、陈品宇（Pin - Yu Chen）、帕耶尔·达斯（Payel Das）、阿米特·杜兰达尔（Amit Dhurandhar）、英基特·帕迪（Inkit Padhi）和德夫利娜·达斯（Devleena Das）。对预训练语言模型进行重新编程以填充抗体序列。收录于《国际机器学习会议》（International Conference on Machine Learning），2023年。

Suvir Mirchandani, Fei Xia, Pete Florence, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, Andy Zeng, et al. Large language models as general pattern machines. In Proceedings of the 7th Annual Conference on Robot Learning, 2023.

苏维尔·米尔钱达尼（Suvir Mirchandani）、夏飞（Fei Xia）、皮特·弗洛伦斯（Pete Florence）、丹尼·德里斯（Danny Driess）、蒙特塞拉特·冈萨雷斯·阿雷纳斯（Montserrat Gonzalez Arenas）、卡尼什卡·拉奥（Kanishka Rao）、多尔萨·萨迪格（Dorsa Sadigh）、安迪·曾（Andy Zeng）等。大语言模型作为通用模式机器。收录于《第七届机器人学习年度会议论文集》（Proceedings of the 7th Annual Conference on Robot Learning），2023年。

Diganta Misra, Agam Goyal, Bharat Runwal, and Pin Yu Chen. Reprogramming under constraints: Revisiting efficient and reliable transferability of lottery tickets. arXiv preprint arXiv:2308.14969, 2023.

迪甘塔·米斯拉（Diganta Misra）、阿加姆·戈亚尔（Agam Goyal）、巴拉特·伦瓦尔（Bharat Runwal）和陈品宇（Pin Yu Chen）。约束条件下的重新编程：重新审视彩票券的高效可靠可迁移性。预印本arXiv:2308.14969，2023年。

Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In International Conference on Learning Representations, 2023.

聂玉琪（Yuqi Nie）、阮南（Nam H Nguyen）、潘瓦迪·辛通（Phanwadee Sinthong）和贾扬特·卡拉格纳纳姆（Jayant Kalagnanam）。时间序列价值64字：基于Transformer的长期预测。发表于2023年国际学习表征会议。

OpenAI. Gpt-4 technical report, 2023.

OpenAI。《GPT - 4技术报告》，2023年。

Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. In International Conference on Learning Representations, 2020.

鲍里斯·N·奥列什金（Boris N Oreshkin）、德米特里·卡尔波夫（Dmitri Carpov）、尼古拉斯·查帕多斯（Nicolas Chapados）和约书亚·本吉奥（Yoshua Bengio）。N - beats：用于可解释时间序列预测的神经基扩展分析。发表于2020年国际学习表征会议。

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32, 2019.

亚当·帕兹克（Adam Paszke）、山姆·格罗斯（Sam Gross）、弗朗西斯科·马萨（Francisco Massa）、亚当·勒雷尔（Adam Lerer）、詹姆斯·布拉德伯里（James Bradbury）、格雷戈里·查南（Gregory Chanan）、特雷弗·基林（Trevor Killeen）、林泽明（Zeming Lin）、娜塔莉亚·吉梅尔申（Natalia Gimelshein）、卢卡·安蒂加（Luca Antiga）等。PyTorch：一种命令式风格的高性能深度学习库。《神经信息处理系统进展》，第32卷，2019年。

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

亚历克·拉德福德（Alec Radford）、杰弗里·吴（Jeffrey Wu）、雷翁·蔡尔德（Rewon Child）、大卫·栾（David Luan）、达里奥·阿莫迪（Dario Amodei）、伊利亚·苏茨克维（Ilya Sutskever）等。语言模型是无监督多任务学习者。OpenAI博客，第1卷第8期：第9页，2019年。

Stephen H Schneider and Robert E Dickinson. Climate modeling. Reviews of Geophysics, 12(3): 447-493, 1974.

斯蒂芬·H·施耐德（Stephen H Schneider）和罗伯特·E·迪金森（Robert E Dickinson）。气候建模。《地球物理学评论》（Reviews of Geophysics），12(3): 447 - 493，1974年。

Yihong Tang, Ao Qu, Andy HF Chow, William HK Lam, SC Wong, and Wei Ma. Domain adversarial spatial-temporal network: a transferable framework for short-term traffic forecasting across cities. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pp. 1905-1915, 2022.

唐一鸿（Yihong Tang）、曲骜（Ao Qu）、周瀚峰（Andy HF Chow）、林晓辉（William HK Lam）、黄少聪（SC Wong）和马伟（Wei Ma）。领域对抗时空网络：一种跨城市短期交通预测的可迁移框架。见《第31届ACM国际信息与知识管理大会论文集》（Proceedings of the 31st ACM International Conference on Information & Knowledge Management），第1905 - 1915页，2022年。

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

雨果·图弗龙（Hugo Touvron）、蒂博·拉夫里尔（Thibaut Lavril）、高蒂埃·伊扎卡尔（Gautier Izacard）、泽维尔·马蒂内（Xavier Martinet）、玛丽 - 安妮·拉肖（Marie - Anne Lachaux）、蒂莫泰·拉克鲁瓦（Timothée Lacroix）、巴蒂斯特·罗齐耶尔（Baptiste Rozière）、纳曼·戈亚尔（Naman Goyal）、埃里克·汉布罗（Eric Hambro）、费萨尔·阿扎尔（Faisal Azhar）等。羊驼模型（Llama）：开放且高效的基础语言模型。预印本arXiv:2302.13971，2023年。

Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200-212, 2021.

玛丽亚·钦普凯利（Maria Tsimpoukelli）、雅各布·L·梅尼克（Jacob L Menick）、塞尔坎·卡比（Serkan Cabi）、S·M·埃斯拉米（SM Eslami）、奥里奥尔·维尼亚尔斯（Oriol Vinyals）和费利克斯·希尔（Felix Hill）。基于冻结语言模型的多模态少样本学习。《神经信息处理系统进展》（Advances in Neural Information Processing Systems），34:200 - 212，2021年。

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.

阿希什·瓦斯瓦尼（Ashish Vaswani）、诺姆·沙泽尔（Noam Shazeer）、尼基·帕尔马（Niki Parmar）、雅各布·乌斯库赖特（Jakob Uszkoreit）、利昂·琼斯（Llion Jones）、艾丹·N·戈麦斯（Aidan N Gomez）、卢卡斯·凯泽（Łukasz Kaiser）和伊利亚·波洛苏欣（Illia Polosukhin）。你只需要注意力机制。《神经信息处理系统进展》，第30卷，2017年。

Ria Vinod, Pin-Yu Chen, and Payel Das. Reprogramming language models for molecular representation learning. In Annual Conference on Neural Information Processing Systems, 2020.

里亚·维诺德（Ria Vinod）、陈品宇（Pin-Yu Chen）和帕耶尔·达斯（Payel Das）。为分子表征学习重新编程语言模型。《神经信息处理系统年度会议》，2020年。

Yan Wang, Zhixuan Chu, Xin Ouyang, Simeng Wang, Hongyan Hao, Yue Shen, Jinjie Gu, Siqiao Xue, James Y Zhang, Qing Cui, et al. Enhancing recommender systems with large language model reasoning graphs. arXiv preprint arXiv:2308.10835, 2023.

王岩、褚志轩、欧阳鑫、王思萌、郝红岩、沈悦、顾金杰、薛思乔、詹姆斯·Y·张（James Y Zhang）、崔清等。用大语言模型推理图增强推荐系统。预印本arXiv:2308.10835，2023年。

Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Transformers in time series: A survey. In International Joint Conference on Artificial Intelligence, 2023.

文青松、周田、张朝丽、陈伟奇、马子清、闫俊驰和孙亮。时间序列中的Transformer：综述。《国际人工智能联合会议》，2023年。

Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Etsformer: Exponential smoothing transformers for time-series forecasting. arXiv preprint arXiv:2202.01381, 2022.

杰拉尔德·吴（Gerald Woo）、刘承浩（Chenghao Liu）、多延·萨胡（Doyen Sahoo）、阿克沙特·库马尔（Akshat Kumar）和史蒂文·霍伊（Steven Hoi）。Etsformer：用于时间序列预测的指数平滑变压器。预印本arXiv:2202.01381，2022年。

Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419-22430, 2021.

吴海旭（Haixu Wu）、徐杰辉（Jiehui Xu）、王建民（Jianmin Wang）和龙明盛（Mingsheng Long）。Autoformer：用于长期序列预测的具有自相关的分解变压器。《神经信息处理系统进展》，34:22419 - 22430，2021年。

Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In International Conference on Learning Representations, 2023.

吴海旭（Haixu Wu）、胡腾格（Tengge Hu）、刘永（Yong Liu）、周航（Hang Zhou）、王建民（Jianmin Wang）和龙明盛（Mingsheng Long）。Timesnet：用于通用时间序列分析的时间二维变化建模。发表于《国际学习表征会议》，2023年。

Hao Xue and Flora D Salim. Prompt-based time series forecasting: A new task and dataset. arXiv preprint arXiv:2210.08964, 2022.

薛浩（Hao Xue）和弗洛拉·D·萨利姆（Flora D Salim）。基于提示的时间序列预测：一项新任务和数据集。预印本arXiv:2210.08964，2022年。

Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Chen. Voice2series: Reprogramming acoustic models for time series classification. In International Conference on Machine Learning, pp. 11808-11819. PMLR, 2021.

杨朝瀚（Chao-Han Huck Yang）、蔡昀芸（Yun-Yun Tsai）和平宇·陈（Pin-Yu Chen）。Voice2series：为时间序列分类重新编程声学模型。见《国际机器学习会议论文集》，第11808 - 11819页。机器学习研究会议录（PMLR），2021年。

Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023.

尹书康（Shukang Yin）、傅超友（Chaoyou Fu）、赵思睿（Sirui Zhao）、李可（Ke Li）、孙兴（Xing Sun）、徐彤（Tong Xu）和陈恩红（Enhong Chen）。多模态大语言模型综述。预印本arXiv:2306.13549，2023年。

Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp. 11121-11128, 2023.

曾爱玲（Ailing Zeng）、陈慕溪（Muxi Chen）、张磊（Lei Zhang）和徐强（Qiang Xu）。Transformer对时间序列预测有效吗？见《美国人工智能协会（AAAI）人工智能会议论文集》，第37卷，第11121 - 11128页，2023年。

Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong Liu, James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, et al. Self-supervised learning for time series analysis: Taxonomy, progress, and prospects. arXiv preprint arXiv:2306.10125, 2023.

张可欣（Kexin Zhang）、文青松（Qingsong Wen）、张朝丽（Chaoli Zhang）、蔡荣耀（Rongyao Cai）、金明（Ming Jin）、刘勇（Yong Liu）、詹姆斯·张（James Zhang）、梁宇轩（Yuxuan Liang）、庞冠松（Guansong Pang）、宋东进（Dongjin Song）等。时间序列分析的自监督学习：分类、进展与展望。预印本arXiv:2306.10125，2023年。

Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and Jian Li. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures. arXiv preprint arXiv:2207.01186, 2022a.

张天平（Tianping Zhang）、张一卓（Yizhuo Zhang）、曹伟（Wei Cao）、卞江（Jiang Bian）、易晓晗（Xiaohan Yi）、郑顺（Shun Zheng）和李建（Jian Li）。少即是多：采用轻采样多层感知机结构实现快速多元时间序列预测。预印本 arXiv:2207.01186，2022a。

Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. Advances in Neural Information Processing Systems, 35:3988-4003, 2022b.

张翔（Xiang Zhang）、赵子渊（Ziyuan Zhao）、西奥多罗斯·齐利加里迪斯（Theodoros Tsiligkaridis）和马林卡·齐特尼克（Marinka Zitnik）。通过时频一致性进行时间序列的自监督对比预训练。《神经信息处理系统进展》，35:3988 - 4003，2022b。

Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 11106-11115, 2021.

周浩毅（Haoyi Zhou）、张上航（Shanghang Zhang）、彭杰奇（Jieqi Peng）、张帅（Shuai Zhang）、李建新（Jianxin Li）、熊辉（Hui Xiong）和张万财（Wancai Zhang）。Informer：超越高效Transformer的长序列时间序列预测方法。《AAAI人工智能会议论文集》，第35卷，第11106 - 11115页，2021年。

Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International Conference on Machine Learning, pp. 27268-27286. PMLR, 2022.

周天（Tian Zhou）、马子清（Ziqing Ma）、文青松（Qingsong Wen）、王雪（Xue Wang）、孙亮（Liang Sun）和金榕（Rong Jin）。Fedformer：用于长期序列预测的频率增强分解Transformer。《国际机器学习会议》，第27268 - 27286页。机器学习研究会议录，2022年。

Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained lm. Advances in Neural Information Processing Systems, 36, 2023a.

周天（Tian Zhou）、牛培松（Peisong Niu）、王雪（Xue Wang）、孙亮（Liang Sun）和金榕（Rong Jin）。一法通吃：通过预训练语言模型助力通用时间序列分析。《神经信息处理系统进展》，36，2023a。

Yunyi Zhou, Zhixuan Chu, Yijia Ruan, Ge Jin, Yuchen Huang, and Sheng Li. ptse: A multi-model ensemble method for probabilistic time series forecasting. In The 32nd International Joint Conference on Artificial Intelligence, 2023b.

周云逸（Yunyi Zhou）、褚志轩（Zhixuan Chu）、阮一佳（Yijia Ruan）、金戈（Ge Jin）、黄雨晨（Yuchen Huang）和李盛（Sheng Li）。ptse：一种用于概率时间序列预测的多模型集成方法。发表于第32届国际人工智能联合会议，2023b。

## A MORE RELATED WORK

## 更多相关工作

Task-specific Learning. We furnish an extension of the related work on task-specific learning, focusing particularly on the most related models to which we made comparisons. Recent works improve Transformer (Vaswani et al. 2017) for time series forecasting by incorporating signal processing principles like patching, exponential smoothing, decomposition, and frequency analysis. For example, PatchTST (Nie et al., 2023) segments time series into patches as input tokens to Transformer. This retains local semantics, reduces computation/memory for attention, and allows longer history. It improves long-term forecast accuracy over other Transformer models. It also achieves excellent performance on self-supervised pretraining and transfer learning. ETSformer (Woo et al., 2022) incorporates exponential smoothing principles into Transformer attention to improve accuracy and efficiency. It uses exponential smoothing attention and frequency attention to replace standard self-attention. FEDformer (Zhou et al., 2022) combines Transformer with seasonal-trend decomposition. The decomposition captures the global profile while Transformer captures detailed structures. It also uses frequency enhancement for long-term prediction. This provides better performance and efficiency than the standard Transformer. Autoformer (Wu et al. 2021) uses a decomposition architecture with auto-correlation to enable progressive decomposition capacities for complex series. Auto-correlation is designed based on series periodicity to conduct dependency discovery and representation aggregation. It outperforms self-attention in efficiency and accuracy.

特定任务学习。我们对特定任务学习的相关工作进行了扩展，特别关注我们进行比较的最相关模型。近期的研究通过融入诸如分块、指数平滑、分解和频率分析等信号处理原理，对用于时间序列预测的Transformer模型（Vaswani等人，2017年）进行了改进。例如，PatchTST模型（Nie等人，2023年）将时间序列分割成块，作为Transformer的输入令牌。这保留了局部语义，减少了注意力机制的计算量和内存占用，并允许使用更长的历史数据。与其他Transformer模型相比，它提高了长期预测的准确性。它在自监督预训练和迁移学习方面也取得了出色的表现。ETSformer模型（Woo等人，2022年）将指数平滑原理融入Transformer的注意力机制中，以提高准确性和效率。它使用指数平滑注意力和频率注意力来替代标准的自注意力机制。FEDformer模型（Zhou等人，2022年）将Transformer与季节性趋势分解相结合。分解过程捕捉全局特征，而Transformer捕捉详细结构。它还使用频率增强技术进行长期预测。与标准的Transformer相比，它具有更好的性能和效率。Autoformer模型（Wu等人，2021年）使用具有自相关的分解架构，为复杂序列实现渐进式分解能力。自相关是基于序列的周期性设计的，用于进行依赖关系发现和表示聚合。在效率和准确性方面，它优于自注意力机制。

Although these methods enhance efficiency and accuracy compared to vanilla Transformer, they are mostly designed and optimized for narrow prediction tasks within specific domains. These models are typically trained end-to-end on small, domain-specific datasets. While achieving strong performance on their target tasks, such specialized models sacrifice versatility and generalizability across the diverse range of time series data encountered in the real world. The narrow focus limits their applicability to new datasets and tasks. To advance time series forecasting, there is a need for more flexible, widely applicable models that can adapt to new data distributions and tasks without extensive retraining. Ideal models would learn robust time series representations that transfer knowledge across domains. Developing such broadly capable forecasting models remains an open challenge. According to our discussions of related previous work, recent studies have begun to explore model versatility through pre-training and architectural innovations. However, further efforts are needed to realize the truly general-purpose forecasting systems that we are advancing in this research.

尽管与普通的Transformer相比，这些方法提高了效率和准确性，但它们大多是为特定领域内的狭义预测任务而设计和优化的。这些模型通常在特定领域的小型数据集上进行端到端训练。虽然这些专用模型在目标任务上表现出色，但它们牺牲了对现实世界中各种时间序列数据的通用性和泛化能力。这种狭隘的关注点限制了它们在新数据集和任务上的适用性。为了推动时间序列预测的发展，需要更灵活、更广泛适用的模型，这些模型能够在无需大量重新训练的情况下适应新的数据分布和任务。理想的模型应该学习到强大的时间序列表示，以便在不同领域之间传递知识。开发具有如此广泛能力的预测模型仍然是一个有待解决的挑战。根据我们对相关前期工作的讨论，近期的研究已经开始通过预训练和架构创新来探索模型的通用性。然而，要实现我们在本研究中所推进的真正通用的预测系统，还需要付出更多努力。

Cross-modality Adaptation. We provide an extended overview of related work in cross-modality adaptation, with a particular focus on recent advancements in model reprogramming for time series and other data modalities. Model reprogramming is a resource-efficient cross-domain learning approach that involves adapting a well-developed, pre-trained model from one domain (source) to address tasks in a different domain (target) without the need for model fine-tuning, even when these domains are significantly distinct, as noted by Chen (2022). In the context of time series data, Voice2Series (Yang et al., 2021) adapts an acoustic model from speech recognition for time series classification by transforming the time series to fit the model and remapping outputs to new labels. Similarly, LLMTime (Gruver et al. 2023) adapts LLMs for zero-shot time series forecasting, focusing on the effective tokenization of input time series for the backbone LLM, which then generates forecasts autoregressively. Diverging from these methods, TIME-LLM does not edit the input time series directly. Instead, it proposes reprogramming time series with the source data modality along with prompting to unleash the full potential of LLMs as versatile forecasters in standard, few-shot, and zero-shot scenarios. Other notable works in this field, mostly in biology, include R2DL (Vinod et al. 2020) and ReproBert (Melnyk et al. 2023), which reprogram amino acids using word em-beddings. A key distinction with our patch reprogramming approach is that, unlike the complete set of amino acids, time series patches do not form a complete set. Thus, we propose optimizing a small set of text prototypes and their mapping to time series patches, rather than directly optimizing a large transformation matrix between two complete sets, such as vocabulary and amino acids.

跨模态适应。我们对跨模态适应领域的相关工作进行了全面概述，特别关注了针对时间序列和其他数据模态的模型重编程方面的最新进展。正如陈（Chen，2022）所指出的，模型重编程是一种资源高效的跨领域学习方法，它可以将一个在某个领域（源领域）中训练良好的预训练模型应用到不同的领域（目标领域）中执行任务，而无需对模型进行微调，即使这些领域之间存在显著差异。在时间序列数据的背景下，语音转序列模型（Voice2Series，杨等人，2021）通过将时间序列进行转换以适应模型，并将输出重新映射到新的标签，从而将语音识别中的声学模型应用于时间序列分类。同样，大语言模型时间序列预测模型（LLMTime，格鲁弗等人，2023）将大语言模型（LLMs）应用于零样本时间序列预测，重点在于为骨干大语言模型对输入时间序列进行有效的分词，然后由大语言模型自回归地生成预测结果。与这些方法不同，时间大语言模型（TIME - LLM）并不直接编辑输入的时间序列。相反，它提出结合源数据模态对时间序列进行重编程，并使用提示技术，以充分发挥大语言模型在标准、少样本和零样本场景下作为通用预测器的潜力。该领域其他值得注意的工作大多集中在生物学领域，包括R2DL（维诺德等人，2020）和重编程BERT模型（ReproBert，梅尔尼克等人，2023），它们使用词嵌入对氨基酸进行重编程。与我们的补丁重编程方法的一个关键区别在于，与完整的氨基酸集合不同，时间序列补丁并不构成一个完整的集合。因此，我们建议优化一小部分文本原型及其与时间序列补丁的映射关系，而不是直接优化两个完整集合（如词汇表和氨基酸）之间的大型转换矩阵。

## B EXPERIMENTAL DETAILS

## B 实验细节

### B.1 IMPLEMENTATION

### B.1 实现

We mainly follow the experimental configurations in (Wu et al. 2023) across all baselines within a unified evaluation pipeline in https://github.com/thuml/Time-Series-Library for fair comparisons. We use Llama-7B (Touvron et al. 2023) as the default backbone model unless stated otherwise. All our experiments are repeated three times and we report the averaged results. Our model implementation is on PyTorch (Paszke et al. 2019) with all experiments conducted on NVIDIA A100-80G GPUs. Our detailed model configurations are in Appendix B.4, and our code is made available at https://github.com/KimMeen/Time-LLM.

为了进行公平比较，我们在https://github.com/thuml/Time - Series - Library的统一评估流程中，针对所有基线模型主要遵循（Wu等人，2023年）中的实验配置。除非另有说明，我们默认使用Llama - 7B（Touvron等人，2023年）作为骨干模型。我们所有的实验都重复三次，并报告平均结果。我们的模型在PyTorch（Paszke等人，2019年）上实现，所有实验均在NVIDIA A100 - 80G GPU上进行。我们详细的模型配置见附录B.4，代码可在https://github.com/KimMeen/Time - LLM获取。

Technical Details. We provide additional technical details of TIME-LLM in three aspects: (1) the learning of text prototypes, (2) the calculation of trends and lags in time series for use in prompts, and (3) the implementation of the output projection. To identify a small set of text prototypes ${\mathbf{E}}^{\prime } \in  {\mathbb{R}}^{{V}^{\prime } \times  D}$ from $\mathbf{E} \in  {\mathbb{R}}^{V \times  D}$ ,we learn a matrix $\mathbf{W} \in  {\mathbb{R}}^{{V}^{\prime } \times  V}$ as the intermediary. To describe the overall time series trend in natural language, we calculate the sum of differences between consecutive time steps. A sum greater than 0 indicates an upward trend, while a lesser sum denotes a downward trend. In addition, we calculate the top-5 lags of the time series, identified by computing the autocorrelation using fast Fourier transformation and selecting the five lags with the highest correlation values. After we pack and feedforward the prompt and patch embeddings ${\mathbf{O}}^{\left( i\right) } \in  {\mathbb{R}}^{P \times  D}$ through the frozen LLM, we discard the prefixal part and obtain the output representations, denoted as ${\widetilde{\mathbf{O}}}^{i} \in  {\mathbb{R}}^{P \times  D}$ . Subsequently,we follow PatchTST (Nie et al. 2023) and flatten ${\widetilde{\mathbf{O}}}^{i}$ into a 1D tensor with the length $P \times  D$ ,which is then linear projected as $\overline{{\mathbf{Y}}^{i}} \in  {\mathbb{R}}^{H}$ .

技术细节。我们从三个方面提供了TIME - LLM的额外技术细节：（1）文本原型的学习；（2）用于提示的时间序列中趋势和滞后的计算；（3）输出投影的实现。为了从$\mathbf{E} \in  {\mathbb{R}}^{V \times  D}$中识别出一小部分文本原型${\mathbf{E}}^{\prime } \in  {\mathbb{R}}^{{V}^{\prime } \times  D}$，我们学习一个矩阵$\mathbf{W} \in  {\mathbb{R}}^{{V}^{\prime } \times  V}$作为中间媒介。为了用自然语言描述整体时间序列趋势，我们计算连续时间步之间的差值之和。和大于0表示上升趋势，和较小则表示下降趋势。此外，我们计算时间序列的前5个滞后值，通过使用快速傅里叶变换计算自相关性并选择相关性值最高的5个滞后值来确定。在我们将提示和补丁嵌入${\mathbf{O}}^{\left( i\right) } \in  {\mathbb{R}}^{P \times  D}$打包并通过冻结的大语言模型（LLM）进行前馈传播后，我们丢弃前缀部分并获得输出表示，记为${\widetilde{\mathbf{O}}}^{i} \in  {\mathbb{R}}^{P \times  D}$。随后，我们遵循PatchTST（聂等人，2023年）的方法，将${\widetilde{\mathbf{O}}}^{i}$展平为长度为$P \times  D$的一维张量，然后将其线性投影为$\overline{{\mathbf{Y}}^{i}} \in  {\mathbb{R}}^{H}$。

### B.2 DATASET DETAILS

### B.2 数据集详情

Dataset statistics are summarized in Tab. 8. We evaluate the long-term forecasting performance on the well-established eight different benchmarks, including four ETT datasets (Zhou et al. 2021) (i.e., ETTh1, ETTh2, ETTm1, and ETTm2), Weather, Electricity, Traffic, and ILI from (Wu et al., 2023). Furthermore, we evaluate the performance of short-term forecasting on the M4 benchmark (Makri-dakis et al., 2018) and the quarterly dataset in the M3 benchmark (Makridakis & Hibon, 2000).

数据集统计信息总结于表 8。我们在八个成熟的不同基准数据集上评估长期预测性能，包括四个 ETT 数据集（周等人，2021 年）（即 ETTh1、ETTh2、ETTm1 和 ETTm2）、天气（Weather）、电力（Electricity）、交通（Traffic）和流感样疾病（ILI）数据集（来自吴等人，2023 年）。此外，我们在 M4 基准数据集（马克里达基斯等人，2018 年）和 M3 基准数据集中的季度数据集（马克里达基斯和希邦，2000 年）上评估短期预测性能。

<!-- Media -->

Table 8: Dataset statistics are from (Wu et al. 2023). The dimension indicates the number of time series (i.e., channels), and the dataset size is organized in (training, validation, testing).

表 8：数据集统计信息来自（吴等人，2023 年）。维度表示时间序列的数量（即通道数），数据集大小按（训练集、验证集、测试集）组织。

<table><tr><td>Tasks</td><td>Dataset</td><td>Dim.</td><td>Series Length</td><td>Dataset Size</td><td>Frequency</td><td>Domain</td></tr><tr><td rowspan="8">Long-term Forecasting</td><td>ETTm1</td><td>7</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(34465, 11521, 11521)</td><td>15 min</td><td>Temperature</td></tr><tr><td>ETTm2</td><td>7</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(34465, 11521, 11521)</td><td>15 min</td><td>Temperature</td></tr><tr><td>ETTh1</td><td>7</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(8545, 2881, 2881)</td><td>1 hour</td><td>Temperature</td></tr><tr><td>ETTh2</td><td>7</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(8545, 2881, 2881)</td><td>1 hour</td><td>Temperature</td></tr><tr><td>Electricity</td><td>321</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(18317, 2633, 5261)</td><td>1 hour</td><td>Electricity</td></tr><tr><td>Traffic</td><td>862</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(12185, 1757, 3509)</td><td>1 hour</td><td>Transportation</td></tr><tr><td>Weather</td><td>21</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(36792, 5271, 10540)</td><td>10 min</td><td>Weather</td></tr><tr><td>ILI</td><td>7</td><td>$\{ {24},{36},{48},{60}\}$</td><td>(617, 74, 170)</td><td>1 week</td><td>Illness</td></tr><tr><td rowspan="7">Short-term Forecasting</td><td>M3-Quarterly</td><td>1</td><td>8</td><td>(756, 0, 756)</td><td>Quarterly</td><td>Multiple</td></tr><tr><td>M4-Yearly</td><td>1</td><td>6</td><td>(23000,0,23000)</td><td>Yearly</td><td>Demographic</td></tr><tr><td>M4-Quarterly</td><td>1</td><td>8</td><td>(24000,0,24000)</td><td>Quarterly</td><td>Finance</td></tr><tr><td>M4-Monthly</td><td>1</td><td>18</td><td>(48000,0,48000)</td><td>Monthly</td><td>Industry</td></tr><tr><td>M4-Weakly</td><td>1</td><td>13</td><td>(359, 0, 359)</td><td>Weakly</td><td>Macro</td></tr><tr><td>M4-Daily</td><td>1</td><td>14</td><td>(4227, 0, 4227)</td><td>Daily</td><td>Micro</td></tr><tr><td>M4-Hourly</td><td>1</td><td>48</td><td>(414, 0, 414)</td><td>Hourly</td><td>Other</td></tr></table>

<table><tbody><tr><td>任务</td><td>数据集</td><td>维度</td><td>序列长度</td><td>数据集大小</td><td>频率</td><td>领域</td></tr><tr><td rowspan="8">长期预测</td><td>ETTm1</td><td>7</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(34465, 11521, 11521)</td><td>15分钟</td><td>温度</td></tr><tr><td>ETTm2</td><td>7</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(34465, 11521, 11521)</td><td>15分钟</td><td>温度</td></tr><tr><td>ETTh1（原文未变，可能为特定专业术语）</td><td>7</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(8545, 2881, 2881)</td><td>1小时</td><td>温度</td></tr><tr><td>ETTh2（原文未变，可能为特定专业术语）</td><td>7</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(8545, 2881, 2881)</td><td>1小时</td><td>温度</td></tr><tr><td>电力</td><td>321</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(18317, 2633, 5261)</td><td>1小时</td><td>电力</td></tr><tr><td>交通</td><td>862</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(12185, 1757, 3509)</td><td>1小时</td><td>运输</td></tr><tr><td>天气</td><td>21</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(36792, 5271, 10540)</td><td>10分钟</td><td>天气</td></tr><tr><td>流感样疾病（ILI）</td><td>7</td><td>$\{ {24},{36},{48},{60}\}$</td><td>(617, 74, 170)</td><td>1周</td><td>疾病</td></tr><tr><td rowspan="7">短期预测</td><td>M3-季度</td><td>1</td><td>8</td><td>(756, 0, 756)</td><td>季度</td><td>多个；多样的</td></tr><tr><td>M4-年度</td><td>1</td><td>6</td><td>(23000,0,23000)</td><td>年度</td><td>人口统计学的</td></tr><tr><td>M4季度数据</td><td>1</td><td>8</td><td>(24000,0,24000)</td><td>季度</td><td>金融</td></tr><tr><td>M4月度数据</td><td>1</td><td>18</td><td>(48000,0,48000)</td><td>月度</td><td>行业</td></tr><tr><td>M4周度数据</td><td>1</td><td>13</td><td>(359, 0, 359)</td><td>微弱地</td><td>宏观</td></tr><tr><td>M4-每日</td><td>1</td><td>14</td><td>(4227, 0, 4227)</td><td>每日</td><td>微观</td></tr><tr><td>M4-每小时</td><td>1</td><td>48</td><td>(414, 0, 414)</td><td>每小时</td><td>其他</td></tr></tbody></table>

<!-- Media -->

The Electricity Transformer Temperature (ETT; An indicator reflective of long-term electric power deployment) benchmark is comprised of two years of data, sourced from two counties in China, and is subdivided into four distinct datasets, each with varying sampling rates: ETTh1 and ETTh2, which are sampled at a 1-hour level, and ETTm1 and ETTm2, which are sampled at a 15-minute level. Each entry within the ETT datasets includes six power load features and a target variable, termed "oil temperature". The Electricity dataset comprises records of electricity consumption from 321 customers, measured at a 1-hour sampling rate. The Weather dataset includes one-year records from 21 meteorological stations located in Germany, with a sampling rate of 10 minutes. The Traffic dataset includes data on the occupancy rates of the freeway system, recorded from 862 sensors across the State of California, with a sampling rate of 1 hour. The influenza-like illness (ILI) dataset contains records of patients experiencing severe influenza with complications.

电力变压器温度（ETT；反映长期电力部署情况的指标）基准数据集包含两年的数据，数据来源于中国的两个县，并细分为四个不同的数据集，每个数据集的采样率不同：ETTh1和ETTh2的采样率为每小时一次，ETTm1和ETTm2的采样率为每15分钟一次。ETT数据集中的每个条目包含六个电力负荷特征和一个目标变量，即“油温”。电力数据集包含321个用户的用电记录，采样率为每小时一次。天气数据集包含德国21个气象站的一年记录，采样率为每10分钟一次。交通数据集包含加利福尼亚州862个传感器记录的高速公路系统占有率数据，采样率为每小时一次。流感样疾病（ILI）数据集包含患有严重流感并伴有并发症的患者记录。

The M4 benchmark comprises ${100}\mathrm{\;K}$ time series,amassed from various domains commonly present in business, financial, and economic forecasting. These time series have been partitioned into six distinctive datasets, each with varying sampling frequencies that range from yearly to hourly. The M3-Quarterly dataset comprises 756 quarterly sampled time series in the M3 benchmark. These series are categorized into five different domains: demographic, micro, macro, industry, and finance.

M4基准包含${100}\mathrm{\;K}$个时间序列，这些时间序列来自商业、金融和经济预测中常见的各个领域。这些时间序列被划分为六个不同的数据集，每个数据集的采样频率各不相同，从每年到每小时不等。M3季度数据集包含M3基准中756个按季度采样的时间序列。这些序列被分为五个不同的领域：人口统计、微观、宏观、行业和金融。

### B.3 EVALUATION METRICS

### B.3 评估指标

For evaluation metrics, we utilize the mean square error (MSE) and mean absolute error (MAE) for long-term forecasting. In terms of the short-term forecasting on M4 benchmark, we adopt the symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weighted average (OWA) as in N-BEATS (Oreshkin et al. 2020). Note that OWA is a specific metric utilized in the M4 competition. The calculations of these metrics are as follows:

对于评估指标，我们在长期预测中使用均方误差（MSE）和平均绝对误差（MAE）。在M4基准的短期预测方面，我们采用对称平均绝对百分比误差（SMAPE）、平均绝对缩放误差（MASE）和总体加权平均值（OWA），这与N - BEATS（奥列什金等人，2020年）中的方法一致。请注意，OWA是M4竞赛中使用的特定指标。这些指标的计算方法如下：

$$
\mathrm{{MSE}} = \frac{1}{H}\mathop{\sum }\limits_{{h = 1}}^{T}{\left( {\mathbf{Y}}_{h} - {\widehat{\mathbf{Y}}}_{h}\right) }^{2},\;\mathrm{{MAE}} = \frac{1}{H}\mathop{\sum }\limits_{{h = 1}}^{H}\left| {{\mathbf{Y}}_{h} - {\widehat{\mathbf{Y}}}_{h}}\right| ,
$$

$$
\text{ SMAPE } = \frac{200}{H}\mathop{\sum }\limits_{{h = 1}}^{H}\frac{\left| {\mathbf{Y}}_{h} - {\widehat{\mathbf{Y}}}_{h}\right| }{\left| {\mathbf{Y}}_{h}\right|  + \left| {\widehat{\mathbf{Y}}}_{h}\right| },\;\text{ MAPE } = \frac{100}{H}\mathop{\sum }\limits_{{h = 1}}^{H}\frac{\left| {\mathbf{Y}}_{h} - {\widehat{\mathbf{Y}}}_{h}\right| }{\left| {\mathbf{Y}}_{h}\right| },
$$

$$
\text{MASE} = \frac{1}{H}\mathop{\sum }\limits_{{h = 1}}^{H}\frac{\left| {\mathbf{Y}}_{h} - {\widehat{\mathbf{Y}}}_{h}\right| }{\frac{1}{H - s}\mathop{\sum }\limits_{{j = s + 1}}^{H}\left| {{\mathbf{Y}}_{j} - {\mathbf{Y}}_{j - s}}\right| },\;\text{OWA} = \frac{1}{2}\left\lbrack  {\frac{\text{ SMAPE }}{{\text{ SMAPE }}_{\text{Naive2 }}} + \frac{\text{ MASE }}{{\text{ MASE }}_{\text{Naive2 }}}}\right\rbrack  \text{,}
$$

where $s$ is the periodicity of the time series data. $H$ denotes the number of data points (i.e.,prediction horizon in our cases). ${\mathbf{Y}}_{h}$ and ${\widehat{\mathbf{Y}}}_{h}$ are the $h$ -th ground truth and prediction where $h \in  \{ 1,\cdots ,H\}$ .

其中 $s$ 是时间序列数据的周期性。$H$ 表示数据点的数量（即，在我们的案例中为预测范围）。${\mathbf{Y}}_{h}$ 和 ${\widehat{\mathbf{Y}}}_{h}$ 分别是第 $h$ 个真实值和预测值，其中 $h \in  \{ 1,\cdots ,H\}$。

### B.4 MODEL CONFIGURATIONS

### B.4 模型配置

The configurations of our models, relative to varied tasks and datasets, are consolidated in Tab. 9 By default, the Adam optimizer (Kingma & Ba, 2015) is employed throughout all experiments. Specifically,the quantity of text prototypes ${V}^{\prime }$ is held constant at 100 and 1000 for short-term and long-term forecasting tasks, respectively. We utilize the Llama-7B model at full capacity, maintaining the backbone model layers at 32 across all tasks as a standard. The term input length $T$ signifies the number of time steps present in the original input time series data. Patch dimensions ${d}_{m}$ represent the hidden dimensions of the embedded time series patches prior to reprogramming. Lastly, heads $K$ correlate to the multi-head cross-attention utilized for patch reprogramming. In the four rightmost columns of Tab. 9, we detail the configurations related to model training.

我们的模型针对不同任务和数据集的配置汇总于表9。默认情况下，所有实验均采用Adam优化器（Kingma & Ba，2015）。具体而言，文本原型数量${V}^{\prime }$在短期和长期预测任务中分别固定为100和1000。我们充分利用Llama - 7B模型，将所有任务的骨干模型层数统一设定为32。术语输入长度$T$表示原始输入时间序列数据中的时间步数。补丁维度${d}_{m}$表示重新编程之前嵌入的时间序列补丁的隐藏维度。最后，头数$K$与用于补丁重新编程的多头交叉注意力相关。在表9最右侧的四列中，我们详细列出了与模型训练相关的配置。

<!-- Media -->

Table 9: An overview of the experimental configurations for TIME-LLM. "LTF" and "STF" denote long-term and short-term forecasting, respectively.

表9：TIME - LLM实验配置概述。“LTF”和“STF”分别表示长期和短期预测。

<table><tr><td rowspan="2">Task-Dataset / Configuration</td><td colspan="5">Model Hyperparameter</td><td colspan="4">Training Process</td></tr><tr><td>Text Prototype ${V}^{\prime }$</td><td>Backbone Layers</td><td>Input Length $T$</td><td>Patch Dim. ${d}_{\mathrm{m}}$</td><td>Heads $K$</td><td>${\mathrm{{LR}}}^{ * }$</td><td>Loss</td><td>Batch Size</td><td>Epochs</td></tr><tr><td>LTF - ETTh1</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-3}$</td><td>MSE</td><td>16</td><td>50</td></tr><tr><td>LTF - ETTh2</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-3}$</td><td>MSE</td><td>16</td><td>50</td></tr><tr><td>LTF - ETTm1</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-3}$</td><td>MSE</td><td>16</td><td>100</td></tr><tr><td>LTF - ETTm2</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-3}$</td><td>MSE</td><td>16</td><td>100</td></tr><tr><td>LTF - Weather</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-2}$</td><td>MSE</td><td>8</td><td>100</td></tr><tr><td>LTF - Electricity</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-2}$</td><td>MSE</td><td>8</td><td>100</td></tr><tr><td>LTF - Traffic</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-2}$</td><td>MSE</td><td>8</td><td>100</td></tr><tr><td>LTF - ILI</td><td>100</td><td>32</td><td>96</td><td>16</td><td>8</td><td>${10}^{-2}$</td><td>MSE</td><td>16</td><td>50</td></tr><tr><td>STF - M3-Quarterly</td><td>100</td><td>32</td><td>$2 \times  {H}^{ \dagger  }$</td><td>32</td><td>8</td><td>${10}^{-4}$</td><td>SMAPE</td><td>32</td><td>50</td></tr><tr><td>STF - M4</td><td>100</td><td>32</td><td>$2 \times  {H}^{ \dagger  }$</td><td>32</td><td>8</td><td>${10}^{-4}$</td><td>SMAPE</td><td>32</td><td>50</td></tr></table>

<table><tbody><tr><td rowspan="2">任务数据集/配置</td><td colspan="5">模型超参数</td><td colspan="4">训练过程</td></tr><tr><td>文本原型 ${V}^{\prime }$</td><td>骨干网络层</td><td>输入长度 $T$</td><td>补丁维度。${d}_{\mathrm{m}}$</td><td>头数 $K$</td><td>${\mathrm{{LR}}}^{ * }$</td><td>损失</td><td>批量大小</td><td>轮数</td></tr><tr><td>长时预测 - ETTh1</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-3}$</td><td>均方误差（Mean Squared Error，MSE）</td><td>16</td><td>50</td></tr><tr><td>长时预测 - ETTh2数据集（Long - Term Forecasting - ETTh2）</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-3}$</td><td>均方误差（Mean Squared Error，MSE）</td><td>16</td><td>50</td></tr><tr><td>长时预测 - ETTm1数据集（Long - Term Forecasting - ETTm1）</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-3}$</td><td>均方误差（Mean Squared Error，MSE）</td><td>16</td><td>100</td></tr><tr><td>长时预测 - ETTm2数据集（Long - Term Forecasting - ETTm2）</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-3}$</td><td>均方误差（Mean Squared Error，MSE）</td><td>16</td><td>100</td></tr><tr><td>长时预测 - 气象数据集（Long - Term Forecasting - Weather）</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-2}$</td><td>均方误差（Mean Squared Error，MSE）</td><td>8</td><td>100</td></tr><tr><td>长时预测 - 电力数据集（Long - Term Forecasting - Electricity）</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-2}$</td><td>均方误差（Mean Squared Error，MSE）</td><td>8</td><td>100</td></tr><tr><td>长期预测 - 交通（LTF - Traffic）</td><td>1000</td><td>32</td><td>512</td><td>16</td><td>8</td><td>${10}^{-2}$</td><td>均方误差（Mean Squared Error，MSE）</td><td>8</td><td>100</td></tr><tr><td>长期预测 - 流感样疾病（LTF - ILI）</td><td>100</td><td>32</td><td>96</td><td>16</td><td>8</td><td>${10}^{-2}$</td><td>均方误差（Mean Squared Error，MSE）</td><td>16</td><td>50</td></tr><tr><td>短期预测 - M3季度（STF - M3-Quarterly）</td><td>100</td><td>32</td><td>$2 \times  {H}^{ \dagger  }$</td><td>32</td><td>8</td><td>${10}^{-4}$</td><td>对称平均绝对百分比误差（SMAPE）</td><td>32</td><td>50</td></tr><tr><td>短期预测 - M4（STF - M4）</td><td>100</td><td>32</td><td>$2 \times  {H}^{ \dagger  }$</td><td>32</td><td>8</td><td>${10}^{-4}$</td><td>对称平均绝对百分比误差（SMAPE）</td><td>32</td><td>50</td></tr></tbody></table>

$\dagger  H$ represents the forecasting horizon of the M4 and M3 datasets.

$\dagger  H$表示M4和M3数据集的预测范围。

$*$ LR means the initial learning rate.

$*$ LR表示初始学习率。

<!-- Media -->

## C HYPERPARAMETER SENSITIVITY

## C 超参数敏感性

We conduct a hyperparameter sensitivity analysis focusing on the four important hyperparameters within TIME-LLM: namely, the number of backbone model layers, the number of text prototypes ${V}^{\prime }$ ,the time series input length $T$ ,and the number of patch reprogramming cross-attention heads $K$ . The correlated results can be found in Fig. 6. From our analysis,we derive the following observations: (1) There is a positive correlation between the number of Transformer layers in the backbone LLM and the performance of TIME-LLM, affirming that the scaling law is preserved post-LLM reprogramming.; (2) Generally, acquiring more text prototypes enhances performance. We hypothesize that a limited number of prototypes ${V}^{\prime }$ might induce noise when aggregating language cues, consequently obstructing the efficient learning of highly representative prototypes essential for characterizing the input time series patches; (3) The input time length $T$ exhibits a direct relation with forecasting accuracy, particularly evident when predicting extended horizons. This observation is logical and is in congruence with conventional time series models; (4) Increasing the number of attention heads during the reprogramming of input patches proves to be advantageous.

我们针对TIME - LLM中的四个重要超参数进行了超参数敏感性分析，即骨干模型层数、文本原型数量${V}^{\prime }$、时间序列输入长度$T$以及补丁重编程交叉注意力头数量$K$。相关结果如图6所示。通过分析，我们得出以下结论：（1）骨干大语言模型（LLM）中的Transformer层数与TIME - LLM的性能呈正相关，这证实了大语言模型重编程后仍遵循缩放定律；（2）一般来说，获取更多的文本原型可以提高性能。我们推测，有限数量的原型${V}^{\prime }$在聚合语言线索时可能会引入噪声，从而阻碍对表征输入时间序列补丁至关重要的高代表性原型的有效学习；（3）输入时间长度$T$与预测准确性呈直接关系，在预测较长时间范围时尤为明显。这一观察结果符合逻辑，也与传统时间序列模型一致；（4）在输入补丁重编程过程中增加注意力头的数量是有益的。

<!-- Media -->

<!-- figureText: — predict-720 Analysis on Number of Attention Heads Number of Rerogramming Attention Heads Analysis on Input Length - predict-720 - predict-336 — predict-192 -->

<img src="https://cdn.noedgeai.com/01957f5d-9de5-7380-ae8c-08021ee888f7_16.jpg?x=335&y=772&w=1126&h=902&r=0"/>

Figure 6: Analysis of hyperparameter sensitivity on ETTh1 dataset.

图6：ETTh1数据集上超参数敏感性分析。

<!-- Media -->

## D LONG-TERM AND SHORT-TERM FORECASTING

## D 长期和短期预测

### D.1 LONG-TERM FORECASTING

### D.1 长期预测

By solely reprogramming the smallest Llama model while keeping it intact, TIME-LLM attains SOTA performance in 36 out of 40 instances across eight time series benchmarks. This underscores the considerable potential of LLMs as robust and reliable time series forecasters. Furthermore, we benchmark the proposed method against other well-established baselines in Tab. 11. This comparison includes three notable statistical methods (AutoARIMA, AutoTheta, and AutoETS) (Herzen et al., 2022) and two recent time series models, N-HiTS (Challu et al., 2023b) and N-BEATS (Ore-shkin et al. 2020). Remarkably, TIME-LLM secures SOTA performance across all cases, surpassing the second-best results by significant margins of over 22% and 16% in terms of MSE and MAE.

通过仅对最小的Llama模型进行重新编程并保持其完整性，TIME - LLM在八个时间序列基准测试的40个实例中有36个达到了最优（SOTA）性能。这凸显了大语言模型（LLMs）作为强大且可靠的时间序列预测器的巨大潜力。此外，我们在表11中将所提出的方法与其他成熟的基线方法进行了基准测试。这种比较包括三种著名的统计方法（自动自回归积分滑动平均模型（AutoARIMA）、自动西塔模型（AutoTheta）和自动指数平滑模型（AutoETS））（赫尔岑（Herzen）等人，2022年）以及两种近期的时间序列模型，即N - HiTS（查卢（Challu）等人，2023b）和N - BEATS（奥列什金（Ore - shkin）等人，2020年）。值得注意的是，TIME - LLM在所有情况下都取得了最优性能，在均方误差（MSE）和平均绝对误差（MAE）方面分别比次优结果高出22%和16%以上。

<!-- Media -->

Table 10: Full long-term forecasting results. We set the forecasting horizons $H \in  \{ {24},{36},{48},{60}\}$ for ILI and $\{ {96},{192},{336},{720}\}$ for the others. A lower value indicates better performance. Red: the best,Blue: the second best.

表10：完整的长期预测结果。我们将流感样病例（ILI）的预测期设定为$H \in  \{ {24},{36},{48},{60}\}$，其他的设定为$\{ {96},{192},{336},{720}\}$。数值越低表示性能越好。红色：最佳，蓝色：次佳。

<table><tr><td>Methods</td><td colspan="2">TIME-LLM</td><td colspan="2">GPT4TS</td><td colspan="2">DLinear</td><td colspan="2">PatchTST</td><td colspan="2">TimesNet</td><td colspan="2">FEDformer</td><td colspan="2">Autoformer</td><td colspan="2">Stationary</td><td colspan="2">ETSformer</td><td colspan="2">LightTS</td><td colspan="2">Informer</td><td colspan="2">Reformer</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="5">96 2023-03 192 336 720 Avg</td><td>0.362</td><td>0.392</td><td>0.376</td><td>0.397</td><td>0.375</td><td>0.399</td><td>$\underline{0.370}$</td><td>0.399</td><td>0.384</td><td>0.402</td><td>0.376</td><td>0.419</td><td>0.449</td><td>0.459</td><td>0.513</td><td>0.491</td><td>0.494</td><td>0.479</td><td>0.424</td><td>0.432</td><td>0.865</td><td>0.713</td><td>0.837</td><td>0.728</td></tr><tr><td>0.398</td><td>$\underline{0.418}$</td><td>0.416</td><td>0.418</td><td>$\underline{0.405}$</td><td>0.416</td><td>0.413</td><td>0.421</td><td>0.436</td><td>0.429</td><td>0.420</td><td>0.448</td><td>0.500</td><td>0.482</td><td>0.534</td><td>0.504</td><td>0.538</td><td>0.504</td><td>0.475</td><td>0.462</td><td>1.008</td><td>0.792</td><td>0.923</td><td>0.766</td></tr><tr><td>$\underline{0.430}$</td><td>0.427</td><td>0.442</td><td>$\underline{0.433}$</td><td>0.439</td><td>0.443</td><td>0.422</td><td>0.436</td><td>0.491</td><td>0.469</td><td>0.459</td><td>0.465</td><td>0.521</td><td>0.496</td><td>0.588</td><td>0.535</td><td>0.574</td><td>0.521</td><td>0.518</td><td>0.488</td><td>1.107</td><td>0.809</td><td>1.097</td><td>0.835</td></tr><tr><td>0.442</td><td>0.457</td><td>0.477</td><td>0.456</td><td>0.472</td><td>0.490</td><td>0.447</td><td>0.466</td><td>0.521</td><td>0.500</td><td>0.506</td><td>0.507</td><td>0.514</td><td>0.512</td><td>0.643</td><td>0.616</td><td>0.562</td><td>0.535</td><td>0.547</td><td>0.533</td><td>1.181</td><td>0.865</td><td>1.257</td><td>0.889</td></tr><tr><td>0.408</td><td>0.423</td><td>0.465</td><td>0.455</td><td>0.422</td><td>0.437</td><td>$\underline{0.413}$</td><td>$\underline{0.430}$</td><td>0.458</td><td>0.450</td><td>0.440</td><td>0.460</td><td>0.496</td><td>0.487</td><td>0.570</td><td>0.537</td><td>0.542</td><td>0.510</td><td>0.491</td><td>0.479</td><td>1.040</td><td>0.795</td><td>1.029</td><td>0.805</td></tr><tr><td rowspan="5">96 ${ETTh2}$ 192 336 720 Avg</td><td>0.268</td><td>0.328</td><td>0.285</td><td>0.342</td><td>0.289</td><td>0.353</td><td>0.274</td><td>0.336</td><td>0.340</td><td>0.374</td><td>0.358</td><td>0.397</td><td>0.346</td><td>0.388</td><td>0.476</td><td>0.458</td><td>0.340</td><td>0.391</td><td>0.397</td><td>0.437</td><td>3.755</td><td>1.525</td><td>2.626</td><td>1.317</td></tr><tr><td>0.329</td><td>0.375</td><td>0.354</td><td>0.389</td><td>0.383</td><td>0.418</td><td>$\underline{0.339}$</td><td>$\underline{0.379}$</td><td>0.402</td><td>0.414</td><td>0.429</td><td>0.439</td><td>0.456</td><td>0.452</td><td>0.512</td><td>0.493</td><td>0.430</td><td>0.439</td><td>0.520</td><td>0.504</td><td>5.602</td><td>1.931</td><td>11.12</td><td>2.979</td></tr><tr><td>0.368</td><td>0.409</td><td>0.373</td><td>0.407</td><td>0.448</td><td>0.465</td><td>0.329</td><td>0.380</td><td>0.452</td><td>0.452</td><td>0.496</td><td>0.487</td><td>0.482</td><td>0.486</td><td>0.552</td><td>0.551</td><td>0.485</td><td>0.479</td><td>0.626</td><td>0.559</td><td>4.721</td><td>1.835</td><td>9.323</td><td>2.769</td></tr><tr><td>0.372</td><td>0.420</td><td>0.406</td><td>0.441</td><td>0.605</td><td>0.551</td><td>0.379</td><td>0.422</td><td>0.462</td><td>0.468</td><td>0.463</td><td>0.474</td><td>0.515</td><td>0.511</td><td>0.562</td><td>0.560</td><td>0.500</td><td>0.497</td><td>0.863</td><td>0.672</td><td>3.647</td><td>1.625</td><td>3.874</td><td>1.697</td></tr><tr><td>$\underline{0.334}$</td><td>$\underline{0.383}$</td><td>0.381</td><td>0.412</td><td>0.431</td><td>0.446</td><td>0.330</td><td>0.379</td><td>0.414</td><td>0.427</td><td>0.437</td><td>0.449</td><td>0.450</td><td>0.459</td><td>0.526</td><td>0.516</td><td>0.439</td><td>0.452</td><td>0.602</td><td>0.543</td><td>4.431</td><td>1.729</td><td>6.736</td><td>2.191</td></tr><tr><td rowspan="5">96 ${ETTm1}$ 192 336 720 Avg</td><td>0.272</td><td>0.334</td><td>0.292</td><td>0.346</td><td>0.299</td><td>0.343</td><td>0.290</td><td>0.342</td><td>0.338</td><td>0.375</td><td>0.379</td><td>0.419</td><td>0.505</td><td>0.475</td><td>0.386</td><td>0.398</td><td>0.375</td><td>0.398</td><td>0.374</td><td>0.400</td><td>0.672</td><td>0.571</td><td>0.538</td><td>0.528</td></tr><tr><td>0.310</td><td>0.358</td><td>0.332</td><td>0.372</td><td>0.335</td><td>0.365</td><td>0.332</td><td>0.369</td><td>0.374</td><td>0.387</td><td>0.426</td><td>0.441</td><td>0.553</td><td>0.496</td><td>0.459</td><td>0.444</td><td>0.408</td><td>0.410</td><td>0.400</td><td>0.407</td><td>0.795</td><td>0.669</td><td>0.658</td><td>0.592</td></tr><tr><td>0.352</td><td>0.384</td><td>0.366</td><td>0.394</td><td>0.369</td><td>0.386</td><td>0.366</td><td>0.392</td><td>0.410</td><td>0.411</td><td>0.445</td><td>0.459</td><td>0.621</td><td>0.537</td><td>0.495</td><td>0.464</td><td>0.435</td><td>0.428</td><td>0.438</td><td>0.438</td><td>1.212</td><td>0.871</td><td>0.898</td><td>0.721</td></tr><tr><td>0.383</td><td>0.411</td><td>0.417</td><td>0.421</td><td>0.425</td><td>0.421</td><td>$\underline{0.416}$</td><td>$\underline{0.420}$</td><td>0.478</td><td>0.450</td><td>0.543</td><td>0.490</td><td>0.671</td><td>0.561</td><td>0.585</td><td>0.516</td><td>0.499</td><td>0.462</td><td>0.527</td><td>0.502</td><td>1.166</td><td>0.823</td><td>1.102</td><td>0.841</td></tr><tr><td>0.329</td><td>0.372</td><td>0.388</td><td>0.403</td><td>0.357</td><td>$\underline{0.378}$</td><td>$\underline{0.351}$</td><td>0.380</td><td>0.400</td><td>0.406</td><td>0.448</td><td>0.452</td><td>0.588</td><td>0.517</td><td>0.481</td><td>0.456</td><td>0.429</td><td>0.425</td><td>0.435</td><td>0.437</td><td>0.961</td><td>0.734</td><td>0.799</td><td>0.671</td></tr><tr><td rowspan="5">96 ${ETTm2}$ 192 336 720 Avg</td><td>0.161</td><td>0.253</td><td>0.173</td><td>0.262</td><td>0.167</td><td>0.269</td><td>$\underline{0.165}$</td><td>$\underline{0.255}$</td><td>0.187</td><td>0.267</td><td>0.203</td><td>0.287</td><td>0.255</td><td>0.339</td><td>0.192</td><td>0.274</td><td>0.189</td><td>0.280</td><td>0.209</td><td>0.308</td><td>0.365</td><td>0.453</td><td>0.658</td><td>0.619</td></tr><tr><td>0.219</td><td>0.293</td><td>0.229</td><td>0.301</td><td>0.224</td><td>0.303</td><td>0.220</td><td>0.292</td><td>0.249</td><td>0.309</td><td>0.269</td><td>0.328</td><td>0.281</td><td>0.340</td><td>0.280</td><td>0.339</td><td>0.253</td><td>0.319</td><td>0.311</td><td>0.382</td><td>0.533</td><td>0.563</td><td>1.078</td><td>0.827</td></tr><tr><td>0.271</td><td>0.329</td><td>0.286</td><td>0.341</td><td>0.281</td><td>0.342</td><td>0.274</td><td>0.329</td><td>0.321</td><td>0.351</td><td>0.325</td><td>0.366</td><td>0.339</td><td>0.372</td><td>0.334</td><td>0.361</td><td>0.314</td><td>0.357</td><td>0.442</td><td>0.466</td><td>1.363</td><td>0.887</td><td>1.549</td><td>0.972</td></tr><tr><td>0.352</td><td>0.379</td><td>0.378</td><td>0.401</td><td>0.397</td><td>0.421</td><td>0.362</td><td>0.385</td><td>0.408</td><td>0.403</td><td>0.421</td><td>0.415</td><td>0.433</td><td>0.432</td><td>0.417</td><td>0.413</td><td>0.414</td><td>0.413</td><td>0.675</td><td>0.587</td><td>3.379</td><td>1.338</td><td>2.631</td><td>1.242</td></tr><tr><td>0.251</td><td>0.313</td><td>0.284</td><td>0.339</td><td>0.267</td><td>0.333</td><td>$\underline{0.255}$</td><td>$\underline{0.315}$</td><td>0.291</td><td>0.333</td><td>0.305</td><td>0.349</td><td>0.327</td><td>0.371</td><td>0.306</td><td>0.347</td><td>0.293</td><td>0.342</td><td>0.409</td><td>0.436</td><td>1.410</td><td>0.810</td><td>1.479</td><td>0.915</td></tr><tr><td rowspan="5">96 Weather 192 336 720 Avg</td><td>0.147</td><td>0.201</td><td>0.162</td><td>0.212</td><td>0.176</td><td>0.237</td><td>0.149</td><td>0.198</td><td>0.172</td><td>0.220</td><td>0.217</td><td>0.296</td><td>0.266</td><td>0.336</td><td>0.173</td><td>0.223</td><td>0.197</td><td>0.281</td><td>0.182</td><td>0.242</td><td>0.300</td><td>0.384</td><td>0.689</td><td>0.596</td></tr><tr><td>0.189</td><td>0.234</td><td>0.204</td><td>0.248</td><td>0.220</td><td>0.282</td><td>0.194</td><td>0.241</td><td>0.219</td><td>0.261</td><td>0.276</td><td>0.336</td><td>0.307</td><td>0.367</td><td>0.245</td><td>0.285</td><td>0.237</td><td>0.312</td><td>0.227</td><td>0.287</td><td>0.598</td><td>0.544</td><td>0.752</td><td>0.638</td></tr><tr><td>$\underline{0.262}$</td><td>0.279</td><td>0.254</td><td>0.286</td><td>0.265</td><td>0.319</td><td>0.245</td><td>$\underline{0.282}$</td><td>0.280</td><td>0.306</td><td>0.339</td><td>0.380</td><td>0.359</td><td>0.395</td><td>0.321</td><td>0.338</td><td>0.298</td><td>0.353</td><td>0.282</td><td>0.334</td><td>0.578</td><td>0.523</td><td>0.639</td><td>0.596</td></tr><tr><td>0.304</td><td>0.316</td><td>0.326</td><td>0.337</td><td>0.333</td><td>0.362</td><td>0.314</td><td>0.334</td><td>0.365</td><td>0.359</td><td>0.403</td><td>0.428</td><td>0.419</td><td>0.428</td><td>0.414</td><td>0.410</td><td>0.352</td><td>0.288</td><td>0.352</td><td>0.386</td><td>1.059</td><td>0.741</td><td>1.130</td><td>0.792</td></tr><tr><td>0.225</td><td>0.257</td><td>0.237</td><td>0.270</td><td>0.248</td><td>0.300</td><td>0.225</td><td>$\underline{0.264}$</td><td>0.259</td><td>0.287</td><td>0.309</td><td>0.360</td><td>0.338</td><td>0.382</td><td>0.288</td><td>0.314</td><td>0.271</td><td>0.334</td><td>0.261</td><td>0.312</td><td>0.634</td><td>0.548</td><td>0.803</td><td>0.656</td></tr><tr><td rowspan="5">Electricity96192336720Avg</td><td>0.131</td><td>$\underline{0.224}$</td><td>0.139</td><td>0.238</td><td>0.140</td><td>0.237</td><td>0.129</td><td>0.222</td><td>0.168</td><td>0.272</td><td>0.193</td><td>0.308</td><td>0.201</td><td>0.317</td><td>0.169</td><td>0.273</td><td>0.187</td><td>0.304</td><td>0.207</td><td>0.307</td><td>0.274</td><td>0.368</td><td>0.312</td><td>0.402</td></tr><tr><td>0.152</td><td>0.241</td><td>0.153</td><td>0.251</td><td>0.153</td><td>0.249</td><td>0.157</td><td>0.240</td><td>0.184</td><td>0.289</td><td>0.201</td><td>0.315</td><td>0.222</td><td>0.334</td><td>0.182</td><td>0.286</td><td>0.199</td><td>0.315</td><td>0.213</td><td>0.316</td><td>0.296</td><td>0.386</td><td>0.348</td><td>0.433</td></tr><tr><td>0.160</td><td>0.248</td><td>0.169</td><td>0.266</td><td>0.169</td><td>0.267</td><td>0.163</td><td>0.259</td><td>0.198</td><td>0.300</td><td>0.214</td><td>0.329</td><td>0.231</td><td>0.338</td><td>0.200</td><td>0.304</td><td>0.212</td><td>0.329</td><td>0.230</td><td>0.333</td><td>0.300</td><td>0.394</td><td>0.350</td><td>0.433</td></tr><tr><td>0.192</td><td>0.298</td><td>0.206</td><td>0.297</td><td>0.203</td><td>0.301</td><td>0.197</td><td>0.290</td><td>0.220</td><td>0.320</td><td>0.246</td><td>0.355</td><td>0.254</td><td>0.361</td><td>0.222</td><td>0.321</td><td>0.233</td><td>0.345</td><td>0.265</td><td>0.360</td><td>0.373</td><td>0.439</td><td>0.340</td><td>0.420</td></tr><tr><td>0.158</td><td>0.252</td><td>0.167</td><td>0.263</td><td>0.166</td><td>0.263</td><td>$\underline{0.161}$</td><td>0.252</td><td>0.192</td><td>0.295</td><td>0.214</td><td>0.327</td><td>0.227</td><td>0.338</td><td>0.193</td><td>0.296</td><td>0.208</td><td>0.323</td><td>0.229</td><td>0.329</td><td>0.311</td><td>0.397</td><td>0.338</td><td>0.422</td></tr><tr><td rowspan="5">96 HK\$’000 192 336 720 Avg</td><td>0.362</td><td>0.248</td><td>0.388</td><td>0.282</td><td>0.410</td><td>0.282</td><td>0.360</td><td>0.249</td><td>0.593</td><td>0.321</td><td>0.587</td><td>0.366</td><td>0.613</td><td>0.388</td><td>0.612</td><td>0.338</td><td>0.607</td><td>0.392</td><td>0.615</td><td>0.391</td><td>0.719</td><td>0.391</td><td>0.732</td><td>0.423</td></tr><tr><td>0.374</td><td>0.247</td><td>0.407</td><td>0.290</td><td>0.423</td><td>0.287</td><td>0.379</td><td>0.256</td><td>0.617</td><td>0.336</td><td>0.604</td><td>0.373</td><td>0.616</td><td>0.382</td><td>0.613</td><td>0.340</td><td>0.621</td><td>0.399</td><td>0.601</td><td>0.382</td><td>0.696</td><td>0.379</td><td>0.733</td><td>0.420</td></tr><tr><td>0.385</td><td>$\underline{0.271}$</td><td>0.412</td><td>0.294</td><td>0.436</td><td>0.296</td><td>$\underline{0.392}$</td><td>0.264</td><td>0.629</td><td>0.336</td><td>0.621</td><td>0.383</td><td>0.622</td><td>0.337</td><td>0.618</td><td>0.328</td><td>0.622</td><td>0.396</td><td>0.613</td><td>0.386</td><td>0.777</td><td>0.420</td><td>0.742</td><td>0.420</td></tr><tr><td>0.430</td><td>0.288</td><td>0.450</td><td>0.312</td><td>0.466</td><td>0.315</td><td>0.432</td><td>0.286</td><td>0.640</td><td>0.350</td><td>0.626</td><td>0.382</td><td>0.660</td><td>0.408</td><td>0.653</td><td>0.355</td><td>0.632</td><td>0.396</td><td>0.658</td><td>0.407</td><td>0.864</td><td>0.472</td><td>0.755</td><td>0.423</td></tr><tr><td>0.388</td><td>$\underline{0.264}$</td><td>0.414</td><td>0.294</td><td>0.433</td><td>0.295</td><td>$\underline{0.390}$</td><td>0.263</td><td>0.620</td><td>0.336</td><td>0.610</td><td>0.376</td><td>0.628</td><td>0.379</td><td>0.624</td><td>0.340</td><td>0.621</td><td>0.396</td><td>0.622</td><td>0.392</td><td>0.764</td><td>0.416</td><td>0.741</td><td>0.422</td></tr><tr><td rowspan="5">24 36 四 48 60 Avg</td><td>1.285</td><td>0.727</td><td>2.063</td><td>0.881</td><td>2.215</td><td>1.081</td><td>1.319</td><td>0.754</td><td>2.317</td><td>0.934</td><td>3.228</td><td>1.260</td><td>3.483</td><td>1.287</td><td>2.294</td><td>0.945</td><td>2.527</td><td>1.020</td><td>8.313</td><td>2.144</td><td>5.764</td><td>1.677</td><td>4.400</td><td>1.382</td></tr><tr><td>1.404</td><td>0.814</td><td>1.868</td><td>0.892</td><td>1.963</td><td>0.963</td><td>1.430</td><td>0.834</td><td>1.972</td><td>0.920</td><td>2.679</td><td>1.080</td><td>3.103</td><td>1.148</td><td>1.825</td><td>0.848</td><td>2.615</td><td>1.007</td><td>6.631</td><td>1.902</td><td>4.755</td><td>1.467</td><td>4.783</td><td>1.448</td></tr><tr><td>1.523</td><td>0.807</td><td>1.790</td><td>0.884</td><td>2.130</td><td>1.024</td><td>1.553</td><td>0.815</td><td>2.238</td><td>0.940</td><td>2.622</td><td>1.078</td><td>2.669</td><td>1.085</td><td>2.010</td><td>0.900</td><td>2.359</td><td>0.972</td><td>7.299</td><td>1.982</td><td>4.763</td><td>1.469</td><td>4.832</td><td>1.465</td></tr><tr><td>1.531</td><td>0.854</td><td>1.979</td><td>0.957</td><td>2.368</td><td>1.096</td><td>1.470</td><td>0.788</td><td>2.027</td><td>0.928</td><td>2.857</td><td>1.157</td><td>2.770</td><td>1.125</td><td>2.178</td><td>0.963</td><td>2.487</td><td>1.016</td><td>7.283</td><td>1.985</td><td>5.264</td><td>1.564</td><td>4.882</td><td>1.483</td></tr><tr><td>1.435</td><td>$\underline{0.801}$</td><td>1.925</td><td>0.903</td><td>2.169</td><td>1.041</td><td>1.443</td><td>0.797</td><td>2.139</td><td>0.931</td><td>2.847</td><td>1.144</td><td>3.006</td><td>1.161</td><td>2.077</td><td>0.914</td><td>2.497</td><td>1.004</td><td>7.382</td><td>2.003</td><td>5.137</td><td>1.544</td><td>4.724</td><td>1.445</td></tr><tr><td>${1}^{\text{st }}$ Count</td><td colspan="2">36│</td><td colspan="2">0</td><td colspan="2">1</td><td colspan="2">$\underline{17}$</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td></tr></table>

<table><tbody><tr><td>方法</td><td colspan="2">TIME大语言模型（TIME-LLM）</td><td colspan="2">GPT4时间序列模型（GPT4TS）</td><td colspan="2">深度线性模型（DLinear）</td><td colspan="2">补丁时间序列变换器（PatchTST）</td><td colspan="2">时间网络（TimesNet）</td><td colspan="2">FEDformer（联邦变换器）</td><td colspan="2">Autoformer（自动变换器）</td><td colspan="2">平稳的</td><td colspan="2">ETSformer（ETS变换器）</td><td colspan="2">LightTS（轻量级时间序列）</td><td colspan="2">Informer（信息者）</td><td colspan="2">改革者</td></tr><tr><td>指标</td><td>均方误差（Mean Squared Error）</td><td>平均绝对误差（Mean Absolute Error）</td><td>均方误差（Mean Squared Error）</td><td>平均绝对误差（Mean Absolute Error）</td><td>均方误差（Mean Squared Error）</td><td>平均绝对误差（Mean Absolute Error）</td><td>均方误差（Mean Squared Error）</td><td>平均绝对误差（Mean Absolute Error）</td><td>均方误差（Mean Squared Error）</td><td>平均绝对误差（Mean Absolute Error）</td><td>均方误差（Mean Squared Error）</td><td>平均绝对误差（Mean Absolute Error）</td><td>均方误差（Mean Squared Error）</td><td>平均绝对误差（Mean Absolute Error）</td><td>均方误差（Mean Squared Error）</td><td>平均绝对误差（Mean Absolute Error）</td><td>均方误差（Mean Squared Error）</td><td>平均绝对误差（Mean Absolute Error）</td><td>均方误差（Mean Squared Error）</td><td>平均绝对误差（Mean Absolute Error）</td><td>均方误差（Mean Squared Error）</td><td>平均绝对误差（Mean Absolute Error）</td><td>均方误差（Mean Squared Error）</td><td>平均绝对误差（Mean Absolute Error）</td></tr><tr><td rowspan="5">96 2023年3月 192 336 720 平均值</td><td>0.362</td><td>0.392</td><td>0.376</td><td>0.397</td><td>0.375</td><td>0.399</td><td>$\underline{0.370}$</td><td>0.399</td><td>0.384</td><td>0.402</td><td>0.376</td><td>0.419</td><td>0.449</td><td>0.459</td><td>0.513</td><td>0.491</td><td>0.494</td><td>0.479</td><td>0.424</td><td>0.432</td><td>0.865</td><td>0.713</td><td>0.837</td><td>0.728</td></tr><tr><td>0.398</td><td>$\underline{0.418}$</td><td>0.416</td><td>0.418</td><td>$\underline{0.405}$</td><td>0.416</td><td>0.413</td><td>0.421</td><td>0.436</td><td>0.429</td><td>0.420</td><td>0.448</td><td>0.500</td><td>0.482</td><td>0.534</td><td>0.504</td><td>0.538</td><td>0.504</td><td>0.475</td><td>0.462</td><td>1.008</td><td>0.792</td><td>0.923</td><td>0.766</td></tr><tr><td>$\underline{0.430}$</td><td>0.427</td><td>0.442</td><td>$\underline{0.433}$</td><td>0.439</td><td>0.443</td><td>0.422</td><td>0.436</td><td>0.491</td><td>0.469</td><td>0.459</td><td>0.465</td><td>0.521</td><td>0.496</td><td>0.588</td><td>0.535</td><td>0.574</td><td>0.521</td><td>0.518</td><td>0.488</td><td>1.107</td><td>0.809</td><td>1.097</td><td>0.835</td></tr><tr><td>0.442</td><td>0.457</td><td>0.477</td><td>0.456</td><td>0.472</td><td>0.490</td><td>0.447</td><td>0.466</td><td>0.521</td><td>0.500</td><td>0.506</td><td>0.507</td><td>0.514</td><td>0.512</td><td>0.643</td><td>0.616</td><td>0.562</td><td>0.535</td><td>0.547</td><td>0.533</td><td>1.181</td><td>0.865</td><td>1.257</td><td>0.889</td></tr><tr><td>0.408</td><td>0.423</td><td>0.465</td><td>0.455</td><td>0.422</td><td>0.437</td><td>$\underline{0.413}$</td><td>$\underline{0.430}$</td><td>0.458</td><td>0.450</td><td>0.440</td><td>0.460</td><td>0.496</td><td>0.487</td><td>0.570</td><td>0.537</td><td>0.542</td><td>0.510</td><td>0.491</td><td>0.479</td><td>1.040</td><td>0.795</td><td>1.029</td><td>0.805</td></tr><tr><td rowspan="5">96 [乳胶0] 192 336 720 平均值</td><td>0.268</td><td>0.328</td><td>0.285</td><td>0.342</td><td>0.289</td><td>0.353</td><td>0.274</td><td>0.336</td><td>0.340</td><td>0.374</td><td>0.358</td><td>0.397</td><td>0.346</td><td>0.388</td><td>0.476</td><td>0.458</td><td>0.340</td><td>0.391</td><td>0.397</td><td>0.437</td><td>3.755</td><td>1.525</td><td>2.626</td><td>1.317</td></tr><tr><td>0.329</td><td>0.375</td><td>0.354</td><td>0.389</td><td>0.383</td><td>0.418</td><td>$\underline{0.339}$</td><td>$\underline{0.379}$</td><td>0.402</td><td>0.414</td><td>0.429</td><td>0.439</td><td>0.456</td><td>0.452</td><td>0.512</td><td>0.493</td><td>0.430</td><td>0.439</td><td>0.520</td><td>0.504</td><td>5.602</td><td>1.931</td><td>11.12</td><td>2.979</td></tr><tr><td>0.368</td><td>0.409</td><td>0.373</td><td>0.407</td><td>0.448</td><td>0.465</td><td>0.329</td><td>0.380</td><td>0.452</td><td>0.452</td><td>0.496</td><td>0.487</td><td>0.482</td><td>0.486</td><td>0.552</td><td>0.551</td><td>0.485</td><td>0.479</td><td>0.626</td><td>0.559</td><td>4.721</td><td>1.835</td><td>9.323</td><td>2.769</td></tr><tr><td>0.372</td><td>0.420</td><td>0.406</td><td>0.441</td><td>0.605</td><td>0.551</td><td>0.379</td><td>0.422</td><td>0.462</td><td>0.468</td><td>0.463</td><td>0.474</td><td>0.515</td><td>0.511</td><td>0.562</td><td>0.560</td><td>0.500</td><td>0.497</td><td>0.863</td><td>0.672</td><td>3.647</td><td>1.625</td><td>3.874</td><td>1.697</td></tr><tr><td>$\underline{0.334}$</td><td>$\underline{0.383}$</td><td>0.381</td><td>0.412</td><td>0.431</td><td>0.446</td><td>0.330</td><td>0.379</td><td>0.414</td><td>0.427</td><td>0.437</td><td>0.449</td><td>0.450</td><td>0.459</td><td>0.526</td><td>0.516</td><td>0.439</td><td>0.452</td><td>0.602</td><td>0.543</td><td>4.431</td><td>1.729</td><td>6.736</td><td>2.191</td></tr><tr><td rowspan="5">96 ${ETTm1}$ 192 336 720 平均值</td><td>0.272</td><td>0.334</td><td>0.292</td><td>0.346</td><td>0.299</td><td>0.343</td><td>0.290</td><td>0.342</td><td>0.338</td><td>0.375</td><td>0.379</td><td>0.419</td><td>0.505</td><td>0.475</td><td>0.386</td><td>0.398</td><td>0.375</td><td>0.398</td><td>0.374</td><td>0.400</td><td>0.672</td><td>0.571</td><td>0.538</td><td>0.528</td></tr><tr><td>0.310</td><td>0.358</td><td>0.332</td><td>0.372</td><td>0.335</td><td>0.365</td><td>0.332</td><td>0.369</td><td>0.374</td><td>0.387</td><td>0.426</td><td>0.441</td><td>0.553</td><td>0.496</td><td>0.459</td><td>0.444</td><td>0.408</td><td>0.410</td><td>0.400</td><td>0.407</td><td>0.795</td><td>0.669</td><td>0.658</td><td>0.592</td></tr><tr><td>0.352</td><td>0.384</td><td>0.366</td><td>0.394</td><td>0.369</td><td>0.386</td><td>0.366</td><td>0.392</td><td>0.410</td><td>0.411</td><td>0.445</td><td>0.459</td><td>0.621</td><td>0.537</td><td>0.495</td><td>0.464</td><td>0.435</td><td>0.428</td><td>0.438</td><td>0.438</td><td>1.212</td><td>0.871</td><td>0.898</td><td>0.721</td></tr><tr><td>0.383</td><td>0.411</td><td>0.417</td><td>0.421</td><td>0.425</td><td>0.421</td><td>$\underline{0.416}$</td><td>$\underline{0.420}$</td><td>0.478</td><td>0.450</td><td>0.543</td><td>0.490</td><td>0.671</td><td>0.561</td><td>0.585</td><td>0.516</td><td>0.499</td><td>0.462</td><td>0.527</td><td>0.502</td><td>1.166</td><td>0.823</td><td>1.102</td><td>0.841</td></tr><tr><td>0.329</td><td>0.372</td><td>0.388</td><td>0.403</td><td>0.357</td><td>$\underline{0.378}$</td><td>$\underline{0.351}$</td><td>0.380</td><td>0.400</td><td>0.406</td><td>0.448</td><td>0.452</td><td>0.588</td><td>0.517</td><td>0.481</td><td>0.456</td><td>0.429</td><td>0.425</td><td>0.435</td><td>0.437</td><td>0.961</td><td>0.734</td><td>0.799</td><td>0.671</td></tr><tr><td rowspan="5">96 ${ETTm2}$ 192 336 720 平均值</td><td>0.161</td><td>0.253</td><td>0.173</td><td>0.262</td><td>0.167</td><td>0.269</td><td>$\underline{0.165}$</td><td>$\underline{0.255}$</td><td>0.187</td><td>0.267</td><td>0.203</td><td>0.287</td><td>0.255</td><td>0.339</td><td>0.192</td><td>0.274</td><td>0.189</td><td>0.280</td><td>0.209</td><td>0.308</td><td>0.365</td><td>0.453</td><td>0.658</td><td>0.619</td></tr><tr><td>0.219</td><td>0.293</td><td>0.229</td><td>0.301</td><td>0.224</td><td>0.303</td><td>0.220</td><td>0.292</td><td>0.249</td><td>0.309</td><td>0.269</td><td>0.328</td><td>0.281</td><td>0.340</td><td>0.280</td><td>0.339</td><td>0.253</td><td>0.319</td><td>0.311</td><td>0.382</td><td>0.533</td><td>0.563</td><td>1.078</td><td>0.827</td></tr><tr><td>0.271</td><td>0.329</td><td>0.286</td><td>0.341</td><td>0.281</td><td>0.342</td><td>0.274</td><td>0.329</td><td>0.321</td><td>0.351</td><td>0.325</td><td>0.366</td><td>0.339</td><td>0.372</td><td>0.334</td><td>0.361</td><td>0.314</td><td>0.357</td><td>0.442</td><td>0.466</td><td>1.363</td><td>0.887</td><td>1.549</td><td>0.972</td></tr><tr><td>0.352</td><td>0.379</td><td>0.378</td><td>0.401</td><td>0.397</td><td>0.421</td><td>0.362</td><td>0.385</td><td>0.408</td><td>0.403</td><td>0.421</td><td>0.415</td><td>0.433</td><td>0.432</td><td>0.417</td><td>0.413</td><td>0.414</td><td>0.413</td><td>0.675</td><td>0.587</td><td>3.379</td><td>1.338</td><td>2.631</td><td>1.242</td></tr><tr><td>0.251</td><td>0.313</td><td>0.284</td><td>0.339</td><td>0.267</td><td>0.333</td><td>$\underline{0.255}$</td><td>$\underline{0.315}$</td><td>0.291</td><td>0.333</td><td>0.305</td><td>0.349</td><td>0.327</td><td>0.371</td><td>0.306</td><td>0.347</td><td>0.293</td><td>0.342</td><td>0.409</td><td>0.436</td><td>1.410</td><td>0.810</td><td>1.479</td><td>0.915</td></tr><tr><td rowspan="5">96 天气 192 336 720 平均值</td><td>0.147</td><td>0.201</td><td>0.162</td><td>0.212</td><td>0.176</td><td>0.237</td><td>0.149</td><td>0.198</td><td>0.172</td><td>0.220</td><td>0.217</td><td>0.296</td><td>0.266</td><td>0.336</td><td>0.173</td><td>0.223</td><td>0.197</td><td>0.281</td><td>0.182</td><td>0.242</td><td>0.300</td><td>0.384</td><td>0.689</td><td>0.596</td></tr><tr><td>0.189</td><td>0.234</td><td>0.204</td><td>0.248</td><td>0.220</td><td>0.282</td><td>0.194</td><td>0.241</td><td>0.219</td><td>0.261</td><td>0.276</td><td>0.336</td><td>0.307</td><td>0.367</td><td>0.245</td><td>0.285</td><td>0.237</td><td>0.312</td><td>0.227</td><td>0.287</td><td>0.598</td><td>0.544</td><td>0.752</td><td>0.638</td></tr><tr><td>$\underline{0.262}$</td><td>0.279</td><td>0.254</td><td>0.286</td><td>0.265</td><td>0.319</td><td>0.245</td><td>$\underline{0.282}$</td><td>0.280</td><td>0.306</td><td>0.339</td><td>0.380</td><td>0.359</td><td>0.395</td><td>0.321</td><td>0.338</td><td>0.298</td><td>0.353</td><td>0.282</td><td>0.334</td><td>0.578</td><td>0.523</td><td>0.639</td><td>0.596</td></tr><tr><td>0.304</td><td>0.316</td><td>0.326</td><td>0.337</td><td>0.333</td><td>0.362</td><td>0.314</td><td>0.334</td><td>0.365</td><td>0.359</td><td>0.403</td><td>0.428</td><td>0.419</td><td>0.428</td><td>0.414</td><td>0.410</td><td>0.352</td><td>0.288</td><td>0.352</td><td>0.386</td><td>1.059</td><td>0.741</td><td>1.130</td><td>0.792</td></tr><tr><td>0.225</td><td>0.257</td><td>0.237</td><td>0.270</td><td>0.248</td><td>0.300</td><td>0.225</td><td>$\underline{0.264}$</td><td>0.259</td><td>0.287</td><td>0.309</td><td>0.360</td><td>0.338</td><td>0.382</td><td>0.288</td><td>0.314</td><td>0.271</td><td>0.334</td><td>0.261</td><td>0.312</td><td>0.634</td><td>0.548</td><td>0.803</td><td>0.656</td></tr><tr><td rowspan="5">电力96192336720平均值</td><td>0.131</td><td>$\underline{0.224}$</td><td>0.139</td><td>0.238</td><td>0.140</td><td>0.237</td><td>0.129</td><td>0.222</td><td>0.168</td><td>0.272</td><td>0.193</td><td>0.308</td><td>0.201</td><td>0.317</td><td>0.169</td><td>0.273</td><td>0.187</td><td>0.304</td><td>0.207</td><td>0.307</td><td>0.274</td><td>0.368</td><td>0.312</td><td>0.402</td></tr><tr><td>0.152</td><td>0.241</td><td>0.153</td><td>0.251</td><td>0.153</td><td>0.249</td><td>0.157</td><td>0.240</td><td>0.184</td><td>0.289</td><td>0.201</td><td>0.315</td><td>0.222</td><td>0.334</td><td>0.182</td><td>0.286</td><td>0.199</td><td>0.315</td><td>0.213</td><td>0.316</td><td>0.296</td><td>0.386</td><td>0.348</td><td>0.433</td></tr><tr><td>0.160</td><td>0.248</td><td>0.169</td><td>0.266</td><td>0.169</td><td>0.267</td><td>0.163</td><td>0.259</td><td>0.198</td><td>0.300</td><td>0.214</td><td>0.329</td><td>0.231</td><td>0.338</td><td>0.200</td><td>0.304</td><td>0.212</td><td>0.329</td><td>0.230</td><td>0.333</td><td>0.300</td><td>0.394</td><td>0.350</td><td>0.433</td></tr><tr><td>0.192</td><td>0.298</td><td>0.206</td><td>0.297</td><td>0.203</td><td>0.301</td><td>0.197</td><td>0.290</td><td>0.220</td><td>0.320</td><td>0.246</td><td>0.355</td><td>0.254</td><td>0.361</td><td>0.222</td><td>0.321</td><td>0.233</td><td>0.345</td><td>0.265</td><td>0.360</td><td>0.373</td><td>0.439</td><td>0.340</td><td>0.420</td></tr><tr><td>0.158</td><td>0.252</td><td>0.167</td><td>0.263</td><td>0.166</td><td>0.263</td><td>$\underline{0.161}$</td><td>0.252</td><td>0.192</td><td>0.295</td><td>0.214</td><td>0.327</td><td>0.227</td><td>0.338</td><td>0.193</td><td>0.296</td><td>0.208</td><td>0.323</td><td>0.229</td><td>0.329</td><td>0.311</td><td>0.397</td><td>0.338</td><td>0.422</td></tr><tr><td rowspan="5">96 港元千元 192 336 720 平均值</td><td>0.362</td><td>0.248</td><td>0.388</td><td>0.282</td><td>0.410</td><td>0.282</td><td>0.360</td><td>0.249</td><td>0.593</td><td>0.321</td><td>0.587</td><td>0.366</td><td>0.613</td><td>0.388</td><td>0.612</td><td>0.338</td><td>0.607</td><td>0.392</td><td>0.615</td><td>0.391</td><td>0.719</td><td>0.391</td><td>0.732</td><td>0.423</td></tr><tr><td>0.374</td><td>0.247</td><td>0.407</td><td>0.290</td><td>0.423</td><td>0.287</td><td>0.379</td><td>0.256</td><td>0.617</td><td>0.336</td><td>0.604</td><td>0.373</td><td>0.616</td><td>0.382</td><td>0.613</td><td>0.340</td><td>0.621</td><td>0.399</td><td>0.601</td><td>0.382</td><td>0.696</td><td>0.379</td><td>0.733</td><td>0.420</td></tr><tr><td>0.385</td><td>$\underline{0.271}$</td><td>0.412</td><td>0.294</td><td>0.436</td><td>0.296</td><td>$\underline{0.392}$</td><td>0.264</td><td>0.629</td><td>0.336</td><td>0.621</td><td>0.383</td><td>0.622</td><td>0.337</td><td>0.618</td><td>0.328</td><td>0.622</td><td>0.396</td><td>0.613</td><td>0.386</td><td>0.777</td><td>0.420</td><td>0.742</td><td>0.420</td></tr><tr><td>0.430</td><td>0.288</td><td>0.450</td><td>0.312</td><td>0.466</td><td>0.315</td><td>0.432</td><td>0.286</td><td>0.640</td><td>0.350</td><td>0.626</td><td>0.382</td><td>0.660</td><td>0.408</td><td>0.653</td><td>0.355</td><td>0.632</td><td>0.396</td><td>0.658</td><td>0.407</td><td>0.864</td><td>0.472</td><td>0.755</td><td>0.423</td></tr><tr><td>0.388</td><td>$\underline{0.264}$</td><td>0.414</td><td>0.294</td><td>0.433</td><td>0.295</td><td>$\underline{0.390}$</td><td>0.263</td><td>0.620</td><td>0.336</td><td>0.610</td><td>0.376</td><td>0.628</td><td>0.379</td><td>0.624</td><td>0.340</td><td>0.621</td><td>0.396</td><td>0.622</td><td>0.392</td><td>0.764</td><td>0.416</td><td>0.741</td><td>0.422</td></tr><tr><td rowspan="5">24 36 四 48 60 平均值</td><td>1.285</td><td>0.727</td><td>2.063</td><td>0.881</td><td>2.215</td><td>1.081</td><td>1.319</td><td>0.754</td><td>2.317</td><td>0.934</td><td>3.228</td><td>1.260</td><td>3.483</td><td>1.287</td><td>2.294</td><td>0.945</td><td>2.527</td><td>1.020</td><td>8.313</td><td>2.144</td><td>5.764</td><td>1.677</td><td>4.400</td><td>1.382</td></tr><tr><td>1.404</td><td>0.814</td><td>1.868</td><td>0.892</td><td>1.963</td><td>0.963</td><td>1.430</td><td>0.834</td><td>1.972</td><td>0.920</td><td>2.679</td><td>1.080</td><td>3.103</td><td>1.148</td><td>1.825</td><td>0.848</td><td>2.615</td><td>1.007</td><td>6.631</td><td>1.902</td><td>4.755</td><td>1.467</td><td>4.783</td><td>1.448</td></tr><tr><td>1.523</td><td>0.807</td><td>1.790</td><td>0.884</td><td>2.130</td><td>1.024</td><td>1.553</td><td>0.815</td><td>2.238</td><td>0.940</td><td>2.622</td><td>1.078</td><td>2.669</td><td>1.085</td><td>2.010</td><td>0.900</td><td>2.359</td><td>0.972</td><td>7.299</td><td>1.982</td><td>4.763</td><td>1.469</td><td>4.832</td><td>1.465</td></tr><tr><td>1.531</td><td>0.854</td><td>1.979</td><td>0.957</td><td>2.368</td><td>1.096</td><td>1.470</td><td>0.788</td><td>2.027</td><td>0.928</td><td>2.857</td><td>1.157</td><td>2.770</td><td>1.125</td><td>2.178</td><td>0.963</td><td>2.487</td><td>1.016</td><td>7.283</td><td>1.985</td><td>5.264</td><td>1.564</td><td>4.882</td><td>1.483</td></tr><tr><td>1.435</td><td>$\underline{0.801}$</td><td>1.925</td><td>0.903</td><td>2.169</td><td>1.041</td><td>1.443</td><td>0.797</td><td>2.139</td><td>0.931</td><td>2.847</td><td>1.144</td><td>3.006</td><td>1.161</td><td>2.077</td><td>0.914</td><td>2.497</td><td>1.004</td><td>7.382</td><td>2.003</td><td>5.137</td><td>1.544</td><td>4.724</td><td>1.445</td></tr><tr><td>${1}^{\text{st }}$ 计数</td><td colspan="2">36│</td><td colspan="2">0</td><td colspan="2">1</td><td colspan="2">$\underline{17}$</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td></tr></tbody></table>

<!-- Media -->

### D.2 SHORT-TERM FORECASTING

### D.2 短期预测

Our complete results on short-term forecasting are presented in Tab. 12. TIME-LLM consistently outperforms the majority of baseline models in most cases. Notably, we surpass GPT4TS by a large margin (e.g., 8.7% overall, 13.4% on M4-Yearly, and an average of 21.5% on M4-Hourly, M4-Daily, and M4-Weekly), as well as TimesNet (e.g., 10% overall, 14.1% on M4-Yearly, and an average of 30.1% on M4-Hourly, M4-Daily, and M4-Weekly). Compared to the recent state-of-the-art forecasting models, N-HiTS and PatchTST, TIME-LLM exhibits comparable or superior performances without any parameter updates on the backbone LLM.

我们关于短期预测的完整结果展示在表12中。在大多数情况下，TIME - LLM始终优于大多数基线模型。值得注意的是，我们大幅超越了GPT4TS（例如，总体上超越8.7%，在M4年度数据上超越13.4%，在M4小时级、M4日级和M4周级数据上平均超越21.5%），也超越了TimesNet（例如，总体上超越10%，在M4年度数据上超越14.1%，在M4小时级、M4日级和M4周级数据上平均超越30.1%）。与近期最先进的预测模型N - HiTS和PatchTST相比，TIME - LLM在主干大语言模型（LLM）没有任何参数更新的情况下，表现出相当或更优的性能。

In addition, we conduct a comparative analysis between TIME-LLM and the top-performing models on the M3-Quarterly dataset, with the findings presented in Tab. 13. We provide additional metrics, namely MRAE and MAPE, alongside the default SMAPE used in the M3 competition. On this dataset, TIME-LLM attains on-par performance compared to TimesNet and PatchTST, outperforming GPT4TS by substantial margins, achieving reductions of over 23%, 35%, and 26% in SMAPE, MRAE, and MAPE, respectively.

此外，我们在M3季度数据集上对TIME - 大语言模型（TIME - LLM）和表现最佳的模型进行了比较分析，结果见表13。除了M3竞赛中默认使用的对称平均绝对百分比误差（SMAPE）外，我们还提供了另外两个指标，即平均绝对比误差（MRAE）和平均绝对百分比误差（MAPE）。在该数据集上，TIME - 大语言模型（TIME - LLM）的表现与时间网络（TimesNet）和补丁时间序列变换器（PatchTST）相当，大幅优于GPT4TS，对称平均绝对百分比误差（SMAPE）、平均绝对比误差（MRAE）和平均绝对百分比误差（MAPE）分别降低了超过23%、35%和26%。

## E FEW-SHOT AND ZERO-SHOT FORECASTING

## 少样本和零样本预测

### E.1 FEW-SHOT FORECASTING

### E.1 少样本预测

Our full results in few-shot forecasting tasks are detailed in Tab. 14 and Tab. 15 Within the scope of 10% few-shot learning, TIME-LLM secures SOTA performance in 32 out of 35 cases, spanning seven different time series benchmarks. Our approach's advantage becomes even more pronounced in the context of $5\%$ few-shot scenarios,achieving SOTA results in 21 out of 32 cases. We attribute this to the successful knowledge activation in our reprogrammed LLM.

我们在小样本预测任务中的完整结果详见表14和表15。在10%小样本学习范围内，TIME-LLM在35种情况中的32种里取得了最优（SOTA）性能，涵盖了七个不同的时间序列基准。在$5\%$小样本场景下，我们方法的优势更加明显，在32种情况中的21种里取得了最优结果。我们将此归因于我们重新编程的大语言模型（LLM）中知识的成功激活。

<!-- Media -->

Table 11: Additional comparison with other baselines in long-term forecasting tasks. We set the forecasting horizons $H \in  \{ {24},{36},{48},{60}\}$ for ILI and $\{ {96},{192},{336},{720}\}$ for the others. A lower value indicates better performance. Red: the best, Blue: the second best.

表11：在长期预测任务中与其他基线模型的额外比较。我们将流感样病例（ILI）的预测时长设置为$H \in  \{ {24},{36},{48},{60}\}$，其他的设置为$\{ {96},{192},{336},{720}\}$。数值越低表示性能越好。红色：最优；蓝色：次优。

<table><tr><td colspan="2">Methods</td><td colspan="2">TIME-LLM</td><td colspan="2">N-BEATS</td><td colspan="2">N-HiTS</td><td colspan="2">AutoARIMA</td><td colspan="2">AutoTheta</td><td colspan="2">AutoETS</td></tr><tr><td colspan="2">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="5">${ETTh1}$</td><td>96</td><td>0.362</td><td>0.392</td><td>0.496</td><td>0.475</td><td>0.392</td><td>0.407</td><td>0.933</td><td>0.635</td><td>1.266</td><td>0.758</td><td>1.264</td><td>0.756</td></tr><tr><td>192</td><td>0.398</td><td>0.418</td><td>0.544</td><td>0.504</td><td>0.442</td><td>0.438</td><td>0.868</td><td>0.621</td><td>1.188</td><td>0.749</td><td>1.181</td><td>0.745</td></tr><tr><td>336</td><td>0.430</td><td>0.427</td><td>0.592</td><td>0.533</td><td>0.497</td><td>0.471</td><td>0.964</td><td>0.663</td><td>1.310</td><td>0.799</td><td>1.292</td><td>0.792</td></tr><tr><td>720</td><td>0.442</td><td>0.457</td><td>0.639</td><td>0.588</td><td>0.559</td><td>0.533</td><td>1.043</td><td>0.705</td><td>1.510</td><td>0.882</td><td>1.405</td><td>0.842</td></tr><tr><td>Avg</td><td>0.408</td><td>0.423</td><td>0.568</td><td>0.525</td><td>0.473</td><td>0.462</td><td>0.952</td><td>0.656</td><td>1.319</td><td>0.797</td><td>1.286</td><td>0.784</td></tr><tr><td rowspan="5">${ETTh2}$</td><td>96</td><td>0.268</td><td>0.328</td><td>0.384</td><td>0.431</td><td>0.321</td><td>0.368</td><td>0.390</td><td>0.417</td><td>0.461</td><td>0.430</td><td>0.444</td><td>0.403</td></tr><tr><td>192</td><td>0.329</td><td>0.375</td><td>0.496</td><td>0.493</td><td>0.398</td><td>0.421</td><td>0.545</td><td>0.492</td><td>0.754</td><td>0.537</td><td>0.771</td><td>0.461</td></tr><tr><td>336</td><td>0.368</td><td>0.409</td><td>0.585</td><td>0.542</td><td>0.453</td><td>0.459</td><td>0.697</td><td>0.562</td><td>1.355</td><td>0.683</td><td>1.526</td><td>0.522</td></tr><tr><td>720</td><td>0.372</td><td>0.420</td><td>0.792</td><td>0.651</td><td>0.775</td><td>0.609</td><td>0.907</td><td>0.658</td><td>3.971</td><td>1.061</td><td>5.183</td><td>0.633</td></tr><tr><td>Avg</td><td>0.334</td><td>0.383</td><td>0.564</td><td>0.529</td><td>0.487</td><td>$\underline{0.464}$</td><td>0.635</td><td>0.532</td><td>1.635</td><td>0.678</td><td>1.981</td><td>0.505</td></tr><tr><td rowspan="5">${ETTm1}$</td><td>96</td><td>0.272</td><td>0.334</td><td>0.393</td><td>0.412</td><td>0.327</td><td>0.368</td><td>1.091</td><td>0.661</td><td>1.211</td><td>0.704</td><td>1.519</td><td>0.768</td></tr><tr><td>192</td><td>0.310</td><td>0.358</td><td>0.425</td><td>0.427</td><td>0.376</td><td>0.400</td><td>1.119</td><td>0.682</td><td>1.237</td><td>0.724</td><td>1.535</td><td>0.784</td></tr><tr><td>336</td><td>0.352</td><td>0.384</td><td>0.464</td><td>0.454</td><td>0.407</td><td>0.423</td><td>1.125</td><td>0.698</td><td>1.231</td><td>0.735</td><td>1.472</td><td>0.782</td></tr><tr><td>720</td><td>0.383</td><td>0.411</td><td>0.521</td><td>0.488</td><td>0.471</td><td>0.456</td><td>1.243</td><td>0.745</td><td>1.394</td><td>0.801</td><td>1.591</td><td>0.825</td></tr><tr><td>Avg</td><td>0.329</td><td>0.372</td><td>0.451</td><td>0.445</td><td>0.395</td><td>0.412</td><td>1.145</td><td>0.697</td><td>1.268</td><td>0.741</td><td>1.529</td><td>0.790</td></tr><tr><td rowspan="5">${ETTm2}$</td><td>96</td><td>0.161</td><td>0.253</td><td>0.204</td><td>0.302</td><td>0.188</td><td>0.273</td><td>0.435</td><td>0.375</td><td>0.245</td><td>0.316</td><td>0.359</td><td>0.333</td></tr><tr><td>192</td><td>0.219</td><td>0.293</td><td>0.282</td><td>0.358</td><td>0.274</td><td>0.338</td><td>0.995</td><td>0.494</td><td>0.413</td><td>0.401</td><td>0.756</td><td>0.396</td></tr><tr><td>336</td><td>0.271</td><td>0.329</td><td>0.378</td><td>0.425</td><td>0.384</td><td>0.406</td><td>2.324</td><td>0.648</td><td>0.790</td><td>0.528</td><td>1.747</td><td>0.467</td></tr><tr><td>720</td><td>0.352</td><td>0.379</td><td>0.555</td><td>0.523</td><td>0.501</td><td>0.488</td><td>9.064</td><td>1.020</td><td>2.451</td><td>0.847</td><td>6.856</td><td>0.639</td></tr><tr><td>Avg</td><td>0.251</td><td>0.313</td><td>0.355</td><td>0.402</td><td>0.337</td><td>0.376</td><td>3.205</td><td>0.634</td><td>0.975</td><td>0.523</td><td>2.430</td><td>0.459</td></tr><tr><td rowspan="5">${Weather}$</td><td>96</td><td>0.147</td><td>0.201</td><td>0.185</td><td>0.244</td><td>0.160</td><td>0.222</td><td>0.255</td><td>0.273</td><td>0.279</td><td>0.266</td><td>0.331</td><td>0.277</td></tr><tr><td>192</td><td>0.189</td><td>0.234</td><td>0.225</td><td>0.282</td><td>0.202</td><td>0.265</td><td>0.390</td><td>0.353</td><td>0.337</td><td>0.316</td><td>0.498</td><td>0.345</td></tr><tr><td>336</td><td>0.262</td><td>0.279</td><td>0.274</td><td>0.323</td><td>0.253</td><td>0.303</td><td>0.775</td><td>0.457</td><td>0.472</td><td>0.385</td><td>0.898</td><td>0.423</td></tr><tr><td>720</td><td>0.304</td><td>0.316</td><td>0.340</td><td>0.373</td><td>0.323</td><td>0.354</td><td>2.898</td><td>0.707</td><td>0.818</td><td>0.526</td><td>2.820</td><td>0.580</td></tr><tr><td>Avg</td><td>0.225</td><td>0.257</td><td>0.256</td><td>0.306</td><td>0.235</td><td>0.286</td><td>1.080</td><td>0.448</td><td>0.477</td><td>0.373</td><td>1.137</td><td>0.406</td></tr><tr><td rowspan="5">Electricity</td><td>96</td><td>0.131</td><td>0.224</td><td>0.233</td><td>0.327</td><td>0.184</td><td>0.275</td><td>0.520</td><td>0.466</td><td>0.653</td><td>0.532</td><td>0.650</td><td>0.526</td></tr><tr><td>192</td><td>0.152</td><td>0.241</td><td>0.246</td><td>0.340</td><td>0.190</td><td>0.282</td><td>0.581</td><td>0.499</td><td>0.713</td><td>0.561</td><td>0.704</td><td>0.549</td></tr><tr><td>336</td><td>0.160</td><td>0.248</td><td>0.262</td><td>0.355</td><td>0.205</td><td>0.298</td><td>0.602</td><td>0.515</td><td>0.797</td><td>0.603</td><td>0.766</td><td>0.577</td></tr><tr><td>720</td><td>0.192</td><td>0.298</td><td>0.296</td><td>0.383</td><td>0.239</td><td>0.330</td><td>0.685</td><td>0.558</td><td>1.023</td><td>0.688</td><td>0.901</td><td>0.628</td></tr><tr><td>Avg</td><td>0.158</td><td>0.252</td><td>0.259</td><td>0.351</td><td>0.205</td><td>0.296</td><td>0.597</td><td>0.510</td><td>0.797</td><td>0.596</td><td>0.755</td><td>0.570</td></tr><tr><td rowspan="5">Traffic</td><td>96</td><td>0.362</td><td>0.248</td><td>0.608</td><td>0.447</td><td>0.410</td><td>0.329</td><td>1.068</td><td>0.694</td><td>3.207</td><td>1.219</td><td>3.254</td><td>1.221</td></tr><tr><td>192</td><td>0.374</td><td>0.247</td><td>0.605</td><td>0.448</td><td>0.414</td><td>0.330</td><td>1.380</td><td>0.775</td><td>3.407</td><td>1.262</td><td>3.569</td><td>1.264</td></tr><tr><td>336</td><td>0.385</td><td>0.271</td><td>0.618</td><td>0.454</td><td>0.428</td><td>0.337</td><td>1.448</td><td>0.790</td><td>3.473</td><td>1.274</td><td>3.971</td><td>1.275</td></tr><tr><td>720</td><td>0.430</td><td>0.288</td><td>0.650</td><td>0.467</td><td>0.456</td><td>0.354</td><td>1.481</td><td>0.799</td><td>3.952</td><td>1.382</td><td>6.784</td><td>1.379</td></tr><tr><td>Avg</td><td>0.388</td><td>0.264</td><td>0.620</td><td>0.454</td><td>0.427</td><td>0.338</td><td>1.344</td><td>0.765</td><td>3.510</td><td>1.284</td><td>4.395</td><td>1.285</td></tr><tr><td rowspan="5">ILI</td><td>24</td><td>1.285</td><td>0.727</td><td>6.809</td><td>1.870</td><td>2.675</td><td>1.080</td><td>4.909</td><td>1.329</td><td>5.991</td><td>1.510</td><td>4.869</td><td>1.315</td></tr><tr><td>36</td><td>1.404</td><td>0.814</td><td>6.850</td><td>1.890</td><td>3.081</td><td>1.194</td><td>5.079</td><td>1.440</td><td>5.922</td><td>1.539</td><td>4.917</td><td>1.422</td></tr><tr><td>48</td><td>1.523</td><td>0.807</td><td>6.788</td><td>1.876</td><td>2.973</td><td>1.176</td><td>4.276</td><td>1.339</td><td>4.637</td><td>1.329</td><td>3.966</td><td>1.301</td></tr><tr><td>60</td><td>1.531</td><td>0.854</td><td>6.908</td><td>1.893</td><td>3.259</td><td>1.232</td><td>3.855</td><td>1.276</td><td>4.378</td><td>1.345</td><td>3.540</td><td>1.229</td></tr><tr><td>Avg</td><td>1.435</td><td>0.801</td><td>6.839</td><td>1.882</td><td>$\underline{2.997}$</td><td>1.171</td><td>4.530</td><td>1.346</td><td>5.232</td><td>1.431</td><td>4.323</td><td>1.317</td></tr><tr><td colspan="2">${1}^{\text{st }}$ Count</td><td colspan="2">40</td><td colspan="2">0</td><td colspan="2">1</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td></tr></table>

<table><tbody><tr><td colspan="2">方法</td><td colspan="2">时间大语言模型（TIME-LLM）</td><td colspan="2">N - 节拍模型（N-BEATS）</td><td colspan="2">N - 分层时间序列模型（N-HiTS）</td><td colspan="2">自动自回归积分滑动平均模型（AutoARIMA）</td><td colspan="2">自动西塔模型（AutoTheta）</td><td colspan="2">自动指数平滑法（AutoETS）</td></tr><tr><td colspan="2">指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td rowspan="5">${ETTh1}$</td><td>96</td><td>0.362</td><td>0.392</td><td>0.496</td><td>0.475</td><td>0.392</td><td>0.407</td><td>0.933</td><td>0.635</td><td>1.266</td><td>0.758</td><td>1.264</td><td>0.756</td></tr><tr><td>192</td><td>0.398</td><td>0.418</td><td>0.544</td><td>0.504</td><td>0.442</td><td>0.438</td><td>0.868</td><td>0.621</td><td>1.188</td><td>0.749</td><td>1.181</td><td>0.745</td></tr><tr><td>336</td><td>0.430</td><td>0.427</td><td>0.592</td><td>0.533</td><td>0.497</td><td>0.471</td><td>0.964</td><td>0.663</td><td>1.310</td><td>0.799</td><td>1.292</td><td>0.792</td></tr><tr><td>720</td><td>0.442</td><td>0.457</td><td>0.639</td><td>0.588</td><td>0.559</td><td>0.533</td><td>1.043</td><td>0.705</td><td>1.510</td><td>0.882</td><td>1.405</td><td>0.842</td></tr><tr><td>平均值</td><td>0.408</td><td>0.423</td><td>0.568</td><td>0.525</td><td>0.473</td><td>0.462</td><td>0.952</td><td>0.656</td><td>1.319</td><td>0.797</td><td>1.286</td><td>0.784</td></tr><tr><td rowspan="5">${ETTh2}$</td><td>96</td><td>0.268</td><td>0.328</td><td>0.384</td><td>0.431</td><td>0.321</td><td>0.368</td><td>0.390</td><td>0.417</td><td>0.461</td><td>0.430</td><td>0.444</td><td>0.403</td></tr><tr><td>192</td><td>0.329</td><td>0.375</td><td>0.496</td><td>0.493</td><td>0.398</td><td>0.421</td><td>0.545</td><td>0.492</td><td>0.754</td><td>0.537</td><td>0.771</td><td>0.461</td></tr><tr><td>336</td><td>0.368</td><td>0.409</td><td>0.585</td><td>0.542</td><td>0.453</td><td>0.459</td><td>0.697</td><td>0.562</td><td>1.355</td><td>0.683</td><td>1.526</td><td>0.522</td></tr><tr><td>720</td><td>0.372</td><td>0.420</td><td>0.792</td><td>0.651</td><td>0.775</td><td>0.609</td><td>0.907</td><td>0.658</td><td>3.971</td><td>1.061</td><td>5.183</td><td>0.633</td></tr><tr><td>平均值</td><td>0.334</td><td>0.383</td><td>0.564</td><td>0.529</td><td>0.487</td><td>$\underline{0.464}$</td><td>0.635</td><td>0.532</td><td>1.635</td><td>0.678</td><td>1.981</td><td>0.505</td></tr><tr><td rowspan="5">${ETTm1}$</td><td>96</td><td>0.272</td><td>0.334</td><td>0.393</td><td>0.412</td><td>0.327</td><td>0.368</td><td>1.091</td><td>0.661</td><td>1.211</td><td>0.704</td><td>1.519</td><td>0.768</td></tr><tr><td>192</td><td>0.310</td><td>0.358</td><td>0.425</td><td>0.427</td><td>0.376</td><td>0.400</td><td>1.119</td><td>0.682</td><td>1.237</td><td>0.724</td><td>1.535</td><td>0.784</td></tr><tr><td>336</td><td>0.352</td><td>0.384</td><td>0.464</td><td>0.454</td><td>0.407</td><td>0.423</td><td>1.125</td><td>0.698</td><td>1.231</td><td>0.735</td><td>1.472</td><td>0.782</td></tr><tr><td>720</td><td>0.383</td><td>0.411</td><td>0.521</td><td>0.488</td><td>0.471</td><td>0.456</td><td>1.243</td><td>0.745</td><td>1.394</td><td>0.801</td><td>1.591</td><td>0.825</td></tr><tr><td>平均值</td><td>0.329</td><td>0.372</td><td>0.451</td><td>0.445</td><td>0.395</td><td>0.412</td><td>1.145</td><td>0.697</td><td>1.268</td><td>0.741</td><td>1.529</td><td>0.790</td></tr><tr><td rowspan="5">${ETTm2}$</td><td>96</td><td>0.161</td><td>0.253</td><td>0.204</td><td>0.302</td><td>0.188</td><td>0.273</td><td>0.435</td><td>0.375</td><td>0.245</td><td>0.316</td><td>0.359</td><td>0.333</td></tr><tr><td>192</td><td>0.219</td><td>0.293</td><td>0.282</td><td>0.358</td><td>0.274</td><td>0.338</td><td>0.995</td><td>0.494</td><td>0.413</td><td>0.401</td><td>0.756</td><td>0.396</td></tr><tr><td>336</td><td>0.271</td><td>0.329</td><td>0.378</td><td>0.425</td><td>0.384</td><td>0.406</td><td>2.324</td><td>0.648</td><td>0.790</td><td>0.528</td><td>1.747</td><td>0.467</td></tr><tr><td>720</td><td>0.352</td><td>0.379</td><td>0.555</td><td>0.523</td><td>0.501</td><td>0.488</td><td>9.064</td><td>1.020</td><td>2.451</td><td>0.847</td><td>6.856</td><td>0.639</td></tr><tr><td>平均值</td><td>0.251</td><td>0.313</td><td>0.355</td><td>0.402</td><td>0.337</td><td>0.376</td><td>3.205</td><td>0.634</td><td>0.975</td><td>0.523</td><td>2.430</td><td>0.459</td></tr><tr><td rowspan="5">${Weather}$</td><td>96</td><td>0.147</td><td>0.201</td><td>0.185</td><td>0.244</td><td>0.160</td><td>0.222</td><td>0.255</td><td>0.273</td><td>0.279</td><td>0.266</td><td>0.331</td><td>0.277</td></tr><tr><td>192</td><td>0.189</td><td>0.234</td><td>0.225</td><td>0.282</td><td>0.202</td><td>0.265</td><td>0.390</td><td>0.353</td><td>0.337</td><td>0.316</td><td>0.498</td><td>0.345</td></tr><tr><td>336</td><td>0.262</td><td>0.279</td><td>0.274</td><td>0.323</td><td>0.253</td><td>0.303</td><td>0.775</td><td>0.457</td><td>0.472</td><td>0.385</td><td>0.898</td><td>0.423</td></tr><tr><td>720</td><td>0.304</td><td>0.316</td><td>0.340</td><td>0.373</td><td>0.323</td><td>0.354</td><td>2.898</td><td>0.707</td><td>0.818</td><td>0.526</td><td>2.820</td><td>0.580</td></tr><tr><td>平均值</td><td>0.225</td><td>0.257</td><td>0.256</td><td>0.306</td><td>0.235</td><td>0.286</td><td>1.080</td><td>0.448</td><td>0.477</td><td>0.373</td><td>1.137</td><td>0.406</td></tr><tr><td rowspan="5">电力</td><td>96</td><td>0.131</td><td>0.224</td><td>0.233</td><td>0.327</td><td>0.184</td><td>0.275</td><td>0.520</td><td>0.466</td><td>0.653</td><td>0.532</td><td>0.650</td><td>0.526</td></tr><tr><td>192</td><td>0.152</td><td>0.241</td><td>0.246</td><td>0.340</td><td>0.190</td><td>0.282</td><td>0.581</td><td>0.499</td><td>0.713</td><td>0.561</td><td>0.704</td><td>0.549</td></tr><tr><td>336</td><td>0.160</td><td>0.248</td><td>0.262</td><td>0.355</td><td>0.205</td><td>0.298</td><td>0.602</td><td>0.515</td><td>0.797</td><td>0.603</td><td>0.766</td><td>0.577</td></tr><tr><td>720</td><td>0.192</td><td>0.298</td><td>0.296</td><td>0.383</td><td>0.239</td><td>0.330</td><td>0.685</td><td>0.558</td><td>1.023</td><td>0.688</td><td>0.901</td><td>0.628</td></tr><tr><td>平均值</td><td>0.158</td><td>0.252</td><td>0.259</td><td>0.351</td><td>0.205</td><td>0.296</td><td>0.597</td><td>0.510</td><td>0.797</td><td>0.596</td><td>0.755</td><td>0.570</td></tr><tr><td rowspan="5">交通流量</td><td>96</td><td>0.362</td><td>0.248</td><td>0.608</td><td>0.447</td><td>0.410</td><td>0.329</td><td>1.068</td><td>0.694</td><td>3.207</td><td>1.219</td><td>3.254</td><td>1.221</td></tr><tr><td>192</td><td>0.374</td><td>0.247</td><td>0.605</td><td>0.448</td><td>0.414</td><td>0.330</td><td>1.380</td><td>0.775</td><td>3.407</td><td>1.262</td><td>3.569</td><td>1.264</td></tr><tr><td>336</td><td>0.385</td><td>0.271</td><td>0.618</td><td>0.454</td><td>0.428</td><td>0.337</td><td>1.448</td><td>0.790</td><td>3.473</td><td>1.274</td><td>3.971</td><td>1.275</td></tr><tr><td>720</td><td>0.430</td><td>0.288</td><td>0.650</td><td>0.467</td><td>0.456</td><td>0.354</td><td>1.481</td><td>0.799</td><td>3.952</td><td>1.382</td><td>6.784</td><td>1.379</td></tr><tr><td>平均值</td><td>0.388</td><td>0.264</td><td>0.620</td><td>0.454</td><td>0.427</td><td>0.338</td><td>1.344</td><td>0.765</td><td>3.510</td><td>1.284</td><td>4.395</td><td>1.285</td></tr><tr><td rowspan="5">流感样疾病（ILI）</td><td>24</td><td>1.285</td><td>0.727</td><td>6.809</td><td>1.870</td><td>2.675</td><td>1.080</td><td>4.909</td><td>1.329</td><td>5.991</td><td>1.510</td><td>4.869</td><td>1.315</td></tr><tr><td>36</td><td>1.404</td><td>0.814</td><td>6.850</td><td>1.890</td><td>3.081</td><td>1.194</td><td>5.079</td><td>1.440</td><td>5.922</td><td>1.539</td><td>4.917</td><td>1.422</td></tr><tr><td>48</td><td>1.523</td><td>0.807</td><td>6.788</td><td>1.876</td><td>2.973</td><td>1.176</td><td>4.276</td><td>1.339</td><td>4.637</td><td>1.329</td><td>3.966</td><td>1.301</td></tr><tr><td>60</td><td>1.531</td><td>0.854</td><td>6.908</td><td>1.893</td><td>3.259</td><td>1.232</td><td>3.855</td><td>1.276</td><td>4.378</td><td>1.345</td><td>3.540</td><td>1.229</td></tr><tr><td>平均值</td><td>1.435</td><td>0.801</td><td>6.839</td><td>1.882</td><td>$\underline{2.997}$</td><td>1.171</td><td>4.530</td><td>1.346</td><td>5.232</td><td>1.431</td><td>4.323</td><td>1.317</td></tr><tr><td colspan="2">${1}^{\text{st }}$ 计数</td><td colspan="2">40</td><td colspan="2">0</td><td colspan="2">1</td><td colspan="2">0</td><td colspan="2">0</td><td colspan="2">0</td></tr></tbody></table>

Table 12: Full short-term time series forecasting results. The forecasting horizons are in [6, 48] and the last three rows are weighted averaged from all datasets under different sampling intervals. A lower value indicates better performance. Red: the best, Blue: the second best.

表12：完整的短期时间序列预测结果。预测范围为[6, 48]，最后三行是不同采样间隔下所有数据集的加权平均值。数值越低表示性能越好。红色：最佳，蓝色：次佳。

<table><tr><td/><td>Methods</td><td>TIME-LLM</td><td>GPT4TS</td><td>TimesNet</td><td>PatchTST</td><td>N-HiTS</td><td>N-BEATS</td><td>ETSformer</td><td>LightTS</td><td>DLinear</td><td>FEDformer</td><td>Stationary</td><td>Autoformer</td><td>Informer</td><td>Reformer</td></tr><tr><td/><td>SMAPE</td><td>13.419</td><td>15.11</td><td>15.378</td><td>13.477</td><td>13.422</td><td>13.487</td><td>18.009</td><td>14.247</td><td>16.965</td><td>14.021</td><td>13.717</td><td>13.974</td><td>14.727</td><td>16.169</td></tr><tr><td>Yearly</td><td>MASE</td><td>3.005</td><td>3.565</td><td>3.554</td><td>3.019</td><td>3.056</td><td>3.036</td><td>4.487</td><td>3.109</td><td>4.283</td><td>3.036</td><td>3.078</td><td>3.134</td><td>3.418</td><td>3.800</td></tr><tr><td/><td>OWA</td><td>0.789</td><td>0.911</td><td>0.918</td><td>0.792</td><td>0.795</td><td>0.795</td><td>1.115</td><td>0.827</td><td>1.058</td><td>0.811</td><td>0.807</td><td>0.822</td><td>0.881</td><td>0.973</td></tr><tr><td/><td>SMAPE</td><td>10.110</td><td>10.597</td><td>10.465</td><td>10.38</td><td>10.185</td><td>10.564</td><td>13.376</td><td>11.364</td><td>12.145</td><td>11.1</td><td>10.958</td><td>11.338</td><td>11.360</td><td>13.313</td></tr><tr><td>Quarterly</td><td>MASE</td><td>1.178</td><td>1.253</td><td>1.227</td><td>1.233</td><td>1.18</td><td>1.252</td><td>1.906</td><td>1.328</td><td>1.520</td><td>1.35</td><td>1.325</td><td>1.365</td><td>1.401</td><td>1.775</td></tr><tr><td/><td>OWA</td><td>0.889</td><td>0.938</td><td>0.923</td><td>0.921</td><td>$\underline{0.893}$</td><td>0.936</td><td>1.302</td><td>1.000</td><td>1.106</td><td>0.996</td><td>0.981</td><td>1.012</td><td>1.027</td><td>1.252</td></tr><tr><td/><td>SMAPE</td><td>12.980</td><td>13.258</td><td>13.513</td><td>12.959</td><td>13.059</td><td>13.089</td><td>14.588</td><td>14.014</td><td>13.514</td><td>14.403</td><td>13.917</td><td>13.958</td><td>14.062</td><td>20.128</td></tr><tr><td>Monthly</td><td>MASE</td><td>0.963</td><td>1.003</td><td>1.039</td><td>0.97</td><td>1.013</td><td>0.996</td><td>1.368</td><td>1.053</td><td>1.037</td><td>1.147</td><td>1.097</td><td>1.103</td><td>1.141</td><td>2.614</td></tr><tr><td/><td>OWA</td><td>0.903</td><td>0.931</td><td>0.957</td><td>0.905</td><td>0.929</td><td>0.922</td><td>1.149</td><td>0.981</td><td>0.956</td><td>1.038</td><td>0.998</td><td>1.002</td><td>1.024</td><td>1.927</td></tr><tr><td rowspan="3">Others</td><td>SMAPE</td><td>4.795</td><td>6.124</td><td>6.913</td><td>4.952</td><td>4.711</td><td>6.599</td><td>7.267</td><td>15.880</td><td>6.709</td><td>7.148</td><td>6.302</td><td>5.485</td><td>24.460</td><td>32.491</td></tr><tr><td>MASE</td><td>3.178</td><td>4.116</td><td>4.507</td><td>3.347</td><td>3.054</td><td>4.43</td><td>5.240</td><td>11.434</td><td>4.953</td><td>4.041</td><td>4.064</td><td>3.865</td><td>20.960</td><td>33.355</td></tr><tr><td>OWA</td><td>1.006</td><td>1.259</td><td>1.438</td><td>1.049</td><td>0.977</td><td>1.393</td><td>1.591</td><td>3.474</td><td>1.487</td><td>1.389</td><td>1.304</td><td>1.187</td><td>5.879</td><td>8.679</td></tr><tr><td>Average</td><td>SMAPE</td><td>11.983</td><td>12.69</td><td>12.88</td><td>12.059</td><td>12.035</td><td>12.25</td><td>14.718</td><td>13.525</td><td>13.639</td><td>13.16</td><td>12.780</td><td>12.909</td><td>14.086</td><td>18.200</td></tr><tr><td/><td>MASE</td><td>1.595</td><td>1.808</td><td>1.836</td><td>1.623</td><td>1.625</td><td>1.698</td><td>2.408</td><td>2.111</td><td>2.095</td><td>1.775</td><td>1.756</td><td>1.771</td><td>2.718</td><td>4.223</td></tr><tr><td/><td>OWA</td><td>0.859</td><td>0.94</td><td>0.955</td><td>0.869</td><td>0.869</td><td>0.896</td><td>1.172</td><td>1.051</td><td>1.051</td><td>0.949</td><td>0.930</td><td>0.939</td><td>1.230</td><td>1.775</td></tr></table>

<table><tbody><tr><td></td><td>方法</td><td>TIME大语言模型（TIME-LLM）</td><td>GPT4时间序列模型（GPT4TS）</td><td>时间网络（TimesNet）</td><td>补丁时间序列变换器（PatchTST）</td><td>神经层次时间序列预测模型（N-HiTS）</td><td>N-BEATS（N-节拍）</td><td>ETSformer（ETS变换器）</td><td>LightTS（轻量级时间序列）</td><td>DLinear（深度线性）</td><td>FEDformer（联邦变换器）</td><td>平稳的</td><td>自动变换器（Autoformer）</td><td>信息器（Informer）</td><td>改革器（Reformer）</td></tr><tr><td></td><td>对称平均绝对百分比误差（SMAPE）</td><td>13.419</td><td>15.11</td><td>15.378</td><td>13.477</td><td>13.422</td><td>13.487</td><td>18.009</td><td>14.247</td><td>16.965</td><td>14.021</td><td>13.717</td><td>13.974</td><td>14.727</td><td>16.169</td></tr><tr><td>每年</td><td>平均绝对尺度误差（MASE）</td><td>3.005</td><td>3.565</td><td>3.554</td><td>3.019</td><td>3.056</td><td>3.036</td><td>4.487</td><td>3.109</td><td>4.283</td><td>3.036</td><td>3.078</td><td>3.134</td><td>3.418</td><td>3.800</td></tr><tr><td></td><td>OWA（办公软件网页版，Outlook Web App）</td><td>0.789</td><td>0.911</td><td>0.918</td><td>0.792</td><td>0.795</td><td>0.795</td><td>1.115</td><td>0.827</td><td>1.058</td><td>0.811</td><td>0.807</td><td>0.822</td><td>0.881</td><td>0.973</td></tr><tr><td></td><td>对称平均绝对百分比误差（SMAPE）</td><td>10.110</td><td>10.597</td><td>10.465</td><td>10.38</td><td>10.185</td><td>10.564</td><td>13.376</td><td>11.364</td><td>12.145</td><td>11.1</td><td>10.958</td><td>11.338</td><td>11.360</td><td>13.313</td></tr><tr><td>季度的</td><td>平均绝对尺度误差（MASE）</td><td>1.178</td><td>1.253</td><td>1.227</td><td>1.233</td><td>1.18</td><td>1.252</td><td>1.906</td><td>1.328</td><td>1.520</td><td>1.35</td><td>1.325</td><td>1.365</td><td>1.401</td><td>1.775</td></tr><tr><td></td><td>OWA（办公软件网页版，Outlook Web App）</td><td>0.889</td><td>0.938</td><td>0.923</td><td>0.921</td><td>$\underline{0.893}$</td><td>0.936</td><td>1.302</td><td>1.000</td><td>1.106</td><td>0.996</td><td>0.981</td><td>1.012</td><td>1.027</td><td>1.252</td></tr><tr><td></td><td>对称平均绝对百分比误差（SMAPE）</td><td>12.980</td><td>13.258</td><td>13.513</td><td>12.959</td><td>13.059</td><td>13.089</td><td>14.588</td><td>14.014</td><td>13.514</td><td>14.403</td><td>13.917</td><td>13.958</td><td>14.062</td><td>20.128</td></tr><tr><td>每月的</td><td>平均绝对尺度误差（MASE）</td><td>0.963</td><td>1.003</td><td>1.039</td><td>0.97</td><td>1.013</td><td>0.996</td><td>1.368</td><td>1.053</td><td>1.037</td><td>1.147</td><td>1.097</td><td>1.103</td><td>1.141</td><td>2.614</td></tr><tr><td></td><td>OWA（办公软件网页版，Outlook Web App）</td><td>0.903</td><td>0.931</td><td>0.957</td><td>0.905</td><td>0.929</td><td>0.922</td><td>1.149</td><td>0.981</td><td>0.956</td><td>1.038</td><td>0.998</td><td>1.002</td><td>1.024</td><td>1.927</td></tr><tr><td rowspan="3">其他</td><td>对称平均绝对百分比误差（SMAPE）</td><td>4.795</td><td>6.124</td><td>6.913</td><td>4.952</td><td>4.711</td><td>6.599</td><td>7.267</td><td>15.880</td><td>6.709</td><td>7.148</td><td>6.302</td><td>5.485</td><td>24.460</td><td>32.491</td></tr><tr><td>平均绝对尺度误差（MASE）</td><td>3.178</td><td>4.116</td><td>4.507</td><td>3.347</td><td>3.054</td><td>4.43</td><td>5.240</td><td>11.434</td><td>4.953</td><td>4.041</td><td>4.064</td><td>3.865</td><td>20.960</td><td>33.355</td></tr><tr><td>OWA（办公软件网页版，Outlook Web App）</td><td>1.006</td><td>1.259</td><td>1.438</td><td>1.049</td><td>0.977</td><td>1.393</td><td>1.591</td><td>3.474</td><td>1.487</td><td>1.389</td><td>1.304</td><td>1.187</td><td>5.879</td><td>8.679</td></tr><tr><td>平均值；平均的</td><td>对称平均绝对百分比误差（SMAPE）</td><td>11.983</td><td>12.69</td><td>12.88</td><td>12.059</td><td>12.035</td><td>12.25</td><td>14.718</td><td>13.525</td><td>13.639</td><td>13.16</td><td>12.780</td><td>12.909</td><td>14.086</td><td>18.200</td></tr><tr><td></td><td>平均绝对尺度误差（MASE）</td><td>1.595</td><td>1.808</td><td>1.836</td><td>1.623</td><td>1.625</td><td>1.698</td><td>2.408</td><td>2.111</td><td>2.095</td><td>1.775</td><td>1.756</td><td>1.771</td><td>2.718</td><td>4.223</td></tr><tr><td></td><td>OWA（办公软件网页版，Outlook Web App）</td><td>0.859</td><td>0.94</td><td>0.955</td><td>0.869</td><td>0.869</td><td>0.896</td><td>1.172</td><td>1.051</td><td>1.051</td><td>0.949</td><td>0.930</td><td>0.939</td><td>1.230</td><td>1.775</td></tr></tbody></table>

<!-- Media -->

### E.2 ZERO-SHOT FORECASTING

### E.2 零样本预测

The full results of zero-shot forecasting are summarized in Tab. 16. TIME-LLM remarkably surpasses the six most competitive time series models in zero-shot adaptation. Overall, we observe over 23.5% and 12.4% MSE and MAE reductions across all baselines on average. Our improvements are consistently significant on those typical cross-domain scenarios (e.g.,ETTh2 $\rightarrow$ ETTh1 and ETTm2 $\rightarrow$ ETTm1),over 20.8% and 11.3% on average w.r.t. MSE and MAE. Significantly, TIME-LLM exhibits superior performance gains in comparison to LLMTime (Gruver et al. 2023), which employs a similarly sized backbone LLM (7B) and is the latest effort in leveraging LLMs for zero-shot time series forecasting. We attribute this success to our reprogramming framework being better at activating the LLM's knowledge transfer and reasoning capabilities in a resource-efficient manner when performing time series tasks.

零样本预测的完整结果总结在表16中。在零样本适应方面，TIME - LLM显著超越了六个最具竞争力的时间序列模型。总体而言，与所有基线模型相比，均方误差（MSE）和平均绝对误差（MAE）平均分别降低了23.5%和12.4%。在那些典型的跨领域场景（例如，ETTh2 $\rightarrow$ ETTh1和ETTm2 $\rightarrow$ ETTm1）中，我们的改进始终显著，均方误差和平均绝对误差平均分别降低了20.8%和11.3%。值得注意的是，与LLMTime（格鲁弗等人，2023年）相比，TIME - LLM表现出更优的性能提升，LLMTime采用了规模相近的基础大语言模型（70亿参数），是利用大语言模型进行零样本时间序列预测的最新成果。我们将这一成功归因于我们的重新编程框架在执行时间序列任务时，能够以资源高效的方式更好地激活大语言模型的知识迁移和推理能力。

<!-- Media -->

Table 13: Additional short-term time series forecasting results on M3 (Quarterly). The forecasting horizon is 8. A lower value indicates better performance. Red: the best, Blue: the second best.

表13：M3数据集（季度数据）的额外短期时间序列预测结果。预测期为8。数值越低表示性能越好。红色：最佳，蓝色：次佳。

<table><tr><td>Methods</td><td>TIME-LLM</td><td>GPT4TS</td><td>TimesNet</td><td>PatchTST</td><td>N-HiTS</td><td>N-BEATS</td><td>DLinear</td><td>FEDformer</td></tr><tr><td>SMAPE</td><td>11.171</td><td>14.453</td><td>10.410</td><td>12.380</td><td>12.616</td><td>18.640</td><td>15.028</td><td>12.927</td></tr><tr><td>MRAE</td><td>3.282</td><td>5.035</td><td>3.310</td><td>2.401</td><td>4.271</td><td>4.612</td><td>2.793</td><td>3.653</td></tr><tr><td>MAPE</td><td>0.151</td><td>0.203</td><td>0.140</td><td>0.154</td><td>0.168</td><td>0.247</td><td>0.196</td><td>0.174</td></tr></table>

<table><tbody><tr><td>方法</td><td>TIME大语言模型（TIME-LLM）</td><td>GPT4时间序列模型（GPT4TS）</td><td>时间网络（TimesNet）</td><td>补丁时间序列变换器（PatchTST）</td><td>神经层次时间序列预测模型（N-HiTS）</td><td>N - 节拍（N - BEATS）</td><td>D线性（DLinear）</td><td>联邦变换器（FEDformer）</td></tr><tr><td>对称平均绝对百分比误差（SMAPE）</td><td>11.171</td><td>14.453</td><td>10.410</td><td>12.380</td><td>12.616</td><td>18.640</td><td>15.028</td><td>12.927</td></tr><tr><td>平均相对绝对误差（MRAE）</td><td>3.282</td><td>5.035</td><td>3.310</td><td>2.401</td><td>4.271</td><td>4.612</td><td>2.793</td><td>3.653</td></tr><tr><td>平均绝对百分比误差（MAPE）</td><td>0.151</td><td>0.203</td><td>0.140</td><td>0.154</td><td>0.168</td><td>0.247</td><td>0.196</td><td>0.174</td></tr></tbody></table>

<!-- Media -->

## F ABLATION STUDY

## 消融研究

The full ablation results are in Tab. 17. We additionally compare the model performance under reprogramming and fine-tuning (with QLoRA Dettmers et al. (2023)) protocols. Our results indicate a clear performance gain of our approach compared to the QLoRA variant (A.5) by 19% in average.

完整的消融实验结果见表17。我们还比较了在重新编程和微调（采用QLoRA，德特默斯等人（2023年）提出）协议下的模型性能。我们的结果表明，与QLoRA变体（A.5）相比，我们的方法平均性能提升了19%。

## G EFFICIENCY COMPARISON WITH MODEL FINE-TUNING

## 与模型微调的效率比较

Setups. We compare the efficiency of model fine-tuning (with QLoRA Dettmers et al. (2023)) and our proposed model reprogramming in this section with two different backbones, that is, Llama in 1/4 capacity (first 8 Transformer layers) and full capacity. Here, we adhere to the long-term forecasting protocol on ETTh1 to forecast two different steps (that is, 96 and 336 in this case) ahead. For the evaluation metrics, we report the total number of trainable parameters (in million), GPU memory (in mebibyte), and running time (seconds per iteration).

实验设置。在本节中，我们比较了模型微调（采用QLoRA，德特默斯等人（2023年）提出）和我们提出的模型重新编程的效率，使用了两种不同的主干网络，即1/4容量（前8个Transformer层）和全容量的Llama。在这里，我们遵循ETTh1上的长期预测协议来预测未来两个不同的步长（在这种情况下为96和336）。对于评估指标，我们报告了可训练参数的总数（以百万为单位）、GPU内存（以兆字节为单位）和运行时间（每次迭代的秒数）。

Results. Our results are given in Tab. 18. We see that model reprogramming remarkably results in better efficiency compared to parameter-efficient fine-tuning (PEFT) with QLoRA on long-range

结果。我们的结果见表18。我们发现，与使用QLoRA进行参数高效微调（PEFT）相比，模型重新编程在长距离任务上显著提高了效率

<!-- Media -->

Table 14: Full few-shot learning results on 10% training data. We use the same protocol as in Tab. 1

表14：在10%训练数据上的全少样本学习结果。我们使用与表1相同的协议

<table><tr><td colspan="2">Methods</td><td colspan="2">TIME-LLM</td><td colspan="2">GPT4TS</td><td colspan="2">DLinear</td><td>PatchTST</td><td colspan="2">TimesNet</td><td colspan="2">FEDformer</td><td colspan="2">Autoformer</td><td colspan="2">Stationary</td><td colspan="2">ETSformer</td><td colspan="2">LightTS</td><td colspan="2">Informer</td><td colspan="2">Reformer</td></tr><tr><td colspan="2">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSEMAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="5">${ETTh1}$</td><td>96</td><td>0.448</td><td>0.460</td><td>0.458</td><td>0.456</td><td>0.492</td><td>0.495</td><td>0.5160.485</td><td>0.861</td><td>0.628</td><td>0.512</td><td>0.499</td><td>0.613</td><td>0.552</td><td>0.918</td><td>0.639</td><td>1.112</td><td>0.806</td><td>1.298</td><td>0.838</td><td>1.179</td><td>0.792</td><td>1.184</td><td>0.790</td></tr><tr><td>192</td><td>0.484</td><td>0.483</td><td>0.570</td><td>0.516</td><td>$\underline{0.565}$</td><td>0.538</td><td>0.5980.524</td><td>0.797</td><td>0.593</td><td>0.624</td><td>0.555</td><td>0.722</td><td>0.598</td><td>0.915</td><td>0.629</td><td>1.155</td><td>0.823</td><td>1.322</td><td>0.854</td><td>1.199</td><td>0.806</td><td>1.295</td><td>0.850</td></tr><tr><td>336</td><td>0.589</td><td>0.540</td><td>0.608</td><td>0.535</td><td>0.721</td><td>0.622</td><td>0.6570.550</td><td>0.941</td><td>0.648</td><td>0.691</td><td>0.574</td><td>0.750</td><td>0.619</td><td>0.939</td><td>0.644</td><td>1.179</td><td>0.832</td><td>1.347</td><td>0.870</td><td>1.202</td><td>0.811</td><td>1.294</td><td>0.854</td></tr><tr><td>720</td><td>0.700</td><td>0.604</td><td>$\underline{0.725}$</td><td>0.591</td><td>0.986</td><td>0.743</td><td>0.7620.610</td><td>0.877</td><td>0.641</td><td>0.728</td><td>0.614</td><td>0.721</td><td>0.616</td><td>0.887</td><td>0.645</td><td>1.273</td><td>0.874</td><td>1.534</td><td>0.947</td><td>1.217</td><td>0.825</td><td>1.223</td><td>0.838</td></tr><tr><td>Avg</td><td>0.556</td><td>0.522</td><td>$\underline{0.590}$</td><td>$\underline{0.525}$</td><td>0.691</td><td>0.600</td><td>0.6330.542</td><td>0.869</td><td>0.628</td><td>0.639</td><td>0.561</td><td>0.702</td><td>0.596</td><td>0.915</td><td>0.639</td><td>1.180</td><td>0.834</td><td>1.375</td><td>0.877</td><td>1.199</td><td>0.809</td><td>1.249</td><td>0.833</td></tr><tr><td rowspan="5">${ETTh2}$</td><td>96</td><td>0.275</td><td>0.326</td><td>$\underline{0.331}$</td><td>0.374</td><td>0.357</td><td>0.411</td><td>0.3530.389</td><td>0.378</td><td>0.409</td><td>0.382</td><td>0.416</td><td>0.413</td><td>0.451</td><td>0.389</td><td>0.411</td><td>0.678</td><td>0.619</td><td>2.022</td><td>1.006</td><td>3.837</td><td>1.508</td><td>3.788</td><td>1.533</td></tr><tr><td>192</td><td>0.374</td><td>0.373</td><td>0.402</td><td>0.411</td><td>0.569</td><td>0.519</td><td>0.4030.414</td><td>0.490</td><td>0.467</td><td>0.478</td><td>0.474</td><td>0.474</td><td>0.477</td><td>0.473</td><td>0.455</td><td>0.785</td><td>0.666</td><td>2.329</td><td>1.104</td><td>3.856</td><td>1.513</td><td>3.552</td><td>1.483</td></tr><tr><td>336</td><td>0.406</td><td>0.429</td><td>0.406</td><td>0.433</td><td>0.671</td><td>0.572</td><td>0.4260.441</td><td>0.537</td><td>0.494</td><td>0.504</td><td>0.501</td><td>0.547</td><td>0.543</td><td>0.507</td><td>0.480</td><td>0.839</td><td>0.694</td><td>2.453</td><td>1.122</td><td>3.952</td><td>1.526</td><td>3.395</td><td>1.526</td></tr><tr><td>720</td><td>0.427</td><td>0.449</td><td>0.449</td><td>0.464</td><td>0.824</td><td>0.648</td><td>0.4770.480</td><td>0.510</td><td>0.491</td><td>0.499</td><td>0.509</td><td>0.516</td><td>0.523</td><td>0.477</td><td>0.472</td><td>1.273</td><td>0.874</td><td>3.816</td><td>1.407</td><td>3.842</td><td>1.503</td><td>3.205</td><td>1.401</td></tr><tr><td>Avg</td><td>0.370</td><td>0.394</td><td>$\underline{0.397}$</td><td>$\underline{0.421}$</td><td>0.605</td><td>0.538</td><td>0.4150.431</td><td>0.479</td><td>0.465</td><td>0.466</td><td>0.475</td><td>0.488</td><td>0.499</td><td>0.462</td><td>0.455</td><td>0.894</td><td>0.713</td><td>2.655</td><td>1.160</td><td>3.872</td><td>1.513</td><td>3.485</td><td>1.486</td></tr><tr><td rowspan="5">${ETTm1}$</td><td>96</td><td>0.346</td><td>0.388</td><td>0.390</td><td>0.404</td><td>0.352</td><td>0.392</td><td>0.4100.419</td><td>0.583</td><td>0.501</td><td>0.578</td><td>0.518</td><td>0.774</td><td>0.614</td><td>0.761</td><td>0.568</td><td>0.911</td><td>0.688</td><td>0.921</td><td>0.682</td><td>1.162</td><td>0.785</td><td>1.442</td><td>0.847</td></tr><tr><td>192</td><td>0.373</td><td>0.416</td><td>0.429</td><td>0.423</td><td>0.382</td><td>0.412</td><td>0.4370.434</td><td>0.630</td><td>0.528</td><td>0.617</td><td>0.546</td><td>0.754</td><td>0.592</td><td>0.781</td><td>0.574</td><td>0.955</td><td>0.703</td><td>0.957</td><td>0.701</td><td>1.172</td><td>0.793</td><td>1.444</td><td>0.862</td></tr><tr><td>336</td><td>0.413</td><td>0.426</td><td>0.469</td><td>0.439</td><td>0.419</td><td>0.434</td><td>0.4760.454</td><td>0.725</td><td>0.568</td><td>0.998</td><td>0.775</td><td>0.869</td><td>0.677</td><td>0.803</td><td>0.587</td><td>0.991</td><td>0.719</td><td>0.998</td><td>0.716</td><td>1.227</td><td>0.908</td><td>1.450</td><td>0.866</td></tr><tr><td>720</td><td>0.485</td><td>0.476</td><td>0.569</td><td>0.498</td><td>0.490</td><td>0.477</td><td>0.6810.556</td><td>0.769</td><td>0.549</td><td>0.693</td><td>0.579</td><td>0.810</td><td>0.630</td><td>0.844</td><td>0.581</td><td>1.062</td><td>0.747</td><td>1.007</td><td>0.719</td><td>1.207</td><td>0.797</td><td>1.366</td><td>0.850</td></tr><tr><td>Avg</td><td>0.404</td><td>0.427</td><td>0.464</td><td>0.441</td><td>$\underline{0.411}$</td><td>$\underline{0.429}$</td><td>0.5010.466</td><td>0.677</td><td>0.537</td><td>0.722</td><td>0.605</td><td>0.802</td><td>0.628</td><td>0.797</td><td>0.578</td><td>0.980</td><td>0.714</td><td>0.971</td><td>0.705</td><td>1.192</td><td>0.821</td><td>1.426</td><td>0.856</td></tr><tr><td rowspan="5">${ETT}{m2}$</td><td>96</td><td>0.177</td><td>0.261</td><td>0.188</td><td>0.269</td><td>0.213</td><td>0.303</td><td>0.1910.274</td><td>0.212</td><td>0.285</td><td>0.291</td><td>0.399</td><td>0.352</td><td>0.454</td><td>0.229</td><td>0.308</td><td>0.331</td><td>0.430</td><td>0.813</td><td>0.688</td><td>3.203</td><td>1.407</td><td>4.195</td><td>1.628</td></tr><tr><td>192</td><td>0.241</td><td>0.314</td><td>0.251</td><td>0.309</td><td>0.278</td><td>0.345</td><td>0.2520.317</td><td>0.270</td><td>0.323</td><td>0.307</td><td>0.379</td><td>0.694</td><td>0.691</td><td>0.291</td><td>0.343</td><td>0.400</td><td>0.464</td><td>1.008</td><td>0.768</td><td>3.112</td><td>1.387</td><td>4.042</td><td>1.601</td></tr><tr><td>336</td><td>0.274</td><td>0.327</td><td>0.307</td><td>0.346</td><td>0.338</td><td>0.385</td><td>0.3060.353</td><td>0.323</td><td>0.353</td><td>0.543</td><td>0.559</td><td>2.408</td><td>1.407</td><td>0.348</td><td>0.376</td><td>0.469</td><td>0.498</td><td>1.031</td><td>0.775</td><td>3.255</td><td>1.421</td><td>3.963</td><td>1.585</td></tr><tr><td>720</td><td>0.417</td><td>0.390</td><td>0.426</td><td>0.417</td><td>0.436</td><td>0.440</td><td>0.4330.427</td><td>0.474</td><td>0.449</td><td>0.712</td><td>0.614</td><td>1.913</td><td>1.166</td><td>0.461</td><td>0.438</td><td>0.589</td><td>0.557</td><td>1.096</td><td>0.791</td><td>3.909</td><td>1.543</td><td>3.711</td><td>1.532</td></tr><tr><td>Avg</td><td>0.277</td><td>0.323</td><td>0.293</td><td>$\underline{0.335}$</td><td>0.316</td><td>0.368</td><td>0.2960.343</td><td>0.320</td><td>0.353</td><td>0.463</td><td>0.488</td><td>1.342</td><td>0.930</td><td>0.332</td><td>0.366</td><td>0.447</td><td>0.487</td><td>0.987</td><td>0.756</td><td>3.370</td><td>1.440</td><td>3.978</td><td>1.587</td></tr><tr><td rowspan="5">${Weather}$</td><td>96</td><td>0.161</td><td>0.210</td><td>0.163</td><td>0.215</td><td>0.171</td><td>0.224</td><td>0.1650.215</td><td>0.184</td><td>0.230</td><td>0.188</td><td>0.253</td><td>0.221</td><td>0.297</td><td>0.192</td><td>0.234</td><td>0.199</td><td>0.272</td><td>0.217</td><td>0.269</td><td>0.374</td><td>0.401</td><td>0.335</td><td>0.380</td></tr><tr><td>192</td><td>0.204</td><td>0.248</td><td>0.210</td><td>0.254</td><td>0.215</td><td>0.263</td><td>0.2100.257</td><td>0.245</td><td>0.283</td><td>0.250</td><td>0.304</td><td>0.270</td><td>0.322</td><td>0.269</td><td>0.295</td><td>0.279</td><td>0.332</td><td>0.259</td><td>0.304</td><td>0.552</td><td>0.478</td><td>0.522</td><td>0.462</td></tr><tr><td>336</td><td>0.261</td><td>0.302</td><td>0.256</td><td>0.292</td><td>0.258</td><td>0.299</td><td>0.2590.297</td><td>0.305</td><td>0.321</td><td>0.312</td><td>0.346</td><td>0.320</td><td>0.351</td><td>0.370</td><td>0.357</td><td>0.356</td><td>0.386</td><td>0.303</td><td>0.334</td><td>724</td><td>0.541</td><td>0.715</td><td>0.535</td></tr><tr><td>720</td><td>0.309</td><td>0.332</td><td>0.321</td><td>0.339</td><td>0.320</td><td>0.346</td><td>0.3320.346</td><td>0.381</td><td>0.371</td><td>0.387</td><td>0.393</td><td>0.390</td><td>0.396</td><td>0.441</td><td>0.405</td><td>0.437</td><td>0.448</td><td>0.377</td><td>0.382</td><td>0.739</td><td>0.558</td><td>0.611</td><td>0.500</td></tr><tr><td>Avg</td><td>0.234</td><td>0.273</td><td>$\underline{0.238}$</td><td>$\underline{0.275}$</td><td>0.241</td><td>0.283</td><td>0.2420.279</td><td>0.279</td><td>0.301</td><td>0.284</td><td>0.324</td><td>0.300</td><td>0.342</td><td>0.318</td><td>0.323</td><td>0.318</td><td>0.360</td><td>0.289</td><td>0.322</td><td>0.597</td><td>0.495</td><td>0.546</td><td>0.469</td></tr><tr><td rowspan="5">Electricity</td><td>96</td><td>0.139</td><td>0.241</td><td>0.139</td><td>0.237</td><td>0.150</td><td>0.253</td><td>0.1400.238</td><td>0.299</td><td>0.373</td><td>0.231</td><td>0.323</td><td>0.261</td><td>0.348</td><td>0.420</td><td>0.466</td><td>0.599</td><td>0.587</td><td>0.350</td><td>0.425</td><td>1.259</td><td>0.919</td><td>0.993</td><td>0.784</td></tr><tr><td>192</td><td>0.151</td><td>0.248</td><td>0.156</td><td>0.252</td><td>0.164</td><td>0.264</td><td>0.1600.255</td><td>0.305</td><td>0.379</td><td>0.261</td><td>0.356</td><td>0.338</td><td>0.406</td><td>0.411</td><td>0.459</td><td>0.620</td><td>0.598</td><td>0.376</td><td>0.448</td><td>1.160</td><td>0.873</td><td>0.938</td><td>0.753</td></tr><tr><td>336</td><td>0.169</td><td>0.270</td><td>0.175</td><td>0.270</td><td>0.181</td><td>0.282</td><td>0.1800.276</td><td>0.319</td><td>0.391</td><td>0.360</td><td>0.445</td><td>0.410</td><td>0.474</td><td>0.434</td><td>0.473</td><td>0.662</td><td>0.619</td><td>0.428</td><td>0.485</td><td>1.157</td><td>0.872</td><td>0.925</td><td>0.745</td></tr><tr><td>720</td><td>0.240</td><td>0.322</td><td>0.233</td><td>0.317</td><td>0.223</td><td>0.321</td><td>0.2410.323</td><td>0.369</td><td>0.426</td><td>0.530</td><td>0.585</td><td>0.715</td><td>0.685</td><td>0.510</td><td>0.521</td><td>0.757</td><td>0.664</td><td>0.611</td><td>0.597</td><td>1.203</td><td>0.898</td><td>1.004</td><td>0.790</td></tr><tr><td>Avg</td><td>0.175</td><td>0.270</td><td>$\underline{0.176}$</td><td>0.269</td><td>0.180</td><td>0.280</td><td>0.1800.273</td><td>0.323</td><td>0.392</td><td>0.346</td><td>0.427</td><td>0.431</td><td>0.478</td><td>0.444</td><td>0.480</td><td>0.660</td><td>0.617</td><td>0.441</td><td>0.489</td><td>1.195</td><td>0.891</td><td>0.965</td><td>0.768</td></tr><tr><td rowspan="5">Traffic</td><td>96</td><td>0.418</td><td>0.291</td><td>0.414</td><td>0.297</td><td>0.419</td><td>0.298</td><td>0.4030.289</td><td>0.719</td><td>0.416</td><td>0.639</td><td>0.400</td><td>0.672</td><td>0.405</td><td>1.412</td><td>0.802</td><td>1.643</td><td>0.855</td><td>1.157</td><td>0.636</td><td>1.557</td><td>0.821</td><td>1.527</td><td>0.815</td></tr><tr><td>192</td><td>0.414</td><td>0.296</td><td>0.426</td><td>0.301</td><td>0.434</td><td>0.305</td><td>0.4150.296</td><td>0.748</td><td>0.428</td><td>0.637</td><td>0.416</td><td>0.727</td><td>0.424</td><td>1.419</td><td>0.806</td><td>1.641</td><td>0.854</td><td>1.207</td><td>0.661</td><td>1.454</td><td>0.765</td><td>1.538</td><td>0.817</td></tr><tr><td>336</td><td>0.421</td><td>0.311</td><td>0.434</td><td>0.303</td><td>0.449</td><td>0.313</td><td>0.4260.304</td><td>0.853</td><td>0.471</td><td>0.655</td><td>0.427</td><td>0.749</td><td>0.454</td><td>1.443</td><td>0.815</td><td>1.711</td><td>0.878</td><td>1.334</td><td>0.713</td><td>1.521</td><td>0.812</td><td>1.550</td><td>0.819</td></tr><tr><td>720</td><td>0.462</td><td>0.327</td><td>0.487</td><td>0.337</td><td>0.484</td><td>0.336</td><td>0.4740.331</td><td>1.485</td><td>0.825</td><td>0.722</td><td>0.456</td><td>0.847</td><td>0.499</td><td>1.539</td><td>0.837</td><td>2.660</td><td>1.157</td><td>1.292</td><td>0.726</td><td>1.605</td><td>0.846</td><td>1.588</td><td>0.833</td></tr><tr><td>Avg</td><td>0.429</td><td>0.306</td><td>0.440</td><td>0.310</td><td>0.447</td><td>0.313</td><td>$\underline{0.430}$0.305</td><td>0.951</td><td>0.535</td><td>0.663</td><td>0.425</td><td>0.749</td><td>0.446</td><td>1.453</td><td>0.815</td><td>1.914</td><td>0.936</td><td>1.248</td><td>0.684</td><td>1.534</td><td>0.811</td><td>1.551</td><td>0.821</td></tr><tr><td colspan="2">${1}^{\text{st }}$ Count</td><td colspan="2">32</td><td colspan="2">9</td><td>3</td><td/><td>3</td><td>0</td><td/><td/><td>0</td><td>0</td><td/><td/><td>0</td><td/><td>0</td><td>0</td><td/><td>0</td><td/><td>0</td><td/></tr></table>

<table><tbody><tr><td colspan="2">方法</td><td colspan="2">TIME大语言模型（TIME-LLM）</td><td colspan="2">GPT4时间序列模型（GPT4TS）</td><td colspan="2">深度线性模型（DLinear）</td><td>补丁时间序列变换器（PatchTST）</td><td colspan="2">时间网络（TimesNet）</td><td colspan="2">FEDformer（联邦变换器）</td><td colspan="2">Autoformer（自动变换器）</td><td colspan="2">平稳的</td><td colspan="2">ETSformer（ETS变换器）</td><td colspan="2">LightTS（轻量级时间序列）</td><td colspan="2">Informer（信息者）</td><td colspan="2">改革者</td></tr><tr><td colspan="2">指标</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差与平均绝对误差（MSEMAE）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td></tr><tr><td rowspan="5">${ETTh1}$</td><td>96</td><td>0.448</td><td>0.460</td><td>0.458</td><td>0.456</td><td>0.492</td><td>0.495</td><td>0.5160.485</td><td>0.861</td><td>0.628</td><td>0.512</td><td>0.499</td><td>0.613</td><td>0.552</td><td>0.918</td><td>0.639</td><td>1.112</td><td>0.806</td><td>1.298</td><td>0.838</td><td>1.179</td><td>0.792</td><td>1.184</td><td>0.790</td></tr><tr><td>192</td><td>0.484</td><td>0.483</td><td>0.570</td><td>0.516</td><td>$\underline{0.565}$</td><td>0.538</td><td>0.5980.524</td><td>0.797</td><td>0.593</td><td>0.624</td><td>0.555</td><td>0.722</td><td>0.598</td><td>0.915</td><td>0.629</td><td>1.155</td><td>0.823</td><td>1.322</td><td>0.854</td><td>1.199</td><td>0.806</td><td>1.295</td><td>0.850</td></tr><tr><td>336</td><td>0.589</td><td>0.540</td><td>0.608</td><td>0.535</td><td>0.721</td><td>0.622</td><td>0.6570.550</td><td>0.941</td><td>0.648</td><td>0.691</td><td>0.574</td><td>0.750</td><td>0.619</td><td>0.939</td><td>0.644</td><td>1.179</td><td>0.832</td><td>1.347</td><td>0.870</td><td>1.202</td><td>0.811</td><td>1.294</td><td>0.854</td></tr><tr><td>720</td><td>0.700</td><td>0.604</td><td>$\underline{0.725}$</td><td>0.591</td><td>0.986</td><td>0.743</td><td>0.7620.610</td><td>0.877</td><td>0.641</td><td>0.728</td><td>0.614</td><td>0.721</td><td>0.616</td><td>0.887</td><td>0.645</td><td>1.273</td><td>0.874</td><td>1.534</td><td>0.947</td><td>1.217</td><td>0.825</td><td>1.223</td><td>0.838</td></tr><tr><td>平均值</td><td>0.556</td><td>0.522</td><td>$\underline{0.590}$</td><td>$\underline{0.525}$</td><td>0.691</td><td>0.600</td><td>0.6330.542</td><td>0.869</td><td>0.628</td><td>0.639</td><td>0.561</td><td>0.702</td><td>0.596</td><td>0.915</td><td>0.639</td><td>1.180</td><td>0.834</td><td>1.375</td><td>0.877</td><td>1.199</td><td>0.809</td><td>1.249</td><td>0.833</td></tr><tr><td rowspan="5">${ETTh2}$</td><td>96</td><td>0.275</td><td>0.326</td><td>$\underline{0.331}$</td><td>0.374</td><td>0.357</td><td>0.411</td><td>0.3530.389</td><td>0.378</td><td>0.409</td><td>0.382</td><td>0.416</td><td>0.413</td><td>0.451</td><td>0.389</td><td>0.411</td><td>0.678</td><td>0.619</td><td>2.022</td><td>1.006</td><td>3.837</td><td>1.508</td><td>3.788</td><td>1.533</td></tr><tr><td>192</td><td>0.374</td><td>0.373</td><td>0.402</td><td>0.411</td><td>0.569</td><td>0.519</td><td>0.4030.414</td><td>0.490</td><td>0.467</td><td>0.478</td><td>0.474</td><td>0.474</td><td>0.477</td><td>0.473</td><td>0.455</td><td>0.785</td><td>0.666</td><td>2.329</td><td>1.104</td><td>3.856</td><td>1.513</td><td>3.552</td><td>1.483</td></tr><tr><td>336</td><td>0.406</td><td>0.429</td><td>0.406</td><td>0.433</td><td>0.671</td><td>0.572</td><td>0.4260.441</td><td>0.537</td><td>0.494</td><td>0.504</td><td>0.501</td><td>0.547</td><td>0.543</td><td>0.507</td><td>0.480</td><td>0.839</td><td>0.694</td><td>2.453</td><td>1.122</td><td>3.952</td><td>1.526</td><td>3.395</td><td>1.526</td></tr><tr><td>720</td><td>0.427</td><td>0.449</td><td>0.449</td><td>0.464</td><td>0.824</td><td>0.648</td><td>0.4770.480</td><td>0.510</td><td>0.491</td><td>0.499</td><td>0.509</td><td>0.516</td><td>0.523</td><td>0.477</td><td>0.472</td><td>1.273</td><td>0.874</td><td>3.816</td><td>1.407</td><td>3.842</td><td>1.503</td><td>3.205</td><td>1.401</td></tr><tr><td>平均值</td><td>0.370</td><td>0.394</td><td>$\underline{0.397}$</td><td>$\underline{0.421}$</td><td>0.605</td><td>0.538</td><td>0.4150.431</td><td>0.479</td><td>0.465</td><td>0.466</td><td>0.475</td><td>0.488</td><td>0.499</td><td>0.462</td><td>0.455</td><td>0.894</td><td>0.713</td><td>2.655</td><td>1.160</td><td>3.872</td><td>1.513</td><td>3.485</td><td>1.486</td></tr><tr><td rowspan="5">${ETTm1}$</td><td>96</td><td>0.346</td><td>0.388</td><td>0.390</td><td>0.404</td><td>0.352</td><td>0.392</td><td>0.4100.419</td><td>0.583</td><td>0.501</td><td>0.578</td><td>0.518</td><td>0.774</td><td>0.614</td><td>0.761</td><td>0.568</td><td>0.911</td><td>0.688</td><td>0.921</td><td>0.682</td><td>1.162</td><td>0.785</td><td>1.442</td><td>0.847</td></tr><tr><td>192</td><td>0.373</td><td>0.416</td><td>0.429</td><td>0.423</td><td>0.382</td><td>0.412</td><td>0.4370.434</td><td>0.630</td><td>0.528</td><td>0.617</td><td>0.546</td><td>0.754</td><td>0.592</td><td>0.781</td><td>0.574</td><td>0.955</td><td>0.703</td><td>0.957</td><td>0.701</td><td>1.172</td><td>0.793</td><td>1.444</td><td>0.862</td></tr><tr><td>336</td><td>0.413</td><td>0.426</td><td>0.469</td><td>0.439</td><td>0.419</td><td>0.434</td><td>0.4760.454</td><td>0.725</td><td>0.568</td><td>0.998</td><td>0.775</td><td>0.869</td><td>0.677</td><td>0.803</td><td>0.587</td><td>0.991</td><td>0.719</td><td>0.998</td><td>0.716</td><td>1.227</td><td>0.908</td><td>1.450</td><td>0.866</td></tr><tr><td>720</td><td>0.485</td><td>0.476</td><td>0.569</td><td>0.498</td><td>0.490</td><td>0.477</td><td>0.6810.556</td><td>0.769</td><td>0.549</td><td>0.693</td><td>0.579</td><td>0.810</td><td>0.630</td><td>0.844</td><td>0.581</td><td>1.062</td><td>0.747</td><td>1.007</td><td>0.719</td><td>1.207</td><td>0.797</td><td>1.366</td><td>0.850</td></tr><tr><td>平均值</td><td>0.404</td><td>0.427</td><td>0.464</td><td>0.441</td><td>$\underline{0.411}$</td><td>$\underline{0.429}$</td><td>0.5010.466</td><td>0.677</td><td>0.537</td><td>0.722</td><td>0.605</td><td>0.802</td><td>0.628</td><td>0.797</td><td>0.578</td><td>0.980</td><td>0.714</td><td>0.971</td><td>0.705</td><td>1.192</td><td>0.821</td><td>1.426</td><td>0.856</td></tr><tr><td rowspan="5">${ETT}{m2}$</td><td>96</td><td>0.177</td><td>0.261</td><td>0.188</td><td>0.269</td><td>0.213</td><td>0.303</td><td>0.1910.274</td><td>0.212</td><td>0.285</td><td>0.291</td><td>0.399</td><td>0.352</td><td>0.454</td><td>0.229</td><td>0.308</td><td>0.331</td><td>0.430</td><td>0.813</td><td>0.688</td><td>3.203</td><td>1.407</td><td>4.195</td><td>1.628</td></tr><tr><td>192</td><td>0.241</td><td>0.314</td><td>0.251</td><td>0.309</td><td>0.278</td><td>0.345</td><td>0.2520.317</td><td>0.270</td><td>0.323</td><td>0.307</td><td>0.379</td><td>0.694</td><td>0.691</td><td>0.291</td><td>0.343</td><td>0.400</td><td>0.464</td><td>1.008</td><td>0.768</td><td>3.112</td><td>1.387</td><td>4.042</td><td>1.601</td></tr><tr><td>336</td><td>0.274</td><td>0.327</td><td>0.307</td><td>0.346</td><td>0.338</td><td>0.385</td><td>0.3060.353</td><td>0.323</td><td>0.353</td><td>0.543</td><td>0.559</td><td>2.408</td><td>1.407</td><td>0.348</td><td>0.376</td><td>0.469</td><td>0.498</td><td>1.031</td><td>0.775</td><td>3.255</td><td>1.421</td><td>3.963</td><td>1.585</td></tr><tr><td>720</td><td>0.417</td><td>0.390</td><td>0.426</td><td>0.417</td><td>0.436</td><td>0.440</td><td>0.4330.427</td><td>0.474</td><td>0.449</td><td>0.712</td><td>0.614</td><td>1.913</td><td>1.166</td><td>0.461</td><td>0.438</td><td>0.589</td><td>0.557</td><td>1.096</td><td>0.791</td><td>3.909</td><td>1.543</td><td>3.711</td><td>1.532</td></tr><tr><td>平均值</td><td>0.277</td><td>0.323</td><td>0.293</td><td>$\underline{0.335}$</td><td>0.316</td><td>0.368</td><td>0.2960.343</td><td>0.320</td><td>0.353</td><td>0.463</td><td>0.488</td><td>1.342</td><td>0.930</td><td>0.332</td><td>0.366</td><td>0.447</td><td>0.487</td><td>0.987</td><td>0.756</td><td>3.370</td><td>1.440</td><td>3.978</td><td>1.587</td></tr><tr><td rowspan="5">${Weather}$</td><td>96</td><td>0.161</td><td>0.210</td><td>0.163</td><td>0.215</td><td>0.171</td><td>0.224</td><td>0.1650.215</td><td>0.184</td><td>0.230</td><td>0.188</td><td>0.253</td><td>0.221</td><td>0.297</td><td>0.192</td><td>0.234</td><td>0.199</td><td>0.272</td><td>0.217</td><td>0.269</td><td>0.374</td><td>0.401</td><td>0.335</td><td>0.380</td></tr><tr><td>192</td><td>0.204</td><td>0.248</td><td>0.210</td><td>0.254</td><td>0.215</td><td>0.263</td><td>0.2100.257</td><td>0.245</td><td>0.283</td><td>0.250</td><td>0.304</td><td>0.270</td><td>0.322</td><td>0.269</td><td>0.295</td><td>0.279</td><td>0.332</td><td>0.259</td><td>0.304</td><td>0.552</td><td>0.478</td><td>0.522</td><td>0.462</td></tr><tr><td>336</td><td>0.261</td><td>0.302</td><td>0.256</td><td>0.292</td><td>0.258</td><td>0.299</td><td>0.2590.297</td><td>0.305</td><td>0.321</td><td>0.312</td><td>0.346</td><td>0.320</td><td>0.351</td><td>0.370</td><td>0.357</td><td>0.356</td><td>0.386</td><td>0.303</td><td>0.334</td><td>724</td><td>0.541</td><td>0.715</td><td>0.535</td></tr><tr><td>720</td><td>0.309</td><td>0.332</td><td>0.321</td><td>0.339</td><td>0.320</td><td>0.346</td><td>0.3320.346</td><td>0.381</td><td>0.371</td><td>0.387</td><td>0.393</td><td>0.390</td><td>0.396</td><td>0.441</td><td>0.405</td><td>0.437</td><td>0.448</td><td>0.377</td><td>0.382</td><td>0.739</td><td>0.558</td><td>0.611</td><td>0.500</td></tr><tr><td>平均值</td><td>0.234</td><td>0.273</td><td>$\underline{0.238}$</td><td>$\underline{0.275}$</td><td>0.241</td><td>0.283</td><td>0.2420.279</td><td>0.279</td><td>0.301</td><td>0.284</td><td>0.324</td><td>0.300</td><td>0.342</td><td>0.318</td><td>0.323</td><td>0.318</td><td>0.360</td><td>0.289</td><td>0.322</td><td>0.597</td><td>0.495</td><td>0.546</td><td>0.469</td></tr><tr><td rowspan="5">电力</td><td>96</td><td>0.139</td><td>0.241</td><td>0.139</td><td>0.237</td><td>0.150</td><td>0.253</td><td>0.1400.238</td><td>0.299</td><td>0.373</td><td>0.231</td><td>0.323</td><td>0.261</td><td>0.348</td><td>0.420</td><td>0.466</td><td>0.599</td><td>0.587</td><td>0.350</td><td>0.425</td><td>1.259</td><td>0.919</td><td>0.993</td><td>0.784</td></tr><tr><td>192</td><td>0.151</td><td>0.248</td><td>0.156</td><td>0.252</td><td>0.164</td><td>0.264</td><td>0.1600.255</td><td>0.305</td><td>0.379</td><td>0.261</td><td>0.356</td><td>0.338</td><td>0.406</td><td>0.411</td><td>0.459</td><td>0.620</td><td>0.598</td><td>0.376</td><td>0.448</td><td>1.160</td><td>0.873</td><td>0.938</td><td>0.753</td></tr><tr><td>336</td><td>0.169</td><td>0.270</td><td>0.175</td><td>0.270</td><td>0.181</td><td>0.282</td><td>0.1800.276</td><td>0.319</td><td>0.391</td><td>0.360</td><td>0.445</td><td>0.410</td><td>0.474</td><td>0.434</td><td>0.473</td><td>0.662</td><td>0.619</td><td>0.428</td><td>0.485</td><td>1.157</td><td>0.872</td><td>0.925</td><td>0.745</td></tr><tr><td>720</td><td>0.240</td><td>0.322</td><td>0.233</td><td>0.317</td><td>0.223</td><td>0.321</td><td>0.2410.323</td><td>0.369</td><td>0.426</td><td>0.530</td><td>0.585</td><td>0.715</td><td>0.685</td><td>0.510</td><td>0.521</td><td>0.757</td><td>0.664</td><td>0.611</td><td>0.597</td><td>1.203</td><td>0.898</td><td>1.004</td><td>0.790</td></tr><tr><td>平均值</td><td>0.175</td><td>0.270</td><td>$\underline{0.176}$</td><td>0.269</td><td>0.180</td><td>0.280</td><td>0.1800.273</td><td>0.323</td><td>0.392</td><td>0.346</td><td>0.427</td><td>0.431</td><td>0.478</td><td>0.444</td><td>0.480</td><td>0.660</td><td>0.617</td><td>0.441</td><td>0.489</td><td>1.195</td><td>0.891</td><td>0.965</td><td>0.768</td></tr><tr><td rowspan="5">交通</td><td>96</td><td>0.418</td><td>0.291</td><td>0.414</td><td>0.297</td><td>0.419</td><td>0.298</td><td>0.4030.289</td><td>0.719</td><td>0.416</td><td>0.639</td><td>0.400</td><td>0.672</td><td>0.405</td><td>1.412</td><td>0.802</td><td>1.643</td><td>0.855</td><td>1.157</td><td>0.636</td><td>1.557</td><td>0.821</td><td>1.527</td><td>0.815</td></tr><tr><td>192</td><td>0.414</td><td>0.296</td><td>0.426</td><td>0.301</td><td>0.434</td><td>0.305</td><td>0.4150.296</td><td>0.748</td><td>0.428</td><td>0.637</td><td>0.416</td><td>0.727</td><td>0.424</td><td>1.419</td><td>0.806</td><td>1.641</td><td>0.854</td><td>1.207</td><td>0.661</td><td>1.454</td><td>0.765</td><td>1.538</td><td>0.817</td></tr><tr><td>336</td><td>0.421</td><td>0.311</td><td>0.434</td><td>0.303</td><td>0.449</td><td>0.313</td><td>0.4260.304</td><td>0.853</td><td>0.471</td><td>0.655</td><td>0.427</td><td>0.749</td><td>0.454</td><td>1.443</td><td>0.815</td><td>1.711</td><td>0.878</td><td>1.334</td><td>0.713</td><td>1.521</td><td>0.812</td><td>1.550</td><td>0.819</td></tr><tr><td>720</td><td>0.462</td><td>0.327</td><td>0.487</td><td>0.337</td><td>0.484</td><td>0.336</td><td>0.4740.331</td><td>1.485</td><td>0.825</td><td>0.722</td><td>0.456</td><td>0.847</td><td>0.499</td><td>1.539</td><td>0.837</td><td>2.660</td><td>1.157</td><td>1.292</td><td>0.726</td><td>1.605</td><td>0.846</td><td>1.588</td><td>0.833</td></tr><tr><td>平均值</td><td>0.429</td><td>0.306</td><td>0.440</td><td>0.310</td><td>0.447</td><td>0.313</td><td>$\underline{0.430}$0.305</td><td>0.951</td><td>0.535</td><td>0.663</td><td>0.425</td><td>0.749</td><td>0.446</td><td>1.453</td><td>0.815</td><td>1.914</td><td>0.936</td><td>1.248</td><td>0.684</td><td>1.534</td><td>0.811</td><td>1.551</td><td>0.821</td></tr><tr><td colspan="2">${1}^{\text{st }}$ 计数</td><td colspan="2">32</td><td colspan="2">9</td><td>3</td><td></td><td>3</td><td>0</td><td></td><td></td><td>0</td><td>0</td><td></td><td></td><td>0</td><td></td><td>0</td><td>0</td><td></td><td>0</td><td></td><td>0</td><td></td></tr></tbody></table>

Table 15: Full few-shot learning results on 5% training data. We use the same protocol as in Tab. 1 '-' means that 5% time series is not sufficient to constitute a training set.

表15：在5%训练数据上的完整小样本学习结果。我们采用与表1相同的协议。“-”表示5%的时间序列不足以构成一个训练集。

<table><tr><td colspan="2">Methods</td><td colspan="2">TIME-LLM</td><td>GPT4TS</td><td colspan="2">DLinear</td><td colspan="2">PatchTST</td><td colspan="2">TimesNet</td><td colspan="2">FEDformer</td><td colspan="2">Autoformer</td><td colspan="2">Stationary</td><td colspan="2">ETSformer</td><td colspan="2">LightTS</td><td colspan="2">Informer</td><td colspan="2">Reformer</td></tr><tr><td colspan="2">Metric</td><td>MSE</td><td>MAE</td><td>MSEMAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="5"/><td>96</td><td>0.483</td><td>0.464</td><td>0.5430.506</td><td>0.547</td><td>$\underline{0.503}$</td><td>0.557</td><td>0.519</td><td>0.892</td><td>0.625</td><td>0.593</td><td>0.529</td><td>0.681</td><td>0.570</td><td>0.952</td><td>0.650</td><td>1.169</td><td>0.832</td><td>1.483</td><td>0.91</td><td>1.225</td><td>0.812</td><td>1.198</td><td>0.795</td></tr><tr><td>192</td><td>0.629</td><td>0.540</td><td>0.7480.580</td><td>0.720</td><td>0.604</td><td>0.711</td><td>0.570</td><td>0.940</td><td>0.665</td><td>$\underline{0.652}$</td><td>$\underline{0.563}$</td><td>0.725</td><td>0.602</td><td>0.943</td><td>0.645</td><td>1.221</td><td>0.853</td><td>1.525</td><td>0.93</td><td>1.249</td><td>0.828</td><td>1.273</td><td>0.853</td></tr><tr><td>336</td><td>0.768</td><td>0.626</td><td>$\underline{0.754}$$\underline{0.595}$</td><td>0.984</td><td>0.727</td><td>0.816</td><td>0.619</td><td>0.945</td><td>0.653</td><td>0.731</td><td>0.594</td><td>0.761</td><td>0.624</td><td>0.935</td><td>0.644</td><td>1.179</td><td>0.832</td><td>1.347</td><td>0.87</td><td>1.202</td><td>0.811</td><td>1.254</td><td>0.857</td></tr><tr><td>720</td><td>-</td><td>-</td><td>--</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Avg</td><td>0.627</td><td>0.543</td><td>0.681$\underline{0.560}$</td><td>0.750</td><td>0.611</td><td>0.694</td><td>0.569</td><td>0.925</td><td>0.647</td><td>$\underline{0.658}$</td><td>0.562</td><td>0.722</td><td>0.598</td><td>0.943</td><td>0.646</td><td>1.189</td><td>0.839</td><td>1.451</td><td>0.903</td><td>1.225</td><td>0.817</td><td>1.241</td><td>0.835</td></tr><tr><td rowspan="5"/><td>96</td><td>0.336</td><td>0.397</td><td>$\underline{0.376}$$\underline{0.421}$</td><td>0.442</td><td>0.456</td><td>0.401</td><td>0.421</td><td>0.409</td><td>0.420</td><td>0.390</td><td>0.424</td><td>0.428</td><td>0.468</td><td>0.408</td><td>0.423</td><td>0.678</td><td>0.619</td><td>2.022</td><td>1.006</td><td>3.837</td><td>1.508</td><td>3.753</td><td>1.518</td></tr><tr><td>192</td><td>0.406</td><td>0.425</td><td>0.4180.441</td><td>0.617</td><td>0.542</td><td>0.452</td><td>0.455</td><td>0.483</td><td>0.464</td><td>0.457</td><td>0.465</td><td>0.496</td><td>0.504</td><td>0.497</td><td>0.468</td><td>0.845</td><td>0.697</td><td>3.534</td><td>1.348</td><td>3.975</td><td>1.933</td><td>3.516</td><td>1.473</td></tr><tr><td>336</td><td>0.405</td><td>0.432</td><td>0.4080.439</td><td>1.424</td><td>0.849</td><td>0.464</td><td>0.469</td><td>0.499</td><td>0.479</td><td>0.477</td><td>0.483</td><td>0.486</td><td>0.496</td><td>0.507</td><td>0.481</td><td>0.905</td><td>0.727</td><td>4.063</td><td>1.451</td><td>3.956</td><td>1.520</td><td>3.312</td><td>1.427</td></tr><tr><td>720</td><td>-</td><td>-</td><td>--</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Avg</td><td>0.382</td><td>0.418</td><td>$\underline{0.400}$$\underline{0.433}$</td><td>0.694</td><td>0.577</td><td>0.827</td><td>0.615</td><td>0.439</td><td>0.448</td><td>0.463</td><td>0.454</td><td>0.441</td><td>0.457</td><td>0.470</td><td>0.489</td><td>0.809</td><td>0.681</td><td>3.206</td><td>1.268</td><td>3.922</td><td>1.653</td><td>3.527</td><td>1.472</td></tr><tr><td rowspan="5">${ETTm1}$</td><td>96</td><td>0.316</td><td>0.377</td><td>0.3860.405</td><td>0.332</td><td>0.374</td><td>0.399</td><td>0.414</td><td>0.606</td><td>0.518</td><td>0.628</td><td>0.544</td><td>0.726</td><td>0.578</td><td>0.823</td><td>0.587</td><td>1.031</td><td>0.747</td><td>1.048</td><td>0.733</td><td>1.130</td><td>0.775</td><td>1.234</td><td>0.798</td></tr><tr><td>192</td><td>0.450</td><td>0.464</td><td>0.4400.438</td><td>0.358</td><td>0.390</td><td>0.441</td><td>0.436</td><td>0.681</td><td>0.539</td><td>0.666</td><td>0.566</td><td>0.750</td><td>0.591</td><td>0.844</td><td>0.591</td><td>1.087</td><td>0.766</td><td>1.097</td><td>0.756</td><td>1.150</td><td>0.788</td><td>1.287</td><td>0.839</td></tr><tr><td>336</td><td>0.450</td><td>0.424</td><td>0.4850.459</td><td>0.402</td><td>0.416</td><td>0.499</td><td>0.467</td><td>0.786</td><td>0.597</td><td>0.807</td><td>0.628</td><td>0.851</td><td>0.659</td><td>0.870</td><td>0.603</td><td>1.138</td><td>0.787</td><td>1.147</td><td>0.775</td><td>1.198</td><td>0.809</td><td>1.288</td><td>0.842</td></tr><tr><td>720</td><td>0.483</td><td>0.471</td><td>0.5770.499</td><td>0.511</td><td>0.489</td><td>0.767</td><td>0.587</td><td>0.796</td><td>0.593</td><td>0.822</td><td>0.633</td><td>0.857</td><td>0.655</td><td>0.893</td><td>0.611</td><td>1.245</td><td>0.831</td><td>1.200</td><td>0.799</td><td>1.175</td><td>0.794</td><td>1.247</td><td>0.828</td></tr><tr><td>Avg</td><td>$\underline{0.425}$</td><td>$\underline{0.434}$</td><td>0.4720.450</td><td>0.400</td><td>0.417</td><td>0.526</td><td>0.476</td><td>0.717</td><td>0.561</td><td>0.730</td><td>0.592</td><td>0.796</td><td>0.620</td><td>0.857</td><td>0.598</td><td>1.125</td><td>0.782</td><td>1.123</td><td>0.765</td><td>1.163</td><td>0.791</td><td>1.264</td><td>0.826</td></tr><tr><td rowspan="5">${ETT}{m2}$</td><td>96</td><td>0.174</td><td>0.261</td><td>0.1990.280</td><td>0.236</td><td>0.326</td><td>0.206</td><td>0.288</td><td>0.220</td><td>0.299</td><td>0.229</td><td>0.320</td><td>0.232</td><td>0.322</td><td>0.238</td><td>0.316</td><td>0.404</td><td>0.485</td><td>1.108</td><td>0.772</td><td>3.599</td><td>1.478</td><td>3.883</td><td>1.545</td></tr><tr><td>192</td><td>0.215</td><td>0.287</td><td>0.2560.316</td><td>0.306</td><td>0.373</td><td>0.264</td><td>0.324</td><td>0.311</td><td>0.361</td><td>0.394</td><td>0.361</td><td>0.291</td><td>0.357</td><td>0.298</td><td>0.349</td><td>0.479</td><td>0.521</td><td>1.317</td><td>0.850</td><td>3.578</td><td>1.475</td><td>3.553</td><td>1.484</td></tr><tr><td>336</td><td>0.273</td><td>0.330</td><td>0.3180.353</td><td>0.380</td><td>0.423</td><td>0.334</td><td>0.367</td><td>0.338</td><td>0.366</td><td>0.378</td><td>0.427</td><td>0.478</td><td>0.517</td><td>0.353</td><td>0.380</td><td>0.552</td><td>0.555</td><td>1.415</td><td>0.879</td><td>3.561</td><td>1.473</td><td>3.446</td><td>1.460</td></tr><tr><td>720</td><td>0.433</td><td>0.412</td><td>0.4600.436</td><td>0.674</td><td>0.583</td><td>0.454</td><td>0.432</td><td>0.509</td><td>0.465</td><td>0.523</td><td>0.510</td><td>0.553</td><td>0.538</td><td>0.475</td><td>0.445</td><td>0.701</td><td>0.627</td><td>1.822</td><td>0.984</td><td>3.896</td><td>1.533</td><td>3.445</td><td>1.460</td></tr><tr><td>Avg</td><td>0.274</td><td>0.323</td><td>$\underline{0.308}$$\underline{0.346}$</td><td>0.399</td><td>0.426</td><td>0.314</td><td>0.352</td><td>0.344</td><td>0.372</td><td>0.381</td><td>0.404</td><td>0.388</td><td>0.433</td><td>0.341</td><td>0.372</td><td>0.534</td><td>0.547</td><td>1.415</td><td>0.871</td><td>3.658</td><td>1.489</td><td>3.581</td><td>1.487</td></tr><tr><td rowspan="5">${Weather}$</td><td>96</td><td>0.172</td><td>0.263</td><td>0.1750.230</td><td>0.184</td><td>0.242</td><td>0.171</td><td>0.224</td><td>0.207</td><td>0.253</td><td>0.229</td><td>0.309</td><td>0.227</td><td>0.299</td><td>0.215</td><td>0.252</td><td>0.218</td><td>0.295</td><td>0.230</td><td>0.285</td><td>0.497</td><td>0.497</td><td>0.406</td><td>0.435</td></tr><tr><td>192</td><td>0.224</td><td>0.271</td><td>0.2270.276</td><td>0.228</td><td>0.283</td><td>0.230</td><td>0.277</td><td>0.272</td><td>0.307</td><td>0.265</td><td>0.317</td><td>0.278</td><td>0.333</td><td>0.290</td><td>0.307</td><td>0.294</td><td>0.331</td><td>0.274</td><td>0.323</td><td>0.620</td><td>0.545</td><td>0.446</td><td>0.450</td></tr><tr><td>336</td><td>0.282</td><td>0.321</td><td>0.2860.322</td><td>0.279</td><td>0.322</td><td>0.294</td><td>0.326</td><td>0.313</td><td>0.328</td><td>0.353</td><td>0.392</td><td>0.351</td><td>0.393</td><td>0.353</td><td>0.348</td><td>0.359</td><td>0.398</td><td>0.318</td><td>0.355</td><td>0.649</td><td>0.547</td><td>0.465</td><td>0.459</td></tr><tr><td>720</td><td>0.366</td><td>0.381</td><td>0.3660.379</td><td>0.364</td><td>0.388</td><td>0.384</td><td>0.387</td><td>0.400</td><td>0.385</td><td>0.391</td><td>0.394</td><td>0.387</td><td>0.389</td><td>0.452</td><td>0.407</td><td>0.461</td><td>0.461</td><td>0.401</td><td>0.418</td><td>0.570</td><td>0.522</td><td>0.471</td><td>0.468</td></tr><tr><td>Avg</td><td>0.260</td><td>0.309</td><td>$\underline{0.263}$0.301</td><td>$\underline{0.263}$</td><td>0.308</td><td>0.269</td><td>$\underline{0.303}$</td><td>0.298</td><td>0.318</td><td>0.309</td><td>0.353</td><td>0.310</td><td>0.353</td><td>0.327</td><td>0.328</td><td>0.333</td><td>0.371</td><td>0.305</td><td>0.345</td><td>0.584</td><td>0.527</td><td>0.447</td><td>0.453</td></tr><tr><td rowspan="5">Electricity</td><td>96</td><td>0.147</td><td>0.242</td><td>0.1430.241</td><td>0.150</td><td>0.251</td><td>$\underline{0.145}$</td><td>0.244</td><td>0.315</td><td>0.389</td><td>0.235</td><td>0.322</td><td>0.297</td><td>0.367</td><td>0.484</td><td>0.518</td><td>0.697</td><td>0.638</td><td>0.639</td><td>0.609</td><td>1.265</td><td>0.919</td><td>1.414</td><td>0.855</td></tr><tr><td>192</td><td>0.158</td><td>0.241</td><td>0.1590.255</td><td>0.163</td><td>0.263</td><td>0.163</td><td>0.260</td><td>0.318</td><td>0.396</td><td>0.247</td><td>0.341</td><td>0.308</td><td>0.375</td><td>0.501</td><td>0.531</td><td>0.718</td><td>0.648</td><td>0.772</td><td>0.678</td><td>1.298</td><td>0.939</td><td>1.240</td><td>0.919</td></tr><tr><td>336</td><td>0.178</td><td>0.277</td><td>0.1790.274</td><td>0.175</td><td>0.278</td><td>0.183</td><td>0.281</td><td>0.340</td><td>0.415</td><td>0.267</td><td>0.356</td><td>0.354</td><td>0.411</td><td>0.574</td><td>0.578</td><td>0.758</td><td>0.667</td><td>0.901</td><td>0.745</td><td>1.302</td><td>0.942</td><td>1.253</td><td>0.921</td></tr><tr><td>720</td><td>0.224</td><td>0.312</td><td>0.2330.323</td><td>0.219</td><td>0.311</td><td>0.233</td><td>0.323</td><td>0.635</td><td>0.613</td><td>0.318</td><td>0.394</td><td>0.426</td><td>0.466</td><td>0.952</td><td>0.786</td><td>1.028</td><td>0.788</td><td>1.200</td><td>0.871</td><td>1.259</td><td>0.919</td><td>1.249</td><td>0.921</td></tr><tr><td>Avg</td><td>0.179</td><td>0.268</td><td>0.178$\underline{0.273}$</td><td>0.176</td><td>0.275</td><td>0.181</td><td>0.277</td><td>0.402</td><td>0.453</td><td>0.266</td><td>0.353</td><td>0.346</td><td>0.404</td><td>0.627</td><td>0.603</td><td>0.800</td><td>0.685</td><td>0.878</td><td>0.725</td><td>1.281</td><td>0.929</td><td>1.289</td><td>0.904</td></tr><tr><td rowspan="5"/><td>96</td><td>0.414</td><td>0.291</td><td>0.4190.298</td><td>0.427</td><td>0.304</td><td>0.404</td><td>0.286</td><td>0.854</td><td>0.492</td><td>0.670</td><td>0.421</td><td>0.795</td><td>0.481</td><td>1.468</td><td>0.821</td><td>1.643</td><td>0.855</td><td>1.157</td><td>0.636</td><td>1.557</td><td>0.821</td><td>1.586</td><td>0.841</td></tr><tr><td>192</td><td>0.419</td><td>0.291</td><td>0.4340.305</td><td>0.447</td><td>0.315</td><td>0.412</td><td>0.294</td><td>0.894</td><td>0.517</td><td>0.653</td><td>0.405</td><td>0.837</td><td>0.503</td><td>1.509</td><td>0.838</td><td>1.856</td><td>0.928</td><td>1.688</td><td>0.848</td><td>1.596</td><td>0.834</td><td>1.602</td><td>0.844</td></tr><tr><td>336</td><td>0.437</td><td>0.314</td><td>0.4490.313</td><td>0.478</td><td>0.333</td><td>0.439</td><td>0.310</td><td>0.853</td><td>0.471</td><td>0.707</td><td>0.445</td><td>0.867</td><td>0.523</td><td>1.602</td><td>0.860</td><td>2.080</td><td>0.999</td><td>1.826</td><td>0.903</td><td>1.621</td><td>0.841</td><td>1.668</td><td>0.868</td></tr><tr><td>720</td><td>-</td><td>-</td><td>--</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Avg</td><td>$\underline{0.423}$</td><td>$\underline{0.298}$</td><td>0.4340.305</td><td>0.450</td><td>0.317</td><td>0.418</td><td>0.296</td><td>0.867</td><td>0.493</td><td>0.676</td><td>0.423</td><td>0.833</td><td>0.502</td><td>1.526</td><td>0.839</td><td>1.859</td><td>0.927</td><td>1.557</td><td>0.795</td><td>1.591</td><td>0.832</td><td>1.618</td><td>0.851</td></tr><tr><td colspan="2">${1}^{\text{st }}$ Count</td><td colspan="2">21</td><td>6</td><td/><td>7</td><td/><td>6</td><td/><td>0</td><td/><td>1</td><td/><td>0│</td><td/><td>0</td><td>│</td><td>0</td><td>0</td><td/><td/><td>0</td><td>0</td><td/></tr></table>

<table><tbody><tr><td colspan="2">方法</td><td colspan="2">TIME大语言模型（TIME-LLM）</td><td>GPT4时间序列模型（GPT4TS）</td><td colspan="2">深度线性模型（DLinear）</td><td colspan="2">补丁时间序列变换器（PatchTST）</td><td colspan="2">时间网络（TimesNet）</td><td colspan="2">FEDformer（联邦变压器）</td><td colspan="2">Autoformer（自动变压器）</td><td colspan="2">平稳的</td><td colspan="2">ETSformer（ETS变压器）</td><td colspan="2">LightTS（轻量级时间序列）</td><td colspan="2">Informer（信息者）</td><td colspan="2">改革者</td></tr><tr><td colspan="2">指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差与平均绝对误差（MSEMAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td rowspan="5"></td><td>96</td><td>0.483</td><td>0.464</td><td>0.5430.506</td><td>0.547</td><td>$\underline{0.503}$</td><td>0.557</td><td>0.519</td><td>0.892</td><td>0.625</td><td>0.593</td><td>0.529</td><td>0.681</td><td>0.570</td><td>0.952</td><td>0.650</td><td>1.169</td><td>0.832</td><td>1.483</td><td>0.91</td><td>1.225</td><td>0.812</td><td>1.198</td><td>0.795</td></tr><tr><td>192</td><td>0.629</td><td>0.540</td><td>0.7480.580</td><td>0.720</td><td>0.604</td><td>0.711</td><td>0.570</td><td>0.940</td><td>0.665</td><td>$\underline{0.652}$</td><td>$\underline{0.563}$</td><td>0.725</td><td>0.602</td><td>0.943</td><td>0.645</td><td>1.221</td><td>0.853</td><td>1.525</td><td>0.93</td><td>1.249</td><td>0.828</td><td>1.273</td><td>0.853</td></tr><tr><td>336</td><td>0.768</td><td>0.626</td><td>$\underline{0.754}$$\underline{0.595}$</td><td>0.984</td><td>0.727</td><td>0.816</td><td>0.619</td><td>0.945</td><td>0.653</td><td>0.731</td><td>0.594</td><td>0.761</td><td>0.624</td><td>0.935</td><td>0.644</td><td>1.179</td><td>0.832</td><td>1.347</td><td>0.87</td><td>1.202</td><td>0.811</td><td>1.254</td><td>0.857</td></tr><tr><td>720</td><td>-</td><td>-</td><td>--</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>平均值</td><td>0.627</td><td>0.543</td><td>0.681$\underline{0.560}$</td><td>0.750</td><td>0.611</td><td>0.694</td><td>0.569</td><td>0.925</td><td>0.647</td><td>$\underline{0.658}$</td><td>0.562</td><td>0.722</td><td>0.598</td><td>0.943</td><td>0.646</td><td>1.189</td><td>0.839</td><td>1.451</td><td>0.903</td><td>1.225</td><td>0.817</td><td>1.241</td><td>0.835</td></tr><tr><td rowspan="5"></td><td>96</td><td>0.336</td><td>0.397</td><td>$\underline{0.376}$$\underline{0.421}$</td><td>0.442</td><td>0.456</td><td>0.401</td><td>0.421</td><td>0.409</td><td>0.420</td><td>0.390</td><td>0.424</td><td>0.428</td><td>0.468</td><td>0.408</td><td>0.423</td><td>0.678</td><td>0.619</td><td>2.022</td><td>1.006</td><td>3.837</td><td>1.508</td><td>3.753</td><td>1.518</td></tr><tr><td>192</td><td>0.406</td><td>0.425</td><td>0.4180.441</td><td>0.617</td><td>0.542</td><td>0.452</td><td>0.455</td><td>0.483</td><td>0.464</td><td>0.457</td><td>0.465</td><td>0.496</td><td>0.504</td><td>0.497</td><td>0.468</td><td>0.845</td><td>0.697</td><td>3.534</td><td>1.348</td><td>3.975</td><td>1.933</td><td>3.516</td><td>1.473</td></tr><tr><td>336</td><td>0.405</td><td>0.432</td><td>0.4080.439</td><td>1.424</td><td>0.849</td><td>0.464</td><td>0.469</td><td>0.499</td><td>0.479</td><td>0.477</td><td>0.483</td><td>0.486</td><td>0.496</td><td>0.507</td><td>0.481</td><td>0.905</td><td>0.727</td><td>4.063</td><td>1.451</td><td>3.956</td><td>1.520</td><td>3.312</td><td>1.427</td></tr><tr><td>720</td><td>-</td><td>-</td><td>--</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>平均值</td><td>0.382</td><td>0.418</td><td>$\underline{0.400}$$\underline{0.433}$</td><td>0.694</td><td>0.577</td><td>0.827</td><td>0.615</td><td>0.439</td><td>0.448</td><td>0.463</td><td>0.454</td><td>0.441</td><td>0.457</td><td>0.470</td><td>0.489</td><td>0.809</td><td>0.681</td><td>3.206</td><td>1.268</td><td>3.922</td><td>1.653</td><td>3.527</td><td>1.472</td></tr><tr><td rowspan="5">${ETTm1}$</td><td>96</td><td>0.316</td><td>0.377</td><td>0.3860.405</td><td>0.332</td><td>0.374</td><td>0.399</td><td>0.414</td><td>0.606</td><td>0.518</td><td>0.628</td><td>0.544</td><td>0.726</td><td>0.578</td><td>0.823</td><td>0.587</td><td>1.031</td><td>0.747</td><td>1.048</td><td>0.733</td><td>1.130</td><td>0.775</td><td>1.234</td><td>0.798</td></tr><tr><td>192</td><td>0.450</td><td>0.464</td><td>0.4400.438</td><td>0.358</td><td>0.390</td><td>0.441</td><td>0.436</td><td>0.681</td><td>0.539</td><td>0.666</td><td>0.566</td><td>0.750</td><td>0.591</td><td>0.844</td><td>0.591</td><td>1.087</td><td>0.766</td><td>1.097</td><td>0.756</td><td>1.150</td><td>0.788</td><td>1.287</td><td>0.839</td></tr><tr><td>336</td><td>0.450</td><td>0.424</td><td>0.4850.459</td><td>0.402</td><td>0.416</td><td>0.499</td><td>0.467</td><td>0.786</td><td>0.597</td><td>0.807</td><td>0.628</td><td>0.851</td><td>0.659</td><td>0.870</td><td>0.603</td><td>1.138</td><td>0.787</td><td>1.147</td><td>0.775</td><td>1.198</td><td>0.809</td><td>1.288</td><td>0.842</td></tr><tr><td>720</td><td>0.483</td><td>0.471</td><td>0.5770.499</td><td>0.511</td><td>0.489</td><td>0.767</td><td>0.587</td><td>0.796</td><td>0.593</td><td>0.822</td><td>0.633</td><td>0.857</td><td>0.655</td><td>0.893</td><td>0.611</td><td>1.245</td><td>0.831</td><td>1.200</td><td>0.799</td><td>1.175</td><td>0.794</td><td>1.247</td><td>0.828</td></tr><tr><td>平均值</td><td>$\underline{0.425}$</td><td>$\underline{0.434}$</td><td>0.4720.450</td><td>0.400</td><td>0.417</td><td>0.526</td><td>0.476</td><td>0.717</td><td>0.561</td><td>0.730</td><td>0.592</td><td>0.796</td><td>0.620</td><td>0.857</td><td>0.598</td><td>1.125</td><td>0.782</td><td>1.123</td><td>0.765</td><td>1.163</td><td>0.791</td><td>1.264</td><td>0.826</td></tr><tr><td rowspan="5">${ETT}{m2}$</td><td>96</td><td>0.174</td><td>0.261</td><td>0.1990.280</td><td>0.236</td><td>0.326</td><td>0.206</td><td>0.288</td><td>0.220</td><td>0.299</td><td>0.229</td><td>0.320</td><td>0.232</td><td>0.322</td><td>0.238</td><td>0.316</td><td>0.404</td><td>0.485</td><td>1.108</td><td>0.772</td><td>3.599</td><td>1.478</td><td>3.883</td><td>1.545</td></tr><tr><td>192</td><td>0.215</td><td>0.287</td><td>0.2560.316</td><td>0.306</td><td>0.373</td><td>0.264</td><td>0.324</td><td>0.311</td><td>0.361</td><td>0.394</td><td>0.361</td><td>0.291</td><td>0.357</td><td>0.298</td><td>0.349</td><td>0.479</td><td>0.521</td><td>1.317</td><td>0.850</td><td>3.578</td><td>1.475</td><td>3.553</td><td>1.484</td></tr><tr><td>336</td><td>0.273</td><td>0.330</td><td>0.3180.353</td><td>0.380</td><td>0.423</td><td>0.334</td><td>0.367</td><td>0.338</td><td>0.366</td><td>0.378</td><td>0.427</td><td>0.478</td><td>0.517</td><td>0.353</td><td>0.380</td><td>0.552</td><td>0.555</td><td>1.415</td><td>0.879</td><td>3.561</td><td>1.473</td><td>3.446</td><td>1.460</td></tr><tr><td>720</td><td>0.433</td><td>0.412</td><td>0.4600.436</td><td>0.674</td><td>0.583</td><td>0.454</td><td>0.432</td><td>0.509</td><td>0.465</td><td>0.523</td><td>0.510</td><td>0.553</td><td>0.538</td><td>0.475</td><td>0.445</td><td>0.701</td><td>0.627</td><td>1.822</td><td>0.984</td><td>3.896</td><td>1.533</td><td>3.445</td><td>1.460</td></tr><tr><td>平均值</td><td>0.274</td><td>0.323</td><td>$\underline{0.308}$$\underline{0.346}$</td><td>0.399</td><td>0.426</td><td>0.314</td><td>0.352</td><td>0.344</td><td>0.372</td><td>0.381</td><td>0.404</td><td>0.388</td><td>0.433</td><td>0.341</td><td>0.372</td><td>0.534</td><td>0.547</td><td>1.415</td><td>0.871</td><td>3.658</td><td>1.489</td><td>3.581</td><td>1.487</td></tr><tr><td rowspan="5">${Weather}$</td><td>96</td><td>0.172</td><td>0.263</td><td>0.1750.230</td><td>0.184</td><td>0.242</td><td>0.171</td><td>0.224</td><td>0.207</td><td>0.253</td><td>0.229</td><td>0.309</td><td>0.227</td><td>0.299</td><td>0.215</td><td>0.252</td><td>0.218</td><td>0.295</td><td>0.230</td><td>0.285</td><td>0.497</td><td>0.497</td><td>0.406</td><td>0.435</td></tr><tr><td>192</td><td>0.224</td><td>0.271</td><td>0.2270.276</td><td>0.228</td><td>0.283</td><td>0.230</td><td>0.277</td><td>0.272</td><td>0.307</td><td>0.265</td><td>0.317</td><td>0.278</td><td>0.333</td><td>0.290</td><td>0.307</td><td>0.294</td><td>0.331</td><td>0.274</td><td>0.323</td><td>0.620</td><td>0.545</td><td>0.446</td><td>0.450</td></tr><tr><td>336</td><td>0.282</td><td>0.321</td><td>0.2860.322</td><td>0.279</td><td>0.322</td><td>0.294</td><td>0.326</td><td>0.313</td><td>0.328</td><td>0.353</td><td>0.392</td><td>0.351</td><td>0.393</td><td>0.353</td><td>0.348</td><td>0.359</td><td>0.398</td><td>0.318</td><td>0.355</td><td>0.649</td><td>0.547</td><td>0.465</td><td>0.459</td></tr><tr><td>720</td><td>0.366</td><td>0.381</td><td>0.3660.379</td><td>0.364</td><td>0.388</td><td>0.384</td><td>0.387</td><td>0.400</td><td>0.385</td><td>0.391</td><td>0.394</td><td>0.387</td><td>0.389</td><td>0.452</td><td>0.407</td><td>0.461</td><td>0.461</td><td>0.401</td><td>0.418</td><td>0.570</td><td>0.522</td><td>0.471</td><td>0.468</td></tr><tr><td>平均值</td><td>0.260</td><td>0.309</td><td>$\underline{0.263}$0.301</td><td>$\underline{0.263}$</td><td>0.308</td><td>0.269</td><td>$\underline{0.303}$</td><td>0.298</td><td>0.318</td><td>0.309</td><td>0.353</td><td>0.310</td><td>0.353</td><td>0.327</td><td>0.328</td><td>0.333</td><td>0.371</td><td>0.305</td><td>0.345</td><td>0.584</td><td>0.527</td><td>0.447</td><td>0.453</td></tr><tr><td rowspan="5">电力</td><td>96</td><td>0.147</td><td>0.242</td><td>0.1430.241</td><td>0.150</td><td>0.251</td><td>$\underline{0.145}$</td><td>0.244</td><td>0.315</td><td>0.389</td><td>0.235</td><td>0.322</td><td>0.297</td><td>0.367</td><td>0.484</td><td>0.518</td><td>0.697</td><td>0.638</td><td>0.639</td><td>0.609</td><td>1.265</td><td>0.919</td><td>1.414</td><td>0.855</td></tr><tr><td>192</td><td>0.158</td><td>0.241</td><td>0.1590.255</td><td>0.163</td><td>0.263</td><td>0.163</td><td>0.260</td><td>0.318</td><td>0.396</td><td>0.247</td><td>0.341</td><td>0.308</td><td>0.375</td><td>0.501</td><td>0.531</td><td>0.718</td><td>0.648</td><td>0.772</td><td>0.678</td><td>1.298</td><td>0.939</td><td>1.240</td><td>0.919</td></tr><tr><td>336</td><td>0.178</td><td>0.277</td><td>0.1790.274</td><td>0.175</td><td>0.278</td><td>0.183</td><td>0.281</td><td>0.340</td><td>0.415</td><td>0.267</td><td>0.356</td><td>0.354</td><td>0.411</td><td>0.574</td><td>0.578</td><td>0.758</td><td>0.667</td><td>0.901</td><td>0.745</td><td>1.302</td><td>0.942</td><td>1.253</td><td>0.921</td></tr><tr><td>720</td><td>0.224</td><td>0.312</td><td>0.2330.323</td><td>0.219</td><td>0.311</td><td>0.233</td><td>0.323</td><td>0.635</td><td>0.613</td><td>0.318</td><td>0.394</td><td>0.426</td><td>0.466</td><td>0.952</td><td>0.786</td><td>1.028</td><td>0.788</td><td>1.200</td><td>0.871</td><td>1.259</td><td>0.919</td><td>1.249</td><td>0.921</td></tr><tr><td>平均值</td><td>0.179</td><td>0.268</td><td>0.178$\underline{0.273}$</td><td>0.176</td><td>0.275</td><td>0.181</td><td>0.277</td><td>0.402</td><td>0.453</td><td>0.266</td><td>0.353</td><td>0.346</td><td>0.404</td><td>0.627</td><td>0.603</td><td>0.800</td><td>0.685</td><td>0.878</td><td>0.725</td><td>1.281</td><td>0.929</td><td>1.289</td><td>0.904</td></tr><tr><td rowspan="5"></td><td>96</td><td>0.414</td><td>0.291</td><td>0.4190.298</td><td>0.427</td><td>0.304</td><td>0.404</td><td>0.286</td><td>0.854</td><td>0.492</td><td>0.670</td><td>0.421</td><td>0.795</td><td>0.481</td><td>1.468</td><td>0.821</td><td>1.643</td><td>0.855</td><td>1.157</td><td>0.636</td><td>1.557</td><td>0.821</td><td>1.586</td><td>0.841</td></tr><tr><td>192</td><td>0.419</td><td>0.291</td><td>0.4340.305</td><td>0.447</td><td>0.315</td><td>0.412</td><td>0.294</td><td>0.894</td><td>0.517</td><td>0.653</td><td>0.405</td><td>0.837</td><td>0.503</td><td>1.509</td><td>0.838</td><td>1.856</td><td>0.928</td><td>1.688</td><td>0.848</td><td>1.596</td><td>0.834</td><td>1.602</td><td>0.844</td></tr><tr><td>336</td><td>0.437</td><td>0.314</td><td>0.4490.313</td><td>0.478</td><td>0.333</td><td>0.439</td><td>0.310</td><td>0.853</td><td>0.471</td><td>0.707</td><td>0.445</td><td>0.867</td><td>0.523</td><td>1.602</td><td>0.860</td><td>2.080</td><td>0.999</td><td>1.826</td><td>0.903</td><td>1.621</td><td>0.841</td><td>1.668</td><td>0.868</td></tr><tr><td>720</td><td>-</td><td>-</td><td>--</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>平均值</td><td>$\underline{0.423}$</td><td>$\underline{0.298}$</td><td>0.4340.305</td><td>0.450</td><td>0.317</td><td>0.418</td><td>0.296</td><td>0.867</td><td>0.493</td><td>0.676</td><td>0.423</td><td>0.833</td><td>0.502</td><td>1.526</td><td>0.839</td><td>1.859</td><td>0.927</td><td>1.557</td><td>0.795</td><td>1.591</td><td>0.832</td><td>1.618</td><td>0.851</td></tr><tr><td colspan="2">${1}^{\text{st }}$ 计数</td><td colspan="2">21</td><td>6</td><td></td><td>7</td><td></td><td>6</td><td></td><td>0</td><td></td><td>1</td><td></td><td>0│</td><td></td><td>0</td><td>│</td><td>0</td><td>0</td><td></td><td></td><td>0</td><td>0</td><td></td></tr></tbody></table>

<!-- Media -->

forecasting tasks in terms of the total number of trainable parameters, GPU memory overhead, and training speed. Quantitatively,there is an ${71.2}\%$ trainable parameter reduction on average over four scenarios, leading to 23.1% smaller memory consumption and 25.3% faster training speed.

从可训练参数总数、GPU内存开销和训练速度等方面来看预测任务。从数量上看，在四种场景下平均可训练参数减少了${71.2}\%$，这使得内存消耗降低了23.1%，训练速度提高了25.3%。

## H ERROR BARS

## H 误差线

All experiments have been conducted three times, and we present the standard deviations of our model and the runner-up model here. The comparisons between our method and the second-best method, PatchTST (Nie et al., 2023), on long-term forecasting tasks, are delineated in Tab. 19 In this table, the average MSE and MAE have been reported across four ETT datasets, complete with standard deviations. Furthermore, Tab. 20 contrasts the effectiveness of our method with that of the second-best method, N-HiTS (Challu et al. 2023a), employing varying M4 datasets for the comparison.

所有实验均进行了三次，我们在此展示我们的模型和亚军模型的标准差。在长期预测任务中，我们的方法与次优方法PatchTST（聂等人，2023年）的比较情况在表19中进行了描述。在该表中，报告了四个ETT数据集上的平均均方误差（MSE）和平均绝对误差（MAE），并给出了标准差。此外，表20对比了我们的方法与次优方法N - HiTS（查卢等人，2023a）的有效性，采用了不同的M4数据集进行比较。

## I VISUALIZATION

## I 可视化

In this part, we visualize the forecasting results of TIME-LLM compared with the state-of-the-art and representative methods (e.g., GPT4TS (Zhou et al., 2023a), PatchTST (Nie et al., 2023), and Autoformer (Wu et al. 2021)) in various scenarios to demonstrate the superior performance of TIME-LLM.

在这部分，我们将TIME - LLM的预测结果与最先进的代表性方法（例如，GPT4TS（Zhou等人，2023a）、PatchTST（Nie等人，2023）和Autoformer（Wu等人，2021））在各种场景下进行可视化对比，以展示TIME - LLM的卓越性能。

In Fig. 7 and Fig. 8, the long-term (input-96-predict-96) and short-term (input-36-predict-36) forecasts of various approaches are compared with the ground truth. Here, TIME-LLM showcases forecasting accuracy that is notably superior compared to GPT4TS, PatchTST, and a classical Transformer-based method, Autoformer.

在图7和图8中，将各种方法的长期（输入96步预测96步）和短期（输入36步预测36步）预测结果与真实值进行了比较。在这里，与GPT4TS、PatchTST和基于Transformer的经典方法Autoformer相比，TIME - LLM的预测准确性显著更优。

We also offer visual comparisons of the forecasting results in both few-shot and zero-shot scenarios, as depicted in Fig. 9 and Fig. 10. We adhere to the long-term (input-96-predict-96) forecasting setup in both cases. TIME-LLM exhibits remarkable superiority in forecasting with limited data-a fact that becomes particularly salient when compared to GPT4TS.

我们还对少样本和零样本场景下的预测结果进行了可视化对比，如图9和图10所示。在这两种情况下，我们都采用长期（输入96步预测96步）预测设置。TIME - LLM在数据有限的情况下进行预测时表现出显著的优越性，与GPT4TS相比，这一事实尤为突出。

<!-- Media -->

Table 19: Standard deviations of our approach and the second-best method (PatchTST) on all time series datasets for long-term forecasting.

表19：我们的方法和次优方法（PatchTST）在所有用于长期预测的时间序列数据集上的标准差。

<table><tr><td>Model</td><td colspan="2">TIME-LLM</td><td colspan="2">PatchTST (2023)</td></tr><tr><td>Dataset</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>ETTh1</td><td>${0.408} \pm  {0.011}$</td><td>${0.423} \pm  {0.012}$</td><td>${0.413} \pm  {0.001}$</td><td>${0.430} \pm  {0.002}$</td></tr><tr><td>ETTh2</td><td>${0.334} \pm  {0.005}$</td><td>${0.383} \pm  {0.009}$</td><td>${0.330} \pm  {0.002}$</td><td>${0.379} \pm  {0.007}$</td></tr><tr><td>ETTm1</td><td>${0.329} \pm  {0.006}$</td><td>${0.372} \pm  {0.007}$</td><td>${0.351} \pm  {0.006}$</td><td>${0.380} \pm  {0.002}$</td></tr><tr><td>ETTm2</td><td>${0.251} \pm  {0.002}$</td><td>${0.313} \pm  {0.003}$</td><td>${0.255} \pm  {0.003}$</td><td>${0.315} \pm  {0.002}$</td></tr><tr><td>Weather</td><td>${0.225} \pm  {0.009}$</td><td>${0.257} \pm  {0.008}$</td><td>${0.225} \pm  {0.001}$</td><td>${0.264} \pm  {0.001}$</td></tr><tr><td>Electricity</td><td>${0.158} \pm  {0.004}$</td><td>${0.252} \pm  {0.007}$</td><td>${0.161} \pm  {0.001}$</td><td>${0.252} \pm  {0.001}$</td></tr><tr><td>Traffic</td><td>${0.388} \pm  {0.001}$</td><td>${0.264} \pm  {0.006}$</td><td>${0.390} \pm  {0.003}$</td><td>${0.263} \pm  {0.003}$</td></tr><tr><td>ILI</td><td>${1.435} \pm  {0.011}$</td><td>${0.801} \pm  {0.008}$</td><td>${1.443} \pm  {0.012}$</td><td>${0.797} \pm  {0.002}$</td></tr></table>

<table><tbody><tr><td>模型</td><td colspan="2">TIME大语言模型（TIME-LLM）</td><td colspan="2">PatchTST模型（PatchTST (2023)）</td></tr><tr><td>数据集</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td>ETTh1</td><td>${0.408} \pm  {0.011}$</td><td>${0.423} \pm  {0.012}$</td><td>${0.413} \pm  {0.001}$</td><td>${0.430} \pm  {0.002}$</td></tr><tr><td>ETTh2</td><td>${0.334} \pm  {0.005}$</td><td>${0.383} \pm  {0.009}$</td><td>${0.330} \pm  {0.002}$</td><td>${0.379} \pm  {0.007}$</td></tr><tr><td>ETTm1</td><td>${0.329} \pm  {0.006}$</td><td>${0.372} \pm  {0.007}$</td><td>${0.351} \pm  {0.006}$</td><td>${0.380} \pm  {0.002}$</td></tr><tr><td>ETTm2</td><td>${0.251} \pm  {0.002}$</td><td>${0.313} \pm  {0.003}$</td><td>${0.255} \pm  {0.003}$</td><td>${0.315} \pm  {0.002}$</td></tr><tr><td>气象</td><td>${0.225} \pm  {0.009}$</td><td>${0.257} \pm  {0.008}$</td><td>${0.225} \pm  {0.001}$</td><td>${0.264} \pm  {0.001}$</td></tr><tr><td>电力</td><td>${0.158} \pm  {0.004}$</td><td>${0.252} \pm  {0.007}$</td><td>${0.161} \pm  {0.001}$</td><td>${0.252} \pm  {0.001}$</td></tr><tr><td>交通流量；交通</td><td>${0.388} \pm  {0.001}$</td><td>${0.264} \pm  {0.006}$</td><td>${0.390} \pm  {0.003}$</td><td>${0.263} \pm  {0.003}$</td></tr><tr><td>流感样疾病（Influenza-like Illness）</td><td>${1.435} \pm  {0.011}$</td><td>${0.801} \pm  {0.008}$</td><td>${1.443} \pm  {0.012}$</td><td>${0.797} \pm  {0.002}$</td></tr></tbody></table>

Table 18: Efficiency comparison between model reprogramming and parameter-efficient fine-tuning (PEFT) with QLoRA (Dettmers et al. 2023) on ETTh1 dataset in forecasting two different steps ahead.

表18：在ETTh1数据集上对未来两个不同步长进行预测时，模型重新编程与使用QLoRA（德特默斯等人，2023年）的参数高效微调（PEFT）的效率比较。

<table><tr><td/><td>Length</td><td colspan="3">ETTh1-96</td><td colspan="3">ETTh1-336</td></tr><tr><td colspan="2">Metric</td><td>Trainable Param. (M)</td><td>Mem. (MiB)</td><td>Speed(s/iter)</td><td>Trainable Param. (M)</td><td>Mem. (MiB)</td><td>Speed(s/iter)</td></tr><tr><td rowspan="2">Llama (8)</td><td>QLoRA</td><td>12.60</td><td>14767</td><td>0.237</td><td>12.69</td><td>15982</td><td>0.335</td></tr><tr><td>Reprogram</td><td>5.62</td><td>11370</td><td>0.184</td><td>5.71</td><td>13188</td><td>0.203</td></tr><tr><td rowspan="2">Llama (32)</td><td>QLoRA</td><td>50.29</td><td>45226</td><td>0.697</td><td>50.37</td><td>49374</td><td>0.732</td></tr><tr><td>Reprogram</td><td>6.39</td><td>32136</td><td>0.517</td><td>6.48</td><td>37988</td><td>0.632</td></tr></table>

<table><tbody><tr><td></td><td>长度</td><td colspan="3">ETTh1 - 96</td><td colspan="3">ETTh1 - 336</td></tr><tr><td colspan="2">指标</td><td>可训练参数（百万）</td><td>内存（兆字节）</td><td>速度（秒/迭代）</td><td>可训练参数（百万）</td><td>内存（兆字节）</td><td>速度（秒/迭代）</td></tr><tr><td rowspan="2">羊驼模型（8）</td><td>量化低秩自适应（QLoRA）</td><td>12.60</td><td>14767</td><td>0.237</td><td>12.69</td><td>15982</td><td>0.335</td></tr><tr><td>重新编程</td><td>5.62</td><td>11370</td><td>0.184</td><td>5.71</td><td>13188</td><td>0.203</td></tr><tr><td rowspan="2">羊驼模型（32）</td><td>量化低秩自适应（QLoRA）</td><td>50.29</td><td>45226</td><td>0.697</td><td>50.37</td><td>49374</td><td>0.732</td></tr><tr><td>重新编程</td><td>6.39</td><td>32136</td><td>0.517</td><td>6.48</td><td>37988</td><td>0.632</td></tr></tbody></table>

Table 20: Standard deviations of our TIME-LLM and the second-best method (N-HiTS) on M4 datasets for short-term forecasting.

表20：我们的TIME-LLM和次优方法（N-HiTS）在M4数据集上进行短期预测的标准差。

<table><tr><td>Model</td><td colspan="3">TIME-LLM</td><td colspan="3">N-HiTS (2023a)</td></tr><tr><td>Dataset</td><td>SMAPE</td><td>MAPE</td><td>OWA</td><td>SMAPE</td><td>MAPE</td><td>OWA</td></tr><tr><td>Yearly</td><td>${13.419} \pm  {0.117}$</td><td>${3.005} \pm  {0.011}$</td><td>${0.789} \pm  {0.003}$</td><td>${13.422} \pm  {0.009}$</td><td>${3.056} \pm  {0.017}$</td><td/></tr><tr><td>Quarterly</td><td>${10.110} \pm  {0.107}$</td><td>${1.178} \pm  {0.009}$</td><td>${0.889} \pm  {0.007}$</td><td>${10.185} \pm  {0.107}$</td><td>${1.180} \pm  {0.007}$</td><td>${0.893} \pm  {0.001}$</td></tr><tr><td>Monthly</td><td>${12.980} \pm  {0.102}$</td><td>${0.963} \pm  {0.005}$</td><td>${0.903} \pm  {0.001}$</td><td>${13.059} \pm  {0.101}$</td><td>${1.013} \pm  {0.007}$</td><td>${0.929} \pm  {0.005}$</td></tr><tr><td>Others</td><td>${4.795} \pm  {0.117}$</td><td>${3.178} \pm  {0.012}$</td><td>${1.006} \pm  {0.009}$</td><td>${4.711} \pm  {0.117}$</td><td>${3.054} \pm  {0.011}$</td><td>${0.997} \pm  {0.012}$</td></tr><tr><td>Averaged</td><td>${11.983} \pm  {0.011}$</td><td>${1.595} \pm  {0.021}$</td><td>${0.859} \pm  {0.002}$</td><td>${12.035} \pm  {0.111}$</td><td>${1.625} \pm  {0.012}$</td><td>${0.869} \pm  {0.005}$</td></tr></table>

<table><tbody><tr><td>模型</td><td colspan="3">TIME-大语言模型（TIME-LLM）</td><td colspan="3">N-HiTS模型（2023a）</td></tr><tr><td>数据集</td><td>对称平均绝对百分比误差（SMAPE）</td><td>平均绝对百分比误差（MAPE）</td><td>OWA（办公软件网页版）</td><td>对称平均绝对百分比误差（SMAPE）</td><td>平均绝对百分比误差（MAPE）</td><td>OWA（办公软件网页版）</td></tr><tr><td>每年</td><td>${13.419} \pm  {0.117}$</td><td>${3.005} \pm  {0.011}$</td><td>${0.789} \pm  {0.003}$</td><td>${13.422} \pm  {0.009}$</td><td>${3.056} \pm  {0.017}$</td><td></td></tr><tr><td>每季度</td><td>${10.110} \pm  {0.107}$</td><td>${1.178} \pm  {0.009}$</td><td>${0.889} \pm  {0.007}$</td><td>${10.185} \pm  {0.107}$</td><td>${1.180} \pm  {0.007}$</td><td>${0.893} \pm  {0.001}$</td></tr><tr><td>每月</td><td>${12.980} \pm  {0.102}$</td><td>${0.963} \pm  {0.005}$</td><td>${0.903} \pm  {0.001}$</td><td>${13.059} \pm  {0.101}$</td><td>${1.013} \pm  {0.007}$</td><td>${0.929} \pm  {0.005}$</td></tr><tr><td>其他</td><td>${4.795} \pm  {0.117}$</td><td>${3.178} \pm  {0.012}$</td><td>${1.006} \pm  {0.009}$</td><td>${4.711} \pm  {0.117}$</td><td>${3.054} \pm  {0.011}$</td><td>${0.997} \pm  {0.012}$</td></tr><tr><td>平均的</td><td>${11.983} \pm  {0.011}$</td><td>${1.595} \pm  {0.021}$</td><td>${0.859} \pm  {0.002}$</td><td>${12.035} \pm  {0.111}$</td><td>${1.625} \pm  {0.012}$</td><td>${0.869} \pm  {0.005}$</td></tr></tbody></table>

<img src="https://cdn.noedgeai.com/01957f5d-9de5-7380-ae8c-08021ee888f7_23.jpg?x=310&y=234&w=1175&h=286&r=0"/>

Figure 7: Long-term forecasting cases from ETTh1 by different models under the input-96-predict- 96 settings. Blue lines are the ground truths and orange lines are the model predictions.

图7：在输入96步预测96步的设置下，不同模型对ETTh1数据集的长期预测情况。蓝色线条为真实值，橙色线条为模型预测值。

<img src="https://cdn.noedgeai.com/01957f5d-9de5-7380-ae8c-08021ee888f7_23.jpg?x=311&y=641&w=1175&h=283&r=0"/>

Figure 8: Short-term forecasting from the M4 dataset by different models under the input-36-predict- 18 settings.

图8：在输入36步预测18步的设置下，不同模型对M4数据集的短期预测情况。

<img src="https://cdn.noedgeai.com/01957f5d-9de5-7380-ae8c-08021ee888f7_23.jpg?x=308&y=1048&w=1177&h=273&r=0"/>

Figure 9: Few-shot forecasting cases from ETTm1 by different models under the input-96-predict- 96 settings. Blue lines are the ground truths and orange lines are the model predictions.

图9：在输入96步预测96步的设置下，不同模型对ETTm1数据集的少样本预测情况。蓝色线条为真实值，橙色线条为模型预测值。

<!-- figureText: (a)Time-LLM -->

<img src="https://cdn.noedgeai.com/01957f5d-9de5-7380-ae8c-08021ee888f7_23.jpg?x=310&y=1444&w=1175&h=286&r=0"/>

Figure 10: Zero-shot forecasting cases from ETTh1→ETTh2 by different models under the input- 96-predict-96 settings. Blue lines are the ground truths and orange lines are the model predictions.

图10：在输入96步预测96步的设置下，不同模型从ETTh1到ETTh2的零样本预测情况。蓝色线条为真实值，橙色线条为模型预测值。

Table 17: Full ablations on ETTh1 and ETTm1 in predicting 96 and 192 steps ahead (MSE reported).

表17：在ETTh1和ETTm1数据集上对未来96步和192步预测的全消融实验（报告均方误差）。

<table><tr><td rowspan="2">Variant</td><td colspan="4">Long-term Forecasting</td><td colspan="4">Few-shot Forecasting</td></tr><tr><td>ETTh1-96</td><td>ETTh1-192</td><td>ETTm1-96</td><td>ETThm1-192</td><td>ETTh1-96</td><td>ETTh1-192</td><td>ETTm1-96</td><td>ETThm1-192</td></tr><tr><td>A.1 Llama (Default; 32)</td><td>0.362</td><td>0.398</td><td>0.272</td><td>0.310</td><td>0.448</td><td>0.484</td><td>0.346</td><td>0.373</td></tr><tr><td>A.2 Llama (8)</td><td>0.389</td><td>0.412</td><td>0.297</td><td>0.329</td><td>0.567</td><td>0.632</td><td>0.451</td><td>0.490</td></tr><tr><td>A.3 GPT-2 (12)</td><td>0.385</td><td>0.419</td><td>0.306</td><td>0.332</td><td>0.548</td><td>0.617</td><td>0.447</td><td>0.509</td></tr><tr><td>A.4 GPT-2 (6)</td><td>0.394</td><td>0.427</td><td>0.311</td><td>0.342</td><td>0.571</td><td>0.640</td><td>0.468</td><td>0.512</td></tr><tr><td>A. 5 Llama (QLoRA; 32)</td><td>0.391</td><td>0.420</td><td>0.310</td><td>0.338</td><td>0.543</td><td>0.611</td><td>0.578</td><td>0.618</td></tr><tr><td>B. 1 w/o Patch Reprogramming</td><td>0.410</td><td>0.412</td><td>0.310</td><td>0.342</td><td>0.498</td><td>0.570</td><td>0.445</td><td>0.487</td></tr><tr><td>B. 2 w/o Prompt-as-Prefix</td><td>0.398</td><td>0.423</td><td>0.298</td><td>0.339</td><td>0.521</td><td>0.617</td><td>0.432</td><td>0.481</td></tr><tr><td>C.1 w/o Dataset Context</td><td>0.402</td><td>0.417</td><td>0.298</td><td>0.331</td><td>0.491</td><td>0.538</td><td>0.392</td><td>0.447</td></tr><tr><td>C. $2\mathrm{\;w}/\mathrm{o}$ Task Instruction</td><td>0.388</td><td>0.420</td><td>0.285</td><td>0.327</td><td>0.476</td><td>0.529</td><td>0.387</td><td>0.439</td></tr><tr><td>C. $3\mathrm{\;w}/\mathrm{o}$ Statistical Context</td><td>0.391</td><td>0.419</td><td>0.279</td><td>0.347</td><td>0.483</td><td>0.547</td><td>0.421</td><td>0.461</td></tr></table>

<table><tbody><tr><td rowspan="2">变体</td><td colspan="4">长期预测</td><td colspan="4">小样本预测</td></tr><tr><td>ETTh1-96</td><td>ETTh1-192</td><td>ETTm1-96</td><td>ETThm1 - 192</td><td>ETTh1-96</td><td>ETTh1-192</td><td>ETTm1-96</td><td>ETThm1 - 192</td></tr><tr><td>A.1 羊驼模型（默认；32）（Llama）</td><td>0.362</td><td>0.398</td><td>0.272</td><td>0.310</td><td>0.448</td><td>0.484</td><td>0.346</td><td>0.373</td></tr><tr><td>A.2 羊驼模型（8）（Llama）</td><td>0.389</td><td>0.412</td><td>0.297</td><td>0.329</td><td>0.567</td><td>0.632</td><td>0.451</td><td>0.490</td></tr><tr><td>A.3 GPT - 2模型（12）</td><td>0.385</td><td>0.419</td><td>0.306</td><td>0.332</td><td>0.548</td><td>0.617</td><td>0.447</td><td>0.509</td></tr><tr><td>A.4 GPT - 2模型（6）</td><td>0.394</td><td>0.427</td><td>0.311</td><td>0.342</td><td>0.571</td><td>0.640</td><td>0.468</td><td>0.512</td></tr><tr><td>A. 5 羊驼模型（QLoRA；32）（Llama）</td><td>0.391</td><td>0.420</td><td>0.310</td><td>0.338</td><td>0.543</td><td>0.611</td><td>0.578</td><td>0.618</td></tr><tr><td>B. 1 无补丁重编程</td><td>0.410</td><td>0.412</td><td>0.310</td><td>0.342</td><td>0.498</td><td>0.570</td><td>0.445</td><td>0.487</td></tr><tr><td>B. 2 无提示前缀</td><td>0.398</td><td>0.423</td><td>0.298</td><td>0.339</td><td>0.521</td><td>0.617</td><td>0.432</td><td>0.481</td></tr><tr><td>C.1 无数据集上下文</td><td>0.402</td><td>0.417</td><td>0.298</td><td>0.331</td><td>0.491</td><td>0.538</td><td>0.392</td><td>0.447</td></tr><tr><td>C. $2\mathrm{\;w}/\mathrm{o}$ 任务指令</td><td>0.388</td><td>0.420</td><td>0.285</td><td>0.327</td><td>0.476</td><td>0.529</td><td>0.387</td><td>0.439</td></tr><tr><td>C. $3\mathrm{\;w}/\mathrm{o}$ 统计上下文</td><td>0.391</td><td>0.419</td><td>0.279</td><td>0.347</td><td>0.483</td><td>0.547</td><td>0.421</td><td>0.461</td></tr></tbody></table>

Table 16: Full zero-shot learning results on ETT datasets. A lower value indicates better performance. Red: the best, Blue: the second best.

表16：ETT数据集上的全零样本学习结果。数值越低表示性能越好。红色：最佳，蓝色：次佳。

<table><tr><td colspan="2">Methods</td><td colspan="2">TIME-LLM</td><td colspan="2">LLMTime</td><td colspan="2">GPT4TS</td><td colspan="2">DLinear</td><td colspan="2">PatchTST</td><td colspan="2">TimesNet</td><td colspan="2">Autoformer</td></tr><tr><td>Metric</td><td/><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="5">${ETTh1} \rightarrow  {ETTh2}$</td><td>96</td><td>0.279</td><td>0.337</td><td>0.510</td><td>0.576</td><td>0.335</td><td>0.374</td><td>0.347</td><td>0.400</td><td>0.304</td><td>0.350</td><td>0.358</td><td>0.387</td><td>0.469</td><td>0.486</td></tr><tr><td>192</td><td>0.351</td><td>0.374</td><td>0.523</td><td>0.586</td><td>0.412</td><td>0.417</td><td>0.447</td><td>0.460</td><td>0.386</td><td>0.400</td><td>0.427</td><td>0.429</td><td>0.634</td><td>0.567</td></tr><tr><td>336</td><td>0.388</td><td>0.415</td><td>0.640</td><td>0.637</td><td>0.441</td><td>0.444</td><td>0.515</td><td>0.505</td><td>0.414</td><td/><td>0.449</td><td>0.451</td><td>0.655</td><td>0.588</td></tr><tr><td>720</td><td>0.391</td><td>0.420</td><td>2.296</td><td>1.034</td><td>0.438</td><td>0.452</td><td>0.665</td><td>0.589</td><td>0.419</td><td>0.443</td><td>0.448</td><td>0.458</td><td>0.570</td><td>0.549</td></tr><tr><td>Avg</td><td>0.353</td><td>0.387</td><td>0.992</td><td>0.708</td><td>0.406</td><td>0.422</td><td>0.493</td><td>0.488</td><td>$\underline{0.380}$</td><td>0.405</td><td>0.421</td><td>0.431</td><td>0.582</td><td>0.548</td></tr><tr><td rowspan="5">${ETTh1} \rightarrow  {ETTm2}$</td><td>96</td><td>0.189</td><td>0.293</td><td>0.646</td><td>0.563</td><td>0.236</td><td>0.315</td><td>0.255</td><td>0.357</td><td>0.215</td><td>0.304</td><td>0.239</td><td>0.313</td><td>0.352</td><td>0.432</td></tr><tr><td>192</td><td>0.237</td><td>0.312</td><td>0.934</td><td>0.654</td><td>0.287</td><td>0.342</td><td>0.338</td><td>0.413</td><td>0.275</td><td>0.339</td><td>0.291</td><td>0.342</td><td>0.413</td><td>0.460</td></tr><tr><td>336</td><td>0.291</td><td>0.365</td><td>1.157</td><td>0.728</td><td>0.341</td><td>0.374</td><td>0.425</td><td>0.465</td><td>0.334</td><td>0.373</td><td>0.342</td><td>0.371</td><td>0.465</td><td>0.489</td></tr><tr><td>720</td><td>0.372</td><td>0.390</td><td>4.730</td><td>1.531</td><td>0.435</td><td>0.422</td><td>0.640</td><td>0.573</td><td>0.431</td><td>0.424</td><td>0.434</td><td>0.419</td><td>0.599</td><td>0.551</td></tr><tr><td>Avg</td><td>0.273</td><td>0.340</td><td>1.867</td><td>0.869</td><td>0.325</td><td>0.363</td><td>0.415</td><td>0.452</td><td>0.314</td><td>0.360</td><td>0.327</td><td>0.361</td><td>0.457</td><td>0.483</td></tr><tr><td rowspan="5">${ETTh2} \rightarrow  {ETTh1}$</td><td>96</td><td>0.450</td><td>0.452</td><td>1.130</td><td>0.777</td><td>0.732</td><td>0.577</td><td>0.689</td><td>0.555</td><td>0.485</td><td>0.465</td><td>0.848</td><td>0.601</td><td>0.693</td><td>0.569</td></tr><tr><td>192</td><td>0.465</td><td>0.461</td><td>1.242</td><td>0.820</td><td>0.758</td><td>0.559</td><td>0.707</td><td>0.568</td><td>0.565</td><td>0.509</td><td>0.860</td><td>0.610</td><td>0.760</td><td>0.601</td></tr><tr><td>336</td><td>0.501</td><td>0.482</td><td>1.328</td><td>0.864</td><td>0.759</td><td>0.578</td><td>0.710</td><td>0.577</td><td>0.581</td><td>0.515</td><td>0.867</td><td>0.626</td><td>0.781</td><td>0.619</td></tr><tr><td>720</td><td>0.501</td><td>0.502</td><td>4.145</td><td>1.461</td><td>0.781</td><td>0.597</td><td>0.704</td><td>0.596</td><td>0.628</td><td>0.561</td><td>0.887</td><td>0.648</td><td>0.796</td><td>0.644</td></tr><tr><td>Avg</td><td>0.479</td><td>0.474</td><td>1.961</td><td>0.981</td><td>0.757</td><td>0.578</td><td>0.703</td><td>0.574</td><td>$\underline{0.565}$</td><td>0.513</td><td>0.865</td><td>0.621</td><td>0.757</td><td>0.608</td></tr><tr><td rowspan="5">${ETTh2} \rightarrow  {ETTm2}$</td><td>96</td><td>0.174</td><td>0.276</td><td>0.646</td><td>0.563</td><td>0.253</td><td>0.329</td><td>0.240</td><td>0.336</td><td>0.226</td><td>0.309</td><td>0.248</td><td>0.324</td><td>0.263</td><td>0.352</td></tr><tr><td>192</td><td>0.233</td><td>0.315</td><td>0.934</td><td>0.654</td><td>0.293</td><td>0.346</td><td>0.295</td><td>0.369</td><td>0.289</td><td>0.345</td><td>0.296</td><td>0.352</td><td>0.326</td><td>0.389</td></tr><tr><td>336</td><td>0.291</td><td>0.337</td><td>1.157</td><td>0.728</td><td>0.347</td><td>0.376</td><td>0.345</td><td>0.397</td><td>0.348</td><td>0.379</td><td>0.353</td><td>0.383</td><td>0.387</td><td>0.426</td></tr><tr><td>720</td><td>0.392</td><td>0.417</td><td>4.730</td><td>1.531</td><td>0.446</td><td>0.429</td><td>0.432</td><td>0.442</td><td>0.439</td><td>0.427</td><td>0.471</td><td>0.446</td><td>0.487</td><td>0.478</td></tr><tr><td>Avg</td><td>0.272</td><td>0.341</td><td>1.867</td><td>0.869</td><td>0.335</td><td>0.370</td><td>0.328</td><td>0.386</td><td>0.325</td><td>$\underline{0.365}$</td><td>0.342</td><td>0.376</td><td>0.366</td><td>0.411</td></tr><tr><td rowspan="5">${ETTm1} \rightarrow  {ETTh2}$</td><td>96</td><td>0.321</td><td>0.369</td><td>0.510</td><td>0.576</td><td>0.353</td><td>0.392</td><td>0.365</td><td>0.415</td><td>0.354</td><td>0.385</td><td>0.377</td><td>0.407</td><td>0.435</td><td>0.470</td></tr><tr><td>192</td><td>0.389</td><td>0.410</td><td>0.523</td><td>0.586</td><td>0.443</td><td>0.437</td><td>0.454</td><td>0.462</td><td>0.447</td><td>0.434</td><td>0.471</td><td>0.453</td><td>0.495</td><td>0.489</td></tr><tr><td>336</td><td>0.408</td><td>0.433</td><td>0.640</td><td>0.637</td><td>0.469</td><td>0.461</td><td>0.496</td><td>0.494</td><td>0.481</td><td>0.463</td><td>0.472</td><td>0.484</td><td>0.470</td><td>0.472</td></tr><tr><td>720</td><td>0.406</td><td>0.436</td><td>2.296</td><td>1.034</td><td>0.466</td><td/><td>0.541</td><td>0.529</td><td>0.474</td><td>0.471</td><td>0.495</td><td>0.482</td><td>0.480</td><td>0.485</td></tr><tr><td>Avg</td><td>0.381</td><td>0.412</td><td>0.992</td><td>0.708</td><td>$\underline{0.433}$</td><td>0.439</td><td>0.464</td><td>0.475</td><td>0.439</td><td>$\underline{0.438}$</td><td>0.457</td><td>0.454</td><td>0.470</td><td>0.479</td></tr><tr><td rowspan="5">${ETTm1} \rightarrow  {ETTm2}$</td><td>96</td><td>0.169</td><td>0.257</td><td>0.646</td><td>0.563</td><td>0.217</td><td>0.294</td><td>0.221</td><td>0.314</td><td>0.195</td><td>0.271</td><td>0.222</td><td>0.295</td><td>0.385</td><td>0.457</td></tr><tr><td>192</td><td>0.227</td><td>0.318</td><td>0.934</td><td>0.654</td><td>0.277</td><td>0.327</td><td>0.286</td><td>0.359</td><td>0.258</td><td>0.311</td><td>0.288</td><td>0.337</td><td>0.433</td><td>0.469</td></tr><tr><td>336</td><td>0.290</td><td>0.338</td><td>1.157</td><td>0.728</td><td>0.331</td><td>0.360</td><td>0.357</td><td>0.406</td><td>0.317</td><td>0.348</td><td>0.341</td><td>0.367</td><td>0.476</td><td>0.477</td></tr><tr><td>720</td><td>0.375</td><td>0.367</td><td>4.730</td><td>1.531</td><td>0.429</td><td>0.413</td><td>0.476</td><td>0.476</td><td>0.416</td><td>0.404</td><td>0.436</td><td>0.418</td><td>0.582</td><td>0.535</td></tr><tr><td>Avg</td><td>0.268</td><td>0.320</td><td>1.867</td><td>0.869</td><td>0.313</td><td>0.348</td><td>0.335</td><td>0.389</td><td>0.296</td><td>0.334</td><td>0.322</td><td>0.354</td><td>0.469</td><td>0.484</td></tr><tr><td rowspan="5">${ETTm2} \rightarrow  {ETTh2}$</td><td>96</td><td>0.298</td><td>0.356</td><td>0.510</td><td>0.576</td><td>0.360</td><td>0.401</td><td>0.333</td><td>0.391</td><td>0.327</td><td>0.367</td><td>0.360</td><td>0.401</td><td>0.353</td><td>0.393</td></tr><tr><td>192</td><td>0.359</td><td>0.397</td><td>0.523</td><td>0.586</td><td>0.434</td><td>0.437</td><td>0.441</td><td>0.456</td><td>0.411</td><td>0.418</td><td>0.434</td><td>0.437</td><td>0.432</td><td>0.437</td></tr><tr><td>336</td><td>0.367</td><td>0.412</td><td>0.640</td><td>0.637</td><td>0.460</td><td>0.459</td><td>0.505</td><td>0.503</td><td>0.439</td><td>0.447</td><td>0.460</td><td>0.459</td><td>0.452</td><td>0.459</td></tr><tr><td>720</td><td>0.393</td><td>0.434</td><td>2.296</td><td>1.034</td><td>0.485</td><td>0.477</td><td>0.543</td><td>0.534</td><td>0.459</td><td>0.470</td><td>0.485</td><td>0.477</td><td>0.453</td><td>0.467</td></tr><tr><td>Avg</td><td>0.354</td><td>0.400</td><td>0.992</td><td>0.708</td><td>0.435</td><td>0.443</td><td>0.455</td><td>0.471</td><td>0.409</td><td>0.425</td><td>0.435</td><td>0.443</td><td>0.423</td><td>0.439</td></tr><tr><td rowspan="5">${ETTm2} \rightarrow  {ETTm1}$</td><td>96</td><td>0.359</td><td>0.397</td><td>1.179</td><td>0.781</td><td>0.747</td><td>0.558</td><td>0.570</td><td>0.490</td><td>0.491</td><td>0.437</td><td>0.747</td><td>0.558</td><td>0.735</td><td>0.576</td></tr><tr><td>192</td><td>0.390</td><td>0.420</td><td>1.327</td><td>0.846</td><td>0.781</td><td>0.560</td><td>0.590</td><td>0.506</td><td>0.530</td><td>0.470</td><td>0.781</td><td>0.560</td><td>0.753</td><td>0.586</td></tr><tr><td>336</td><td>0.421</td><td>0.445</td><td>1.478</td><td>0.902</td><td>0.778</td><td>0.578</td><td>0.706</td><td>0.567</td><td>0.565</td><td>0.497</td><td>0.778</td><td>0.578</td><td>0.750</td><td>0.593</td></tr><tr><td>720</td><td>0.487</td><td>0.488</td><td>3.749</td><td>1.408</td><td>0.769</td><td>0.573</td><td>0.731</td><td>0.584</td><td>0.686</td><td>0.565</td><td>0.769</td><td>0.573</td><td>0.782</td><td>0.609</td></tr><tr><td>Avg</td><td>0.414</td><td>0.438</td><td>1.933</td><td>0.984</td><td>0.769</td><td>0.567</td><td>0.649</td><td>0.537</td><td>0.568</td><td>0.492</td><td>0.769</td><td>0.567</td><td>0.755</td><td>0.591</td></tr></table>

<table><tbody><tr><td colspan="2">方法</td><td colspan="2">TIME大语言模型（TIME-LLM）</td><td colspan="2">大语言模型时间（LLMTime）</td><td colspan="2">GPT4时间序列（GPT4TS）</td><td colspan="2">深度线性模型（DLinear）</td><td colspan="2">补丁时间序列变换器（PatchTST）</td><td colspan="2">时间网络（TimesNet）</td><td colspan="2">自动变换器（Autoformer）</td></tr><tr><td>指标</td><td></td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td rowspan="5">${ETTh1} \rightarrow  {ETTh2}$</td><td>96</td><td>0.279</td><td>0.337</td><td>0.510</td><td>0.576</td><td>0.335</td><td>0.374</td><td>0.347</td><td>0.400</td><td>0.304</td><td>0.350</td><td>0.358</td><td>0.387</td><td>0.469</td><td>0.486</td></tr><tr><td>192</td><td>0.351</td><td>0.374</td><td>0.523</td><td>0.586</td><td>0.412</td><td>0.417</td><td>0.447</td><td>0.460</td><td>0.386</td><td>0.400</td><td>0.427</td><td>0.429</td><td>0.634</td><td>0.567</td></tr><tr><td>336</td><td>0.388</td><td>0.415</td><td>0.640</td><td>0.637</td><td>0.441</td><td>0.444</td><td>0.515</td><td>0.505</td><td>0.414</td><td></td><td>0.449</td><td>0.451</td><td>0.655</td><td>0.588</td></tr><tr><td>720</td><td>0.391</td><td>0.420</td><td>2.296</td><td>1.034</td><td>0.438</td><td>0.452</td><td>0.665</td><td>0.589</td><td>0.419</td><td>0.443</td><td>0.448</td><td>0.458</td><td>0.570</td><td>0.549</td></tr><tr><td>平均值（Avg）</td><td>0.353</td><td>0.387</td><td>0.992</td><td>0.708</td><td>0.406</td><td>0.422</td><td>0.493</td><td>0.488</td><td>$\underline{0.380}$</td><td>0.405</td><td>0.421</td><td>0.431</td><td>0.582</td><td>0.548</td></tr><tr><td rowspan="5">${ETTh1} \rightarrow  {ETTm2}$</td><td>96</td><td>0.189</td><td>0.293</td><td>0.646</td><td>0.563</td><td>0.236</td><td>0.315</td><td>0.255</td><td>0.357</td><td>0.215</td><td>0.304</td><td>0.239</td><td>0.313</td><td>0.352</td><td>0.432</td></tr><tr><td>192</td><td>0.237</td><td>0.312</td><td>0.934</td><td>0.654</td><td>0.287</td><td>0.342</td><td>0.338</td><td>0.413</td><td>0.275</td><td>0.339</td><td>0.291</td><td>0.342</td><td>0.413</td><td>0.460</td></tr><tr><td>336</td><td>0.291</td><td>0.365</td><td>1.157</td><td>0.728</td><td>0.341</td><td>0.374</td><td>0.425</td><td>0.465</td><td>0.334</td><td>0.373</td><td>0.342</td><td>0.371</td><td>0.465</td><td>0.489</td></tr><tr><td>720</td><td>0.372</td><td>0.390</td><td>4.730</td><td>1.531</td><td>0.435</td><td>0.422</td><td>0.640</td><td>0.573</td><td>0.431</td><td>0.424</td><td>0.434</td><td>0.419</td><td>0.599</td><td>0.551</td></tr><tr><td>平均值（Avg）</td><td>0.273</td><td>0.340</td><td>1.867</td><td>0.869</td><td>0.325</td><td>0.363</td><td>0.415</td><td>0.452</td><td>0.314</td><td>0.360</td><td>0.327</td><td>0.361</td><td>0.457</td><td>0.483</td></tr><tr><td rowspan="5">${ETTh2} \rightarrow  {ETTh1}$</td><td>96</td><td>0.450</td><td>0.452</td><td>1.130</td><td>0.777</td><td>0.732</td><td>0.577</td><td>0.689</td><td>0.555</td><td>0.485</td><td>0.465</td><td>0.848</td><td>0.601</td><td>0.693</td><td>0.569</td></tr><tr><td>192</td><td>0.465</td><td>0.461</td><td>1.242</td><td>0.820</td><td>0.758</td><td>0.559</td><td>0.707</td><td>0.568</td><td>0.565</td><td>0.509</td><td>0.860</td><td>0.610</td><td>0.760</td><td>0.601</td></tr><tr><td>336</td><td>0.501</td><td>0.482</td><td>1.328</td><td>0.864</td><td>0.759</td><td>0.578</td><td>0.710</td><td>0.577</td><td>0.581</td><td>0.515</td><td>0.867</td><td>0.626</td><td>0.781</td><td>0.619</td></tr><tr><td>720</td><td>0.501</td><td>0.502</td><td>4.145</td><td>1.461</td><td>0.781</td><td>0.597</td><td>0.704</td><td>0.596</td><td>0.628</td><td>0.561</td><td>0.887</td><td>0.648</td><td>0.796</td><td>0.644</td></tr><tr><td>平均值（Avg）</td><td>0.479</td><td>0.474</td><td>1.961</td><td>0.981</td><td>0.757</td><td>0.578</td><td>0.703</td><td>0.574</td><td>$\underline{0.565}$</td><td>0.513</td><td>0.865</td><td>0.621</td><td>0.757</td><td>0.608</td></tr><tr><td rowspan="5">${ETTh2} \rightarrow  {ETTm2}$</td><td>96</td><td>0.174</td><td>0.276</td><td>0.646</td><td>0.563</td><td>0.253</td><td>0.329</td><td>0.240</td><td>0.336</td><td>0.226</td><td>0.309</td><td>0.248</td><td>0.324</td><td>0.263</td><td>0.352</td></tr><tr><td>192</td><td>0.233</td><td>0.315</td><td>0.934</td><td>0.654</td><td>0.293</td><td>0.346</td><td>0.295</td><td>0.369</td><td>0.289</td><td>0.345</td><td>0.296</td><td>0.352</td><td>0.326</td><td>0.389</td></tr><tr><td>336</td><td>0.291</td><td>0.337</td><td>1.157</td><td>0.728</td><td>0.347</td><td>0.376</td><td>0.345</td><td>0.397</td><td>0.348</td><td>0.379</td><td>0.353</td><td>0.383</td><td>0.387</td><td>0.426</td></tr><tr><td>720</td><td>0.392</td><td>0.417</td><td>4.730</td><td>1.531</td><td>0.446</td><td>0.429</td><td>0.432</td><td>0.442</td><td>0.439</td><td>0.427</td><td>0.471</td><td>0.446</td><td>0.487</td><td>0.478</td></tr><tr><td>平均值（Avg）</td><td>0.272</td><td>0.341</td><td>1.867</td><td>0.869</td><td>0.335</td><td>0.370</td><td>0.328</td><td>0.386</td><td>0.325</td><td>$\underline{0.365}$</td><td>0.342</td><td>0.376</td><td>0.366</td><td>0.411</td></tr><tr><td rowspan="5">${ETTm1} \rightarrow  {ETTh2}$</td><td>96</td><td>0.321</td><td>0.369</td><td>0.510</td><td>0.576</td><td>0.353</td><td>0.392</td><td>0.365</td><td>0.415</td><td>0.354</td><td>0.385</td><td>0.377</td><td>0.407</td><td>0.435</td><td>0.470</td></tr><tr><td>192</td><td>0.389</td><td>0.410</td><td>0.523</td><td>0.586</td><td>0.443</td><td>0.437</td><td>0.454</td><td>0.462</td><td>0.447</td><td>0.434</td><td>0.471</td><td>0.453</td><td>0.495</td><td>0.489</td></tr><tr><td>336</td><td>0.408</td><td>0.433</td><td>0.640</td><td>0.637</td><td>0.469</td><td>0.461</td><td>0.496</td><td>0.494</td><td>0.481</td><td>0.463</td><td>0.472</td><td>0.484</td><td>0.470</td><td>0.472</td></tr><tr><td>720</td><td>0.406</td><td>0.436</td><td>2.296</td><td>1.034</td><td>0.466</td><td></td><td>0.541</td><td>0.529</td><td>0.474</td><td>0.471</td><td>0.495</td><td>0.482</td><td>0.480</td><td>0.485</td></tr><tr><td>平均值（Avg）</td><td>0.381</td><td>0.412</td><td>0.992</td><td>0.708</td><td>$\underline{0.433}$</td><td>0.439</td><td>0.464</td><td>0.475</td><td>0.439</td><td>$\underline{0.438}$</td><td>0.457</td><td>0.454</td><td>0.470</td><td>0.479</td></tr><tr><td rowspan="5">${ETTm1} \rightarrow  {ETTm2}$</td><td>96</td><td>0.169</td><td>0.257</td><td>0.646</td><td>0.563</td><td>0.217</td><td>0.294</td><td>0.221</td><td>0.314</td><td>0.195</td><td>0.271</td><td>0.222</td><td>0.295</td><td>0.385</td><td>0.457</td></tr><tr><td>192</td><td>0.227</td><td>0.318</td><td>0.934</td><td>0.654</td><td>0.277</td><td>0.327</td><td>0.286</td><td>0.359</td><td>0.258</td><td>0.311</td><td>0.288</td><td>0.337</td><td>0.433</td><td>0.469</td></tr><tr><td>336</td><td>0.290</td><td>0.338</td><td>1.157</td><td>0.728</td><td>0.331</td><td>0.360</td><td>0.357</td><td>0.406</td><td>0.317</td><td>0.348</td><td>0.341</td><td>0.367</td><td>0.476</td><td>0.477</td></tr><tr><td>720</td><td>0.375</td><td>0.367</td><td>4.730</td><td>1.531</td><td>0.429</td><td>0.413</td><td>0.476</td><td>0.476</td><td>0.416</td><td>0.404</td><td>0.436</td><td>0.418</td><td>0.582</td><td>0.535</td></tr><tr><td>平均值（Avg）</td><td>0.268</td><td>0.320</td><td>1.867</td><td>0.869</td><td>0.313</td><td>0.348</td><td>0.335</td><td>0.389</td><td>0.296</td><td>0.334</td><td>0.322</td><td>0.354</td><td>0.469</td><td>0.484</td></tr><tr><td rowspan="5">${ETTm2} \rightarrow  {ETTh2}$</td><td>96</td><td>0.298</td><td>0.356</td><td>0.510</td><td>0.576</td><td>0.360</td><td>0.401</td><td>0.333</td><td>0.391</td><td>0.327</td><td>0.367</td><td>0.360</td><td>0.401</td><td>0.353</td><td>0.393</td></tr><tr><td>192</td><td>0.359</td><td>0.397</td><td>0.523</td><td>0.586</td><td>0.434</td><td>0.437</td><td>0.441</td><td>0.456</td><td>0.411</td><td>0.418</td><td>0.434</td><td>0.437</td><td>0.432</td><td>0.437</td></tr><tr><td>336</td><td>0.367</td><td>0.412</td><td>0.640</td><td>0.637</td><td>0.460</td><td>0.459</td><td>0.505</td><td>0.503</td><td>0.439</td><td>0.447</td><td>0.460</td><td>0.459</td><td>0.452</td><td>0.459</td></tr><tr><td>720</td><td>0.393</td><td>0.434</td><td>2.296</td><td>1.034</td><td>0.485</td><td>0.477</td><td>0.543</td><td>0.534</td><td>0.459</td><td>0.470</td><td>0.485</td><td>0.477</td><td>0.453</td><td>0.467</td></tr><tr><td>平均值（Avg）</td><td>0.354</td><td>0.400</td><td>0.992</td><td>0.708</td><td>0.435</td><td>0.443</td><td>0.455</td><td>0.471</td><td>0.409</td><td>0.425</td><td>0.435</td><td>0.443</td><td>0.423</td><td>0.439</td></tr><tr><td rowspan="5">${ETTm2} \rightarrow  {ETTm1}$</td><td>96</td><td>0.359</td><td>0.397</td><td>1.179</td><td>0.781</td><td>0.747</td><td>0.558</td><td>0.570</td><td>0.490</td><td>0.491</td><td>0.437</td><td>0.747</td><td>0.558</td><td>0.735</td><td>0.576</td></tr><tr><td>192</td><td>0.390</td><td>0.420</td><td>1.327</td><td>0.846</td><td>0.781</td><td>0.560</td><td>0.590</td><td>0.506</td><td>0.530</td><td>0.470</td><td>0.781</td><td>0.560</td><td>0.753</td><td>0.586</td></tr><tr><td>336</td><td>0.421</td><td>0.445</td><td>1.478</td><td>0.902</td><td>0.778</td><td>0.578</td><td>0.706</td><td>0.567</td><td>0.565</td><td>0.497</td><td>0.778</td><td>0.578</td><td>0.750</td><td>0.593</td></tr><tr><td>720</td><td>0.487</td><td>0.488</td><td>3.749</td><td>1.408</td><td>0.769</td><td>0.573</td><td>0.731</td><td>0.584</td><td>0.686</td><td>0.565</td><td>0.769</td><td>0.573</td><td>0.782</td><td>0.609</td></tr><tr><td>平均值（Avg）</td><td>0.414</td><td>0.438</td><td>1.933</td><td>0.984</td><td>0.769</td><td>0.567</td><td>0.649</td><td>0.537</td><td>0.568</td><td>0.492</td><td>0.769</td><td>0.567</td><td>0.755</td><td>0.591</td></tr></tbody></table>

<!-- Media -->