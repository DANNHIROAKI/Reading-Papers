# Approximate Nearest Neighbor Queries in Fixed Dimensions

## 0. Abstract  

给定$d$维欧几里得空间中的一个$n$点集$S \subset E^d$，以及一个查询点$q \in E^d$，我们希望找到$q$的最近邻，即$S$中与$q$的欧几里得距离最小的点。我们的目标是预处理点集$S$，使得查询可以尽可能高效地进行。我们假设维度$d$是与$n$无关的常数。尽管当$d$较小时，这个问题已经有相当不错的解决方案，但随着$d$的增加，这些算法的性能会急剧下降。我们提出了一种用于近似最近邻搜索的随机算法。给定任意点集$S \subset E^d$，以及常数$\epsilon>0$，我们构建一个数据结构，使得对任意查询点$q$，报告的点与$q$的距离至多为真实最近邻距离的$(1+\epsilon)$倍。该算法的期望运行时间为$O\left(\log ^3 n\right)$，空间复杂度为$O(n \log n)$。该数据结构可以在$O\left(n^2\right)$的期望时间内构建。常数因子取决于$d$和$\epsilon$。由于在高维空间中最近邻搜索具有实际的重要性，我们实现了该算法的一个实用变体，并通过实验表明，对于许多点分布，这个算法变体在中等维度下比现有的实用方法找到最近邻的速度显著更快。



## 1. Intro

寻找最近邻是计算几何学和一般搜索算法研究中最基础的问题之一。最近邻问题是：给定$d$维空间中的一个包含$n$个点的集合$S \subset E^d$，以及一个查询点$q \in E^d$，找到$S$中到$q$的欧几里得距离最小的点。我们假设$d$是与$n$无关的常数。当然，可以通过暴力枚举所有$n$个点并计算它们到$q$的距离，在$O(n)$时间内解决该问题。更高效的方法是通过对点集$S$进行预处理，创建一个数据结构，使得给定查询点$q$时，可以快速计算最近邻。

最近邻问题在统计学、模式识别和数据压缩等领域中具有重要意义。我们对该问题的特别兴趣来自于数据压缩在语音处理中的应用，使用了矢量量化技术。该技术依赖于在中等维度(例如8到64维)中高效地解决最近邻查询问题。语音波形被采样，并将样本分组为长度为$d$的向量，随后传输的是一组码字向量中最近邻的索引。据该领域的研究者[10]观察，尽管希望将矢量量化扩展到更高维度(例如8维以上)，但主要瓶颈在于解决这些维度中的最近邻搜索问题的困难。

最近邻问题在一维空间中可以通过二分搜索在$O(\log n)$时间内解决，在平面中可以通过使用Voronoi图和点定位在$O(\log n)$时间内解决[15]。然而，随着维度的增加，解决最近邻问题的难度(无论是时间还是空间复杂度)似乎急剧增加。Clarkson提出了一种基于随机采样点集Voronoi图(RPO树)的随机化算法，在固定维度下其期望运行时间为$O(\log n)$[6]。然而，在最坏情况下，该算法所需的空间大致增长为$O\left(n^{\lceil d / 2\rceil+\delta}\right)$，这对于我们的应用来说空间复杂度太高。Yao和Yao[24]指出，最近邻搜索可以在线性空间和接近线性的时间$O\left(n^{f(d)}\right)$内解决，其中$f(d)=\left(\log \left(2^d-1\right)\right) / d$，但这种微小的渐近改进在实际中没有太大价值。已知对高维问题最实用的方法是Friedman、Bentley和Finkel提出的$k$-d树算法[9]。$k$-d树在期望情况下的运行时间是对数级别的，但这仅在对输入分布有相当严格假设的情况下成立。对于某些输入，$k$-d树算法的运行时间可能糟糕到线性时间，尽管实验表明它的性能通常要好得多。在语音处理领域，已经提出了多种加速朴素线性时间算法的相当随意的技术[5], [2], [18]，然而这些技术仅能带来适度的常数因子改进。

在本文中，我们表明，如果愿意考虑近似最近邻而非精确最近邻，则可以在不依赖输入分布的情况下，实现空间和查询时间的高效表现。给定任意常数$\epsilon>0$，我们称点$p$是查询点$q$的$(1+\epsilon)$-最近邻，如果从$q$到$p$的距离与从$q$到其最近邻的距离之比至多为$(1+\epsilon)$。特别地，我们证明了以下内容：

**定理 1.1：**给定任意$n$个点集$S \subset E^d$以及任意常数$\epsilon>0$，通过随机化，可以在$O\left(n^2\right)$的期望时间内预处理$S$，将结果存储在大小为$O(n \log n)$的数据结构中，从而能够在$O\left(\log ^3 n\right)$的期望时间内回答$(1+\epsilon)$-最近邻查询。

渐近符号隐藏的常数是$\epsilon$和$d$的函数。我们的算法的关键特点是：

- 这是第一个在最近邻搜索(无论是近似还是精确)中实现多对数级查询时间和近线性空间的算法。
- 查询处理的随机运行时间与点集$S$或查询点$q$无关，只取决于随机化的影响。
- 该算法和数据结构简单且易于实现。

我们算法的直接实现唯一的实际缺点在于，在我们的分析中得出的常数因子过大，无法在中等高维度(例如$d \geq 8$)和相对较小的误差因子(例如$\epsilon \leq 0.1$)下使这种方法具有实际竞争力。然而，我们认为这一通用方法非常重要。为了证明这一点，我们提出了算法的一个实用变体。我们在合成数据和实际数据(来自语音压缩应用)上进行了大量实验，结果表明这种通用方法显著快于已知的实用方法，包括$k-d$树。

Matoušek推测，对于半空间空集问题，$\Omega\left(n^{1-1 /\lfloor d / 2\rfloor}\right)$是一个下界[14](假设使用线性空间)。球空集问题是该问题的推广(到无限半径的球体)，因此任何下界也适用于球空集问题。然而，球空集问题可以简化为一个最近邻查询问题(位于球的中心)。鉴于此，如果一个人只能使用大致线性空间，并且期望在高维中实现多对数性能，那么近似可能是合理期望的最佳选择。



## 2. The Randomized Neighborhood Graph  

在本节中，我们讨论用于寻找近似最近邻的算法，适用于点集$S \subset E^d$。我们的方法基于一些简单的技术，可以视为高维中“扁平化”跳跃表的推广[16]。数据结构本身由一个有向图(带有一些额外的结构信息)组成，其顶点集为$S$，并且每个顶点的度数为$O(\log n)$。对于每个点$p \in S$，我们使用一组凸锥体覆盖$E^d$，这些锥体的数量是常数，并且它们共享$p$作为公共顶点，其角直径$\delta$由$\epsilon$的函数上界约束。这些锥体不必是圆形的。Yao[23]给出了一种构造这样一组锥体的方法。以$p$为中心的锥体数量是$d$和$\epsilon$的函数，但与$n$无关。

对于每个锥体，我们从$p$向位于该锥体内的$O(\log n)$个点添加有向边。通过以下随机化过程确定$p$的这些邻居。将$S-\{p\}$中的点随机排列。每个点在此排列中的位置称为它相对于$p$的索引。对于每个以$p$为中心的锥体$c$，我们考虑位于该锥体内的点$r$。如果$r$是该锥体中所有索引较低的点中距离$p$最近的点，则从$p$到$r$添加一条边。结果的邻居集记为$N_c[p]$，并存储在一个列表中。根据标准的概率论推导，一个点的期望度数为$O(\log n)$。如果有必要，在期望情况下通过重复这个过程若干次，我们可以保证每个顶点的度数为$O(\log n)$。注意，如果锥体不为空，则该锥体中至少有一个$p$的邻居(即距离$p$最近的点)。生成的图称为$S$的随机邻居图，记作$N G_d(\delta, S)$。图1展示了将该方法应用于单个锥体的示例。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921202444745.png" alt="image-20240921202444745" style="zoom: 25%;" /> 

Figure 1: Randomized neighborhood graph.  

随机邻居图的一个性质由以下引理给出。

**引理 2.1**：给定任意$\epsilon>0$，存在一个角直径$\delta$(依赖于$\epsilon$)，使得对于任意查询点$q$和任意点$p \in S$，如果$p$不是$q$的$(1+\epsilon)$-最近邻，那么在$N G_d(\delta, S)$中存在一个$p$的邻居，它比$p$更接近$q$。

证明。(概要)假设$p$不是$q$的$(1+\epsilon)$-最近邻。将距离归一化，使得$p$位于以$q$为中心、半径为$(1+\epsilon)$的球面上。(在整个证明过程中，除非另有说明，我们使用“球”一词来表示以查询点为中心的$(d-1)$维超球面。)那么，存在一个点$r$位于以$q$为中心、半径为1的球内。定义从$p$出发穿过这些点的两条射线之间的夹角为等距点之间的角距离，并将这两个球面之间的角距离定义为从每个球面上各取一个等距点之间角距离的下确界。一个简单的几何练习可以证明，对于$\epsilon>0$，这两个球面之间的角距离大于零。设$\delta$为小于该角距离的任意正值。

考虑一个以$p$为顶点包含$r$的锥体。如果$p$与$r$之间有一条边，则证明结束。如果没有，则该锥体内必然存在一个$p$的邻居$s$，它比$r$更接近$p$。同样，通过几何论证并结合我们选择的$\delta$，可以证明$s$比$p$更接近$q$，从而完成证明。

这个引理表明，从任意点$p \in S$出发，我们可以沿着一条距离$q$逐步减少的路径，走向查询点$q$的$(1+\epsilon)$-最近邻。可以想象有许多不同的搜索策略。例如，一个简单的贪心搜索策略是在每个点$p$处，访问$p$的邻居中距离查询点最近的那个点。尽管这种方法直观上很有吸引力，但我们无法给出贪心搜索的渐近性能界。

我们的搜索策略基于一种简单随机策略的修改版。我们先对简单策略及其为何失败做一个直观解释。设$p$是当前搜索访问的点，定义集合$Cl(p)$为点集$S$中到$q$的距离严格小于$p$到$q$的距离的子集。这些点位于以$q$为中心、半径为$\text{dist}(q, p)$的球内。考虑集合$Cl(p)$中相对于$p$具有最低索引的点$r$。由于$r$可能是$Cl(p)$中任何一个点，其概率相等，因此期望情况下，$Cl(p)$中比$r$更接近$q$的点的数量大约为$|Cl(p)| / 2$。因此，如果$r$是$N G_d(\delta, S)$中$p$的邻居，通过从$p$移动到$r$，我们期望能排除一半的剩余点。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921202842689.png" alt="image-20240921202842689" style="zoom:33%;" /> 

Figure 2: Pruning.  

该搜索策略的问题在于，$r$不一定是$p$的邻居，因此这种跳转可能无法实现。为了理解这一点，我们引入一个称为剪枝(pruning)的概念。我们说，位于$Cl(p)$中的点$r$是被剪枝的，如果对于所有以$p$为中心且包含$r$的锥体，存在某个点$r^{\prime}$，它位于相同锥体内且位于$Cl(p)$之外(因此距离$q$比$p$更远)，并且相对于$p$，$r^{\prime}$的索引比$r$低，且$\text{dist}\left(p, r^{\prime}\right)<\text{dist}(p, r)$。参见图2。显然，如果$r$被剪枝了，那么它就不是$p$的邻居。因此，$r^{\prime}$实际上将$r$排除为$p$的可能邻居，但由于我们要求到$q$的路径距离单调递减，我们无法访问$r^{\prime}$。

为了解决剪枝问题，我们利用了一些关于随机邻居图和剪枝的基本性质。我们在此直观地陈述这些性质，但在引理2.2的证明中对它们进行了精确定义，并影响了锥体的角直径$\delta$的选择。首先，因为剪枝发生在锥体内(而不是锥体之间)，它仅局限于靠近球面(以$q$为中心、半径为$\text{dist}(q, p)$的球面)的点。

在陈述第二个事实之前，我们先给出一个定义。我们假设以$S$中点为中心的锥体集合仅在平移上是相等的。邻居图的每条有向边自然与其起点中心的锥体相关联，边的终点位于该锥体内。如果随机邻居图中的一条路径$p_1, p_2, \ldots, p_k$的每条边所关联的锥体共享一个公共轴，则称这条路径为伪线性路径。参见图3(a)。我们对伪线性路径的关注在于它们的行为与一维跳跃表中的路径非常相似，因为路径上的后续锥体包含数据点的一个子集，因此我们可以通过排除点的数量轻松衡量进展。第二个性质是，如果$p$不是$q$的$(1+\epsilon)$-最近邻，则存在一条从$p$出发的伪线性路径，其期望长度为$O(\log n)$，这条路径通向一个比所有被剪枝的点离$q$更近的点(并且该路径可以在$O\left(\log ^2 n\right)$的期望时间内构建)。直观上，正是这一第二个观察使我们能够通过沿着一条对数长度的路径绕过被剪枝的点，从而规避剪枝问题。我们在下面的引理中总结了这些观察结果。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921203921784.png" alt="image-20240921203921784" style="zoom:33%;" />  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921203942393.png" alt="image-20240921203942393" style="zoom:30%;" /> 

**引理 2.2**：给定$E^d$中的一个包含$n$个点的集合$S$和常数$\epsilon>0$，存在$\delta>0$($\delta$是$\epsilon$的函数)，使得对于任意查询点$q \in E^d$和任意点$p \in S$，点集$C l(p) \subset S$(即比$p$距离$q$更近的点集)可以划分为三个子集：$\text{Pr}(p)$(表示“可剪枝”)，$\text{In}(p)$(表示“内部”)和$\text{Im}(p)$(表示“中间”)，使得：

(i) $C l(p)$中被剪枝的点仅位于$\text{Pr}(p)$中

(ii) $\text{In}(p)$中的点都比$\text{Pr}(p) \cup \text{Im}(p)$中的任意点更接近$q$

(iii) 如果$p$不是$q$的$(1+\epsilon)$-最近邻，则存在一条在$N G_d(\delta, S)$中的伪线性路径，从$p$出发通过$\text{Im}(p)$到达$\text{In}(p)$中的一个点，该路径可以在$O\left(\log ^2 n\right)$的期望时间内计算

(iv) 每个点属于哪个子集的判断可以在$O(1)$时间内完成。

证明。(概要)在给出引理的证明之前，我们需要介绍一些术语并做出一些观察。令$\delta$表示锥体的角直径(该值稍后确定)。令$c$表示某个锥体的索引，$\text{cone}_c(p)$表示几何锥体，其顶点位于$p$。我们将视相同索引但以不同点为中心的锥体具有相同的形状和方向。令$\text{core}_c(p)$表示位于$\text{cone}_c(p)$中的点，这些点到锥体中央轴线的角距离是$\delta$的一小部分(任何小于$1/2$的比例都可以)。那么，给定$\text{core}_c(p)$中的任意点$r$，可以证明，对于位于$\text{cone}_c(p)$内且足够接近$p$的任意点$s$(相对于$p$和$r$之间的距离)，$r$位于以$s$为中心的平行锥体$\text{cone}_c(s)$内。第二点观察是，我们可以将空间划分，使得每个点都位于某个锥体的核心中，方法是首先用较小的锥体覆盖空间，这些锥体的直径与核心的直径相同，然后将这些较小的锥体扩展至$\delta$的直径。

现在我们回到引理的证明。将距离归一化，使得$p$位于距离$q$为$(1+\epsilon)$的球面上，称为外球面。令基球面为以$q$为中心、半径为1的球面，内球面同样以$q$为中心，位于基球面和外球面之间，其半径为$1+\alpha \epsilon$，其中$\alpha<1$为适当选择的常数。我们将构建一个足够小的$\delta$，以确保内球面内的任何点都不会被剪枝(这也确保基球面内的任何点都不会被剪枝)。令$C^{\prime}$表示以$p$为中心的锥体的子集，其中的核心与基球面相交。假设$\delta$足够小，则$C^{\prime}$中的每个锥体都会被内球面切割成一个有限的锥体，称为“锥帽”(cap)，其顶点位于$p$，底面位于内球面上。我们选择$\alpha$足够接近1，以便对于基球面内位于某个锥体核心中的任意点$r$，以及该锥体锥帽内的任意点$s$，点$p$到$s$的距离与$p$到$r$的距离之比足够小。这使我们能够利用之前的观察，声称$r$位于以锥帽内任意一点为中心的平行锥体内。

令$\text{Im}(p)$为位于$C^{\prime}$中每个锥体的锥帽内的点集，$\text{In}(p)$为位于内球面上的点集，最后令$\text{Pr}(p)$为所有剩余的点集。参见图3(b)。事实(ii)和(iv)直接从我们的定义中得出。不难证明，对于足够小的$\delta$，$\text{Im}(p)$中的点不会被剪枝，从而得出(i)。为了证明(iii)，回忆一下，如果$p$不是$q$的$(1+\epsilon)$-最近邻，则在基球面内存在一个点$r$，该点位于$C^{\prime}$中的某个锥体的核心内。虽然我们不知道具体是哪一个锥体，但可以依次尝试它们，因为锥体的数量是常数。对于每个锥体索引$c$，我们仅关注位于$\text{cone}_c(p)$内的数据点，并执行以下操作。首先，我们检查是否存在从$p$到内球面上且位于$\text{cone}_c(p)$内的点的边。如果有，证明结束。否则，如果存在从$p$到锥帽中的点的边，我们选择具有最低索引的点$s$，并在点$s$处重复该过程(详情请参见伪代码中的while循环)。如果没有这样的点，我们继续检查下一个锥体索引。

锥帽中最低索引的点$s$是锥帽中的随机点，并且由于以$s$为中心的平行锥体包含在$p$的锥体内，我们期望锥帽中剩余的数据点最多有一半位于$s$的锥体内。因此在期望情况下，经过$O(\log n)$步之后，每一步花费$O(\log n)$时间，我们会终止该过程。这样，该过程的期望成本为$O\left(\log ^2 n\right)$。在终止时，我们保证找到一个位于内球面上的点，因为如果基球面内的点$r$位于$\text{core}_c(p)$中，那么它也位于每个以锥帽中任意点为中心的平行锥体内。因此对于锥体索引$c$，我们最终一定会到达内球面上的一个点。

搜索算法的操作如下。我们假设随机邻居图$N G_d(\delta, S)$已经被计算完毕，这可以在期望时间$O\left(n^2\right)$内轻松完成。初始点$p$可以是$S$中的任意点。设$p$为当前正在访问的点，考虑$p$的邻居中位于$C l(p)$内的最低索引点。如果该点位于$\text{In}(p)$中，我们继续访问该点。如果不在，我们应用前述引理的部分(iii)来找到这样一个点。如果搜索失败，则返回$p$作为近似最近邻。

让我们更详细地描述搜索过程。令$N_c[p]$表示锥体$c$中$p$的邻居集，$N[p]$表示$p$的所有邻居集，NCones表示以一个点为中心的锥体总数。我们将以一个点为中心的锥体从1到NCones编号。令$\text{low}_p(B)$表示相对于点$p$在点集$B$中索引最低的点。while循环计算前述引理部分(iii)中描述的伪线性路径。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921205241841.png" alt="image-20240921205241841" style="zoom: 60%;" />  

注意，所有的集合操作都可以在$O(\log n)$时间内完成，方法是枚举$N[p]$或$N_c[r]$的元素，并对$C l(p), \text{Pr}(p), \text{In}(p)$或$\text{Im}(p)$进行适当的成员测试。为了验证上述过程的正确性，注意如果$p$不是$q$的$(1+\epsilon)$-最近邻，那么引理2.2暗示存在一条伪线性路径通向一个比$p$离$q$更近的点，因此搜索将成功找到这样的点。

为了确定搜索过程的运行时间，我们需要证明在期望情况下，对函数$NN$的递归调用次数是$O(\log n)$。如前所述，期望值是根据在构建$N G_d(\delta, S)$时所有可能的随机排列选择计算的，因此它与$S$和$q$无关。我们的基本断言是：在每次对$NN$的调用中，随着固定的概率，距离$q$比当前点更近的点数减少了一个常数倍。非正式地说，论证基于两种情况：$|\text{Pr}(p) \cup \text{Im}(p)|>|\text{In}(p)|$和$|\text{Pr}(p) \cup \text{Im}(p)| \leq|\text{In}(p)|$。在第一种情况下，经过期望时间$O\left(\log ^2 n\right)$的努力后，我们要么终止，要么递归调用，剩余点集的大小至多为：

$|\text{In}(p)| \leq \cfrac{|\text{Pr}(p) \cup \text{Im}(p)|+|\text{In}(p)|}{2} \leq \cfrac{|C l(p)|}{2}$ 

因此，至少有一半的点被排除在进一步考虑之外。在第二种情况下，以至少$1/2$的概率，$C l(p)$中相对于$p$索引最低的点位于$\text{In}(p)$中，因此不能被剪枝。在这种情况下，使用与简单随机搜索类似的论证，期望$|\text{In}(p)|$中的至少一半的点会被索引最低的点排除(连同$(\text{Pr}(p) \cup \text{Im}(p))$中的所有点)，这意味着期望至少有一半的点会被排除。总结一下，在第一种情况下，我们在$O\left(\log ^2 n\right)$的时间内排除至少一半的点，而在第二种情况下，我们以至少$1/2$的概率在一步中排除一半的点。第二种情况中的每一步的代价是$O(\log n)$(与$p$的邻居数成正比)。

**引理2.3**：对$NN$的期望递归调用次数是$O(\log n)$，因此搜索过程的期望运行时间是$O\left(\log ^3 n\right)$。



## 3. A Practical Variant  

尽管上一节的结果在理论上很有吸引力，但对于最近邻搜索问题的实际实例而言，所提出的算法在竞争力上不如其他实用方法。原因在于，当$\epsilon$较小时，锥体的数量随着$\epsilon$的减小呈渐近增长为$\Omega\left(1 / \epsilon^{d-1}\right)$。在本节中，我们描述了一种邻居图方法的变体，旨在解决实际问题，尽管上一节中展示的正式复杂度和性能界限可能不再适用。我们认为这些结果表明该方法有望成为解决高维空间中最近邻搜索问题的实际方法。

所提出的变体包含以下对前一节中介绍的随机邻居图方案的主要修改：

- 为了减少图的度数，我们使用了一种类似于相对邻居图[13], [21]中的剪枝方案。正如我们将看到的，生成的图会显著稀疏。
- 为了进一步将图的度数减少一个$O(\log n)$的量级，我们放弃了随机化的“跳跃表”构造。我们的经验表明，通过简单地选择一个更好的起始点，可以廉价地模拟由此部分构造引入的“长”边。通过为点集构建一个$k$-$d$树(作为预处理的一部分)，可以从包含查询点的树叶节点中选择起始点。
- 由于生成的图更为稀疏，假设沿着搜索路径的点到查询点的距离会单调递减是不合理的。我们维护一个候选点列表，其中包含所有已访问点的邻居。在未访问的候选点中反复选择最接近查询点的点。结果的搜索路径可能不是单调的，但总是试图向查询点靠近而不重复访问点。

总而言之，我们通过降低邻居图的度数来减少搜索中的复杂度，但这会增加搜索所需的步骤数。在某些最坏情况下，这个方案的性能可能会很差。然而，在大多数实际情况下，搜索能够快速收敛到最近邻。

首先描述修改后的邻居图。它与相对邻居图[13,21]非常相似。一个点集$S \subset E^d$的相对邻居图是一个无向图，其中两个点$p$和$q$相邻，当且仅当没有同时比它们都更接近的第三个点。我们修改后的邻居图是一个基于以下剪枝规则的有向图。对于每个点$p \in S$，我们按照距离$p$的递增顺序考虑$S$中的其他点。我们从序列中删除最接近的点$r$，创建一条从$p$到$r$的有向边，并从进一步考虑中排除所有满足$\text{dist}(p, s)>\text{dist}(r, s)$的点$s$。直观上，由于$r$比$p$和$s$之间的距离更接近它们两者，因此这些点$s$不会被认为是$p$的邻居。这个过程重复进行，直到所有点都被剪枝。这个图等价于Jaromczyk和Kowaluk[11]提出的图，他们在构建相对邻居图时将其作为中间结果。我们称该图为$S$的稀疏邻居图，记作$R N G^*(S)$。

$R N G^*(S)$可以在$O\left(n^2\right)$的时间内轻松计算完成，其中$n=|S|$。该图的一个重要性质是：如果查询点恰好等于$S$中的某个点，那么通过简单的贪心搜索(每一步访问距离查询点更近的邻居)就可以成功找到查询点，沿着一条距离查询点单调递减的路径。其原因在于，如果当前点与查询点之间没有边，那么必须存在一个更近的点，并且该点的边剪枝掉了查询点。

确定$R N G^*$度数的上界与经典数学问题——在$d$维球面上寻找球冠的最密堆积——密切相关。定义球冠的直径为球冠上任意两点之间的最大角度。

**引理 3.1**：给定位于$E^d$中的点集$S$，且点集处于一般位置(没有两点与第三点的距离相等)，$R N G^*(S)$中任何顶点的度数不超过在$d$维球面上直径为$\pi / 3$的球冠的最密堆积数量。

证明：设$p, r, s \in S$为三个点，并且$\angle p r s \leq \pi / 3$。我们声称$p$和$s$不能同时是$r$在$R N G^*(S)$中的邻居，因为通过基本几何可以很容易地证明，添加一条从$r$指向$p$和$s$中较近点的有向边将会剪枝较远的点。因此，如果我们将$r$的邻居集投影到以$r$为中心的单位$d$维球面上，并用半径为$\pi / 6$的球冠包围每个邻居，那么这些球冠之间不会相交，因此它们在$d$维球面上形成了一种堆积。

遗憾的是，对于任意维度，关于这个数量的紧确界尚不清楚。根据Kabatjanskii和Levenštein [12] 对球面堆积的上界，以及Shannon [17] 和Wyner [22] 的下界，当维度趋于无穷时，$R N G^*(S)$中任意顶点$p$的度数上界位于区间

$\left[1.15^{d-1}, 1.32^{d-1}\right] \quad(\text{当 } d \rightarrow \infty)$

遗憾的是，这些界限是关于$d$的渐近值，对于我们感兴趣的较小维度，这些界限显得过于乐观。例如，在24维空间中，最坏情况下的度数可以高达196,560 [20]，而$1.32^{23}$仅为593。不过，我们推测期望度数要远小于最坏情况下的度数。

为了确定$R N G^*(S)$中顶点期望度数的实际界限，我们进行了两项实验研究。第一项研究测量了在维度$d$下图中顶点的期望度数，点集大小为$2^d$，均匀分布在单位立方体内。在这种小规模点集中，边界效应(即在高维空间中更多的点趋向于靠近凸包的现象)显著减少了图的度数。我们进行了第二项实验，尝试推断出点集足够大以至于边界效应可以忽略的情况。在第一个实验中，我们生成了$2^d$个均匀分布的点，并计算了随机点的度数。在第二个实验中，生成了10万个均匀分布在超立方体内的点，并计算位于中心的顶点的度数。在每种情况下，每个维度下的度数都基于100次试验的平均值。结果如图4所示。通过对度数的对数拟合直线，我们推测对于第一个实验，度数为$1.46\left(1.20^d\right)$，对于第二个实验，度数为$2.9\left(1.24^d\right)$(且残差研究表明增长率可能更慢)。尽管度数随着维度呈指数增长，但对于我们感兴趣的维度范围，这个增长率仍在可接受范围内。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921212045160.png" alt="image-20240921212045160" style="zoom: 33%;" /> 

Figure 4: Expected degree of  $\text{RNG}^{*}(S)$ 

我们使用最佳优先策略搜索图。搜索算法从一个点 $p$ 开始，该点是从包含查询点的 $k-d$ 树的一个桶中选择的。我们维护一个最近邻候选集(使用堆来维护)，最初只包含点 $p$。然后我们选择还未访问过的、距离最近的候选点。算法步骤如下所示。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921212238822.png" alt="image-20240921212238822" style="zoom:60%;" />  

终止条件的选择有些微妙。由于该数据结构缺乏其他算法提供的结构化信息，它无法确定何时找到了最近邻。在实践中，终止条件通常基于收敛测试。在这项研究中，我们想测试这种方法相对于其他实用算法(如 $k$ $d$ 树 [9])的可行性，$k$-$d$ 树由 Sproull [19] 进行了实证精炼和分析(Sproull 对 $k$-$d$ 树执行时间的分析在更高维度下远远不如我们的算法。我们通过对他的算法进行一些小的修改，显著提高了运行时间。这在 [1] 中有所描述)，以及一个简单的桶算法，Cleary [7] 针对均匀分布数据进行了分析，Bentley、Weide 和 Yao [4] 也独立进行了分析。由于基于 $R N G^*(S)$ 的算法并不保证找到最近邻(除非所有点都被枚举)，因此我们选择将每个算法在找到最近邻之前考虑的点数(最近邻已离线预计算)作为比较基础。需要注意的是，$k$-$d$ 树算法和桶算法会继续搜索，直到确信这是最近邻，但找到最近邻的时间肯定为其执行时间提供了下限。

实验中使用的点分布如下描述，其中一些由 Bentley [3] 提出。

**均匀分布**：每个坐标从区间 $[0,1]$ 上均匀选择。

**正态分布**：每个坐标从均值为零、方差为一的正态分布中选择。

**聚类正态分布**：从均匀分布中选择十个点，并在每个点放置标准差为 0.05 的正态分布。

**拉普拉斯分布**：每个坐标从均值为零、方差为一的拉普拉斯分布中选择。

为了模拟语音处理应用中常见的分布类型，通过将自回归源的输出分组为长度为 $d$ 的向量，构造了另外两种点分布。自回归源使用以下递推公式生成连续输出：

$X_n = \rho X_{n-1} + W_n$ 

其中，$W_n$ 是一系列均值为零、相互独立且同分布的随机变量。我们在实验中取相关系数 $\rho$ 为 0.9。每个点的生成方式是：首先从对应的非相关分布(正态或拉普拉斯分布)中选择第一个分量，剩余分量则通过上述方程生成。有关如何生成这些自回归过程的更多细节，可以参考 Farvardin 和 Modestino 的研究 [8]。

**共正态分布**：选择 $W_n$ 使得 $X_n$ 的边际密度为方差为 1 的正态分布。

**共拉普拉斯分布**：选择 $W_n$ 使得 $X_n$ 的边际密度为方差为 1 的拉普拉斯分布。

**语音分布**：从一个由以 $8 \mathrm{~kb} / \mathrm{s}$ 采样语音波形得到的 680 万样本构成的数据库中，将连续样本分组以形成所需维度的向量。在 16 维度中，我们得到 425,000 个向量，从前 400,000 个向量中随机选择向量形成数据向量集，并从剩余的 25,000 个向量中随机选择查询向量。

为了避免图表过于繁杂，我们未展示 ClusNorm 和 Co-Normal 分布的结果；仅需指出这些分布的结果与 Co-Laplace 分布非常相似。

图 5 显示了 $k$-d 树算法在终止之前检查的平均点数，针对这些点分布的代表性集合以及 1000 个查询点的结果。在所有实验中，我们在 $E^{16}$ 中构建了优化的 $k$-d 树 [9]。切割平面位于中位数处，垂直于扩展最大的一条坐标轴。每个叶节点包含一个点，这已被证明是 $k$ - $d$ 树算法在检查点数上的最佳性能表现。在每种情况下，数据点和查询点都从相同的分布中选择。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921214759936.png" alt="image-20240921214759936" style="zoom:33%;" /> 

Figure 5: $k$-d 树算法在终止前检查的平均点数

表 1 显示了桶算法在 16 维超立方体中点均匀分布情况下，终止前检查的平均和最大点数及单元数。该超立方体被划分为 $2^{16}$ 个大小相等的单元，并按距离查询点的递增顺序进行检查。我们将此技术的实验限制在均匀分布上，因为将其扩展到无界分布并不容易。对于 100,000 个点，结果与 $k$-d 树的结果相似。

|  NPts  | Avg Pts | Avg Cells |
| :----: | :-----: | :-------: |
|  1000  |   598   |   38988   |
| 10000  |  2886   |   18899   |
| 100000 |  11189  |   7341    |

Table 1: 桶算法检查到终止的点数

由于我们的算法没有明确的终止条件，因此将其与那些必须继续搜索直到确定找到最近邻的算法进行比较并不完全公平。出于这个原因，我们专注于算法在首次找到最近邻之前会访问多少个点(尽管算法可能不知道已经找到了)。我们通过蛮力法离线计算了真实的最近邻。图 6 和表 2 显示了 $k$-$d$ 树和桶算法在找到最近邻之前检查的点数，针对的是与图 5 和表 1 中相同的数据集和查询点。对于 $k$-$d$ 树和桶算法来说，找到最近邻之前看到的点数显著少于算法终止前看到的点数。此外，桶算法在找到最近邻之前看到的点数明显少于 $k$-$d$ 树算法。这一差异的可能解释是，$k$-$d$ 树算法并不严格按照与查询点距离递增的顺序访问单元。(最近我们对 $k$-$d$ 树的变体进行了实证研究，这些变体按距离递增顺序搜索单元，发现这些算法在访问点数方面与 $R N G^*$ 搜索具有竞争力。然而，维持这种顺序的开销相当大。这些结果已在 [1] 中报告)

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921215505245.png" alt="image-20240921215505245" style="zoom:25%;" /> 

Figure 6: $k$-d 树算法在找到最近邻之前检查的平均点数

图 7 显示了基于 $R N G^*(S)$ 的算法在找到最近邻之前检查的平均点数，针对各种分布。我们将算法访问的每个点的所有邻居都计入这个数量(这意味着点可能会被多次计数，但能准确反映运行时间)。图 8 对比了均匀分布数据下 $k$-$d$ 树算法终止前、$k$-$d$ 树算法找到最近邻之前、以及我们算法的检查点数。可以看到，基于 $R N G^*(S)$ 的算法检查的点数远少于 $k$-$d$ 树算法(注意图表使用对数刻度)。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921215850733.png" alt="image-20240921215850733" style="zoom:25%;" /> 

Figure 7: 基于 $R N G^*(S)$ 算法检查的平均点数

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921220017343.png" alt="image-20240921220017343" style="zoom:25%;" /> 

Figure 8: $k$-d 树算法与基于 $R N G^*(S)$ 算法的比较

总结而言，我们可以从测试中得出以下几点观察：

- $k$-d 树算法和桶算法在搜索终止之前就已经找到了最近邻。从某种意义上说，这额外的搜索可以被视为为了保证最终输出的正确性而付出的代价。这表明，在一些应用中，如果快速找到一个最近邻的近似解已经足够，搜索可以在相对较少的步骤后终止。

- 在均匀分布的数据上，$k$-d 树算法和桶算法在终止前所检查的点数表现非常相近。

- 我们基于 $R N G^*(S)$ 的算法在所有点分布上对于大规模点集的表现显著优于 $k$-d 树算法。对于均匀分布，$R N G^*(S)$ 算法与桶算法的表现相当，但后者除了处理均匀数据集之外在其他情况下并不实用。



## 4. Conclusions  

我们提出了一种随机算法，用于在预期的多对数查询时间和 $O(n \log n)$ 空间内计算近似最近邻。由于该算法中涉及的常数相当大，我们还提出了一个更为实用的变体。实验表明，该算法对于许多输入分布以及在高达16维的实际语音数据上非常高效。这项工作引发了一些有趣的开放性问题。最重要的理论问题是如何去除空间和运行时间中的额外对数因子，目标是提供 $O(\log n)$ 的查询时间和 $O(n)$ 的空间。另一个问题是是否可以使结果变为确定性的。再者，能否用更简单的贪婪搜索替换我们的搜索策略，同时仍然保证多对数搜索时间。最重要的实际问题是，随机算法中与维度 $d$ 和误差 $\epsilon$ 相关的常数能否减少，或者 $R N G^*$ 搜索的效率能否从理论上得到证明。