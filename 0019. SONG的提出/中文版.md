## SONG: Approximate Nearest Neighbor Search on GPU

# 0. Abstract

近似最近邻（ANN）搜索是计算机科学中的一个基本问题，广泛应用于机器学习和数据挖掘等领域。最近的研究表明，基于图的ANN方法往往优于其他类型的ANN算法。在典型的基于图的算法中，搜索算法是迭代执行的，执行依赖性限制了其在GPU上的适配。在本文中，我们提出了一个新颖的框架，通过将图搜索算法解耦为三个阶段，从而并行化至关重要的距离计算。此外，为了在GPU上获得更好的并行性，我们提出了针对ANN的优化方法，这些方法消除了动态GPU内存分配，并通过增加计算来减少GPU内存的消耗。通过在6个数据集上的实证比较，本文提出的系统（SONG）与目前在CPU上最先进的ANN方法HNSW以及流行的GPU加速平台Faiss进行了对比。结果表明，SONG的速度比单线程HNSW快50-180倍，同时大幅超越了Faiss。

# 1. INTRODUCTION  

最近邻（NN）搜索自计算机科学早期以来一直是一个基本问题，广泛应用于机器学习、计算机视觉、数据挖掘和信息检索等领域。NN任务的挑战在于找到最近邻而无需扫描数据存储库中的所有数据点。近年来，近似最近邻（ANN）搜索方法变得越来越受欢迎，因为许多应用只需要找到“足够接近”的邻居，而不需要找到精确的最近邻解决方案。

---

**ANN搜索方法。** 在ANN搜索范式中，每个查询只与数据点的一个子集进行比较，而不是整个数据集。为了获取这个子集，提出了多种索引结构，包括概率哈希、量化、球树或KD树变体，以及基于图的搜索等方法。

---

**基于图的ANN方法。** 近年来，基于图的方法备受关注。文献中的大量实验表明，基于图的方法在常见指标上优于其他类型的ANN方法。通常，这些方法构建一个称为**近邻图**（Proximity Graph）的图索引，图的顶点表示数据集中的点，图中的边表示连接节点之间的邻域关系。邻域关系的定义基于各种约束条件，以便使图适用于ANN问题。例如，某些图约束如**德劳内图**和**单调搜索网络**保证从任何起始顶点到查询点存在一条单调递减的路径。为降低近邻图构建的复杂度，某些方法如**GNNS**、**NSG**、**HNSW**等对德劳内图或相对邻域图进行近似，使得这些基于图的ANN方法在处理海量数据时具有可行性，广泛应用于工业实践。 

---

**图上搜索**：虽然这些方法在构建图索引的约束上有所不同，但大多数基于图的方法（如[18]、[19]、[26]、[35]、[46]、[47]、[57]、[58]）共享类似的启发式搜索算法（算法1），该算法是$A^*$启发式搜索的变体。其工作流程与广度优先搜索（BFS）类似，但BFS中的队列被优先队列$q$替代。优先队列按照顶点到查询点$p$的距离进行升序排列。搜索过程从默认起始点开始，随后从优先队列中提取一个顶点（第3行），更新top-K候选点（第4-8行），并将该顶点的邻居插入优先队列中以供未来探索（第13行）。当从优先队列中提取的顶点比已搜索到的top-K结果候选点差时，主要搜索循环（第2-16行）停止。抽象来说，算法贪婪地沿路径前进，以找到查询点的最近邻。图1展示了算法1的示例。

---

**图1**：一个示例说明了在近邻图上的搜索算法（算法1）。星形表示查询点$p$。在该示例中，我们的目标是找到$p$的前$K=3$个最近邻。顶点1为默认起始点。搜索路径$1 \rightarrow 2 \rightarrow 8 \rightarrow 7$用虚线箭头高亮显示。右侧展示了每次迭代中优先队列$q$、top $k$和哈希表的状态。

---

**并行性**：搜索过程是迭代执行的，每次迭代依赖于前一次迭代的进展。执行依赖性阻止了该算法的“令人愉快的并行”（embarrassingly-parallel）解决方案。基于图的ANN传统并行解决方案是在多核CPU上同时执行不同的查询。目前关于基于图的方法在GPU上的应用研究较少。然而，如果不考虑GPU架构，查询并行化方案在GPU平台上无法很好地扩展。同时，越来越多的研究[21]、[22]、[36]、[61]正在探索基于GPU的ANN系统。例如，Faiss[36]是一个流行的基于GPU加速的量化ANN系统。由于量化方法的指令依赖性较低，Faiss能够充分并行化执行，相比其他基于CPU的方法，在处理稠密数据时显示出卓越的性能。另一方面，尽管基于图的方法在CPU上效果更好，但复杂的图结构和图搜索的高执行依赖性使得在GPU上进行适配成为一项挑战。在本文中，我们探讨了GPU适配问题，并为基于图的ANN方法提出了一系列优化组合。

---

**贡献摘要**：

- 我们提出了SONG（“Search ON Graph”的缩写）——一个在GPU上执行基于图的ANN（近似最近邻）搜索系统。据我们所知，SONG是第一个专为GPU设计的基于图的ANN搜索系统。
- 我们开发了一个新框架，将图上搜索算法解耦为三个阶段：候选位置定位、大规模距离计算和数据结构维护，以并行化关键的距离计算性能。与GPU上的广度优先搜索方法不同，我们的框架针对ANN问题中的高维距离计算优化了基于图的ANN搜索。
- 我们提出了一系列针对GPU ANN图搜索的优化数据结构和方法。采用开放地址散列表、Bloom过滤器和Cuckoo过滤器作为高性能散列表，并应用一系列优化技术，如有界优先队列、选择性插入和访问删除，以消除动态内存分配，减少内存消耗。
- 我们提出了一个参数化的图搜索算法——多查询和多步骤探测——用于候选位置定位阶段，允许对性能进行微调。
- 我们通过6个数据集对该系统进行了实验评估，并与HNSW（CPU平台上最先进的ANN方法）和Faiss（流行的GPU ANN系统）进行了比较。结果显示，SONG比单线程HNSW快约50到180倍，同时在性能上也远超Faiss的GPU版本。

# 2. GPU ARCHITECTURE  

在本节中，我们介绍了SONG的GPU架构，这是其特定于GPU优化的基础。

---

**硬件架构和内存层次结构**。图2展示了GPU的硬件架构和内存层次结构。一个GPU包含多个流多处理器（SM）。每个SM包含几十个核心、两个指令调度单元、一个warp调度器、一个寄存器文件和可配置的L1缓存与共享内存。所有的SM共享L2缓存和全局内存。SM内的核心专门用于处理有限的计算指令集。指令调度单元和warp调度器负责下达指令并调度核心执行。寄存器文件是核心可以直接访问的处理器寄存器数组。NVIDIA Volta架构[34]最近引入了L0指令缓存，它比以往的GPU中使用的指令缓冲区更高效。与CPU平台不同，GPU的L1缓存是可配置的，我们可以将L1缓存的一部分分配为共享内存。共享内存可以被显式操作，并且在同一个SM内的所有核心都可以访问。所有SM都可以访问全局内存，L2缓存作为全局内存I/O的缓存。

---

全局内存可以被所有SM中的核心访问。虽然它的容量最大，但其带宽最低，访问延迟最高。共享内存则具有高带宽和低延迟，它也可以被SM内的核心共享。L1和L2这两级缓存用于减少全局内存访问的延迟。当全局内存地址被请求时，连续对齐的地址会合并为一个内存事务。核心必须访问连续的全局内存地址才能高效地从全局内存中获取数据。

---

**GPU编程模型**。在CUDA编程模型中，工作负载以函数的形式提交到GPU，这个函数被称为内核。与CPU平台的定义类似，内核的逻辑实例称为线程。线程以块的形式分组。在物理上，同一个块中的所有线程都位于同一个SM上。在CUDA代码中，线程可以显式地访问深层内存层次结构的不同单元。为了管理数千个并行线程处理不同的数据部分，SM采用了SIMT（单指令多线程）或SIMD（单指令多数据）并行性，通过将一个块中的连续线程分组为一个warp。warp中的所有线程必须同时执行相同的指令。如果代码中有if-else分支，可能会阻塞同一warp中一些线程的执行。处理不同代码分支的线程会顺序执行。

# 3. SONG SYSTEM OVERVIEW  

在本节中，我们识别了算法1的性能瓶颈，并从高级视角介绍了SONG优化算法的主要模块。

---

**距离计算**。对CPU平台上搜索算法的性能分析表明，超过95%的时间花费在距离计算（第11行）上，尤其是在大多数常见的ANN基准数据集上[47]。性能主要由距离计算决定。相比耗时的高维距离计算，其他操作如优先队列维护和内存访问只占不到5%的执行时间。与此相反，GPU具备强大的并行高维距离计算能力，通过其大量核心来提高效率。在CPU上占主要成本的95%在GPU上可以得到显著改善。然而，尽管主要成本大幅降低，GPU上却出现了CPU上不明显的问题。与CPU不同，GPU的内存访问成为主要瓶颈。

---

**内存访问**。CPU上的图搜索算法引入了动态内存分配，并且在搜索过程中占用了大量工作内存。这种情况在GPU上并不具备可扩展性，因为每个线程可用的内存预算要少得多。为了有效地并行化搜索，我们必须限制算法1中优先队列和哈希表的内存消耗。

---

**高级方法**。我们在这里介绍用于解决GPU图搜索算法中单个查询的高级方法。由于非并行近似最近邻搜索在高维数据集上的主要时间花费在距离计算上[47]，我们的GPU图搜索算法着重于利用GPU加速距离计算。为了充分利用GPU的计算带宽，我们必须以批处理的方式计算距离，而不是进行大量独立的成对距离计算函数调用。因此，我们通过将图搜索算法分为三个阶段：候选定位、大批量距离计算和数据结构更新，来优化搜索流程。

---

图3展示了解耦的工作流程。左侧的图（邻接表）继承自图1中的示例。图中第 $i^{t h}$ 行存储了顶点 $i$ 的相邻顶点。例如，图中的第一行表示顶点1连接到顶点2、4、5和7。按照算法1的步骤，在每次迭代中，我们首先从优先队列 $q$ 中提取一个顶点开始搜索。在此示例中，我们正在顶点1上搜索。候选定位阶段从图中获取与顶点1连接的顶点（2、4、5、7）。接着，在大批量距离计算阶段，读取这些顶点的向量值，并使用GPU warp reduction计算它们与查询点 $p$ 的距离。图1中的玩具示例仅用于说明搜索算法的思想。在实际的ANN应用中，维度从几百到上千——我们可以充分利用GPU线程在大批量距离计算阶段进行计算。之后，数据结构维护阶段根据计算出的距离更新优先队列和哈希表，以进行下一次迭代。

图3：展示了GPU warp近似图搜索工作流程的示例。左侧的图（邻接表）继承自图1的示例。阴影单元表示图1中第一次迭代的内存访问模式。

---

这种三阶段工作流程将距离计算和队列维护的依赖性（算法1的第9到15行）解耦为批处理流水线。因此，我们可以通过GPU加速距离计算。然而，这三个阶段仍然是顺序执行的。我们对每个阶段进行了针对GPU架构的优化，以实现更好的性能。

# 4. DATA STRUCTURE MAINTENANCE  

## 4.1. Data Structures on GPU  

为图搜索任务专门设计的数据结构在GPU上有以下优化：

---

**固定度图存储**：基于图的ANN搜索中使用的近似图具有以下特性：每个顶点的度数受固定常数K的限制 [47]。通常将图存储为邻接表需要在GPU内存中保存一个索引来跟踪每个顶点的偏移量。然而，邻接表的索引查找效率不高，因为它需要额外的内存操作。通过将图存储为固定度邻接表，可以消除在邻接表中的额外索引查找。我们可以通过将顶点的索引乘以顶点的固定大小来定位顶点，因为每个顶点占用相同的固定内存量。固定度图存储在GPU的全局内存中（见图3）。

---

**查询操作**：近似图和数据集可以在查询到达之前保存在GPU全局内存中，但查询点需要在运行时从主机CPU的主内存传输到GPU全局内存。在查询的搜索过程中，查询点会被频繁访问，以计算其与图中其他顶点的距离。我们显式地将查询点复制到快速片上共享内存中，以减少GPU全局内存的读取。

---

**并发控制**：尽管有一些研究提出了无锁并发优先队列和哈希表，但这些数据结构设计主要是替代主机端CPU的数据结构，而不是为CUDA内核中的线程设计。此外，每次迭代中仅需执行少量插入操作，因此顺序操作的性能优于复杂的并发数据结构。因此，在我们提出的系统中，优先队列和哈希表由单个线程维护。

---

**内存访问模式**：GPU的共享内存可以在低延迟的情况下被同一块中的所有线程访问。正如前文所述，查询点 $p$ 被复制到共享内存中，因为在距离计算中会频繁访问它。此外，候选点和距离数组被分配为共享内存中的固定长度数组，长度最多为图索引的固定度。将其分配为固定长度数组比动态分配更高效，并将其放入共享内存可消除在warp reduction中的额外通信成本。由于优先队列和哈希表仅由单个线程维护，因此 $q$、topk 和 visited 被分配为其宿主线程的本地内存，其他线程不会访问这些数据结构。图索引和数据则保存在全局内存中。

## 4.2. Hash Table Alternatives  

在这一部分，我们讨论搜索算法中哈希表（visited）的设计及其替代方案。

---

**开放寻址哈希表**：最常见的哈希表实现之一是分离链法，例如GNU GCC的unordered_set哈希表实现采用分离链法来解决哈希冲突。然而，链式解决方案需要动态内存分配（如链表），会破坏GPU的计算性能。因此，在我们的GPU图搜索实现中，采用了另一种哈希冲突解决方法——开放寻址法 [43]。开放寻址法通过在数组中寻找备用位置来解决哈希冲突，直到找到目标记录或未使用的数组槽位。我们为每个线程块在共享内存中分配一个固定长度的数组，其长度与搜索参数$K$成正比，并且可以预先计算。线性探测步骤可以在warp级别并行化——warp中的所有线程探测内存，并通过warp reduction定位插入/删除位置。我们限制了warp级别的并行探测，因为线性探测通常不需要探测很多位置，每个warp中的线程探测一个内存位置（32个线程）通常足以找到有效的插入/删除位置。

---

**布隆过滤器**：我们观察到访问测试不需要精确回答——可以容忍误报，而误漏可能会引起较大的计算开销。误报（visited告诉我们某个元素已被访问但实际上没有）会阻止我们搜索一些未访问的顶点——当被跳过的顶点是查询的最近邻唯一路径时，可能会损失一些搜索精度。另一方面，误漏（visited告诉我们某个顶点未被访问但实际上已被访问）会导致我们再次搜索已访问的顶点，这在搜索和多次将已访问顶点插入优先队列时会带来较大的开销。此外，还会影响数据完整性，因为需要额外检查以避免将同一顶点多次插入优先队列。为了减少内存占用，我们采用了一种概率性数据结构——布隆过滤器 [49]，以替代哈希表。布隆过滤器占用恒定且较小的内存空间，可以在GPU上高效实现。此外，布隆过滤器保证不会有误漏，误报率在理论上受到限制。布隆过滤器通过引入轻微的精度损失（由误报引起）来替代哈希表。误报率与插入布隆过滤器的元素数量相关。实际中，当插入1,000个顶点时，约300个32位整数大小的布隆过滤器误报率低于1%，因此精度损失是可以忽略的。假设数据点的键也是一个32位整数，布隆过滤器方案比哈希表至少减少3倍的内存占用。

## 4.3. Bounded Priority Queue Optimization  

搜索算法（算法1）维护了一个优先队列，用来存储当前找到的查询的top-k候选项。在CPU上将优先队列实现为二叉堆是高效的。然而，直接将二叉堆移植到GPU上会遇到问题。二叉堆的实现会消耗无界内存——在搜索过程中，我们会不断地将顶点添加到队列中，队列$q$的大小可能会远远超过$K$。无界内存分配对GPU性能来说是灾难性的。为了有效利用GPU存储$q$，我们有以下观察：

---

**观察1**：图搜索算法（算法1）只使用优先队列$q$中的前$K$个元素。

---

**证明**：当$q$的大小增长到$K+1$时，记$q$中的第$K+1$个元素为$x$。如果在后续的迭代中不会从$q$中提取$x$，那么$x$就不会包含在top-$K$的结果中。否则，我们会在后续的某个时刻从$q$中提取$x$，此时top-$K$列表已经被$K$个元素填满。同时，由于至少有$K$个处理过的元素比$x$更好，算法1第4行的条件得以满足——算法退出，且$x$不会包含在top-$K$的结果中。

---

因此，当$q$的大小增长到$K+1$时，我们可以将表现较差的候选项弹出。图4(a)展示了这一优化的迭代过程。与原始算法（图1）相比，有限优先队列优化消除了对$5, 13, 14$的插入，并将$q$的大小限制在3个元素以内。

---

我们实现了GPU版本的对称最小-最大堆 [3] 作为有限优先队列。插入和弹出$\mathrm{min}/\mathrm{max}$元素的操作都在对数时间内完成。

# 4.4. Selected Insertion Optimization  

由于数据结构的维护主要受内存带宽和延迟的影响，当数据结构的内存占用减少时，此阶段可以得到改进。哈希表和布隆过滤器的内存消耗与插入的数量成正比。我们提出了选择性插入优化，以减少对`visited`的插入操作，从而降低内存消耗。

---

在原始算法中（算法1的第12行），在计算完顶点到查询点的距离后，该顶点被标记为已访问。我们建议在插入之前进行选择：距离大于当前topk中所有$K$个元素的顶点会被过滤掉——只有当顶点是当前距离查询点最近的$K$个顶点之一时，才将其标记为已访问并推入$q$。

---

图4(b)展示了在(a)中有限优先队列优化基础上应用选择性插入的示例。在第$3^{rd}$次迭代中，顶点13和14没有被标记为已访问，也没有被推入$q$，因为在topk中已有$K$个候选项，而13和14的距离比所有topk中的顶点更远。在第$4^{th}$次迭代中，顶点6同样没有被插入$q$或标记为已访问，因为4比topk中的所有候选项更差。由于被过滤掉的顶点未被标记为已访问，未来迭代中处理其邻居时可能会再次计算其距离。请注意，在迭代过程中，topk优先队列中的top-$K$候选项会越来越接近查询点。因此，被过滤掉的顶点在将来的迭代中仍会被过滤。通过这种选择性插入优化，搜索算法的正确性和完整性得以保持——没有顶点会在哈希表或优先队列中多次插入。选择性插入方法减少了插入次数和GPU内存使用，代价是一些距离可能需要多次计算。在这个例子中，最终的`visited`大小为6，比图4(a)少存储了4个元素。距离计算可以完全并行化，而哈希表和优先队列的维护是顺序的。因此，这种计算-空间的权衡能够提高查询性能。

## 4.5. Visited Deletion Optimization  

我们遵循选择性插入的想法，并更进一步来激进地节省更多的GPU内存——为了确保搜索算法的正确性，我们只需要将`visited`作为一个哈希表来显示顶点是否在优先队列$q$和`topk`中。具体而言，当顶点从优先队列$q$中提取并处理后，如果该顶点没有更新`topk`，我们可以将其从`visited`中删除。此外，当`topk`更新时，被弹出的顶点也可以从`visited`哈希表中删除。其直觉与选择性插入优化相似：被删除的顶点（逻辑上重新标记为未访问）与当前top- $K$候选者的距离较大——它们不会在未来的迭代中更新top-$K$候选者或被插入优先队列$q$。图4(c)展示了一个示例。当应用`visited`删除优化时，`visited`哈希表的大小恰好等于$q$（最大为$K$）和`topk`（最大为$K$）的并集。因此，`visited`的大小被限制为$2K$。

---

`visited`删除优化要求对`visited`进行删除操作。哈希表的删除操作可以在常数时间内完成。我们的哈希表替代方案——布隆过滤器——不支持删除，因此我们选择了布谷鸟过滤器（Cuckoo filter）作为哈希表的概率数据结构替代方案，以验证`visited`删除优化。

---

与图1中的原始算法和图4(c)中的运行示例相比，可以观察到，通过应用我们的三种优化，我们使用了近$50\%$更少的内存。在实际应用中，搜索过程需要数百次迭代，这比这个玩具示例节省了更多的内存。

# 5. CANDIDATE LOCATING  

**基础候选位置确定**。在候选位置确定阶段，warp中的一个线程（假设是线程0）负责提取距离查询点最近的顶点ID，并将当前未访问的相邻顶点添加到候选列表中。在图3中，2、4、5、7这些顶点被定位并存储在候选列表中，以便进入距离计算阶段。由于该阶段简单且不涉及复杂计算，一个线程即可高效处理该任务。

---

**多查询warp处理**。在基本的候选定位方法中，当线程0从优先队列中提取顶点时，其他线程是空闲的。为提高线程利用率，我们可以在一个warp中处理多个查询。例如，假设我们在一个warp中处理4个查询，我们为每个查询构建优先队列和哈希表。4个活跃线程（假设是线程0、1、2和3）分别从其对应的优先队列中提取顶点ID。虽然有更多活跃线程，但缺点是我们必须为每个查询创建一套单独的数据结构（优先队列和哈希表）。在warp中选择最佳的查询数量并不明确，实验评估部分（第VIII-C节）中将讨论这一问题。

---

**多步探测**。最近的GPU BFS研究[44]、[45]建议了一种并行扩展相邻顶点的策略。在我们的图搜索问题中，它对应于从优先队列中提取多个顶点，而不是仅提取第一个顶点。多步探测会向候选列表中填充更多顶点。然而，与一般的BFS问题不同的是，在邻近图上进行搜索通常在少量步骤内沿着靠近查询点的方向进行。因此，当前处理顶点的相邻顶点更可能位于优先队列的前列。多步探测可能会在次优候选点上浪费探测的内存访问和距离计算。其效果将在第VIII-C节中进行评估。

# 6. BULK DISTANCE COMPUTATION  

我们现在解决图基ANN方法中繁重距离计算的问题。**批量距离计算阶段**将候选列表中的顶点作为输入，从数据集中获取对应的数据，计算它们到查询点的距离，并将结果输出到共享内存的数组中（见图3）。在这一阶段，块中的所有线程都会参与进来——每个线程负责计算部分维度的距离。随后，线程0通过`shfl down` warp规约将所有warp的部分距离聚合为一个值。聚合后的距离会存储在候选集中的第$i$个元素的`dist_i`中。

---

与简单地为每个候选点并发计算距离相比，上述的并行策略对缓存更友好。在所提出的并行规约中，32个线程被组织起来访问连续的内存地址。如果并发处理候选项，每个线程的内存访问模式是独立的，这会生成更多的缓存未命中。

---

这种并行规约距离计算方法可以应用于常见的ANN距离度量方法，例如p-范数距离、余弦相似度和内积等。

# 7. OUT-OF-GPU-MEMORY DATASETS  

在本节中，我们讨论了解决**GPU内存不足**情况下处理数据集的问题。这个问题在存储高维数据时尤为紧迫。通常情况下，图索引的大小要小得多——它的大小与度数×数据点数量成比例，其中度数是图索引的度数限制，数据点数量是数据集中的点的数量。经验表明，使用16作为度数是足够的——对于数百万个数据点，图索引的大小通常不到1GB。例如，8百万个784维数据点的16度数图索引大小为988 MB，而数据集的大小为24 GB。为了利用GPU加速图搜索，我们需要减少数据集的大小以便将其存储在GPU中。为此，我们采用了随机哈希技术，将高维数据编码为一个位向量（bit vector），这样哈希后的数据集可以适应GPU内存，并且距离计算在低维位上进行。

---

**1-bit随机投影**。在众多概率哈希方法中，本文引入了一种名为“1-bit随机投影”的常用方法。形式上，对于两个数据向量$u, v \in \mathbb{R}^d$，我们生成一个随机向量$r \in \mathbb{R}^d$，其条目服从独立同分布的标准正态分布。然后$\operatorname{Pr}(\operatorname{sgn}(<u, r>)=\operatorname{sgn}(<v, r>)))=1-\frac{\theta(u, v)}{\pi}$，其中$\theta(u, v)$是$u$和$v$之间的角度。如果$r$的条目是从独立同分布的柯西分布中采样的，那么这种碰撞概率与$\chi^2$相似度密切相关。通过使用$h$个独立的随机向量，每个数据点被映射为一个$h$位的向量。位向量之间的汉明距离成为原始数据相似度的一个很好的估计（如果$h$不是太小的话）。在实现中，$h$可以设置为32的倍数，这样位向量可以存储为几个32位无符号整数。这样，位向量的内存占用等于$h / 32$个单精度浮点数的空间。我们在Section VIII-H中研究了哈希在GPU内存不足数据集场景中的性能。

---

虽然本文没有研究，但其他技术如分片（sharding）也适用于这种可扩展性挑战。例如，当考虑多GPU时，我们可以将数据分片给每个GPU，为每个分片构建图索引，在每个GPU上执行图搜索并合并结果。

# 8. EXPERIMENTS  

在本节中，我们对6个实际数据集进行了详细分析，以评估所提出系统的有效性。

---

**实现**。我们使用C++11开发了SONG的原型，并用g++-6.4.0编译，启用了“O3”优化。GPU的CUDA代码则使用CUDA 10.0中的nvcc进行编译，并启用了“-Xptxas -O3”优化。SONG加载了由NSW[46]生成的预构建图索引。我们选择NSW是因为它是一种通用且灵活的邻近图构建算法，其他图构建算法也可以适应我们的加速框架。

---

**硬件系统**。实验在一台服务器上进行。该服务器配有一颗Intel Xeon E5-2660处理器（64位）——8核心16线程，内存为128GB，操作系统为Ubuntu 16.04.4 LTS 64位。使用的GPU为NVIDIA TESLA V100。

---

**GPU内存层次结构**。GPU的L1缓存是可配置的：我们可以将L1缓存的一部分分配为共享内存。L1缓存的延迟较低，带宽高于GPU的全局内存。然而，L1缓存的容量有限，每个SM（Streaming Multiprocessor）为96KB。我们从L1缓存中分配共享内存来存储搜索候选者、top-k结果、工作查询点以及批量计算的距离等数据结构。这些数据结构在图搜索算法中经常被访问，并且它们的大小是有界的，适合放入L1缓存。访问哈希表的大小如果不进行任何优化可能会超出L1缓存的容量，因此需要将其存放在全局内存中。通过选定插入和已访问删除优化，哈希表的大小可以根据搜索参数K进行约束，从而能够将其存储在共享内存中，加速哈希表的探测和更新。数据集和图索引无法放入L1缓存，因此存储在全局内存中。

---

**比较算法**。比较的算法包括HNSW（CPU上的最先进的ANN方法）和Faiss（用于大规模数据的顶级GPU ANN系统）。其他研究[4]，[36]表明，基于树或哈希的方法性能较差，因此我们未将其纳入本研究的对比中。我们从GitHub上获取了Faiss和HNSW的代码，特别是HNSW的多种实现中，我们选择了NMSLIB的实现（在ANN Benchmark中表现最佳）。为了公平比较，我们在一个精细网格上调整了HNSW和Faiss的参数，最终选取它们的最佳结果进行对比。

---

**方法**。每个算法的索引都是预先构建的，索引构建时间不计入实验中。特别地，SONG不生成自己的索引，而是使用与NSW相同的图索引（类似于HNSW，但没有分层结构）。我们运行HNSW时使用单线程，因为HNSW支持查询间的并行处理，我们可以假设HNSW的性能可以线性扩展到线程数。对于Faiss和SONG，我们在一台单GPU上批量执行查询。类似于HNSW，Faiss和SONG也可以通过多GPU卡实现线性扩展。与单线程的HNSW对比，可以让我们看出一块GPU可以替代多少CPU线程。性能评估通过查询时间（每秒查询数）和检索质量（召回率）进行。

---

**查询时间**。我们测量每个算法的实际运行时间，并将回答的每秒查询数（查询吞吐量）作为执行时间的度量标准。我们选择每秒查询数作为指标，而不是查询批次的执行时间，因为每秒查询数可以在不对查询批次大小进行标准化的情况下进行比较。所有实验至少执行3次，结果为平均值。

---

**检索质量**。召回率是ANN算法广泛使用的检索质量度量标准。假设算法返回的候选点集为$A$，查询的正确K近邻集合为$B$，则召回率定义为：$\operatorname{Recall}(A)=\frac{|A \cap B|}{|B|}$。较高的召回率意味着与正确近邻结果的逼近程度更高。

---

数据。我们在6个ANN基准数据集上进行了实验：NYTimes、SIFT、GloVe200、UQ_V、GIST和MNIST8m。数据集的规格如表I所示。测试数据集的维度从128（SIFT）到960（GIST）不等，数据点数量从290,000（NYTimes）到8,090,000（MNIST8m）不等。MNIST8m是最大的一个数据集（24 GB），NYTimes则是最小的（301 MB）。数据集的分布也有差异——NYTimes和GloVe200分布较为偏斜，而SIFT、UQ_V、GIST和MNIST8m的偏斜程度较小。我们使用MNIST8m研究GPU内存不足场景下的表现。

## 8.1. ANN Search Performance Comparisons  

SONG、Faiss 和 HNSW 的 ANN 性能比较结果如图 5 所示。表 II 列出了相对于 Faiss 的详细加速结果。

图 5：SONG、Faiss 和 HNSW 在五个数据集上的性能比较。NYTimes 数据集展示了 top-1、10、50 和 100 的结果，其他数据集展示了 top-10 和 top-100 的结果。纵轴为每秒查询数（QPS），使用对数尺度。在每张图中，曲线越靠近右上角，性能越好。

---

表 II 显示了在中等召回率（0.5）到高召回率（0.95）之间，SONG 相对于 Faiss 的加速比。SONG 在相同召回率下比 Faiss 快 4.8 到 20.2 倍。此外，Faiss 无法在 GloVe200、NYTimes 和 GIST 数据集上达到高召回率。与单线程的 HNSW 相比，SONG 的速度提升约为 50-180 倍，这意味着使用一块 GPU 加速的 SONG 可以在 16 线程 CPU 服务器上比 HNSW 提升约 3-11 倍的速度。

表 II：在中等召回率（0.5）到高召回率（0.95）之间，top-10 的 Faiss 加速比。N/A 表示 Faiss 无法达到给定的召回率。这与之前的研究[4]、[6]、[19]、[36] 和 [61] 的结论一致。top-1 的加速比被省略，但趋势相似，通常会有略高的加速比。

---

在 NYTimes 和 GloVe200 上进行 ANN 搜索较为困难——数据点的分布较为偏斜和聚集。在低召回率范围内（召回率 <60%），Faiss 的性能与 SONG 竞争力相当。作为一种基于量化的方法，Faiss 的性能受其生成的量化码的质量限制。在 NYTimes 和 GloVe200 数据集上，随着召回率的提高，Faiss 的查询速度显著下降。通过增大搜索优先队列的大小，SONG 可以达到 95% 以上的召回率。与此同时，当召回率约为 80% 时，SONG 比 HNSW 快约 100 倍。SIFT 和 UQ_V 数据集对 ANN 搜索非常友好——由于数据集的分布较为均匀，ANN 方法可以迅速找到查询点的邻近点。SONG 在 SIFT 数据集上使用优先队列大小为 100 时可达到 99% 的召回率。GIST 是这五个数据集中维度最大（960 维）的数据集。Faiss 与 SONG 的性能差距较小，因为 Faiss 的量化方法将高维数据编码成较短的代码，因此 Faiss 需要执行的计算较少。尽管如此，由于 SONG 的大规模并行距离计算，SONG 仍然优于 Faiss。在大多数召回率范围内，SONG 比 HNSW 快约 180 倍。

---

**Top-$K$ 最近邻**。随着 $K$ 的增加，所比较算法的趋势一致——图中的曲线向左移动。直观地看，找到 99% 的 top-1 最近邻比找到 top-10 所有正确候选项的 99% 更容易。通过探索相同的搜索空间，问题越困难，召回率下降越明显。

---

**索引内存大小**。表 III 展示了索引内存大小的比较。由于图结构的复杂性，SONG 的图索引消耗的内存比 Faiss 的倒排索引更多。但这一（相对较小的）差异对于 GPU 内存容量来说是可以接受的。

---

**相对于 HNSW 的加速**。我们在图 6 中列出了 SONG 对 HNSW 的加速比，针对 top-10 和 top-100 的结果。对于 SIFT 和 GloVe200，在大多数召回率下，加速比为 50 到 100。NYTimes 是一个有趣的案例：通过我们的优化，SONG 使用更少的内存，在 NYTimes 数据集上，随着召回率的增大，SONG 的每秒查询数下降得比 HNSW 更慢，因此 SONG 的加速比在更大召回率时不断增加。由于 GIST 具有更多的维度，SONG 在 GIST 数据集上的加速比相比 SIFT 和 GloVe200 更为显著——SONG 在更多维度上有更多机会并行距离计算。

## 8.2. Selected Insertion and Visited Deletion Optimizations  

图 7 描述了我们提出的选择性插入和访问删除优化在哈希表及其替代方案中的表现。我们对比了基础哈希表 (SONG-hashtable)、带选择性插入的哈希表 (SONG-hashtable-sel)、带选择性插入和访问删除的哈希表 (SONG-hashtable-seldel) 以及两种概率数据结构替代方案 (SONG-bloomfilter 和 SONG-cuckoofilter) 的性能。

---

对于 SIFT 数据集，启用了选择性插入和访问删除优化的哈希表表现最佳。由于在 SIFT 中我们只需较小的优先队列即可达到较高的召回率，选择性插入对基础哈希表方案的性能提升不显著。但访问删除优化在 SIFT 上表现良好。Bloom 过滤器和 Cuckoo 过滤器方案位于基础哈希表和 hashtable-sel-del 之间，尽管性能不如 hashtable-seldel，但它们消耗的 GPU 内存更少，因此在 GPU 内存不足时可以作为替代解决方案。

---

在数据分布较为"不友好"的 NYTimes 数据集中，为了达到较高的召回率，我们必须将优先队列的大小增至数千。我们观察到类似的趋势。此外，由于哈希表删除操作的开销，hashtable-sel 在初期表现优于其他方案。然而，当召回率达到约 81% 时，它耗尽了内存，性能显著下降。而 hashtable-sel-del 使用的内存更少，因此成为其他方法中最快的解决方案。两种概率数据结构在高召回率区域的表现也很有竞争力，因为它们消耗的内存更少。

## 8.3. Searching Parameter  

本节我们研究了候选定位阶段的多种搜索参数的有效性。在实验中，我们在设置其他参数为 1 的情况下，分别改变一个搜索参数，并将哈希表替代方案固定为 hashtable-sel-del。由于页面限制，我们仅展示了部分实验结果，省略的实验呈现类似趋势。

---

**多查询在一个 warp 中**。如图 8 所示，我们在 warp 中分别处理 1、2 和 4 个查询。尽管在一个 warp 中处理更多查询时活跃线程数增加，但查询性能却下降了。候选定位阶段的主要时间消耗在于从全局内存加载图数据——这是受内存带宽限制的，而不是计算限制。因此，更多的活跃线程并未提高性能。访问图的多个部分使得内存访问模式变得更加不可预测。同时，在 warp 中处理多个查询还会构建优先队列和哈希表的多个副本——这消耗了更多的 GPU 内存。因此，当在 warp 中解决多查询问题时，观察到了较差的性能表现。

---

**多步探测**。图 9 展示了多步探测的效果。作为 GPU BFS 中常见的并行解决方案，多步探测在 ANN 图搜索中并未提高性能。原因在于当前处理的顶点的邻居很可能是优先队列的头部。多步探测在不相关的候选节点上浪费了内存访问和距离计算。在高召回率范围内，性能差距较小，因为我们需要探测很多步才能找到非常准确的最近邻候选节点——在这种情况下，多步探测并没有浪费操作。

## 8.4. Where Does The Time Go?  

我们在图 10 中分析了 GloVe200 和 GIST 上各组件消耗的时间百分比。

---

**数据传输开销**。为了使用 GPU 处理查询，我们首先需要将查询数据从 CPU 内存（主机）传输到 GPU 卡（设备）。查询在 GPU 上完成后，我们需要将存储在 GPU 上的结果复制回 CPU 内存。这两个内存传输分别称为 HtoD 和 DtoH。图 10 左侧展示了数据传输和内核执行的时间分布。可以观察到，内核执行占据了主要的执行时间（在 GloVe200 上超过 96%，在 GIST 上超过 89%）。由于 HtoD 内存传输消耗的时间是常量，随着内核执行变得更加耗时（例如，使用更大的优先队列时），HtoD 所占的百分比会下降。另一方面，设置更大的 K 值会返回更多的候选项，因此 DtoH 的时间百分比略有增加。

---

**三个阶段的时间分布**。图 10 右侧显示了候选定位、批量距离计算和数据结构维护的时间分布。在 GloVe200 和 GIST 上，数据结构维护占据了主要的执行时间。由于 GloVe200 有 200 个维度，而 GIST 有 960 个维度，GIST 上的距离计算比 GloVe200 的时间多 8%-20%。然而，GIST 的维护时间占比更大。在 GloVe200 上，选择性插入过滤了更多的顶点——为了节省 GPU 内存和数据结构维护成本，需要进行更多的距离计算。  

## 8.5. Query Batch Size  

图 11 展示了在不同批处理大小下的查询性能表现。我们从 SIFT 查询集中抽取了 100 和 1000 个查询来构建小批处理。为了研究大批处理情况下的查询性能，我们将 SIFT 查询集扩展到 10 万和 100 万个查询。正如预期的那样，当批处理数量增大时，每秒查询量 (QPS) 也随之增加。当批处理数量较小时，CPU 内存与 GPU 之间的数据传输开销变得不可忽视。此外，GPU 上的成千上万核无法在少量查询时得到充分利用。随着批处理数量的增大，性能逐渐提升，因为传输开销被分摊到批处理中的查询数量上，并且有足够的查询来充分利用 GPU 的大规模并行计算能力。当批处理量达到 10 万时，每秒查询量达到了峰值。进一步增大批处理量（到 100 万）不再提高性能。之前的实验中批处理大小为 1 万——使用更大的批处理时，SONG 相对 HNSW 的加速效果会更明显。

## 8.6. Generalization to Other Graph Methods  

SONG 是一个用于图方法的通用 GPU 框架，能够加速基于图的搜索算法。除了 HNSW，SONG 还可以应用于其他基于图的方法。这里我们以导航扩展图 (NSG) 为例展示泛化能力。我们提取了 NSG 构建的图索引，并使用 SONG 在提取的 NSG 索引上执行查询。如图 12 所示，在高召回率（>0.8）下，SONG 相对 NSG 提供了 30-37 倍的加速效果。

## 8.7. Performance on Various GPUs  

图 13 展示了 SONG 在不同 GPU 上的性能表现。我们对比了 3 款 GPU：NVIDIA TESLA V100（5120 核心，32 GB 内存）、NVIDIA TESLA P40（3840 核心，24 GB 内存）和 NVIDIA TITAN X（3584 核心，12 GB 内存）。SONG 在不同 GPU 上的性能趋势一致——图中的趋势线说明了这一点。不同 GPU 性能差异的趋势与它们的计算能力成正比。 

## 8.8. Out-of-GPU-Memory Dataset  

图 14 中我们研究了哈希降维在超出 GPU 内存数据集上的效果。我们使用 TITAN X 来展示 GPU 内存不足的场景，因为 TITAN X 是我们使用的 3 个 GPU 中内存最小的（12 GB）。MNIST8m 数据集（24 GB）无法直接适配 TITAN X 的 GPU 内存。我们首先从 MNIST8m 中抽取了 100 万个数据点，验证哈希方法的性能。左图部分显示了，使用 128 位哈希的数据集的查询性能与原始的 784 维数据非常接近。在召回率小于 0.9 的范围内，哈希数据集的性能优于原始数据集，因为哈希后的低维数据集在距离计算上更快——计算的维度远低于原始的 784 维。

---

表 4 显示了哈希数据集的大小。使用哈希后，数据集的大小变小了数百倍，可以适应 GPU 内存。例如，128 位哈希使原始数据集缩小了 190 多倍，同时保持了与原始数据集接近的查询性能。应用哈希技术后，MNIST8m 可以适配 GPU 内存。图 14 显示了 MNIST8m 哈希版本与 MNIST1m 的一致性趋势。

## 8.9. CPU Implementation of SONG  

除了 GPU 优化之外，我们还实现了一个 heavily 优化的 SONG CPU 版本。如图 15 所示，我们的 CPU 实现性能优于 NYTimes 和 UQ_V 上的 HNSW。

# 9. RELATED WORK  

**ANN方法**：Flann [51] 是基于复合树算法的近似最近邻（ANN）库。Annoy 基于二叉搜索森林。FALCONN [1] 是一种多探针哈希近似最近邻方法。作为树型和哈希算法的代表实现，它们的性能逊色于基于图的ANN方法 [19]。在基于图的ANN方法中，HNSW [47] 基于层次化图结构，DPG [42] 基于从kNN图中选取的无向图。NSG [19] 只有一个包含导航节点的图，搜索总是从该节点开始。它们共享相同的ANN图搜索方法，而SONG能够加速图结构ANN家族中的大多数算法。Faiss [36] 是最快的基于GPU量化的ANN库，而我们的GPU ANN系统是基于图结构的。

---

**GPU图搜索**：GPU图搜索的数据布局在[37], [38], [48], [53]–[55]中被研究过。它们通过划分或流式处理图数据，使其能够适应GPU内存。在我们的ANN搜索应用中，我们假设GPU内存充足。iBFS [44], GunRock [62], Enterprise [45], in-cache query [28] 和 GTS [38] 构建了多个前沿并同时进行搜索。我们的ANN图搜索不同于普通的BFS——我们从优先队列中提取顶点进行下一次迭代。Virtual Warp [29], CuSha [37], Fine Par [65] 和 MapGraph [20] 提出了减少warp发散的算法。在我们的ANN应用中，搜索架构必须重新设计以计算高维距离。

---

**最大内积搜索（MIPS）**：MIPS问题近年来引起了广泛关注，研究人员和实践者发现了广泛的相关应用，例如，通过考虑每个向量的权重（如广告的出价值）来使用ANN匹配用户与广告 [15]。最新的MIPS方法[67]已采用SONG作为基础算法。

# 10. CONCLUSIONS  

在本文中，我们介绍了SONG——一个在GPU上执行基于图的近似最近邻（ANN）搜索系统。我们展示了一种新的框架，将图搜索算法解耦为三个阶段，以并行化高维距离计算。我们提出了一系列适用于GPU ANN图搜索的数据结构和优化方案，并展示了选定插入和已访问删除优化，以减少GPU内存消耗。我们通过实验评估了SONG，并与HNSW和Faiss在六个实际数据集上进行了比较。结果证实了SONG的有效性，其速度比单线程HNSW快50-180倍，同时大幅超越了Faiss的性能。