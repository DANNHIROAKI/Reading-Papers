## 3.4. 并非所有质心对每个查询都重要

我们==进一步假设，对于给定查询，段落嵌入簇中的一个小子集往往比其他簇在确定相关性方面更为重要==。如果确实如此，那么我们可以优先计算这些高度加权的质心，并==舍弃其余的质心==，因为我们知道它们对最终排名贡献不大。我们通过随机抽样15个MS MARCO v1查询并绘制所有查询词项中每个质心的最高相关性得分的经验累积分布函数（eCDF），如图4所示，以测试该理论。我们发现有一小部分高度加权的质心，其相关性得分的幅度明显高于所有其他质心。尽管图4中未显示，我们在LoTTE池查询中重复了此实验，发现了非常相似的得分分布。

# 4. PLAID

图5展示了PLAID评分管道，该管道包括多个连续阶段，用于检索、过滤和排序。第一阶段通过计算每个质心相对于查询嵌入的相关性得分来生成初始候选集。在中间阶段，PLAID使用质心交互和质心剪枝的创新技术来积极且有效地过滤候选段落。最后，PLAID使用完全重构的段落嵌入对最终候选集进行排名。以下是对这些模块的详细讨论。

![img](https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241211204041524.png) 

## 4.1. 候选生成

给定查询嵌入矩阵 $Q$ 和索引中的质心向量列表 $C$，PLAID通过矩阵乘法计算词级查询-质心相关性得分 $S_{c, q}$：

- $S_{c, q}=C \cdot Q^T$

然后标识对于每个查询词项“接近”前 $t$ 个质心的段落作为初始候选集。如果段落中的一个或多个词项在索引期间通过 $k$-均值聚类分配到该质心，则视为该段落接近该质心。这个值 $t$ 在原始ColBERTv2中被称为nprobe，我们在PLAID ColBERTv2中保留了这一术语。

---

PLAID ColBERTv2的初始候选生成与对应的原始ColBERTv2阶段在两个关键方面有所不同。首先，虽然原始ColBERTv2保存了一个倒排列表，将质心映射到其对应的嵌入ID，但PLAID ColBERTv2将倒排列表结构化为一个从质心到对应唯一段落ID的映射。与存储嵌入ID相比，存储段落ID更有优势，因为段落数量远少于嵌入数量，意味着倒排列表总体上需要存储更少的信息。这也使PLAID ColBERTv2能够在倒排列表中使用32位整数，而非可能的64位长整型。在实际应用中，这在MS MARCO v2 [6] 倒排列表中带来了$2.7 \times$的空间节省（从71 GB减少到27 GB，拥有1.4亿段落）。

---

其次，如果初始候选集过大（由超参数ncandidates指定），原始ColBERTv2会通过对候选嵌入向量子集（特别是倒排列表中质心ID映射到嵌入ID的嵌入）进行评分和排名来修剪候选集，使用完整的残差解压缩，这在§3.2中已讨论是非常耗费资源的。相比之下，PLAID ColBERTv2不对初始候选集大小施加任何限制，因为后续阶段可以通过质心交互和剪枝来廉价地过滤候选段落。

## 4.2. 质心交互

质心交互通过在标准MaxSim公式中使用每个词的最近质心来廉价地近似每个段落的相关性。通过将质心交互作为额外的过滤阶段，评分管道可以跳过对大量候选段落的昂贵嵌入重构过程，从而显著加速端到端检索。直观地说，质心交互使PLAID能够模拟传统的词袋检索，其中质心相关性得分承担了BM25等系统中词项相关性得分的角色。然而，由于PLAID使用的是向量表示（特别是查询的向量），因此PLAID在查询时计算质心相关性得分，而不是传统的预计算词项相关性得分。

---

该过程如下所示。回想一下公式2中的 $S_{c, q}$ 储存了每个质心相对于查询词项的相关性得分。假设 $I$ 是映射到候选集中每个词项的质心索引列表。此外，设 $S_{c, q}[i]$ 表示 $S_{c, q}$ 的第 $i$ 行。那么PLAID构建基于质心的近似得分 $\tilde{D}$ 如下：

- $\tilde{D}=\left[\begin{array}{c}S_{c, q}\left[I_1\right] \\ S_{c, q}\left[I_2\right] \\ \ldots \\ S_{c, q}\left[I_{|\tilde{D}|}\right]\end{array}\right]$ 

然后，为了使用 $\tilde{D}$ 对候选段落进行排序，PLAID计算 $\operatorname{MaxSim}$ 得分 $S_{\tilde{D}}$ 如下：

- $\displaystyle{}S_{\tilde{D}}=\sum_i^{|Q|} \max _{j=1}^{|\tilde{D}|} \tilde{D}_{i, j}$  

从 $S_{\tilde{D}}$ 中选出的最相关的前 $k$ 个段落构成过滤后的候选段落集。

---

PLAID包含了优化的内核以高效部署质心交互（以及更广泛的MaxSim操作）；我们将在$\S 4.5$中讨论这些内容。

## 4.3. 质心剪枝

作为另一项优化，PLAID利用了$\S 3.3$中的观察，在构建 $\tilde{D}$ 之前首先修剪低幅度的质心得分。在此过滤阶段，PLAID仅对其最大对应质心得分达到给定阈值 $t_{c s}$ 的词项进行评分。具体来说，$\tilde{D}$ 只包含满足以下条件的词项：

- $\displaystyle{}\max _{j=1}^{|Q|} S_{c, q_{i, j}} \geq t_{c s}$ 

---

我们引入超参数ndocs来表示第二阶段选择的候选文档数。我们通过实验发现，从第三阶段选择 $\frac{\text { ndocs }}{4}$ 个候选项可以获得良好的结果；我们在$\S 5$中展示的所有结果均使用了这一启发式方法。

## 4.4. 评分

与原始ColBERTv2一样，PLAID将通过残差解压缩重构最终候选段落集的原始嵌入，并使用MaxSim对这些段落进行排序。设 $D$ 为解压后最终候选集的嵌入向量，那么最终得分 $S_{q, d}$ 按照公式1计算。

第$\S 4.5$节讨论了==用于加速MaxSim和解压操作的快速内核。==

## 4.5. 快速内核：无填充的MaxSim和优化的解压缩

图2a显示，==索引查找操作是原始ColBERTv2的主要开销来源之一==。导致这些查找成本高的原因之一是，它们需要通过填充附加的最大段落长度维度来调整2D索引张量的形状，使其成为3D张量，从而在不规则的词项列表上实现批量MaxSim操作。为避免这种填充，我们实现了自定义的C++代码，直接在打包的2D索引张量上计算MaxSim得分（即将不同长度的多个2D子张量沿同一维度连接的张量）。我们的内核遍历每个段落对应的词项向量，以针对每个查询词项计算每段落的最大得分，然后在所有查询词项上汇总每段落的最大得分。这种设计易于在段落间并行化，并且通过为每个查询词项分配一个输出向量来存储最大得分并在原位更新该向量，实现了 $O(|Q|)$ 的每线程内存使用。而基于填充的方法则需要 $O(|D| \cdot |Q|)$ 的空间。我们已将此设计整合进优化的质心交互实现中以及最后的MaxSim操作（图5中的第4阶段）。PLAID仅在CPU上实现了这些内核，对应的GPU内核尚待开发。

---

ColBERTv2的残差解压缩方案计算出一组质心向量，确定来自这些质心的一组 $2^b$ 个可能的增量，然后存储每个嵌入向量对应的增量索引。具体来说，每个压缩的8位值存储 $\frac{8}{b}$ 个范围在 $\left[0,2^b\right)$ 内的索引。ColBERTv2在残差解压缩中产生了显著的开销，如图2a所示。这部分原因是由于解压缩的原始实现比较简单，需从压缩表示中显式解开比特并执行昂贵的位移和求和操作以恢复原始值。为优化此过程，PLAID预先计算出所有可能的 $2^8$ 索引列表，并将这些输出存储在查找表中，使得解压缩函数只需从查找表中检索索引而无需手动解开比特。我们为此查找表解压缩实现了CPU和GPU上的优化。GPU实现使用了自定义CUDA内核，为压缩的残差张量中的每个字节分配一个独立的线程（线程块大小计算为 $\cfrac{b \cdot d}{8}$，对于$d$ 维的嵌入向量）。CPU实现则以单个段落为单位进行并行解压。

# 5. 评估

我们的评估旨在回答以下研究问题：
(1) PLAID如何影响跨IR基准的端到端延迟和检索质量？（§5.2）
(2) PLAID的每项优化对性能提升有多大贡献？（§5.3）
(3) PLAID在语料库大小和并行度方面的扩展性如何？（§5.4）

## 5.1 设置

**PLAID 实现**. PLAID 引擎包括质心交互以及残差解压缩优化。我们以模块化的方式将 PLAID 作为对 ColBERTv2 基于 PyTorch 的实现的扩展，特别是在其搜索组件上。对于 CPU 执行，我们将质心交互和解压缩操作完全实现为多线程 C++ 代码。对于 GPU，我们在 PyTorch 中实现了质心交互，并提供了用于快速解压缩的 CUDA 内核。总体而言，PLAID 包括大约 300 行额外的 Python 代码和 700 行 C++ 代码。

---

**数据集**. 我们的评估包括四个不同的IR基准的数据集，详见表1。我们在 MS MARCO v1 和 Wikipedia Open QA 基准上执行域内评估，这些任务中的检索器经过了专门训练；我们在基于 StackExchange 的 LoTTE (Santhanam等人, 2021) 和 TREC 2021 深度学习赛道 [6] MS MARCO v2 基准上执行域外评估，其中 ColBERTv2 检索器 [42] 在 MS MARCO v1 上训练。对于 Wikipedia 的评估，我们使用 2018 年 12 月的转储 [18] 和 NaturalQuestions (NQ) 数据集 [23] 中的查询。LoTTE [42] 评估使用的是“pooled”开发数据集，带有“search”风格的查询。对于 MS MARCO v2，我们使用数据的增强段落版本 [2]，包含段落标题，但忽略标题内容。由于我们评估了模型的多个配置，因此我们所有的评估都是使用开发集查询进行的。

---

**系统和超参数**. 我们报告了多个系统的端到端结果：原始 ColBERTv2 和 PLAID ColBERTv2 以及 ColBERT (v1) [22]、BM25 [41]、SPLADEv2 [10] 和 DPR [18]。对于原始 ColBERTv2，我们使用 ColBERTv2 论文中针对每个基准数据集报告的特定超参数。在结果表中，我们用 $p$（nprobe）和 $c$（ncandidates）表示这些超参数。对于 PLAID ColBERTv2，我们评估了三种不同的设置： $k=10$、$k=100$ 和 $k=1000$。参数 $k$ 控制最终评分的文档数量以及第4节中描述的检索超参数。表2列出了每个 $k$ 设置的超参数配置。我们通过实验证明，对最终评分阶段的文档进行 $\frac{\text { ndocs }}{4}$ 的排序可产生较强的结果。对于原始 ColBERTv2 和 PLAID ColBERTv2，我们将所有数据集压缩至每维2位，除 MS MARCO v2 外将其压缩至1位。

---

**硬件**. 我们在配置了28个Intel Xeon Gold 6132 2.6 GHz CPU核心（每核2个线程，总计56个线程）和4个NVIDIA TITAN V GPU的服务器上进行所有实验。每台服务器有两个NUMA插槽，具有大约92 ns的插槽内内存延迟、142 ns的跨插槽内存延迟、72 GBps的插槽内内存带宽和33 GBps的跨插槽内存带宽。每个TITAN V GPU具有12 GB的高带宽内存。

---

**延迟测量**. 在测量端到端结果的延迟时，我们计算所有查询的平均延迟（查询总数见表1），并报告三次试验中的最小平均延迟。对于其他结果，我们在相关章节中描述了具体的测量过程。我们丢弃神经模型的查询编码延迟（ColBERTv1 [22]、原始ColBERTv2 [42]、PLAID ColBERTv2 和 SPLADEv2 [10]），遵循 Mackenzie 等人的做法[30]；此前的研究表明，运行BERT模型的成本可以通过量化、蒸馏等标准技术减到可以忽略不计的水平 [4]。我们在其他空闲的机器上测量延迟。我们在命令前添加 numactl --membind 0 以确保插槽内I/O操作。对于 MS MARCO v2，我们未执行此操作，因为其大型索引可能需要使用两个NUMA节点。对于GPU结果，我们允许使用所有56个线程，但对于仅限CPU的结果，我们限制使用 torch.set_num_threads 设置的1或8个线程。对于非ColBERT系统，我们使用 Mackenzie 等人[30] 报告的单线程延迟数值。请注意，这些数值是在不同硬件设置和不同实现上测量的，仅用来展示PLAID ColBERTv2的竞争性能，而非作为绝对比较。

## 5.2. 端到端结果

表3展示了MS MARCO v1基准的数据内结果。在最保守的设置（𝑘 = 1000）下，PLAID ColBERTv2能够匹配原始ColBERTv2的MRR@10和Recall@100，同时在GPU上实现6.8倍的加速，在CPU上达到45倍的加速。对于一些质量的轻微损失，PLAID ColBERTv2进一步将对原始ColBERTv2的加速提升到在GPU上12.9–22.6倍，在CPU上86.4–145倍。PLAID ColBERTv2还实现了与其他系统相当的延迟（在SPLADEv2的1.6倍以内），同时在检索质量上优于它们。

---

在Wikipedia OpenQA基准的域内评估中，我们观察到了类似的趋势，如表4所示。与原始ColBERTv2相比，PLAID ColBERTv2在GPU上实现了3.7倍的加速，在CPU上实现了22倍的加速，且没有质量损失。在最小质量损失的情况下，GPU加速达到7.6–15倍，CPU加速达到42.3–75.7倍。

---

我们还确认了PLAID在域外环境中的良好表现，正如LoTTE“pooled”数据集上的结果所展示的。在表5中，我们可以看到，在𝑘 = 1000时，PLAID ColBERTv2在GPU上比原始ColBERTv2快2.5倍，在CPU上快9.2倍；此外，这一设置下的质量实际上优于原始ColBERTv2。在一定的质量损失下，PLAID ColBERTv2可以实现GPU加速3.8–7.3倍，CPU加速23.2–42.5倍。请注意，由于LoTTE的平均段落长度约为MS MARCO v1的2倍，PLAID ColBERTv2在LoTTE上实现的CPU延迟比在MS MARCO v1上更大。

---

最后，表6显示PLAID ColBERTv2在MS MARCO v2上的有效扩展能力。MS MARCO v2是一个具有138M段落和9.4B词汇的大规模数据集（约为MS MARCO v1的16倍）。延续在其他数据集上观察到的趋势，我们发现PLAID ColBERTv2在CPU上比原始ColBERTv2快20.8倍，而质量在100段落之内没有损失。当𝑘 = 1000时，我们发现原始ColBERTv2和PLAID ColBERTv2在GPU上都出现了内存不足的问题；我们认为可以通过实现自定义的无填充MaxSim内核来解决这一问题，详见§4.5。

## 5.3. 消融分析

图6展示了对PLAID的性能改进进行消融分析的结果，适用于GPU和CPU执行。我们的测量数据来自于对500个MS MARCO v1查询的随机样本的评估（这会导致与表3中报告的绝对数值有些微差异）。我们将原始ColBERTv2视为基线，然后添加不带剪枝的质心交互阶段（图5中的第3阶段），接着是带有质心剪枝的质心交互阶段（图5中的第2阶段），最后是§4.5中描述的优化内核。在适用的情况下，我们使用表2中描述的𝑘 = 1000设置的超参数（即最保守的设置）。

---

我们发现，评分流程的算法改进和实现优化都是PLAID性能的关键。特别是，仅质心交互阶段就在GPU上实现了5.2倍的加速，在CPU上实现了8.6倍的加速，而添加实现优化后，GPU和CPU上分别进一步加速1.3倍和4.9倍。仅在CPU上启用优化的C++内核而不进行质心交互（未在图6中显示）与完整的PLAID相比，只有3倍的端到端加速，而完整PLAID则达到42.4倍的加速。 

## 5.4. 可扩展性

我们评估了PLAID在数据集规模和并行度（CPU上的线程数）方面的可扩展性。

---

首先，图7展示了我们测量的每个基准数据集的端到端PLAID ColBERTv2延迟与数据集大小（按嵌入数量衡量）的关系。尽管不同数据集的延迟不一定具有直接的可比性（例如，由于段落长度不同），但我们仍然试图从图中分析总体趋势。我们发现，通常PLAID ColBERTv2的延迟似乎随着数据集大小的平方根而增长。这可以直观地理解为，ColBERTv2设置质心的数量与嵌入数量的平方根成比例，并且候选生成的开销与分区数量成反比。

---

接下来，图8展示了PLAID ColBERTv2在不同可用CPU线程数下的延迟，并对不同的$k$值$\{10,100,1000\}$进行测试。我们对500个MS MARCO v1查询的随机样本进行了评估以获得延迟测量。我们观察到PLAID能够利用额外的线程；特别是，当$k=1000$时，使用16个线程执行的速度比单线程执行提高了$4.9\times$。尽管PLAID未能实现完美的线性扩展，我们推测可能的解释包括：现有的vanilla ColBERTv2候选生成步骤中仍存在一些效率低下的地方（本工作中我们未对其进行底层优化），或由于段落长度不均匀导致的线程之间负载不平衡。我们将更深入的性能分析和潜在解决方案留待未来研究。

# 6. 结论

在本研究中，我们提出了PLAID，一个高效的晚交互引擎，通过积极且低成本地过滤候选段落来加速检索。我们展示了仅使用ColBERTv2质心的检索在召回率方面几乎可以与vanilla ColBERTv2媲美，并且质心相关性分数的分布偏向于较低的得分。基于这些见解，我们引入了质心交互技术，并将其整合到PLAID ColBERTv2评分流程的多个阶段。我们还描述了PLAID的高度优化实现，其中包括无填充MaxSim和残差解压操作的自定义内核。在多个IR基准上的评估表明，与vanilla ColBERTv2相比，PLAID ColBERTv2在GPU上实现了2.5–6.8×的加速，在CPU上达到了9.2–45×的加速，并且几乎没有质量损失，同时能够有效扩展到包含1.4亿段落的数据集。