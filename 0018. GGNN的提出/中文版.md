## GGNN: Graph-Based GPU Nearest Neighbor Search  

# 0. Abstract

高维空间中的近似最近邻（ANN）搜索是多个计算机视觉系统的重要组成部分，并且在深度学习中，显式记忆表示使其愈加重要。自从PQT (Wieschollek et al., 2016)、FAISS (Johnson et al., 2021) 和SONG (Zhao et al., 2020) 开始利用GPU提供的大规模并行处理能力以来，基于GPU的实现已成为当今最先进ANN方法的关键资源。尽管这些方法大多使查询速度更快，但对加速构建底层索引结构的关注相对较少。在本文中，我们提出了一种基于最近邻图和图上信息传播的全新GPU友好搜索结构。该方法旨在利用GPU架构，加速索引结构的分层构建和查询执行。实证评估显示，GGNN在构建时间、准确性和搜索速度方面显著超过了当前最先进的CPU和GPU系统。

# 1. INTRODUCTION

近似最近邻（ANN）搜索在许多领域中发挥着关键且长期的作用，包括数据库、计算机视觉、自动驾驶、个性化医疗和机器学习。随着数据收集变得更加容易，创建可扩展且高效的数据结构以检索相似项成为了一个活跃的研究课题。尽管近年来取得了许多进展，但在高维空间中唯一能保证找到精确最近邻的方法仍然是穷尽搜索，这主要是由于维度灾难的影响【4】。即使利用现代硬件，执行高维数据点的穷尽搜索在十亿级数据集上依然是不现实的。因此，大多数流行的方法通过寻找可能是最近邻的条目，放宽了这一问题，并接受最小的准确性损失。

---

除了设计非常快速的基于GPU的近似kNN查询算法外，我们还解决了高效构建分层图搜索结构的问题。在动态增长或变化的数据集上，构建时间变得越来越重要，尤其是在需要即时分析的应用场景中，例如在视频中的对应点匹配和对象识别、神经网络嵌入向量（如DEEP1B【5】）以及推荐系统【6】中。我们的图构建过程明确地以高概率确定数据集中每个点的真实k个最近邻。这对于全局问题的解决具有高度相关性，如n体问题、显著点估计、核密度计算、聚类分析、提取更鲁棒的原型或嵌入中的特征匹配。 

---

为了跟上日益增长的数据规模，现代方法使用专门为GPU大规模并行处理量身定制的索引结构【1】、【2】、【7】、【8】或定制硬件【9】，而不是以前基于CPU的方法【10】-【17】。

---

召回率的质量高度依赖于所选择的搜索结构和实际执行的查询。基于量化或哈希/分箱方案的结构【1】、【2】、【11】-【19】可以高效构建，但在高维度下由于需要穷举并访问相邻的单元，通常会导致较低的召回率。最近，基于图的方法【8】、【20】-【27】实现了更好的召回率。现有构建有效搜索图的方法，如【20】，通常顺序更新不同大小的边列表，强依赖全局内存同步，且难以有效并行化超过少数核心。因此，其构建时间无法很好地扩展，通常以小时甚至天来计算【19】、【27】。

---

在预先计算好的图结构中，查询会遍历图的边缘，以缩短到查询点的距离。它需要计算当前节点的所有邻居到查询点的距离，移动到下一个最佳点，并存储已访问的点。所有决策都是为每个查询独立做出的，这使得查询算法成为并行化的理想候选者。然而，要实现高效的并行化实现，还需要仔细处理一些问题。首先，用于加载每个要比较的向量的可用内存带宽可能成为限制因素。在这里，Optane内存【27】或GPU【3】可以提供解决方案。其次，在GPU上，需要处理每个线程或每个块可用内存有限的问题，以存储已访问的点列表。

---

因此，我们提出了一种适合GPU的查询设计，基于线程块级别，通过全并行的多功能缓存和每个点固定数量的邻居，来实现高效的片上资源利用。此外，我们引入了一种新的快速图构建技术，利用快速并行查询算法，通过迭代地合并多个子图来进行自底向上的构建。**图1**中概述了这种自底向上的构建方案。分层图被用作全局优化替代，以克服局部连通性中的不足，特别是在构建过程中。此外，我们的局部对称链接方法减少了图中冗余链接的数量，避免了内存有限缓存中的溢出带来的负面影响。

图1. kNN图自下而上的构建过程示意图。对于数据集(a)，首先进行随机划分(b)，然后为每个划分构建kNN图(c)。选择一些节点构建一个粗略的kNN图(d)，该图用于在划分之间传播链接。这些链接用于将多个划分合并(e)为最终的kNN图(f)。

---

我们的方法本质上非常适合批处理，因为它可以同时生成多个子图。通过放弃将多个子图合并为一个综合图的最终步骤，可以轻松地为大规模数据集的不同部分获得独立且有效的子图，这些子图原本无法放入内存中。即使在多GPU上同时处理这些子图也很简单，每个查询只需在每个分片上执行一次。这个简单而有效的方法可以实现对多GPU的最佳利用。

---

总结我们主要的贡献，我们提出了一种用于高维数据的极快近似kNN搜索方法，专为GPU设计。我们不仅专注于快速的查询时间，还注重高效构建索引结构，同时仍然实现了高召回率。

---

正如我们的实证评估所表明的，该方案在构建时间和查询时间方面都优于现有的方法。同时，召回率始终保持较高，并且可以在更快的构建时间或查询时间上进行权衡。我们还提出了一种多GPU方案，即使对于具有数十亿高维条目的常见基准数据集，也能够实现超过99%的召回率。

# 2. RELATED WORK  

在加速最近邻搜索的结构设计方面，已经有大量文献存在。除了传统方法【10】、【28】外，大多数流行的技术依赖于簇中的数据量化【1】、【2】、【11】-【19】或构建邻域图【8】、【22】-【27】。为了实现最佳性能，这些方法通常对每个条目计算压缩表示，因为大数据集无法完全放入快速内存。计算压缩的方法有多种策略。虽然哈希方法【29】-【31】生成紧凑的二进制代码，但基于量化的方法通过为每个数据点分配一个唯一标识符，利用质心对数据进行再利用。经验表明，量化方法比各种哈希方法更准确【10】、【11】。

---

**量化方法** 用于最近邻搜索的簇集成方法由Jégou等【11】普及，最初在【32】中引入。这类索引结构如IVFADC【11】将高维搜索空间划分为由矢量量化（VQ）【33】获得的质心集描述的离散Voronoi单元。后来Babenko等【15】扩展了这一概念，通过将高维向量空间分解为正交子空间。在这里，每个向量根据一个独立的码本被分配到每个子空间的一个质心。Wieschollek等【1】提出了一种分层表示码本的方法，并展示了利用GPU的优越性能。Johnson等【2】将IVFADC【11】移植到GPU上，并结合了基于GPU的快速k选择实现，即从给定列表中返回$k$个最小值（这是量化方法的关键部分）。他们是首个通过复制和分片实现多GPU并行化的团队，这项工作形成了"FAISS"库。最终，Chen等【18】提出了一种基于GPU的方法RobustiQ，通过以分层方式扩展【1】中的线量化思想，克服了FAISS的内存限制问题。然而，报告的距离仍然只是实际距离的近似值。

---

所有基于哈希和量化的索引方案都面临着相同的问题，即它们将空间划分为单元。虽然查询所包含的单元可以非常高效地找到，但精确的最近邻可能位于邻居单元的边界之外。在高维度中确定并访问所有邻居单元是一个严重限制这些方法的问题。

---

基于**kNN图**的方法是加速查询过程的另一种方式。我们提出的方法属于这一类。主要思想是将搜索空间中的每个点链接到其附近的k个点。每个查询将从数据集中随机猜测一个起点，然后通过从k个相邻点中选择一个更好的点来改进该猜测。Chen等【23】提出了一种快速分而治之的策略来计算这样的kNN图。Done等【22】介绍了用于加速NN搜索的NN-descent方法。每个点维护自己的最近邻列表以及那些将该点视为最近邻的点。后来，这一方法被扩展【24】以利用MapReduce。EFANNA是一种多层次索引结构，使用截断的KD树来构建kNN图【25】。

---

在理想情况下，增强了额外链接的kNN图可以保证从任意起点开始，NN-descent将收敛到正确的解决方案。然而，在大规模上计算这样一个带有附加链接的图是不现实的。因此，出现了几种至少可以近似这种图的方法【8】、【26】。Fu等【26】提出了NSG作为一种近似方法。为了减少整体边缘数量，他们的优化尝试根据每个节点单独降低出度。他们的方法可以扩展到多个核心，且在基准数据集上性能优于基于GPU的方法【2】。Harwood等【8】提出了一种构建这种图的替代方法。他们从一个相当密集的图开始，删除“阴影边”，即在查询时考虑遍历路径时冗余的边。他们使用GPU展示了有前景的结果，但仅限于较小的数据集，因为其构建时间较长。Malkov等【20】构建了一种分层图结构以加速最近邻搜索。

---

对于基于图的方法，内存吞吐量通常是限制因素。Ren等【27】提出了一种优化的HNSW风格搜索图构建方法，适用于外部数据集，使用异构内存。使用快速的Optane内存使他们能够在高精度下快速查询十亿级数据集。Zhao等【3】提出了基于GPU的“图上搜索”（SONG），使用基于HNSW【20】或NSG【26】构建的索引结构，在大多数情况下相较于基于CPU的查询实现了显著的加速。

---

我们的方法与HNSW【20】最为相似，因为它也使用了分层构建，但在构建任务的并行化方面进行了非常精细的调优，并且在查询过程中有所不同。

# 3. BACKGROUND  

在本节中，我们正式介绍了近似最近邻（ANN）问题的定义和使用的符号。

## 3.1. Nearest Neighbor Search  

最近邻问题从数据集$\mathcal{X}=\left\{x_1, \ldots, x_n\right\}$中检索出与查询$q$距离最近的点$x^{\star}$。为简单起见，这里我们假设一个欧几里得空间（ $\mathcal{X} \subset \mathbb{R}^d, q \in \mathbb{R}^d$ ）和欧几里得距离$\left(|\cdot|_2\right.$ )。因此，$q$的最近邻$x^{\star} \in \mathcal{X}$定义为$x^{\star}=\underset{x \in \mathcal{X}}{\arg \min }|q-x|_2$。

---

类似地，$k$最近邻搜索用于给定的查询$q$从$\mathcal{X}$中检索出$k$个距离最近的条目。由于找到精确的最近邻可能代价高昂，我们可以接受$\mathcal{X}$中的一些点，它们接近于$q$，因此提供一个近似解来解决公式(1)。

## 3.2. KNN Graph  

在kNN图中，数据集$\mathcal{X}$中的每个点$x$表示图$G$中的一个节点。进一步，我们定义$\mathcal{N}_x \subseteq \mathcal{X}$为点$x$的局部邻域，其中包含$k$个元素，如何构建$\mathcal{N}_x$的细节将在下一节讨论。图的边$E$则定义为$(x, y)$，其中$y \in \mathcal{N}_x$。需要注意的是，所得的图是有向图$G=(\mathcal{X}, E)$，其中$E=\left\{(x, y) \mid x \in \mathcal{X}, y \in \mathcal{N}_x\right\}$，并且$(x, y) \in E \nRightarrow(y, x) \in E$。

---

一个用于找到查询点$q$最近邻的贪婪算法是NN-descent【22】。从一个初始猜测点$x \in \mathcal{X}$开始，计算$q$与每个相邻点$y \in \mathcal{N}_x$之间的距离。如果$\mathcal{N}_x$中的任何点$y$比$x$更接近$q$，则将$x$替换为$\mathcal{N}_x$中离$q$最近的点。这个过程不断迭代，直到$\mathcal{N}_x$中没有比$x$更接近$q$的点为止。然而，由于当前的$x$可能没有提供正确搜索方向的边，这种贪婪算法可能会在纯kNN图中陷入局部最小值。

### 3.2.1. Common Pitfalls  

由于NN-descent是贪婪搜索，因此它无法保证找到精确解，主要原因包括以下几点：

---

**连通性**：由于kNN图是有向图，$y$可能直接与$x$相连，成为它的最近邻，因此$y \in \mathcal{N}_x$。但这并不意味着逆向链接也存在（ $y \in \mathcal{N}_x \nRightarrow x \in \mathcal{N}_y$ ）。因此，构建增强（多样化）的kNN搜索图需要处理出边和入边（逆向）的同步问题。

---

**高维空间中的间隙**：由于每个点仅与有限数量的局部邻居相连，存在一些病态情况（即使在二维空间中），相邻点根本没有直接连接。**图3**展示了这样一种情况。由于存在间隙，无法找到真正的最近邻。计算理想化的单调相对邻域图【26】（MRNG）可以避免这个问题，但它需要较大变化的连通性，不适合并行方法，同时还带来了额外的计算负担。

图3. 允许松弛$\xi$可以使搜索摆脱局部最小值$x \in \mathcal{X}$，最终到达查询$q \in \mathbb{R}^d$的解$x^{\star} \in \mathcal{X}$。该边界之外的点不太可能提供有用的链接，因而被丢弃以减少计算成本。（右侧）对比了使用我们的停止标准$\left(\tau_q\right)$和使用不断扩展的最佳列表来终止查询。通过最佳列表的方法，只有在高内存使用情况下才能实现高召回率，这会减慢查询速度。

---

**节点度**：在为任何$x \in \mathcal{X}$选择$\mathcal{N}_x$的基数时，存在一个权衡。较少的边会放大前面描述的问题，但也减少了每步必要的比较数量。相反，较多的边允许贪婪搜索逃离局部邻域，但每次迭代的成本也会增加。

# 4. GPU-BASED NEAREST NEIGHBOR SEARCH  

在给定某种图结构的情况下，为多个查询搜索$k$个最近邻是各种应用的核心操作，也是我们图构建的主要构件（见第5节）。我们的基于GPU的并行kNN图搜索算法的主要目标是在极短的时间内实现高召回率。该算法可以在质量和速度之间进行调整，以适应广泛的使用场景，既能满足高召回率的需求，也能适应对质量要求较低的实时算法。

---

其核心思想是通过为每个查询分配一个线程块来高度并行化搜索。与每个线程处理一个查询的简单方案相比，主要优势在于捆绑足够的片上资源，使所有元数据都可以保存在非常快速的内存中。这至关重要，因为基于图的方法仍然需要从全局内存中加载大量的邻域信息和向量以进行距离计算。此外，线程块的方法通过非常高效的合并内存访问保证了高内存带宽。这种方法还允许更多的并行计算，因为像距离计算、优先队列的维护以及访问列表的管理等常见任务都可以并行化。此外，运行时间不同的各个查询也可以独立调度。

## 4.1. GPU-Based Search With Backtracking  

假设在一个多样化（见第5.2节）的kNN图中有给定的起点$s \subset \mathcal{X}$，对于查询$q \in \mathbb{R}^d$，将执行算法1中的简单贪婪下坡搜索并带有回溯。首先，缓存结构（见第4.1.1节）用查询点和松弛因子$\tau$（见第4.1.2节）进行初始化（init）。然后，通过fetch引入起点$s$到缓存中，计算它们到查询$q$的距离并将其添加到优先队列（prioq）中。作为起点$s$，我们使用分层图顶层的节点（见第5.4节）。

---

在优先队列中没有更多未探索的点之前，最近的未访问点$a$的所有邻居$\mathcal{N}_a$会被fetch到缓存中，实质上是在图中执行深度优先搜索。我们的缓存结构管理着一个排序列表，该列表记录在遍历过程中观察到的$k$个最接近查询的点（best）。因此，best列表作为搜索结果返回。

### 4.1.1. Caching on GPUs  

我们快速基于GPU的kNN搜索的核心是一种多功能缓存（见算法1和2）。GPU的主要缺陷之一是其片上内存（寄存器和共享内存）的有限容量${ }^1$，这些内存可以以低延迟访问。

---

特别是，kNN图算法对于高度动态的结构有很强的需求，例如潜在访问点或已访问点的列表，这些列表的大小无法预测且经常增长。这些元数据在查询期间会频繁使用和修改。

---

缓存的内存布局如**图2**所示。它由三个主要部分组成：

1. **best**：一个排序列表，记录与查询最接近的点及其距离。
2. **prioq**：一个优先队列，作为按距离排序的环形缓冲区管理要访问的点。
3. **visited**：一个环形缓冲区，缓存已访问点的索引，采用先进先出（FIFO）方式。

所有三个部分都存储在共享内存中。总长度是分配给线程数的倍数，因此每个线程有多个工作项。

图2. 我们的缓存由一个**最佳列表**、**优先队列(prioq)和一个已访问列表**组成，完全驻留在共享内存中。最佳列表和prioq都包含索引和距离，并按距离排序。已访问列表仅包含索引以节省内存。prioq和已访问列表实现为环形缓冲区，这使得可以轻松地从prioq前端弹出元素，并在已访问列表满时覆盖旧元素。整个线程块并行访问和维护缓存。

---

缓存的主要方法如算法2所示，接下来会详细描述。需要注意的是，为了提高可读性，防止竞态条件的同步屏障已被省略。更多信息请参考我们发布的开源代码${ }^2$。为了便于可视化，使用下划线表示共享内存变量，普通变量则可以假定存储在寄存器空间中。

---

**Fetch**：给定一个点提议列表p，首先从提议列表中删除已知的元素。每个提议点会与每个线程的工作项并行比较，如果匹配则删除。剩余的提议点逐个处理。查询点到每个提议点的距离并行计算（dist）。每个线程从内存中读取相应的向量元素，并计算逐元素的结果。随后，通过归约操作生成完整的距离。对于归约等并行原语，我们使用NVIDIA的CUB库$^3$，这是高度优化的实现。并行合并加载和处理在GPU利用率方面非常高效。符合标准的点会被推入缓存结构。

---

**Push**：对于给定的索引$p$和距离$d$，push方法在距离排序的best和prioq列表中进行并行插入。插入时，所有距离大于$d$的项$c_i$会临时复制到线程块的寄存器中，除非列表或环形缓冲区的末尾，否则这些项会被写回到后续索引$c_{i+1}$。新元素被插入到左侧项比新项更近的位置，或在列表的开头（如果新项比所有前面的项都更近）。因此，符合条件的点会同时插入best和prioq列表。

由于prioq是环形缓冲区，逻辑的起始和结束位置依赖于头部位置。因此，在物理边界上的索引计算需要进行环绕处理。

---

**Pop**：此单线程例程返回当前优先队列的头部，并管理环形缓冲区。我们的标准在查询过程中单调递减（见第4.1.2节）。因此，如果prioq的头部违反了标准，我们可以安全地终止查询。如果头部有效，该点会从prioq中删除，并在头部位置添加到visited列表中。两个头指针向前移动一步。最后，该点会被返回。

### 4.1.2. Stopping Criterion  

在高维数据上，单纯依赖贪婪下坡搜索的算法很容易陷入局部最小值。而另一方面，完全回溯可能导致访问整个数据集。

---

一个常见的停止标准，例如【20】，是在最佳列表无法通过添加最近但尚未访问的元素得到改进时终止搜索，即当$d>d_{\text{best}_K}$时停止，其中$d$是新元素的距离，$d_{\text{best}_K}$是最佳列表中最后一个元素的距离。为了实现更高的召回率，需要扩展最佳列表（$K$需要增加），可能增加数百个元素。

---

我们提出了一种高效的近似方法，通过向$k$最近邻的距离添加一个自适应的、单调递减的松弛因子$\xi$：$d_{\text{best}_K} \approx d_{\text{best}_k} + \xi$，其中$k \ll K$，这使得我们可以将最佳列表的大小限制在只包含$k$个最近邻（至少10个）。而不是显式跟踪更多邻居的距离，松弛因子提供了一个安全边界，确保能找到通向最近邻的路径。例如，如图3所示。一旦$d > d_{\text{best}_k} + \xi$时，搜索终止，其中$\xi=\tau \cdot \min \left\{d_{\text{best}_1}, d_{\mathrm{nn}_1}^{+}\right\}$。

---

松弛因子$\tau$控制了安全边界的大小。$d_{\text{best}_1}$是特定于查询的，表示与当前找到的最佳匹配相关的边界，而$d_{\mathrm{nn}_1}^{+}$与给定数据库节点的密度相关。具体来说，$d_{\mathrm{nn}_1}^{+}$为全局限制（用于捕获离群点），在图构建过程中计算，表示当前处理的图子集$\mathcal{S}$中所有点到最近邻的最大距离。

$d_{\mathrm{nn}_1}^{+} = \max_{x \in \mathcal{S}} \left\{ \min_{y \in \mathcal{N}_x} \|x - y\|_2 \right\}$，其中$\mathcal{S} \subseteq \mathcal{X}$。

---

使用我们的停止标准可以保持最佳列表的较小大小，从而减少执行查询所需的共享内存量。如图3所示，我们的停止标准即使在高精度下也能保持高效。$\tau$的典型值在0到2之间，较大的值可以提高准确性，但收益递减。

# 5. APPROXIMATE SYMMETRIC NEAREST NEIGHBOR GRAPH CONSTRUCTION

通常，基于CPU的适当kNN搜索图的构建是在全局图上按顺序执行的。边缘要么是一个接一个地附加（例如【20】），要么是一个接一个地剪除（例如【8】、【21】、【26】）。在此过程中，每个节点的边缘列表往往会有很大的变化，从完全空白到包含所有可能点的完整列表不等。尽管这些全局优化方法能够在CPU上生成高质量的图结构，但对于快速的基于GPU的构建，它们并不实用。

---

我们提出了一种基于层次kNN搜索图的并行合并的图构建方法，如**图1**所示。通过将搜索图的构建任务划分为小而可并行化的任务，我们能够有效利用GPU提供的大规模并行处理能力。接下来，我们将介绍我们的并行图构建过程，随后讨论层次化查询过程、通过对称链接进行的图多样化和优化。最后，我们探讨如何最好地执行最终查询以及多GPU配置的实现。

## 5.1. Building the Hierarchical kNN-Graph  

我们的构建过程（算法3）通过递归合并较小的搜索图自下而上地构建kNN搜索图。

---

初始化时，我们将整个数据集$\mathcal{X}$逻辑上划分为小批次，大小为$s$，例如32。这些批次代表高度为1的分层搜索图。我们首先通过执行合并操作来初始化底层中的邻居，该操作为每个点初始化$k$个出边，每个点连接到其最近的邻居。接着，通过对称操作（symoperation）替换最多$k_{sym}$个出边，试图近似一个无向图，详见第5.2节。

---

为了合并各个子图，我们首先从每个需要合并的$g$个图的组中选择点。这些选择的点现在形成代表新顶层的批次。在该顶层上，我们执行与底层相同的操作，即合并，即查询最近邻以填充出边列表并进行图的多样化（sym）。由于任意点的最近邻可能位于任何子图中，批次之间的边缘会被引入。这些步骤将自顶向下重复，直到整个分层搜索图互联。

---

每当到达底层时，我们还会保存每个点到其第一个最近邻的距离$d_{\mathrm{nn}_1}$，并计算数据集的均值和最大值（stats）。这使得我们可以轻松地将停止标准适应每个数据集，而无需任何预计算。最初，由于最近邻图仍较粗糙，最大值易受离群点的影响。因此，在构建过程中，我们将公式（2）中的最大距离$d_{\mathrm{mn}_1}^{+}$替换为平均距离$\bar{d}_{\mathrm{nn}_1}$：

$\bar{d}_{\mathrm{nn}_1} = \frac{1}{|\mathcal{S}|} \sum_{x \in \mathcal{S}}\left\{\min _{y \in \mathcal{N}_x}\|x-y\|_2\right\}$，其中$\mathcal{S} \subseteq \mathcal{X}$。

---

为了选择顶层的点，使用带有$d_{\mathrm{mn}_1}$作为权重的加权水库抽样，方法参考【34】。

由于在每一层中，每个点的构建任务可以并行执行，并且每次只需要少量、有限的内存，因此当利用GPU提供的大规模并行性时，构建过程可以非常快速地完成。

---

**分层kNN图查询**。在顶层中，合并操作可以简单地在每个批次中的$s$个点之间进行暴力搜索。随着层数的降低，批次数量按因子$g$增长。为了在较低层进行高效的最近邻搜索，我们执行分层查询（算法4），逐层利用已经合并的上层来找到进入下层批次的入口点，其余行为与前面介绍的简单查询（第4.1节）相同。在上层找到的最近点的索引会转换为同一在下一层中的索引，并继续搜索，直到到达目标层$l_m$。尽管可以重用已访问点的距离，但其在较低层的邻域会发生变化。因此，在层之间切换后，允许重新访问所有点。

---

与HNSW【20】一样，分层结构使我们能够弥合连通性中的间隙，因为来自顶层的多个入口点可能会从不同的方向在底层接近查询点。

## 5.2. Graph Diversification  

为了从任意方向到达任何点，原则上图中的每条边都应该是无向的。然而，当每个点至少要知道其$k_{\mathrm{nn}}$个最近邻时，对于无向图来说，每个点$x$的总边数会显著变化，因为将$x$作为最近邻的点的数量将严重依赖于局部几何结构。

---

为了获得规则的表示，并允许每步具有恒定工作负载的简单并行遍历，我们的图仅由恰好$k$条有向（即出边）边组成。我们逻辑上将出边分为至少$k_{\mathrm{nn}}=k / 2$的真实最近邻，以及最多$k_{\text {sym}}=k / 2$的逆向链接，这些逆向链接用于近似一个无向图。

---

由于可表示的逆向链接数量有限，因此必须确定哪些逆向链接是必要的。Harwood和Drummond【8】引入了**阴影边**（shadowed links）的概念，这些边通过查询的贪婪探索策略变得冗余。优化整个图以删除所有可能的阴影边需要复杂的全局优化。同样的情况也适用于构建单调相对邻域图【26】。

---

我们对图多样化的处理方法是通过在每个点$z$的$k_{\mathrm{nn}}$最近邻$x_i \in \mathcal{N}_z^{nn}$内的一个小搜索半径内查询缺失的逆向链接。如果从$x_i$到$z$的直接逆向链接存在，则可以直接跳过该查询。如果在允许的范围内没有路径，则会添加该链接（例如，图4）。这些查询会在所有点$z \in \mathcal{X}$上并行执行，从而加快构建过程。

图4. 维护对称链接。如果从$x \in \mathcal{N}_z^{nn}$到$z \in \mathcal{X}$没有找到容易的连接，则添加边$e$，以在$z$和$x$之间传播最近邻信息。（右侧）在检测是否插入对称链接时，我们将访问点的最大距离限制在围绕中点$q_m$的较小球体内，而不是考虑整个搜索半径。

---

详细的对称链接操作如算法5所示。每个点$z$为自己发起查询，从它的$k_{\mathrm{nn}}$最近邻$x_i$开始。在查询过程中，仅考虑每个访问点$\mathcal{N}_{x_i}^{nn}$的$k_{\mathrm{nn}}$最近邻以及在该点已经插入的对称/逆向链接$\mathcal{N}_{x_i}^{\text {sym}}$，因为其他可能仍会被覆盖。如果找到了返回$z$的路径，搜索将继续$z^{\prime}$的下一个邻居。否则，在搜索过程中遇到的、距离$z$最近且逆向链接列表中仍有空闲容量的点处插入指向$z$的链接。

---

为避免在插入操作中的竞争条件，我们使用原子操作来跟踪逆向链接列表的大小。通过这种方式，平均需要少于$k / 4$的逆向链接。如果没有找到候选项，则忽略该链接。然而，在我们的设置中，这种情况非常罕见，$z$仍然可能通过层次结构或多个起点点可达。

### 5.2.1. Additional Symmetric Linking Constraint  

如图4所示，通常的搜索空间将根据$z$和$x_i$来定义。然而，为了生成能够找到所有潜在查询点路径的搜索结构，我们进一步约束其为仍需要找到通往$z$的路径的最坏情况查询。因此，需要一个链接来弥合在中点$q_m$上的最坏情况查询的间隙。请注意，通往$x_i$的路径已知，因为它是直接从$z$已知的。对于对称链接，只有该搜索空间是相关的，即由中心$q_m$和距离$x_i$的圆，但仍包围$z$。在实践中，我们将$q_m$设置为$z + 0.4(x_i - z)$。这样做虽然稍微不那么保守，但减少了所需的对称链接数量。

## 5.3. Graph Refinement  

初始图的构建可能并不会在第一次尝试时产生完美的结果。部分原因是由于在构建覆盖大量数据集的低层次搜索图时，合并因子$g$较大。

---

在我们的实验中，发现对于分层搜索图，使用高度为$L=4$能提供最佳性能。为了从最初由$n / s$个不相连的子图（每个子图由单个大小为$s$的批次组成，通常$s=32$）中创建一个单一的图，我们需要在每一层上合并$g=\sqrt[L-1]{n / s}$个子图，其中每个新的子图的顶层包含从这$g$个子图中抽取的$s$个点（见第5.1节）。例如，当$n=256$时，每一层仅需要合并2个图；而对于$n=10^6$，每一层需要合并大约32个图。合并因子$g$越大，进入低层的入口点就越少，这限制了初始分层查询的成功率，因为每个合并后的子图只有$s / g$个入口点。当$g > s$时，一些点只能通过对称链接桥接子图后才能到达。

---

为了提高图的质量，可以执行改进步骤，如算法6中所述，这些步骤只是简单地在整个搜索图中重复合并和对称链接步骤。虽然在实际的子图合并过程中执行这些改进步骤也是有益的，但我们观察到，在最后执行这些步骤足以提高搜索结构的整体质量。

## 5.4. Query Considerations  

在我们的方法中，图构建期间必须执行分层查询，即逐层搜索最近邻，这是因为搜索索引尚未完全合并。在这种情况下，粗到精的方法能够弥合尚未连接的子图之间的差距，并为感兴趣的区域提供多条路径。从多个分散的点开始可以显著提高质量，特别是当不仅仅需要1-NN（最近邻），还包括距离较远的第$k$个邻居时。然而，对于已收敛的搜索图，这种努力不再必要，因为所有点都已很好地互连。

---

在扁平索引结构中进行搜索没有分层的额外开销。例如，Li等人【21】在扁平图中始终从质心开始查询。然而，随着点的数量增加，跳到质心的次数也会增加。此外，使用单一的起始点，任何目标点将仅从一个方向接近。搜索可能会受到图中缺失的连接或链路问题的影响。

---

在我们的设计中，底层实际上是一个包含所有点的扁平图。当在已完成的搜索图中为新点搜索最近邻时，实际操作中，跳过中间层，直接在底层继续搜索会更为高效。在探索了由顶层组成的$s$个起始点之后，这允许显著减少执行的搜索迭代次数，否则需要在每个中间层进行迭代。实际操作中，与分层方法相比，我们没有观察到召回率的损失，但在时间上有明显节省。

## 5.5. Multi-GPU  

并行构建和搜索算法可以扩展到多个GPU。Johnson等人【2】区分了两种多GPU并行的类型：复制（Replication）和分片（Sharding）。复制方法将整个数据集$\mathcal{X}$复制到多个GPU上，以进一步并行化查询过程。查询集$\mathcal{Q}$被平均分配到可用的GPU中，带来线性速度提升。

---

**分片（Sharding）**将数据集$\mathcal{X}$划分为更小的、独立的子集。这使我们能够处理大规模数据集，否则这些数据集将无法适应GPU内存。当必要时，这些分片可以被交换到主内存或磁盘上。所有GPU并行为各自的分片构建搜索图。由于每个分片的点数减少，复杂性也随之降低，针对相同质量的超线性加速效果是可以实现的。

---

缺点是每个查询都需要处理所有分片。当在每个GPU上查询多个分片时，我们会在后台交换已经处理过的分片来替换新的分片，以隐藏内存延迟。不过，分片交换对每批查询的最短运行时间设定了限制。此外，来自每个分片的独立查询结果需要合并。在每个GPU内，我们使用并行基数排序（radix sort）。跨GPU的最终结果通过CPU上的高效n路归并算法计算得出。

# 6. EMPIRICAL EVALUATION  

在下文中，我们评估了所提出方法及其各个组成部分在多个公开可用的基准数据集上的性能，并在时间和准确性方面报告了定性和定量的结果。

---

**数据集**。我们在不同应用场景下生成的不同大小和维度的数据集上运行了实验：SIFT1M [11] 和 SIFT1B [12] 包含了128维的SIFT向量，DEEP1B [5] 拥有10亿个96维的图像特征向量，NyTimes [35], [36] 和 GloVe 200 [35], [37] 是两个偏斜且聚类的数据集，包含了使用余弦相似度比较的词嵌入的随机投影，最后是960维的数据集GIST [11]。有关数据集的详细信息和我们搜索图的构建细节可参见表4。

---

**硬件**。我们使用了一台配备8个NVIDIA Tesla V100显卡和2个Intel Xeon Gold 5218处理器的机器，除了在SIFT1B和DEEP1B实验中使用全部8个GPU外，其他情况下我们通常只使用单个GPU。关于更多GPU的结果，参见补充材料。

---

**性能指标**。在与其他方法比较时，必须仔细查看所使用的指标。查询性能通常以召回率（R@k）衡量。对于基于量化的方法（例如，[11]），召回率理解为在前k个结果中返回真实最近邻的查询比例。$R @ k=\frac{\left|\mathcal{N}_q^{\mathrm{gt}}(1) \cap \mathcal{N}_q(k)\right|}{\left|\mathcal{N}_q^{\mathrm{gt}}(1)\right|}$。

---

对于使用精确距离计算的方法（如基于图的方法），召回率则理解为前k个结果与前k个真实最近邻的重叠率。为了消除歧义，我们将其称为一致率（C@k）， $C @ k=\frac{\left|\mathcal{N}_q^{\mathrm{gt}}(k) \cap \mathcal{N}_q(k)\right|}{\left|\mathcal{N}_q^{\mathrm{gt}}(k)\right|}$。

---

由于我们的方法使用精确的距离计算，真实最近邻要么作为答案的第一个元素报告，要么根本未找到。因此，我们仅报告R@1 (=C@1)。

---

**批处理大小**。每批执行的查询数量是基于GPU的查询的一个重要因素，我们在图8b中进行了分析。在实验中，我们测量执行所有10000个查询的时间（GIST [11]数据集执行1000个查询），并报告总执行时间除以查询数得到的每查询时间（毫秒/查询）或绘制成每秒查询数。

图8. 查询性能既取决于搜索图的质量，也取决于同时执行的查询数量。如果在构建过程中花费更多时间，则可以更快地执行具有相同准确度的查询（a）。这里没有进行任何细化操作$(r=0)$，以可视化松弛因子$\tau_b$对初始构建的影响。通常，执行几次细化迭代可以进一步提升查询性能。就查询规模而言（b），几千个查询已经能达到很高的性能，进一步增加同时查询的数量能带来更好的表现，性能峰值出现在95k个查询时。

## 6.1. Performance Comparisons  

评估是在默认配置GGNN、速度更快但准确性较低的配置GGNNz，以及8-GPU配置GGNN8上进行的（参见表4中的参数）。

---

作为参考，我们使用了几种最新的基于CPU的方法[20]、[21]、[26]，其中我们在Intel Core i7-9700K上执行了单线程的参考实现。我们进一步与基于GPU的SONG[3]进行比较，并模仿他们的方法，通过我们的查询作为“GGNN-HNSW”查询预构建的HNSW[20]图的底层。使用我们的查询速度更快。此外，它的性能与使用我们的方法构建的类似大小的图上的查询非常相似，但所用时间明显减少。**图5**和表1、**表2**、**表3**中显示了详细的性能比较。在这些表格中，我们与先前发布的结果进行了比较，其中GPU方法[1]、[2]、[8]、[18]使用了NVIDIA GTX Titan X GPU，而[9]使用了Arria 10 GX1150 FPGA。除了在所有数据集上显著更快（在SIFT1M上的查询约为1ms），我们的方法还可以达到非常高的召回率，几乎接近完美。 

---

图5. 各个数据集和 top-$k$查询的性能对比。右上角的结果更优。我们主要与SONG [3]进行对比，使用他们论文中的值并使用相同的GPU。在SIFT1M上，我们还与最新的单核CPU方法 [20], [21], [26] 进行比较，并加入了配置“GGNN-HNSW”，在其中我们使用我们的方法查询由HNSW [20]构建的图，就像SONG [3]一样。我们的查询显著优于SONG，并且在用我们的方法在更短时间内构建的类似大小的搜索图上达到了类似的结果。在查询100个最近邻或更复杂的数据集（如GloVe 200）以及非常高维的数据集（如GIST）时，表现也很出色。在较小的NyTimes数据集上，虽然我们的图构建可能不够理想，但高效的查询仍然大幅超越了SONG。使用8个GPU，甚至可以在像SIFT1B或DEEP1B这样的大规模数据集上，以接近完美的召回率达到每秒10万次查询的速度。

---

表 2. 我们的方法 GGNN 使用了 8 个 GPU。RobustiQ [18] 使用了 2 个 GPU。除此之外，其余方法使用了单个 CPU 或 GPU。我们的方法在 recall@1 上达到了完美的召回率。GGNN 的查询速度受限于主机到设备的内存带宽，但在理想情况下可以以 24.5μs/查询的速度，在 99% recall@1 的情况下执行查询（参见第6.3.1节）。较小的 GGNN 能更快地执行查询，因为它迭代的分片较少，能够完整地放入 GPU 内存中。

---

表 3. 我们的方法 GGNN 使用了 8 个 GPU。FAISS [2] 使用了 4 个 GPU。RobustiQ [18] 使用了 2 个 GPU。除此之外，其余方法使用了单个 CPU。GGNN 受限于主机到设备的内存带宽，在 10k 查询集上查询的最小时间为 233μs/查询（参见第6.3.1节）。为展示 $\tau_{q}$ 的影响并与未来硬件配置进行比较，我们报告了没有当前上传需求时的 GGNN 查询时间。

## 6.2. Search-Graph Construction  

在接下来的部分中，我们探讨构建过程的几个方面，包括在各个构建步骤上花费的时间、数据集大小的影响、结果图在每个点的最近邻方面的质量，以及如何通过增加构建时间来换取更快的查询速度。

### 6.2.1. Construction Time  

表4列出了我们方法的构建时间和索引大小。相比之下，FAISS [2] 报告在DEEP1B上的构建时间为4到24小时，但R@1的召回率却不到50%。HM-ANN [27] 报告在处理十亿级搜索图时，构建时间大约为100小时。使用我们的基于GPU层次结构的图合并算法，可以在DEEP1B上达到99%的R@1召回率，甚至在SIFT1B上达到完美的100% R@1，同时查询速度非常快，而构建时间不到2小时。

### 6.2.2. Construction Time Composition  

图6显示了带有两次细化迭代的各个图构建操作所花费的时间。大部分构建时间花费在合并操作上，特别是涉及底层的合并操作，此时合并操作需要为数据集中的所有点搜索k个最近邻。

图6. SIFT1M搜索图的构建时间拆解。大部分时间花费在合并操作上(86.92%)，一部分时间在对称操作上(13.06%)，几乎没有时间用于为上层选择点(0.02%)。

### 6.2.3. Dataset Size  

高效的图构建需要能够很好地随数据集大小进行扩展。如图7所示，构建时间几乎线性扩展，约为$\approx O\left(n^{1.077}\right)$。这种现象可以解释为，大部分构建时间花费在独立确定每个底层点的邻居上（见第6.2.2节）。这种几乎线性的表现比暴力kNN构建的$O\left(n^2\right)$复杂度好得多。

图7. 在单个GPU上增加SIFT1B子集（最多100M）的构建时间和每秒查询数。构建时间几乎线性依赖于数据集的大小。随着数据集大小的增加，在固定的松弛因子$\tau_q$下，查询性能在执行时间和准确性上略有下降。

### 6.2.4. For-All Queries  

从头开始快速构建kNN搜索图还解决了另一个有趣的问题，即计算数据集中所有点的k个最近邻，这在n体问题中经常遇到。当我们构建完整的层次结构时，最后的合并步骤实际上是在为所有点搜索k个邻居。在表5中，我们展示了考虑共识问题时全体问题的质量。参考图7，此全体问题再次显示出几乎线性扩展的趋势。然而，在分片的情况下，所有点需要针对每个分片测试一次。由于分片数量线性依赖于点的数量，因此复杂度再次接近$O\left(n^2\right)$，但具有非常小的常数，例如，对于SIFT1B数据集，16个分片。

### 6.2.5. The Construction-Query Trade-Off  

我们的图构建算法和查询可以针对速度或准确性进行调整。构建时间由松弛因子$\tau_b$以及执行的细化迭代次数$r$控制。构建时间越长，生成的图越精确。快速组装的图会产生较差的边，查询可能需要访问更多的节点才能满足停止标准。在图8a中，构建时间和查询时间之间的权衡可视化展示了固定召回率下的情况。

图8. 查询性能既取决于搜索图的质量，也取决于同时执行的查询数量。如果在构建过程中花费更多时间，则可以更快地执行具有相同准确度的查询（a）。这里没有进行任何细化操作$(r=0)$，以可视化松弛因子$\tau_b$对初始构建的影响。通常，执行几次细化迭代可以进一步提升查询性能。就查询规模而言（b），几千个查询已经能达到很高的性能，进一步增加同时查询的数量能带来更好的表现，性能峰值出现在95k个查询时。

## 6.3. Query Behaviour  

通过调整松弛因子 $\tau_q$，可以控制查询的运行时间和质量。如表1所示，增加 $\tau_q$ 将在查询过程中引入更大的安全边界，从而在提高召回率的同时需要访问更多的点，导致查询时间延长。

---

观察查询随时间的行为是有帮助的。如图9所示，查询起始点（即顶部层最接近的点）到查询点的初始距离在几次迭代后显著减少。在此处，迭代表示从优先队列中获取最佳点并计算其所有邻居的距离。大部分时间用于仔细探索真最近邻居附近的区域。在找到最佳候选人后，查询被停止准则终止，以避免不必要的迭代。这种效果不仅体现在小部分绘制的查询距离上，还在所有查询的统计直方图中有所体现。

图9. 观察多个查询的行为（a）可以看到，搜索图非常快地找到了目标区域。随后，局部邻域被探索，最后一次改进后不久（通过标记显示），查询由停止准则终止。停止准则的有效性也反映在统计结果中（b）。总迭代次数的分布与实现最后一次改进所需的迭代次数分布密切相关，只落后了少数迭代。

### 6.3.1. Out of GPU-Memory Queries  

在查询亿级规模的数据集时，所需的GPU内存量将迅速超过即使是8个NVIDIA Tesla V100系统的可用内存。尽管SIFT1B数据集加上低配置的GGNN${ }^{8 \ddagger}$图仍能适应这些GPU，但如果使用高配置的GGNN${ }^8$图，则将超出可用内存2个分片（4.2 GiB）。在DEEP1B数据集中，仅数据集本身就已经超过了可用内存容量，且使用GGNN${ }^8$图时，每次查询需要额外加载28 GiB的内存。这导致受主机到设备内存传输吞吐量（ $\approx 12 \mathrm{GiB} / \mathrm{s}$ [38]）的限制，例如对于10,000个查询，在SIFT1B上查询的最低时间为$35 \mu \mathrm{~s} /$查询，而在DEEP1B上则为$233 \mu \mathrm{~s} /$查询。这些限制在图5中有所体现。需要注意的是，这段加载时间可以通过执行整体成本更高的查询（例如，更多查询，增加 $\tau_q$ 或更大的缓存）完全隐藏。为了与未来具备足够内存的设备进行轻松对比，我们还报告了不涉及内存传输的计算时间。

# 7. CONCLUSION  

我们提出的并行GPU基础的搜索算法在速度方面远远超越了所有最先进的算法，并且在保持回忆率超过$99 \%$的同时，成为最快的ANN（近似最近邻）查询技术。高效的查询算法进一步加速了子图的并行构建和合并。我们的层次化构建方案创建了高质量的kNN图，并通过图多样化链接实现了高效的遍历。这一方法很容易部署在多GPU系统中，以应对非常大规模的数据集。它是第一个能够在不到一小时内为亿级规模数据集构建有效搜索结构（R@1 $\geq 0.99$）的ANN搜索方法。对于百万级规模的数据集，通常可以在几秒钟内构建出足够质量的搜索图，从而允许快速的最近邻搜索，也可以作为更大规模GPU算法（例如机器学习应用）中的中间数据结构的一部分。

---

目前，我们的方法对所有访问过的高维点计算确切的距离。由于距离计算的次数主导了查询时间，使用数据向量的压缩表示可能会进一步加速计算。































































