# Informer：超越高效Transformer的长序列时间序列预测

#### 摘要

许多现实世界的应用需要对长序列时间序列进行预测，例如电力消耗规划。长序列时间序列预测（LSTF）要求模型具有较高的预测能力，即能够高效地捕捉输出和输入之间的精确长程依赖关系。最近的研究表明，Transformer在提高预测能力方面具有潜力。然而，Transformer存在几个严重问题，使其无法直接应用于LSTF，包括二次时间复杂度、高内存使用量以及编码器-解码器架构的固有局限性。为了解决这些问题，我们设计了一种高效的基于Transformer的LSTF模型，名为Informer，具有三个显著特点：（i）ProbSparse自注意力机制，其时间复杂度和内存使用量为$\mathcal{O}(L \log L)$，并且在序列依赖对齐方面具有可比性能。（ii）自注意力蒸馏通过将级联层输入减半来突出主导注意力，并高效处理极长的输入序列。（iii）生成式解码器在概念上简单，能够通过一次前向操作预测长时间序列，而不是逐步预测，从而大幅提高了长序列预测的推理速度。在四个大规模数据集上的广泛实验表明，Informer显著优于现有方法，并为LSTF问题提供了新的解决方案。

# 1 引言

时间序列预测是许多领域的关键组成部分，例如传感器网络监控（Papadimitriou和Yu 2006）、能源和智能电网管理、经济学和金融（Zhu和Shasha 2002）以及疾病传播分析（Matsubara等 2014）。在这些场景中，我们可以利用大量过去行为的时间序列数据来进行长期预测，即长序列时间序列预测（LSTF）。然而，现有方法大多设计用于短期问题设置，例如预测48个点或更少（Hochreiter和Schmidhuber 1997；Li等 2018；Yu等 2017；Liu等 2019；Qin等 2017；Wen等 2017）。越来越长的序列使模型的预测能力受到极大挑战，阻碍了LSTF的研究进展。作为一个实证示例，图（1）展示了一个真实数据集上的预测结果，其中LSTM网络从短期（12个点，0.5天）到长期（480个点，20天）预测了一个电力变压器站的每小时温度。当预测长度超过48个点（图（1p）中的实心星）时，整体性能差距显著，MSE上升到不可接受的水平，推理速度急剧下降，LSTM模型开始失效。

- 图1：（a）LSTF可以覆盖比短序列预测更长的时期，在政策规划和投资保护中具有重要区别。（b）现有方法的预测能力限制了LSTF的性能。例如，从长度$=48$开始，MSE上升到不可接受的高水平，推理速度迅速下降。

---

LSTF的主要挑战是增强预测能力以满足日益增长的长期序列需求，这需要（a）非凡的长程对齐能力和（b）对长序列输入和输出的高效操作。最近，Transformer模型在捕捉长程依赖关系方面表现出优于RNN模型的性能。自注意力机制可以将网络信号的最长传播路径减少到理论最短的$\mathcal{O}(1)$，并避免循环结构，因此Transformer在LSTF问题上显示出巨大潜力。然而，自注意力机制由于其在$L$长度输入/输出上的$L$二次计算和内存消耗，违反了要求（b）。一些大规模Transformer模型在NLP任务上投入资源并取得了令人印象深刻的结果（Brown等 2020），但在数十个GPU上的训练和昂贵的部署成本使得这些模型在现实世界的LSTF问题上难以承受。自注意力机制和Transformer架构的效率成为将其应用于LSTF问题的瓶颈。因此，在本文中，我们试图回答以下问题：我们能否改进Transformer模型，使其在计算、内存和架构上更加高效，同时保持更高的预测能力？

---

Vanilla Transformer (Vaswani et al. 2017) 在解决长序列时间预测（LSTF）问题时存在三个显著的限制：

1. **自注意力机制的二次计算复杂度**。自注意力机制的核心操作，即标准的点积运算，导致每一层的时间复杂度和内存使用量为 $\mathcal{O}\left(L^{2}\right)$。
2. **长输入序列下堆叠层的内存瓶颈**。堆叠 $J$ 个编码器/解码器层使得总内存使用量为 $\mathcal{O}\left(J \cdot L^{2}\right)$，这限制了模型在处理长序列输入时的可扩展性。
3. **预测长输出序列时的速度骤降**。Vanilla Transformer 的动态解码机制使得逐步推理的速度与基于 RNN 的模型一样慢（如图 1b 所示）。

---

在提升自注意力机制效率方面，已有一些相关研究。Sparse Transformer (Child et al. 2019)、LogSparse Transformer (Li et al. 2019) 和 Longformer (Beltagy, Peters, and Cohan 2020) 都采用了启发式方法来应对限制 1，并将自注意力机制的复杂度降低到 $\mathcal{O}(L \log L)$，但它们的效率提升有限 (Qiu et al. 2019)。Reformer (Kitaev, Kaiser, and Levskaya 2019) 也通过局部敏感哈希自注意力实现了 $\mathcal{O}(L \log L)$ 的复杂度，但它仅适用于极长序列。最近，Linformer (Wang et al. 2020) 声称达到了线性复杂度 $\mathcal{O}(L)$，但其投影矩阵无法针对现实世界中的长序列输入固定，可能导致复杂度退化到 $\mathcal{O}\left(L^{2}\right)$。Transformer-XL (Dai et al. 2019) 和 Compressive Transformer (Rae et al. 2019) 使用辅助隐藏状态来捕捉长距离依赖关系，这可能会加剧限制 1，并且不利于突破效率瓶颈。所有这些研究主要集中在限制 1 上，而限制 2 和 3 在 LSTF 问题中仍未得到解决。为了增强预测能力，我们在提出的 Informer 模型中解决了所有这些限制，并实现了超越效率的改进。

---

为此，我们的工作深入探讨了这三个问题。我们研究了自注意力机制中的稀疏性，改进了网络组件，并进行了广泛的实验。本文的贡献总结如下：

- 我们提出了 **Informer**，成功增强了长序列时间预测（LSTF）问题的预测能力，验证了类 Transformer 模型在捕捉长序列时间序列输出与输入之间长距离依赖关系的潜在价值。  
- 我们提出了 **ProbSparse 自注意力机制**，以高效地替代标准的自注意力机制。它在依赖对齐上实现了 $\mathcal{O}(L \log L)$ 的时间复杂度和 $\mathcal{O}(L \log L)$ 的内存使用量。  
- 我们提出了 **自注意力蒸馏操作**，以在 $J$ 层堆叠中突出主导注意力得分，并将总空间复杂度显著降低到 $\mathcal{O}((2-\epsilon) L \log L)$，这有助于接收长序列输入。  
- 我们提出了 **生成式解码器**，只需一次前向步骤即可获取长序列输出，同时避免了推理阶段中累积误差的传播。  

---

图 2：Informer 模型概览。  
- **左侧**：编码器接收大量的长序列输入（绿色序列）。我们使用提出的 ProbSparse 自注意力机制替代标准的自注意力机制。蓝色梯形是自注意力蒸馏操作，用于提取主导注意力，显著减少网络规模。层堆叠副本增强了鲁棒性。  
- **右侧**：解码器接收长序列输入，将目标元素填充为零，测量特征图的加权注意力组合，并以生成式风格即时预测输出元素（橙色序列）。

# 2 预备知识

我们首先给出长序列时间预测（LSTF）问题的定义。在固定大小窗口的滚动预测设置下，我们在时间 $t$ 的输入为 $\mathcal{X}^{t}=\left\{\mathbf{x}_{1}^{t}, \ldots, \mathbf{x}_{L_{x}}^{t} \mid \mathbf{x}_{i}^{t} \in \mathbb{R}^{d_{x}}\right\}$，输出则是预测相应的序列 $\mathcal{Y}^{t}=$ $\left\{\mathbf{y}_{1}^{t}, \ldots, \mathbf{y}_{L_{y}}^{t} \mid \mathbf{y}_{i}^{t} \in \mathbb{R}^{d_{y}}\right\}$。LSTF 问题鼓励输出长度 $L_{y}$ 比之前的工作（Cho et al. 2014, Sutskever, Vinyals, and Le 2014）更长，并且特征维度不局限于单变量情况（$d_{y} \geq 1$）。

**编码器-解码器架构**：许多流行模型被设计为将输入表示 $\mathcal{X}^{t}$ “编码”为隐藏状态表示 $\mathcal{H}^{t}$，并从 $\mathcal{H}^{t}=\left\{\mathbf{h}_{1}^{t}, \ldots, \mathbf{h}_{L_{h}}^{t}\right\}$ 中“解码”出输出表示 $\mathcal{Y}^{t}$。推理过程涉及一个逐步的“动态解码”过程，其中解码器根据前一个状态 $\mathbf{h}_{k}^{t}$ 和第 $k$ 步的其他必要输出计算新的隐藏状态 $\mathbf{h}_{k+1}^{t}$，然后预测第 $(k+1)$ 个序列 $\mathbf{y}_{k+1}^{t}$。

**输入表示**：为了增强时间序列输入的全局位置上下文和局部时间上下文，我们提供了一种统一的输入表示。为避免描述过于琐碎，细节请参见附录 B。

# 3 方法论

现有的时间序列预测方法大致可以分为两类${ }^{11}$：经典时间序列模型作为时间序列预测的可靠工具（Box et al. 2015; Ray 1990; Seeger et al. 2017; Seeger, Salinas, and Flunkert |2016），而深度学习技术主要通过使用 RNN 及其变体开发编码器-解码器预测范式（Hochreiter and Schmidhuber 1997; Li et al. |2018; Yu et al. 2017）。我们提出的 **Informer** 保留了编码器-解码器架构，同时针对 LSTF 问题进行了优化。请参考图 (2) 以获取概览，并参考以下部分以获取详细信息。

## 3.1 高效的自注意力机制

Vaswani et al. 2017 中提出的标准自注意力机制基于元组输入（即查询、键和值）定义，其执行缩放点积运算为 $\mathcal{A}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=$ $\operatorname{Softmax}\left(\mathbf{Q K}^{\top} / \sqrt{d}\right) \mathbf{V}$，其中 $\mathbf{Q} \in \mathbb{R}^{L_{Q} \times d}, \mathbf{K} \in \mathbb{R}^{L_{K} \times d}$, $\mathbf{V} \in \mathbb{R}^{L_{V} \times d}$，$d$ 为输入维度。为了进一步讨论自注意力机制，令 $\mathbf{q}_{i}, \mathbf{k}_{i}, \mathbf{v}_{i}$ 分别表示 $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ 的第 $i$ 行。根据 (Tsai et al. 2019) 中的公式，第 $i$ 个查询的注意力被定义为概率形式的核平滑器：

$$
\begin{equation*}
\mathcal{A}\left(\mathbf{q}_{i}, \mathbf{K}, \mathbf{V}\right)=\sum_{j} \frac{k\left(\mathbf{q}_{i}, \mathbf{k}_{j}\right)}{\sum_{l} k\left(\mathbf{q}_{i}, \mathbf{k}_{l}\right)} \mathbf{v}_{j}=\mathbb{E}_{p\left(\mathbf{k}_{j} \mid \mathbf{q}_{i}\right)}\left[\mathbf{v}_{j}\right] \tag{1}
\end{equation*}
$$

其中 $p\left(\mathbf{k}_{j} \mid \mathbf{q}_{i}\right)=k\left(\mathbf{q}_{i}, \mathbf{k}_{j}\right) / \displaystyle\sum_{l} k\left(\mathbf{q}_{i}, \mathbf{k}_{l}\right)$，$k\left(\mathbf{q}_{i}, \mathbf{k}_{j}\right)$ 选择非对称指数核 $\exp \left(\mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}\right)$。自注意力机制通过计算概率 $p\left(\mathbf{k}_{j} \mid \mathbf{q}_{i}\right)$ 结合值并获取输出。它需要二次的点积计算和 $\mathcal{O}\left(L_{Q} L_{K}\right)$ 的内存使用量，这是提升预测能力时的主要缺点。

---

一些先前的研究表明，自注意力概率分布具有潜在的稀疏性，并且他们在不影响性能的情况下设计了“选择性”计算策略来处理所有 $p\left(\mathbf{k}_{j} \mid \mathbf{q}_{i}\right)$。Sparse Transformer (Child et al. 2019) 结合了行输出和列输入，其中稀疏性源于分离的空间相关性。LogSparse Transformer (Li et al. 2019) 注意到自注意力中的周期性模式，并通过指数步长强制每个单元关注其前一个单元。Longformer (Beltagy, Peters, and Cohan 2020) 将前两项工作扩展到更复杂的稀疏配置。然而，它们仅限于从启发式方法中得出的理论分析，并且对每个多头自注意力采用相同的策略，这限制了它们的进一步改进。

---

为了激发我们的方法，我们首先对标准自注意力机制学习到的注意力模式进行定性评估。“稀疏”自注意力得分形成了长尾分布（详见附录 C），即少数点积对贡献了主要的注意力，而其他点积对生成了微不足道的注意力。那么，接下来的问题是如何区分它们？

---

##### 查询稀疏性度量

从公式 (1) 中，第 $i$ 个查询对所有键的注意力被定义为概率 $p\left(\mathbf{k}_{j} \mid \mathbf{q}_{i}\right)$，输出是其与值 $\mathbf{v}$ 的组合。主要的点积对会使得相应的查询注意力概率分布远离均匀分布。如果 $p\left(\mathbf{k}_{j} \mid \mathbf{q}_{i}\right)$ 接近均匀分布 $q\left(\mathbf{k}_{j} \mid \mathbf{q}_{i}\right)=1 / L_{K}$，自注意力机制将退化为对值 $\mathbf{V}$ 的简单求和，对输入信息来说是冗余的。自然地，分布 $p$ 和 $q$ 之间的“相似性”可以用来区分“重要”的查询。我们通过 Kullback-Leibler 散度 $K L(q \| p)=\ln \displaystyle\sum_{l=1}^{L_{K}} e^{\mathbf{q}_{i} \mathbf{k}_{l}^{\top} / \sqrt{d}}-\cfrac{1}{L_{K}} \displaystyle\sum_{j=1}^{L_{K}} \mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}-\ln L_{K}$ 来度量这种“相似性”。去掉常数项，我们将第 $i$ 个查询的稀疏性度量定义为：

$$
\begin{equation*}
M\left(\mathbf{q}_{i}, \mathbf{K}\right)=\ln \sum_{j=1}^{L_{K}} e^{\frac{\mathbf{q}_{i} \mathbf{k}_{j}^{\top}}{\sqrt{d}}}-\frac{1}{L_{K}} \sum_{j=1}^{L_{K}} \frac{\mathbf{q}_{i} \mathbf{k}_{j}^{\top}}{\sqrt{d}} \tag{2}
\end{equation*}
$$

其中，第一项是 $\mathbf{q}_{i}$ 在所有键上的 Log-Sum-Exp (LSE)，第二项是它们的算术平均值。如果第 $i$ 个查询的 $M\left(\mathbf{q}_{i}, \mathbf{K}\right)$ 较大，其注意力概率 $p$ 更加“多样化”，并且更有可能在长尾自注意力分布的头部区域包含主要的点积对。

---

##### ProbSparse 自注意力

基于上述度量，我们提出了 ProbSparse 自注意力机制，允许每个键仅关注 $u$ 个主导查询：

$$
\begin{equation*}
\mathcal{A}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{Softmax}\left(\frac{\overline{\mathbf{Q}} \mathbf{K}^{\top}}{\sqrt{d}}\right) \mathbf{V} \tag{3}
\end{equation*}
$$

其中，$\overline{\mathbf{Q}}$ 是一个与 $\mathbf{q}$ 大小相同的稀疏矩阵，它仅包含在稀疏性度量 $M(\mathbf{q}, \mathbf{K})$ 下排名前 $u$ 的查询。通过一个常数采样因子 $c$ 控制，我们设置 $u=c \cdot \ln L_{Q}$，这使得 ProbSparse 自注意力机制只需为每个查询-键查找计算 $\mathcal{O}\left(\ln L_{Q}\right)$ 的点积，并且层的内存使用量保持在 $\mathcal{O}\left(L_{K} \ln L_{Q}\right)$。在多头注意力机制下，这种注意力机制为每个头生成不同的稀疏查询-键对，从而避免了严重的信息丢失。

---

然而，遍历所有查询以计算度量 $M\left(\mathbf{q}_{i}, \mathbf{K}\right)$ 需要计算每个点积对，即二次复杂度 $\mathcal{O}\left(L_{Q} L_{K}\right)$，此外 LSE 操作可能存在数值稳定性问题。受此启发，我们提出了一种经验性近似方法，以高效获取查询稀疏性度量。

---

**引理 1**：对于键集 $\mathbf{K}$ 中的每个查询 $\mathbf{q}_{i} \in \mathbb{R}^{d}$ 和 $\mathbf{k}_{j} \in \mathbb{R}^{d}$，我们有如下边界：$\ln L_{K} \leq M\left(\mathbf{q}_{i}, \mathbf{K}\right) \leq$ $\max _{j}\left\{\mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}\right\}-\cfrac{1}{L_{K}} \displaystyle\sum_{j=1}^{L_{K}}\left\{\mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}\right\}+\ln L_{K}$。当 $\mathbf{q}_{i} \in \mathbf{K}$ 时，该边界同样成立。

根据引理 1（证明见附录 D.1），我们提出了 max-mean 度量方法：

$$
\begin{equation*}
\bar{M}\left(\mathbf{q}_{i}, \mathbf{K}\right)=\max _{j}\left\{\frac{\mathbf{q}_{i} \mathbf{k}_{j}^{\top}}{\sqrt{d}}\right\}-\frac{1}{L_{K}} \sum_{j=1}^{L_{K}} \frac{\mathbf{q}_{i} \mathbf{k}_{j}^{\top}}{\sqrt{d}} \tag{4}
\end{equation*}
$$

通过命题 1（见附录 D.2）的边界松弛，Top-$u$ 的范围近似成立。在长尾分布下，我们只需随机采样 $U=L_{K} \ln L_{Q}$ 个点积对来计算 $\bar{M}\left(\mathbf{q}_{i}, \mathbf{K}\right)$，即将其余对填充为零。然后，我们从这些对中选择稀疏的 Top-$u$ 作为 $\overline{\mathbf{Q}}$。$\bar{M}\left(\mathbf{q}_{i}, \mathbf{K}\right)$ 中的 max 操作对零值不敏感，且数值稳定。在实践中，自注意力计算中查询和键的输入长度通常相等，即 $L_{Q}=L_{K}=L$，因此 ProbSparse 自注意力的总时间复杂度和空间复杂度均为 $\mathcal{O}(L \ln L)$。

---

**图 3**：Informer 编码器中的单栈结构。  
(1) 水平栈表示图 (2) 中编码器副本的一个独立部分。  
(2) 所示的主栈接收整个输入序列，第二个栈接收输入的一半切片，后续栈重复此过程。  
(3) 红色层是点积矩阵，通过在每个层上应用自注意力蒸馏操作，它们会逐步减少。  
(4) 将所有栈的特征图连接起来作为编码器的输出。

## 3.2 编码器：在内存使用限制下处理更长序列输入

编码器旨在提取长序列输入的鲁棒长程依赖关系。在输入表示之后，第 $t$ 个序列输入 $\mathcal{X}^{t}$ 被塑造成矩阵 $\mathbf{X}_{\text {en }}^{t} \in \mathbb{R}^{L_{x} \times d_{\text {model }}}$。为了清晰起见，我们在图 (3) 中给出了编码器的草图。

---

**自注意力蒸馏**  
作为 ProbSparse 自注意力机制的自然结果，编码器的特征图中存在值 $\mathbf{V}$ 的冗余组合。我们使用蒸馏操作来突出具有主导特征的优质组合，并在下一层生成一个聚焦的自注意力特征图。它大幅修剪了输入的时间维度，如图 (3) 中注意力块的多头权重矩阵（重叠的红色方块）所示。受膨胀卷积（Yu, Koltun, and Funkhouser 2017; Gupta and Rush 2017）的启发，我们的“蒸馏”过程从第 $j$ 层向前传递到第 $(j+1)$ 层，公式如下：

$$
\begin{equation*}
\mathbf{X}_{j+1}^{t}=\operatorname{MaxPool}\left(\operatorname{ELU}\left(\operatorname{Conv1d}\left(\left[\mathbf{X}_{j}^{t}\right]_{\mathrm{AB}}\right)\right)\right) \tag{5}
\end{equation*}
$$

其中，$[\cdot]_{\mathrm{AB}}$ 表示注意力块。它包含多头 ProbSparse 自注意力和基本操作，其中 Conv1d $(\cdot)$ 在时间维度上执行一维卷积滤波器（核宽度=3），并使用 ELU $(\cdot)$ 激活函数（Clevert, Unterthiner, and Hochreiter 2016）。我们在堆叠一层后添加一个步幅为 2 的最大池化层，并将 $\mathbf{X}^{t}$ 下采样为其一半切片，从而将整体内存使用量减少到 $\mathcal{O}((2-\epsilon) L \log L)$，其中 $\epsilon$ 是一个很小的数。为了增强蒸馏操作的鲁棒性，我们构建了主栈的副本，输入减半，并逐步减少自注意力蒸馏层的数量，每次丢弃一层，如图 (2) 中的金字塔结构，以使它们的输出维度对齐。因此，我们将所有栈的输出连接起来，得到编码器的最终隐藏表示。

## 3.3 解码器：通过一次前向过程生成长序列输出

我们使用图 (2) 中的标准解码器结构（Vaswani et al. 2017），它由两个相同的多头注意力层堆叠而成。然而，为了缓解长预测中的速度下降问题，我们采用了生成式推理。我们将以下向量输入解码器：

$$
\begin{equation*}
\mathbf{X}_{\mathrm{de}}^{t}=\operatorname{Concat}\left(\mathbf{X}_{\text {token }}^{t}, \mathbf{X}_{\mathbf{0}}^{t}\right) \in \mathbb{R}^{\left(L_{\text {token }}+L_{y}\right) \times d_{\text {model }}}, \tag{6}
\end{equation*}
$$

其中，$\mathbf{X}_{\text {token }}^{t} \in \mathbb{R}^{L_{\text {token }} \times d_{\text {model }}}$ 是起始标记，$\mathbf{X}_{\mathbf{0}}^{t} \in$ $\mathbb{R}^{L_{y} \times d_{\text {model }}}$ 是目标序列的占位符（标量设置为 0）。在 ProbSparse 自注意力计算中，通过将掩码点积设置为 $-\infty$ 来应用掩码多头注意力。它防止每个位置关注未来的位置，从而避免了自回归。一个全连接层获取最终输出，其输出维度 $d_{y}$ 取决于我们执行的是单变量预测还是多变量预测。

---

**生成式推理**  
起始标记在 NLP 的“动态解码”（Devlin et al. 2018）中被高效应用，我们将其扩展为生成式方法。与选择特定标志作为标记不同，我们从输入序列中采样一个长度为 $L_{\text {token }}$ 的序列，例如输出序列之前的较早片段。以预测 168 个点为例（实验部分中的 7 天温度预测），我们将目标序列之前已知的 5 天作为“起始标记”，并将生成式推理解码器输入为 $\mathbf{X}_{\mathrm{de}}=\left\{\mathbf{X}_{5 d}, \mathbf{X}_{\mathbf{0}}\right\}$。$\mathbf{X}_{\mathbf{0}}$ 包含目标序列的时间戳，即目标周的上下文。然后，我们提出的解码器通过一次前向过程预测输出，而不是传统编码器-解码器架构中耗时的“动态解码”。在计算效率部分提供了详细的性能比较。

---

**损失函数**  
我们选择预测值与目标序列之间的均方误差（MSE）作为损失函数，损失从解码器的输出通过整个模型反向传播。

# 4 实验

## 4.1 数据集

我们在四个数据集上进行了广泛的实验，包括 2 个为长序列时间预测（LSTF）收集的真实世界数据集和 2 个公开基准数据集。

**ETT（电力变压器温度）${ }^{2}$**：ETT 是电力长期部署中的关键指标。我们收集了中国两个不同县的两年的数据。为了探索 LSTF 问题的粒度，我们创建了单独的数据集，$\left\{\mathrm{ETTh}_{1}, \mathrm{ETTh}_{2}\right\}$ 为 1 小时级别，ETTm ${ }_{1}$ 为 15 分钟级别。每个数据点包括目标值“油温”和 6 个电力负载特征。训练/验证/测试集分别为 $12 / 4 / 4$ 个月。

**ECL（电力消耗负载）${ }^{3}$**：该数据集收集了 321 个客户的电力消耗（千瓦时）。由于数据缺失（Li et al. 2019），我们将数据集转换为两年的每小时消耗，并将 'MT_320' 设为目标值。训练/验证/测试集分别为 $15 / 3 / 4$ 个月。

**Weather ${ }^{4}$**：该数据集包含 2010 年至 2013 年近 1,600 个美国地点的本地气候数据，数据点每小时收集一次。每个数据点包括目标值“湿球温度”和 11 个气候特征。训练/验证/测试集分别为 $28 / 10 / 10$ 个月。

## 4.2 实验细节

我们简要总结了基础知识，更多关于网络组件和设置的信息见附录 E。

**基线方法**：我们选择了五种时间序列预测方法作为对比，包括 ARIMA (Ariyo, Adewumi, and Ayo 2014)、Prophet (Taylor and Letham 2018)、LSTMa (Bahdanau, Cho, and Bengio |2015)、LSTnet (Lai et al. 2018) 和 DeepAR (Flunkert, Salinas, and Gasthaus 2017)。为了更好地探索 ProbSparse 自注意力在我们提出的 Informer 中的性能，我们在实验中加入了标准自注意力变体（Informer ${ }^{\dagger}$）、高效变体 Reformer (Kitaev, Kaiser, and Levskaya 2019) 和最相关的工作 LogSparse 自注意力 (Li et al. 2019)。网络组件的详细信息见附录 E.1。

---

**超参数调优**：我们对超参数进行了网格搜索，详细范围见附录 E.3。Informer 的编码器包含一个 3 层栈和一个 1 层栈（输入为 $1 / 4$），解码器为 2 层。我们提出的方法使用 Adam 优化器进行优化，学习率从 $1 e^{-4}$ 开始，每 epoch 衰减两倍。总 epoch 数为 8，并采用适当的早停策略。我们按照推荐设置对比方法，批量大小为 32。

**设置**：每个数据集的输入进行零均值归一化。在 LSTF 设置下，我们逐步延长预测窗口大小 $L_{y}$，即 $\{1 \mathrm{~d}, 2 \mathrm{~d}, 7 \mathrm{~d}, 14 \mathrm{~d}, 30 \mathrm{~d}, 40 \mathrm{~d}\}$ 用于 $\{E T T h$, ECL, Weather $\}$，$\{6 h, 12 h, 24 h, 72 h, 168 \mathrm{~h}\}$ 用于 ETTm。

**评估指标**：我们使用两个评估指标，包括均方误差（MSE）$=\cfrac{1}{n} \displaystyle\sum_{i=1}^{n}(\mathbf{y}-\hat{\mathbf{y}})^{2}$ 和平均绝对误差（MAE）$=\cfrac{1}{n} \displaystyle\sum_{i=1}^{n}|\mathbf{y}-\hat{\mathbf{y}}|$，用于每个预测窗口（多变量预测取平均值），并以步长 $=1$ 滚动整个数据集。

**平台**：所有模型在单块 Nvidia V100 32GB GPU 上训练/测试。源代码可在 https://github.com/zhouhaoyi/Informer2020 获取。

## 4.3 结果与分析

表 1 和表 2 总结了所有方法在 4 个数据集上的单变量/多变量评估结果。我们逐步延长预测范围以测试预测能力，其中 LSTF 问题设置被精确控制为在单块 GPU 上可处理。最佳结果以粗体标出。

---

**单变量时间序列预测**  

在此设置下，每种方法随时间序列预测单个变量。从表 1 中可以看出： 

(1) 所提出的 Informer 模型在所有数据集上显著提高了推理性能（最后一列的获胜次数），并且其预测误差在增长预测范围内平滑且缓慢上升，这证明了 Informer 在增强 LSTF 问题预测能力方面的成功。  

(2) Informer 在获胜次数上大多优于其标准退化版本 Informer ${ }^{\dagger}$，即 $32>12$，这支持了查询稀疏性假设在提供可比注意力特征图方面的有效性。我们提出的方法也优于最相关的工作 LogTrans 和 Reformer。我们注意到，Reformer 保持动态解码，在 LSTF 中表现不佳，而其他方法受益于生成式解码器作为非自回归预测器。 

(3) Informer 模型显示出比循环神经网络 LSTMa 显著更好的结果。我们的方法在 168、336 和 720 时间点的 MSE 分别减少了 $26.8 \%$、$52.4 \%$ 和 $60.1 \%$。这表明自注意力机制中较短的网络路径比基于 RNN 的模型具有更好的预测能力。  

(4) 所提出的方法在 MSE 上优于 DeepAR、ARIMA 和 Prophet，分别在 168、336 和 720 时间点平均减少了 $49.3 \%$、$61.1 \%$ 和 $65.1 \%$。在 ECL 数据集上，DeepAR 在较短范围（$\leq 336$）表现更好，而我们的方法在较长范围上超越。我们将此归因于一个具体例子，其中预测能力的有效性随着问题的可扩展性而体现。

---

**多变量时间序列预测**  

在此设置下，一些单变量方法不适用，而 LSTnet 是最先进的基线。相反，我们提出的 Informer 通过调整最终的 FCN 层可以轻松从单变量预测转变为多变量预测。从表 2 中可以看出：  

(1) 所提出的 Informer 模型大大优于其他方法，单变量设置中的发现 $1 \& 2$ 在多变量时间序列中仍然成立。  

(2) Informer 模型显示出比基于 RNN 的 LSTMa 和基于 CNN 的 LSTnet 更好的结果，MSE 在 168、336 和 720 时间点平均减少了 $26.6 \%$、$28.2 \%$ 和 $34.3 \%$。与单变量结果相比，压倒性的性能有所减弱，这种现象可能是由于特征维度预测能力的各向异性引起的。这超出了本文的范围，我们将在未来的工作中探讨。

---

**考虑粒度的 LSTF**  

我们进行了额外的比较，以探索不同粒度下的性能。ETTm 1（分钟级别）的序列 $\{96,288,672\}$ 与 ETTh $_{1}$（小时级别）的 $\{24,48,168\}$ 对齐。即使序列处于不同的粒度级别，Informer 仍然优于其他基线方法。

## 4.4 参数敏感性分析

我们在 ETTh1 数据集上对单变量设置下的 Informer 模型进行了敏感性分析。  

**输入长度**：在图 (4) 中，当预测短序列（如 48）时，最初增加编码器/解码器的输入长度会降低性能，但进一步增加会使 MSE 下降，因为它带来了重复的短期模式。然而，在预测长序列（如 168）时，输入越长，MSE 越低。这是因为更长的编码器输入可能包含更多的依赖关系，而更长的解码器标记具有丰富的局部信息。  

**采样因子**：采样因子控制公式 (3) 中 ProbSparse 自注意力的信息带宽。我们从较小的因子（=3）开始，逐步增加，总体性能略有提升，最终趋于稳定（图 4b）。这验证了我们的查询稀疏性假设，即自注意力机制中存在冗余的点积对。在实践中，我们将采样因子设为 $c=5$（红线）。  

**层堆叠组合**：层的复制对自注意力蒸馏具有补充作用，我们研究了每个堆叠 $\{\mathrm{L}, \mathrm{L} / 2, \mathrm{~L} / 4\}$ 的行为（图 4r）。较长的堆叠对输入更敏感，部分原因是它接收了更多的长期信息。我们方法的选择（红线），即结合 $L$ 和 $L / 4$，是最稳健的策略。

## 4.5 消融研究：Informer 的效果如何？

我们还对 ETTh $_{1}$ 进行了额外的消融实验。

**ProbSparse 自注意力机制的性能**  
在总体结果表 1 和表 2 中，我们限制了问题设置，使得标准自注意力的内存使用量可行。在本研究中，我们将我们的方法与 LogTrans 和 Reformer 进行比较，并彻底探索它们的极限性能。为了隔离内存效率问题，我们首先将设置减少为 $\{$ 批量大小 $=8$，头数 $=8$，$\operatorname{dim}=64\}$，并在单变量情况下保持其他设置。在表 3 中，ProbSparse 自注意力显示出比对比方法更好的性能。LogTrans 在极端情况下会出现内存不足（OOM），因为其公开实现是完整注意力的掩码，仍然具有 $\mathcal{O}\left(L^{2}\right)$ 的内存使用量。我们提出的 ProbSparse 自注意力通过公式 (4) 中的查询稀疏性假设带来的简单性避免了这一点（参见附录 E.2 中的伪代码），并达到了更小的内存使用量。

**自注意力蒸馏的性能**  

在本研究中，我们使用 Informer ${ }^{\dagger}$ 作为基准，以消除 ProbSparse 自注意力的额外影响。其他实验设置与单变量时间序列的设置一致。从表 5 中可以看出，Informer ${ }^{\dagger}$ 完成了所有实验，并在利用长序列输入后实现了更好的性能。对比方法 Informer ${ }^{\ddagger}$ 移除了蒸馏操作，在输入较长（$>7720$）时会出现内存不足（OOM）。关于长序列输入在 LSTF 问题中的好处，我们得出结论，自注意力蒸馏值得采用，尤其是在需要更长预测时。

---

**生成式解码器的性能**  

在本研究中，我们验证了我们的解码器在获取“生成式”结果方面的潜在价值。与现有方法不同，标签和输出在训练和推理中被强制对齐，而我们提出的解码器的预测仅依赖于时间戳，可以预测偏移量。从表 6 中可以看出，Informer ${ }^{\ddagger}$ 的整体预测性能在偏移量增加时保持稳定，而对比方法由于动态解码而失败。这证明了解码器在捕捉任意输出之间的长程依赖关系并避免误差累积方面的能力。

## 4.6 计算效率

在多变量设置下，并采用所有方法当前最精细的实现，我们在图 (5) 中进行了严格的运行时比较。在训练阶段，Informer（红线）在基于 Transformer 的方法中实现了最佳的训练效率。在测试阶段，我们的方法通过生成式解码比其他方法快得多。理论时间复杂度和内存使用的比较总结在表 4 中。Informer 的性能与运行时实验一致。请注意，LogTrans 专注于改进自注意力机制，为了公平比较，我们将我们提出的解码器应用于 LogTrans（表 4 中的 $\star$）。

# 5 结论

在本文中，我们研究了长序列时间序列预测问题，并提出了 Informer 来预测长序列。具体来说，我们设计了 ProbSparse 自注意力机制和蒸馏操作，以应对 Vanilla Transformer 中二次时间复杂度和二次内存使用的挑战。此外，精心设计的生成式解码器缓解了传统编码器-解码器架构的限制。在真实世界数据上的实验证明了 Informer 在增强 LSTF 问题预测能力方面的有效性。

# 附录

## 附录A 相关工作

我们以下提供了关于长序列时间序列预测（LSTF）问题的文献综述。

时间序列预测 现有的时间序列预测方法大致可以分为两类：经典模型和基于深度学习的方法。经典时间序列模型作为时间序列预测的可靠工具，具有可解释性和理论保证等吸引人的特性（Box等，2015；Ray，1990）。现代扩展包括对缺失数据的支持（Seeger等，2017）和多种数据类型的支持（Seeger, Salinas, and Flunkert，2016）。基于深度学习的方法主要通过使用RNN及其变体来开发序列到序列的预测范式，取得了突破性的性能（Hochreiter and Schmidhuber，1997；Li等，2018；Yu等，2017）。尽管取得了重大进展，现有算法仍然无法以令人满意的准确性预测长序列时间序列。典型的先进方法（Seeger等，2017；Seeger, Salinas, and Flunkert，2016），尤其是深度学习方法（Yu等，2017；Qin等，2017；Flunkert, Salinas, and Gasthaus，2017；Mukherjee等，2018；Wen等，2017），仍然采用逐步处理的序列到序列预测范式，具有以下局限性：（i）尽管它们可能在一步预测中取得准确结果，但由于动态解码的累积误差，它们往往在LSTF问题中出现较大误差（Liu等，2019；Qin等，2017）。预测精度随着预测序列长度的增加而衰减。（ii）由于梯度消失和内存限制问题（Sutskever, Vinyals, and Le，2014），大多数现有方法无法从时间序列的整个历史行为中学习。在我们的工作中，Informer旨在解决这两个局限性。

---

长序列输入问题 从上述讨论中，我们将第二个局限性称为长序列时间序列输入（LSTI）问题。我们将探讨相关研究，并将其与我们的LSTF问题进行比较。研究人员在实践中通过截断/总结/采样输入序列来处理非常长的序列，但在做出准确预测时可能会丢失有价值的数据。与修改输入不同，截断BPTT（Aicher, Foti, and Fox，2019）仅使用最后的时间步来估计权重更新中的梯度，而辅助损失（Trinh等，2018）通过添加辅助梯度来增强梯度流动。其他尝试包括循环高速公路网络（Zilly等，2017）和自举正则化器（Cao and Xu，2019）。这些方法试图改善循环网络中长路径的梯度流动，但随着LSTI问题中序列长度的增加，性能受到限制。基于CNN的方法（Stoller等，2019；Bai, Kolter, and Koltun，2018）使用卷积滤波器捕捉长期依赖关系，其感受野随着层的堆叠呈指数增长，这损害了序列对齐。在LSTI问题中，主要任务是增强模型接收长序列输入的能力，并从这些输入中提取长程依赖关系。但LSTF问题则寻求增强模型预测长序列输出的能力，这需要在输出和输入之间建立长程依赖关系。因此，上述方法不直接适用于LSTF。

---

注意力模型 Bahdanau等首次提出了加法注意力（Bahdanau, Cho, and Bengio，2015），以改进翻译任务中编码器-解码器架构的词对齐。随后，其变体（Luong, Pham, and Manning，2015）提出了广泛使用的位置、通用和点积注意力。最近，基于自注意力的Transformer（Vaswani等，2017）作为一种新的序列建模思想被提出，并取得了巨大成功，尤其是在NLP领域。通过将其应用于翻译、语音、音乐和图像生成，其更好的序列对齐能力得到了验证。在我们的工作中，Informer利用了其序列对齐能力，并将其应用于LSTF问题。

---

基于Transformer的时间序列模型 最相关的工作（Song等，2018；Ma等，2019；Li等，2019）都始于将Transformer应用于时间序列数据的尝试，但由于使用了原始Transformer，在LSTF预测中失败了。其他一些工作（Child等，2019；Li等，2019）注意到了自注意力机制中的稀疏性，我们已在正文中讨论了它们。

## 附录B 统一的输入表示

RNN模型（Schuster and Paliwal，1997；Hochreiter and Schmidhuber，1997；Chung等，2014；Sutskever, Vinyals, and Le，2014；Qin等，2017；Chang等，2018）通过循环结构本身捕捉时间序列模式，几乎不依赖时间戳。原始Transformer（Vaswani等，2017；Devlin等，2018）使用逐点自注意力机制，时间戳作为局部位置上下文。然而，在LSTF问题中，捕捉长程独立性的能力需要全局信息，如分层时间戳（周、月、年）和不可知时间戳（节假日、事件）。这些在经典自注意力中很少被利用，编码器和解码器之间的查询-键不匹配会导致预测性能的潜在下降。我们提出了一种统一的输入表示来缓解这一问题，图6给出了直观的概述。

---

假设我们有第$t$个序列输入$\mathcal{X}^{t}$和$p$种全局时间戳，输入表示后的特征维度为$d_{\text{model}}$。我们首先通过使用固定的位置嵌入来保留局部上下文：

$$
\begin{align*}
\mathrm{PE}_{(p o s, 2 j)} & =\sin \left(p o s /\left(2 L_{x}\right)^{2 j / d_{\mathrm{model}}}\right) \\
\mathrm{PE}_{(p o s, 2 j+1)} & =\cos \left(p o s /\left(2 L_{x}\right)^{2 j / d_{\mathrm{model}}}\right) \tag{7}
\end{align*}
$$

其中$j \in\left\{1, \ldots,\left\lfloor d_{\text{model}} / 2\right\rfloor\right\}$。每个全局时间戳通过一个可学习的时间戳嵌入$\mathrm{SE}_{(\text{pos})}$来表示，其词汇量有限（最多60，即以分钟为最细粒度）。也就是说，自注意力的相似性计算可以访问全局上下文，并且在长输入上的计算消耗是可承受的。为了对齐维度，我们将标量上下文$\mathbf{x}_{i}^{t}$投影到$d_{\text{model}}$维向量$\mathbf{u}_{i}^{t}$，使用一维卷积滤波器（核宽度=3，步长=1）。因此，我们得到输入向量

$$
\begin{equation*}
\mathcal{X}_{\text{feed }[i]}^{t}=\alpha \mathbf{u}_{i}^{t}+\mathrm{PE}_{\left(L_{x} \times(t-1)+i,\right)}+\sum_{p}\left[\mathrm{SE}_{\left(L_{x} \times(t-1)+i\right)}\right]_{p} \tag{8}
\end{equation*}
$$

其中$i \in\left\{1, \ldots, L_{x}\right\}$，$\alpha$是平衡标量投影与局部/全局嵌入幅度的因子。如果序列输入已归一化，我们推荐$\alpha=1$。

---

图6：Informer的输入表示。输入的嵌入由三个独立的部分组成：标量投影、局部时间戳（位置）和全局时间戳嵌入（分钟、小时、周、月、节假日等）。

## 附录C 自注意力特征图中的长尾分布

我们在ETTh${}_{1}$数据集上运行了原始Transformer，以研究自注意力特征图的分布。我们选择了第1层的\{Head1, Head7\}的注意力分数。图7中的蓝线形成了长尾分布，即少数点积对贡献了主要的注意力，而其他部分可以忽略。

## 附录D 证明细节

### 引理1的证明

证明。对于单个$\mathbf{q}_{i}$，我们可以将离散的键松弛为连续的$d$维变量，即向量$\mathbf{k}_{j}$。查询稀疏性度量定义为$M\left(\mathbf{q}_{i}, \mathbf{K}\right)=\ln \displaystyle\sum_{j=1}^{L_{K}} e^{\mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}}-\cfrac{1}{L_{K}} \displaystyle\sum_{j=1}^{L_{K}}\left(\mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}\right)$。

---

首先，我们来看不等式左侧。对于每个查询$\mathbf{q}_{i}$，$M\left(\mathbf{q}_{i}, \mathbf{K}\right)$的第一项变为固定查询$\mathbf{q}_{i}$与所有键的内积的log-sum-exp，我们可以定义$f_{i}(\mathbf{K})=\ln \displaystyle\sum_{j=1}^{L_{K}} e^{\mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}}$。根据Log-sum-exp网络中的公式（2）（Calafiore, Gaubert, and Possieri，2018）以及进一步分析，函数$f_{i}(\mathbf{K})$是凸的。此外，$f_{i}(\mathbf{K})$加上$\mathbf{k}_{j}$的线性组合使得$M\left(\mathbf{q}_{i}, \mathbf{K}\right)$对于固定查询成为凸函数。然后，我们可以对度量关于单个向量$\mathbf{k}_{j}$求导，得到$\cfrac{\partial M\left(\mathbf{q}_{i}, \mathbf{K}\right)}{\partial \mathbf{k}_{j}}=\cfrac{e^{\mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}}}{\displaystyle\sum_{j=1}^{L_{K}} e^{\mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}}} \cdot \cfrac{\mathbf{q}_{i}}{\sqrt{d}}-\cfrac{1}{L_{K}} \cdot \cfrac{\mathbf{q}_{i}}{\sqrt{d}}$。为了达到最小值，我们令$\vec{\nabla} M\left(\mathbf{q}_{i}\right)=\overrightarrow{0}$，并得到以下条件：$\mathbf{q}_{i} \mathbf{k}_{1}^{\top}+\ln L_{K}=\cdots=$ $\mathbf{q}_{i} \mathbf{k}_{j}^{\top}+\ln L_{K}=\cdots=\ln \displaystyle\sum_{j=1}^{L_{K}} e^{\mathbf{q}_{i} \mathbf{k}_{j}^{\top}}$。自然地，这要求$\mathbf{k}_{1}=\mathbf{k}_{2}=\cdots=\mathbf{k}_{L_{K}}$，并且我们得到度量的最小值为$\ln L_{K}$，即

$$
\begin{equation*}
M\left(\mathbf{q}_{i}, \mathbf{K}\right) \geq \ln L_{K} \tag{9}
\end{equation*}
$$

---

其次，我们来看不等式右侧。如果我们选择最大的内积$\max _{j}\left\{\mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}\right\}$，很容易得到
$$
\begin{align*}
M\left(\mathbf{q}_{i}, \mathbf{K}\right) & =\ln \sum_{j=1}^{L_{K}} e^{\frac{\mathbf{q}_{i} \mathbf{k}_{j}^{\top}}{\sqrt{d}}}-\frac{1}{L_{K}} \sum_{j=1}^{L_{K}}\left(\frac{\mathbf{q}_{i} \mathbf{k}_{j}^{\top}}{\sqrt{d}}\right) \\
& \leq \ln \left(L_{K} \cdot \max _{j}\left\{\frac{\mathbf{q}_{i} \mathbf{k}_{j}^{\top}}{\sqrt{d}}\right\}\right)-\frac{1}{L_{K}} \sum_{j=1}^{L_{K}}\left(\frac{\mathbf{q}_{i} \mathbf{k}_{j}^{\top}}{\sqrt{d}}\right) .  \tag{10}\\
& =\ln L_{K}+\max _{j}\left\{\frac{\mathbf{q}_{i} \mathbf{k}_{j}^{\top}}{\sqrt{d}}\right\}-\frac{1}{L_{K}} \sum_{j=1}^{L_{K}}\left(\frac{\mathbf{q}_{i} \mathbf{k}_{j}^{\top}}{\sqrt{d}}\right)
\end{align*}
$$

结合公式（14）和公式（15），我们得到引理1的结果。当键集与查询集相同时，上述讨论同样成立。

---

命题1。假设$\mathbf{k}_{j} \sim \mathcal{N}(\mu, \Sigma)$，并令$\mathbf{q} \mathbf{k}_{i}$表示集合$\left\{\left(\mathbf{q}_{i} \mathbf{k}_{j}^{\top}\right) / \sqrt{d} \mid j=1, \ldots, L_{K}\right\}$，则对于$\forall M_{m}=$ $\max _{i} M\left(\mathbf{q}_{i}, \mathbf{K}\right)$，存在$\kappa>0$，使得在区间$\forall \mathbf{q}_{1}, \mathbf{q}_{2} \in\left\{\mathbf{q} \mid M(\mathbf{q}, \mathbf{K}) \in\left[M_{m}, M_{m}-\kappa\right)\right\}$内，如果$\bar{M}\left(\mathbf{q}_{1}, \mathbf{K}\right)>$ $\bar{M}\left(\mathbf{q}_{2}, \mathbf{K}\right)$且$\operatorname{Var}\left(\mathbf{q} \mathbf{k}_{1}\right)>\operatorname{Var}\left(\mathbf{q} \mathbf{k}_{2}\right)$，则$M\left(\mathbf{q}_{1}, \mathbf{K}\right)>M\left(\mathbf{q}_{2}, \mathbf{K}\right)$的概率很高。

### 命题1的证明

证明。为了简化进一步的讨论，我们可以记$a_{i, j}=q_{i} k_{j}^{T} / \sqrt{d}$，从而定义数组$A_{i}=\left[a_{i, 1}, \cdots, a_{i, L_{k}}\right]$。此外，我们记$\cfrac{1}{L_{K}} \displaystyle\sum_{j=1}^{L_{K}}\left(\mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}\right)=\operatorname{mean}\left(A_{i}\right)$，则可以记$\bar{M}\left(\mathbf{q}_{i}, \mathbf{K}\right)=\max \left(A_{i}\right)-\operatorname{mean}\left(A_{i}\right), i=1,2$。

---

对于$M\left(\mathbf{q}_{i}, \mathbf{K}\right)$，我们记每个分量$a_{i, j}=$ $\operatorname{mean}\left(A_{i}\right)+\Delta a_{i, j}, j=1, \cdots, L_{k}$，则我们有以下公式：

$$
\begin{aligned}
M\left(\mathbf{q}_{i}, \mathbf{K}\right) & =\ln \sum_{j=1}^{L_{K}} e^{\mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}}-\frac{1}{L_{K}} \sum_{j=1}^{L_{K}}\left(\mathbf{q}_{i} \mathbf{k}_{j}^{\top} / \sqrt{d}\right) \\
& =\ln \left(\Sigma_{j=1}^{L_{k}} e^{\operatorname{mean}\left(A_{i}\right)} e^{\Delta a_{i, j}}\right)-\operatorname{mean}\left(A_{i}\right), \\
& =\ln \left(e^{\operatorname{mean}\left(A_{i}\right)} \Sigma_{j=1}^{L_{k}} e^{\Delta a_{i, j}}\right)-\operatorname{mean}\left(A_{i}\right) \\
& =\ln \left(\Sigma_{j=1}^{L_{k}} e^{\Delta a_{i, j}}\right)
\end{aligned}
$$

并且很容易发现$\Sigma_{j=1}^{L_{k}} \Delta a_{i, j}=0$。

---

我们定义函数$E S\left(A_{i}\right)=\Sigma_{j=1}^{L_{k}} \exp \left(\Delta a_{i, j}\right)$，等价地定义$A_{i}=\left[\Delta a_{i, 1}, \cdots, \Delta a_{i, L_{k}}\right]$，立即我们的命题可以写成等价形式：

对于$\forall A_{1}, A_{2}$，如果

1. $\max \left(A_{1}\right)-\operatorname{mean}\left(A_{1}\right) \geq \max \left(A_{2}\right)-\operatorname{mean}\left(A_{2}\right)$
2. $\operatorname{Var}\left(A_{1}\right)>\operatorname{Var}\left(A_{2}\right)$

那么我们将原始结论重新表述为更一般的形式，即$E S\left(A_{1}\right)>E S\left(A_{2}\right)$的概率很高，且概率与$\operatorname{Var}\left(A_{1}\right)-\operatorname{Var}\left(A_{2}\right)$呈正相关。

---

此外，我们考虑一个精细的情况，$\forall M_{m}=$ $\max _{i} M\left(\mathbf{q}_{i}, \mathbf{K}\right)$存在$\kappa>0$，使得在该区间内$\forall \mathbf{q}_{i}, \mathbf{q}_{j} \in\left\{\mathbf{q} \mid M(\mathbf{q}, \mathbf{K}) \in\left[M_{m}, M_{m}-\kappa\right)\right\}$，如果$\max \left(A_{1}\right)-$ $\operatorname{mean}\left(A_{1}\right) \geq \max \left(A_{2}\right)-\operatorname{mean}\left(A_{2}\right)$且$\operatorname{Var}\left(A_{1}\right)>$ $\operatorname{Var}\left(A_{2}\right)$，则$M\left(\mathbf{q}_{1}, \mathbf{K}\right)>$ $M\left(\mathbf{q}_{2}, \mathbf{K}\right)$的概率很高，这等价于$E S\left(A_{1}\right)>E S\left(A_{2}\right)$。

---

在原始命题中，$\mathbf{k}_{\mathbf{j}} \sim \mathcal{N}(\mu, \Sigma)$服从多元高斯分布，这意味着$k_{1}, \cdots, k_{n}$是独立同分布的高斯分布，因此根据Wienerkhinchin大数定律，$a_{i, j}=q_{i} k_{j}^{T} / \sqrt{d}$是一维高斯分布，当$n \rightarrow \infty$时其期望为0。回到我们的定义，$\Delta a_{1, m} \sim$ $N\left(0, \sigma_{1}^{2}\right), \Delta a_{2, m} \sim N\left(0, \sigma_{2}^{2}\right), \forall m \in 1, \cdots, L_{k}$，我们的命题等价于一个对数正态分布求和问题。

---

对数正态分布求和问题等价于准确近似$\operatorname{ES}\left(A_{1}\right)$的分布，其历史在文献（Dufresne 2008）和（Vargasguzman 2005）中得到了很好的介绍。近似对数正态分布的和是一个众所周知的经验法则，并且对于对数正态分布的和，没有通用的PDF函数。然而，（Romeo, Da Costa, and Bardou 2003）和（Hcine and Bouallegue 2015）指出，在大多数情况下，对数正态分布的和仍然是一个对数正态分布，并且通过应用（Beaulieu 2011）中的中心极限定理，我们可以很好地近似$E S\left(A_{1}\right)$是一个对数正态分布，并且我们有$E\left(E S\left(A_{1}\right)\right)=n e^{\frac{\sigma_{1}^{2}}{2}}$，$\operatorname{Var}\left(E S\left(A_{1}\right)\right)=n e^{\sigma_{1}^{2}}\left(e^{\sigma_{1}^{2}}-1\right)$。同样地，$E\left(E S\left(A_{2}\right)\right)=$ $n e^{\frac{\sigma_{2}^{2}}{2}}$，$\operatorname{Var}\left(E S\left(A_{2}\right)\right)=n e^{\sigma_{2}^{2}}\left(e^{\sigma_{2}^{2}}-1\right)$。

---

我们记$B_{1}=E S\left(A_{1}\right)$，$B_{2}=E S\left(A_{2}\right)$，并且概率$\operatorname{Pr}\left(B_{1}-B_{2}>0\right)$是我们命题在一般条件下的最终结果，其中$\sigma_{1}^{2}>\sigma_{2}^{2}$。对数正态分布的差异仍然是一个难以解决的问题。

---

通过使用（Lo 2012）中给出的定理，该定理提供了对数正态分布的和与差的概率分布的一般近似。即$S_{1}$和$S_{2}$是两个服从随机微分方程$\cfrac{d S_{i}}{S_{i}}=\sigma_{i} d Z_{i}, i=1,2$的对数正态随机变量，其中$d Z_{1,2}$分别表示与$S_{1,2}$相关的标准维纳过程，$\sigma_{i}^{2}=\operatorname{Var}\left(\ln S_{i}\right)$，$S^{ \pm} \equiv$ $S_{1} \pm S_{2}$，$S_{0}^{ \pm} \equiv S_{10} \pm S_{20}$。对于联合概率分布函数$P\left(S_{1}, S_{2}, t ; S_{10}, S_{20}, t_{0}\right)$，$S_{1}$和$S_{2}$在时间$t>t_{0}$的值由它们在初始时间$t_{0}$的初始值$S_{10}$和$S_{20}$提供。上述维纳过程等价于对数正态分布（Weiner and Solbrig 1984），下面的结论以一般形式写成，包含对数正态分布的和与差的近似，分别用$+$表示和，用$-$表示差。

---

在边界条件下

$$
\bar{P}_{ \pm}\left(S^{ \pm}, t ; S_{10}, S_{20}, t_{0} \longrightarrow t\right)=\delta\left(S_{10} \pm S_{20}-S^{ \pm}\right)
$$

它们的闭式概率分布函数由以下公式给出：

$$
\begin{aligned}
  f^{\mathrm{LN}}\left(\tilde{S}^{ \pm}, t ; \tilde{S}_{0}^{ \pm}, t_{0}\right) 
 =\frac{1}{\widetilde{S}^{ \pm} \sqrt{2 \pi \tilde{\sigma}_{ \pm}^{2}\left(t-t_{0}\right)}} \cdot \exp \left\{-\frac{\left[\ln \left(\tilde{S}^{+} / \tilde{S}_{0}^{+}\right)+(1 / 2) \tilde{\sigma}_{ \pm}^{2}\left(t-t_{0}\right)\right]^{2}}{2 \tilde{\sigma}_{ \pm}^{2}\left(t-t_{0}\right)}\right\}
\end{aligned}
$$

这是一个近似正态分布，其中$\tilde{S}^{+}$和$\tilde{S}^{-}$是对数正态随机变量，$\tilde{S}_{0}^{ \pm}$是在$t_{0}$时刻由上述维纳过程定义的初始条件。（需要注意的是，$\tilde{\sigma}_{ \pm}^{2}\left(t-t_{0}\right)$应该足够小以使此近似有效。在我们的模拟实验中，我们假设$t-t_{0}=1$。）由于

$$
\widetilde{S}_{0}^{-}=\left(S_{10}-S_{20}\right)+\left(\frac{\sigma_{-}^{2}}{\sigma_{1}^{2}-\sigma_{2}^{2}}\right)\left(S_{10}+S_{20}\right),
$$

以及

$$
\begin{gathered}
\tilde{\sigma_{-}}=\left(\sigma_{1}^{2}-\sigma_{2}^{2}\right) /\left(2 \sigma_{-}\right) \\
\sigma_{-}=\sqrt{\sigma_{1}^{2}+\sigma_{2}^{2}}
\end{gathered}
$$

注意到$E\left(B_{1}\right)>E\left(B_{2}\right)$，$\operatorname{Var}\left(B_{1}\right)>\operatorname{Var}\left(B_{2}\right)$，近似正态分布的均值和方差与$\sigma_{1}^{2}-\sigma_{2}^{2}$呈正相关。此外，闭式PDF $f^{\mathrm{LN}}\left(\tilde{S}^{ \pm}, t ; \tilde{S}_{0}^{ \pm}, t_{0}\right)$也与$\sigma_{1}^{2}-\sigma_{2}^{2}$呈正相关。由于$\tilde{\sigma}_{ \pm}^{2}\left(t-t_{0}\right)$需要足够小的限制，这种正相关性在我们的数值实验中并不显著。

---

通过使用（Lo 2012）中的Lie-Trotter算子分裂方法，我们可以为$B_{1}-B_{2}$的分布提供说明性的数值示例，其中参数经过精心选择以适合我们在实际LLLT实验中的top-u近似。图表明，当$\sigma_{1}^{2}>\sigma_{2}^{2}$时，$B_{1}>B_{2}$，$E S\left(A_{1}\right)>E S\left(A_{2}\right)$的不等式成立的概率很高。

---

在完成一般条件下命题的证明后，我们可以考虑一个更具体的条件：如果$\mathbf{q}_{1}, \mathbf{q}_{2} \in$ $\left\{\mathbf{q} \mid M(\mathbf{q}, \mathbf{K}) \in\left[M_{m}, M_{m}-\kappa\right)\right\}$，命题仍然以高概率成立。

首先，我们有$M\left(q_{1}, \mathbf{k}\right)=\ln \left(B_{1}\right)>\left(M_{m}-\kappa\right)$在该区间内对$\forall q_{1}, q_{2}$成立。由于我们已经证明了$\left.E\left(B_{1}\right)\right)=n e^{\cfrac{\sigma_{1}^{2}}{2}}$，我们可以得出结论：对于给定区间内的$\forall q_{i}$，存在$\alpha$使得$\sigma_{i}^{2}>\alpha, i=1,2$。由于我们有$\widetilde{S}_{0}^{-}=$ $\left(S_{10}-S_{20}\right)+\left(\cfrac{\sigma_{-}^{2}}{\sigma_{1}^{2}-\sigma_{2}^{2}}\right)\left(S_{10}+S_{20}\right)$，这也显示出与$\sigma_{1}^{2}+\sigma_{2}^{2}>2 \alpha$的正相关性，以及与$\sigma_{1}^{2}-\sigma_{2}^{2}$的正相关性。因此，由于近似正态分布PDF的性质，如果$\sigma_{1}^{2}>\sigma_{2}^{2}$，则$\operatorname{Pr}\left(M\left(q_{1}, \mathbf{k}\right)>M\left(q_{2}, \mathbf{k}\right)\right) \approx \Phi\left(\cfrac{\widetilde{S}_{0}^{-}}{\sigma_{-}}\right)$也显示出与$\sigma_{1}^{2}+\sigma_{2}^{2}>2 \alpha$的正相关性。

---

我们在图8中给出了上述近似的说明性数值示例。在我们的实际LTTnet实验中，我们选择$\widehat{A}_{1}, A_{2}$的Top-k，而不是整个集合。实际上，我们可以做一个简单的假设：在选择$A_{1}, A_{2}$的前$\left\lfloor\cfrac{1}{4} L_{k}\right\rfloor$个变量（记为$A_{1}^{\prime}, A_{2}^{\prime}$）时，方差$\sigma_{1}, \sigma_{2}$没有显著变化，但期望$E\left(A_{1}^{\prime}\right), E\left(A_{2}^{\prime}\right)$明显上升，这导致初始条件$S_{10}, S_{20}$显著上升，因为初始条件将从Top $-\left\lfloor\cfrac{1}{4} L_{k}\right\rfloor$变量中采样，而不是整个集合。

---

在我们的实际LTTnet实验中，我们设置$U$，即选择$A_{1}$和$A_{2}$的前$\left\lfloor\cfrac{1}{4} L_{k}\right\rfloor$个变量，保证在$\left[M_{m}, M_{m}-\kappa\right)$区间内，如图8中的黑色曲线所示，概率超过$99\%$。通常条件2可以放宽，我们可以相信，如果$q_{1}, q_{2}$满足命题中的条件1，则有$M\left(\mathbf{q}_{1}, \mathbf{K}\right)>M\left(\mathbf{q}_{2}, \mathbf{K}\right)$。

## 附录E 可复现性

### 实验细节

提出的Informer模型的细节总结在表7中。对于ProbSparse自注意力机制，我们设$d=32, n=16$，并添加了残差连接、一个逐位置前馈网络层（内层维度为2048）和一个Dropout层（$p=0.1$）。需要注意的是，我们为每个数据集保留了$10\%$的验证数据，因此所有实验都在时间轴上进行了5次随机训练/验证划分选择，结果取5次运行的平均值。所有数据集都进行了标准化处理，使得变量的均值为0，标准差为1。

### ProbSparse自注意力的实现

我们使用Python 3.6和Pytorch 1.0实现了ProbSparse自注意力机制。伪代码在算法1中给出。源代码可在https://github.com/zhouhaoyi/Informer2020获取。所有过程都可以通过高效的向量操作实现，并保持对数级的总内存使用量。屏蔽版本可以通过在第6步应用位置屏蔽并在第7步的均值计算中使用`cumsum($\cdot$)`来实现。在实践中，我们可以使用`sum($\cdot$)`作为`mean($\cdot$)`的简化实现。

### 超参数调优范围

对于所有方法，循环组件的输入长度从$\{24,48,96,168,336,720\}$中选择，适用于ETTh1、ETTh2、Weather和Electricity数据集，而对于ETTm数据集则从$\{24,48,96,192,288,672\}$中选择。对于LSTMa和DeepAR，隐藏状态的大小从$\{32,64,128,256\}$中选择。对于LSTnet，循环层和卷积层的隐藏维度分别从$\{64,128,256\}$和$\{32,64,128\}$中选择，而循环跳跃层的跳跃长度在ETTh1、ETTh2、Weather和ECL数据集中设为24，在ETTm数据集中设为96。对于Informer，编码器的层数从$\{6,4,3,2\}$中选择，解码器的层数设为2。多头注意力的头数从$\{8,16\}$中选择，多头注意力的输出维度设为512。编码器输入序列的长度和解码器起始标记的长度从$\{24,48,96,168,336,480,720\}$中选择，适用于ETTh1、ETTh2、Weather和ECL数据集，而对于ETTm数据集则从$\{24,48,96,192,288,480,672\}$中选择。在实验中，解码器的起始标记是从编码器输入序列中截取的一段，因此解码器起始标记的长度必须小于编码器输入序列的长度。

---

基于RNN的方法在预测窗口上执行动态解码并进行左移。我们提出的Informer系列方法和LogTrans（我们的解码器）执行非动态解码。

## 附录F 额外实验结果

图（9）展示了8个模型的部分预测结果。最相关的工作LogTrans和Reformer显示了可接受的结果。LSTMa模型不适合长序列预测任务。ARIMA和DeepAR能够捕捉长序列的长期趋势。Prophet检测到变化点，并用平滑曲线拟合，效果优于ARIMA和DeepAR。我们提出的模型Informer和Informer ${ }^{\dagger}$显示出显著优于上述方法的结果。

## 附录G 计算基础设施

所有实验均在Nvidia Tesla V100 SXM2 GPU（32GB内存）上进行。其他配置包括2 * Intel Xeon Gold 6148 CPU、384GB DDR4内存和2 * 240GB M.2 SSD，这些配置足以支持所有基线模型。
