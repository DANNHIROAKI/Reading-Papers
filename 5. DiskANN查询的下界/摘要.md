# 摘要  

我们提出了一种简单的确定性数据结构，用于维护一般度量空间中点集$S$，并支持邻近搜索（最近邻查询和范围查询）以及对$S$的更新（插入和删除）。该数据结构由一系列逐渐精细化的$S$的$\epsilon$-网组成，带有指针，使我们能够轻松地在不同尺度之间进行导航。

我们基于度量$S$的“抽象维度”分析了该数据结构的最坏情况复杂度。对于维度受限的度量空间，我们的数据结构非常高效，并且在某种距离计算模型中基本上是最优的。最后，作为一个特例，我们的方法改进了Karger和Ruhl [KR02] 最近提出的方法。

# 1. Intro

最近邻搜索（NNS）问题是指预处理一个位于巨大的（可能是无限的）度量空间$(M, d)$中的点集$S$，使得在给定查询点$q \in M$的情况下，能够高效地找到$S$中最接近$q$的点。高效地计算这样的最近邻是一个经典且基础的问题，具有许多实际应用。这些应用包括数据压缩、数据库查询、机器学习、计算生物学、数据挖掘、模式识别和临时网络等。许多此类应用的一个共同特征是，比较两个元素的代价很高，因此应尽量减少距离计算的次数。

一般的NNS问题有几种不同的形式。例如，宿主度量空间$(M, d)$可能是特定应用的（例如，它可能是拼写检查器或基因组数据中的字符串之间的加权编辑距离），它可能是无限的（例如欧几里得空间），甚至可能如此无结构，以至于在实际中是未知的（例如在表示节点间延迟的对等网络中）。预处理阶段的空间和时间可能会受到限制（例如，限制在$n$的多项式或线性范围内）。所需应用可能要求数据结构是动态的，这意味着它应该高效地支持从$S$中插入和删除点。此外，可能希望数据结构分布在数据点之间，在这种情况下，通信和负载等其他成本衡量标准可能非常重要。最后，可能仅需要近似解。设$q$为查询点，$a \in S$为$S$中距离$q$最近的点。那么一个$(1+\epsilon)$-NNS算法是指，在给定$q$的情况下，返回某个$s \in S$，使得$d(q, s) \leq(1+\epsilon) d(q, a)$。

大多数以往的研究集中在$M=\mathbb{R}^d$且距离根据某些$\ell_p$范数计算的这一重要特殊情况。虽然许多类型的数据可以自然地表示为这种形式，但对于相当多的应用来说，这显然并不适用，因此在一般度量空间中解决最近邻搜索（NNS）问题是值得探讨的。一方面，针对一般度量的结构可能会导致最近邻查询的时间复杂度差至$\Omega(n)$，这在实际中是无法接受的。即使仅要求近似解，这种依赖性也是固有的。一个著名的例子是$S$构成一个均匀度量空间，即$S$中的点间距离都相等，几乎没有提供任何信息。（模型的具体定义和额外的下界见第3节。）另一方面，实际中出现的度量空间通常具有更好的结构，可以在算法上加以利用。

在这种情况下，最近的研究越来越关注以度量的隐含结构为基础来理解NNS的复杂性。在欧几里得空间中，度量复杂性的一个自然且常见的度量标准是欧几里得宿主空间的维度。确实，大多数算法的效率依赖于这个维度。事实上，许多算法的运行时间随着维度的增加呈指数增长，这种现象被称为维度灾难（一些显著的例外是[IM98, KOR98]中的算法）。因此，尝试为抽象度量空间定义类似的维度概念是很自然的。

我们的第一个贡献是概念性的——我们在NNS的背景下提出了一种基于Assouad [Ass83]（来自度量空间分析理论）的抽象维度的自然概念。这一概念得到了技术性贡献的支持——我们==为一般低维度度量空间中的NNS提供了一个高效且动态的数据结构==。其他关于维度的概念也在一般度量空间中的NNS问题上下文中被提出和研究过[Cla99, KR02]，而我们的方案在与这些相比时表现良好。此外，尽管我们的方法是通用的，特别是独立于欧几里得几何学，但它与已知的针对低维欧几里得空间（具有线性预处理空间）的最佳方案[$\mathrm{AMN}^{+}98$]相当。详细内容见第1.3节。

## 1.1. Abstract dimension  

==度量空间$(X, d)$的倍增维度（doubling dimension），在本文中记作$\operatorname{dim}(X)$，是满足以下条件的最小值$\rho$：$X$中的任意集合都可以被$2^\rho$个直径为原来一半的集合覆盖==。（集合$S \subseteq X$的直径定义为$\sup \{d(x, y): x, y \in S\}$。）通常情况下，将$S \subseteq X$中的$x$为中心、半径为$r$的（闭）球定义为$B_S(x, r)=\{y \in S: d(x, y) \leq r\}$；当上下文清楚时，可以省略下标$S$。不难看出，使得$X$中的每个球都可以被$2^\rho$个半径为原来一半的球覆盖的最小值$\rho$与上述定义的倍数在2倍以内——这就是我们在本文中使用的定义。

如果一个度量空间的维度为$O(1)$，则称其为倍增空间。例如，$k$个点的均匀度量空间的维度为$\log k$。一个近似的逆命题也成立（见下面的引理1.2），表明小倍增维度量化了缺乏大规模接近均匀度量的现象。换句话说，这种维度概念衡量了$X$的“体积增长”。我们注意到，Clarkson [Cla99]在NNS问题的背景下使用了一个类似的概念（但名称不同），然而他的结果似乎没有完全发挥出该概念的潜力。（详细内容见第1.3节。）

倍增维度具有一些自然且吸引人的性质。令$(X, d)$为任意度量空间，以下是其中一些较为简单的性质（例如，参见[Hei01]）。

- 对于配备任意范数的$X=\mathbb{R}^k$，$\operatorname{dim}(X) = \Theta(k)$。
- 如果$X$是$Y$的子空间，那么$\operatorname{dim}(X) \leq \operatorname{dim}(Y)$。
- $\operatorname{dim}\left(X_1 \cup \cdots \cup X_m\right) \leq \max _i \operatorname{dim}\left(X_i\right)+\log m$。

在[KR02]中指出，倍增度量（那些具有均匀有界维度的度量）在实际应用中，如对等网络中，自然地出现。在特征识别问题中，数据集$S$通常包含在某个高维空间$\mathbb{R}^L$中的$m$个低维流形的并集中，并且使用的距离函数是$\mathbb{R}^L$的一个范数（通常是$\ell_1$或$\ell_2$范数）。例如，一个流形可能代表对应于单个对象的不同视角的特征向量，因此$m$个对象会形成$m$个流形的并集。在这种情况下，仅使用高维宿主空间的结构进行最近邻搜索会非常昂贵。测量中产生的噪声和误差使情况变得更加复杂，因为它们实际上会导致$m$个集合的并集，而这些集合仅接近于流形。幸运的是，倍增维度对这些小的扰动相当不敏感。

## 1.2 Our results

我们为一般度量空间中的$(1+\epsilon)$-NNS（近似最近邻搜索）提供了一个简单的方案。第2节中描述的数据结构是确定性的，因此保证能够正确回答$(1+\epsilon)$-NNS查询。我们的数据结构是动态的，支持对$S$的插入和删除操作。它还支持范围查询：给定查询点$q \in M$和范围$t$，目标是返回所有满足$d(q, s) \leq t$的点$s \in S$。

这些操作的复杂度取决于$S$的维度和$(S, d)$的长宽比，记作$\Delta=\Delta_S$，其定义为$S$中最大和最小点间距离的比值。在大多数应用中，$\Delta=\operatorname{poly}(|S|)$，因此$\log \Delta=O(\log n)$。

如果$S$是倍增空间，该数据结构使用$O(n)$空间；它以$O(\log \Delta)+ (1 / \epsilon)^{O(1)}$的时间回答$(1+\epsilon)$-NNS查询；并且以$O(\log \Delta \log \log \Delta)$的时间实现插入和删除操作。运行时间呈指数地依赖于$\operatorname{dim}(S)$（见第2节），但在距离预言机模型中（即对$M$的访问仅限于对距离函数$d$的查询），这种依赖性是必要的，如我们在第3节中所示。

在[KR02]中，定义了一个不同的维度概念（隐含地），我们记作$\operatorname{dim}_{\mathrm{KR}}(X)$。这是最小的$k$值，使得对每个点$x \in X$和每个$r>0$，都有$\left|B_X(x, 2r)\right| \leq 2^k\left|B_X(x, r)\right|$。在[GKL03]中证明了以下简单引理；为了完整性，我们在附录中重复了该证明。

**引理 1.1.**（[GKL03]）每个有限度量空间$(X, d)$都满足$\operatorname{dim}(X) \leq 4 \operatorname{dim}_{\mathrm{KR}}(X)$。

反过来的方向（$\operatorname{dim}_{\mathrm{KR}}(X)$与$\operatorname{dim}(X)$的关系的上界）不成立。换句话说，倍增度量（那些具有有界倍增维度的度量）构成了一个严格更大的类，相对于那些具有有界KR维度的度量。这两个概念将在下一节中进一步比较。

当由数据集$S$和查询点$q$（即$S \cup\{q\}$）形成的度量属于KR维度有界的更严格类时，通过稍微修改我们的查询过程（但不修改数据结构），我们能够返回精确的最近邻，并且能够匹配或改进[KR02]的结果；参见第1.3节。

## 1.3 Related work  

几项研究（例如 [Bri95]）表明，度量空间中点之间距离的直方图（或其集中情况）可以（启发式地）反映其维度。Chávez等人 [CNBYM01] 提议定义度量的维度为 $\rho=\mu^2 / 2 \sigma^2$，其中$\mu$和$\sigma^2$分别是直方图的均值和方差，并展示了它在随机实例中影响某种最近邻搜索算法的效率。Faloutsos和Kamel [FK97] 提议测量数据集的分形维度，但这个概念仅适用于欧几里得空间。

Clarkson [Cla99] 为满足某种球体填充性质（这等价于具有$O(1)$的倍增维度）的度量空间设计了两种NNS数据结构。（见表1。）然而，他的数据结构是随机的，非动态的，查询时间为超对数级，且假设数据集$S$和查询点$q$来自同一（未知）概率分布。我们的算法在这些方面改进了 [Cla99]，尽管它们只保证$(1+\epsilon)-\mathrm{NNS}$。

最近，Karger和Ruhl [KR02] 提出了上一节中讨论的维度概念。对于这一概念的一个合理解释是，$l_1$度量下的$k$维网格具有$\Theta(k)$的KR维度。Karger和Ruhl [KR02] 为具有有界KR维度的度量空间展示了一种高效的NNS数据结构。（见表1。）

|              | Dimension                                  |               Space               |           NNS or $(1+\epsilon)$-NNS            |           Insert/Delete           |
| :----------- | :----------------------------------------- | :-------------------------------: | :--------------------------------------------: | :-------------------------------: |
| [Cla99] $^*$ | $\operatorname{dim}(S)=O(1)^{\dagger}$     |        $O(n \log \Delta)$         | $O\left(\log { }^4 n \cdot \log \Delta\right)$ |                 -                 |
| [Cla99] $^*$ | $\operatorname{dim}(S)=O(1)^{\ddagger}$    | $n(\log n)^{O(\log \log \Delta)}$ |        $(\log n)^{O(\log \log \Delta)}$        |                 -                 |
| 本文         | $\operatorname{dim}(S)=O(1)$               |              $O(n)$               |     $O(\log \Delta)+(1 / \epsilon)^{O(1)}$     | $O(\log \Delta \log \log \Delta)$ |
| [KR02] $^*$  | $\operatorname{dim}_{\mathrm{KR}}(S)=O(1)$ |           $O(n \log n)$           |                  $O(\log n)$                   |      $O(\log n \log \log n)$      |
| 本文         | $\operatorname{dim}_{\mathrm{KR}}(S)=O(1)$ |              $O(n)$               |                  $O(\log n)$                   |      $O(\log n \log \log n)$      |
| [AMN’98]     | $O(1)$ Euclidean                           |              $O(n)$               |         $(1 / \epsilon)^{O(1)} \log n$         |            $O(\log n)$            |

$*$ 随机化（Las Vegas）数据结构 
$\dagger$ 假设查询有训练集支持。 
$\ddagger$ 假设查询来自与数据集相同的（未知）分布。

引理1.1表明，每个度量空间的倍增维度本质上不大于其KR维度。因此，我们关于倍增度量的所有结果立即适用于具有有界KR维度的度量。此外，我们可以证明，当我们的算法运行在后者的度量族上时，稍作修改即可找到精确的NNS，运行时间与[KR02]相似，但空间复杂度为线性（而非近线性）。我们的结果在多个方面优于[KR02]。除了扩展到更大范围的度量空间外，它们是确定性的，不需要任何参数，而[KR02]的方法是随机的（LasVegas），其性能依赖于正确设置维度参数。

我们在此讨论KR维度概念的脆弱性。特别是，如[KR02]所指出的，具有有界KR维度的度量空间的子集并不总是具有有界维度。尤其是，实数线的某些子集具有无界的KR维度（虽然每个$\mathbb{R}$的子集肯定具有有界的倍增维度）。这背后的原因很简单；[KR02] 的算法使用随机采样来找到最近邻，从而在度量空间中强加了一定的点布局均匀性。因此尚不清楚他们的方法是否可以扩展到更大类的倍增度量空间。为了看到这种不稳定性的一个简单例子，考虑离散环带$S=\{x \in \mathbb{Z}: 2r > |x| > r\}$。不难看出$\operatorname{dim}_{\mathrm{KR}}(S)=O(1)$（对于任何$r>0$，都是一致的）。另一方面，$\operatorname{dim}_{\mathrm{KR}}(S \cup \{0\})=\Omega(\log r)$，因此向$S$中添加一个点就可以导致KR维度任意增长。

或许最有趣的是，我们的结果与$\left[\mathrm{AMN}^{+} 98\right]$ 针对有界维度欧几里得度量的结果相当（当然，它们也具有有界的倍增维度）；见表1。值得注意的是，仅仅利用点集的体积增长上界而非欧几里得空间的几何结构，即可实现相似的运行时间。

## 1.4 Techniques  

设$(X, d)$为一个度量空间。如果一个子集$Y \subseteq X$满足以下两个条件： (1) 对于每个$x, y \in Y$，有$d(x, y) \geq \epsilon$；(2) $X \subseteq \bigcup_{y \in Y} B(y, \epsilon)$，则称$Y$是$X$的一个$\epsilon$-网。对于任意$\epsilon>0$，这样的网总是存在的。对于有限度量空间，可以通过贪心算法构造这样的网。对于任意度量空间，利用佐恩引理可以容易地证明它们的存在。这种经典的$\epsilon$-网概念是度量空间研究中的标准工具（参见例如[Hei01]），不应与计算几何中最近提出的具有相同名称的概念相混淆。

以下引理是我们分析的关键。

**引理 1.2.** 设$(S, d)$为一个度量空间，$Y \subseteq S$。如果$Y$上诱导的度量的长宽比至多为$\alpha$，且$\alpha \geq 2$，则$|Y| \leq \alpha^{O(\operatorname{dim}(S))}$。

**证明.** 设$d_{\min }=\inf \{d(x, y): x, y \in Y\}$和$d_{\max }=\sup \{d(x, y): x, y \in Y\}$分别为$Y$中点之间的最小和最大距离，并假设$\alpha=\frac{d_{\text{max}}}{d_{\text{min}}}<\infty$。注意到$Y$包含在$S$中的一个以$Y$中任意一点为中心、半径为$2d_{\max} \leq 2\alpha d_{\min}$的球内。应用倍增维度的定义，反复迭代几次后，我们得到这个球，尤其是$Y$，可以被$2^{\operatorname{dim}(S) \cdot O(\log \alpha)}$个半径为$d_{\min}/3$的球覆盖。根据$d_{\min}$的定义，每个这样的球最多覆盖$Y$中的一个点，因此$|Y| \leq 2^{\operatorname{dim}(S) \cdot O(\log \alpha)} \leq \alpha^{O(\operatorname{dim}(S))}$。 

### 1.4.1. A simplifled 3-NNS algorithm  

以下是我们数据结构的简化版本。设$(S, d)$为查询的度量空间，并且为了这段非正式的讨论，假设$S$中点之间的最小距离为$\min \{d(x, y): x, y \in S\}=1$。在这种情况下，长宽比$\Delta$就是$S$的直径。接下来，对于一个子集$R \subseteq S$和一个点$x \in M$，我们定义$d(x, R)=\inf _{y \in R} d(x, y)$。

设$k=\log \Delta$，对于$i=0,1,\ldots, k$，设$Y_i$为$S$的一个$2^i$-网。现在，对于每个点$y \in Y_i$，假设我们有一个点集$L_{y, i}=\left\{z \in Y_{i-1}: d(y, z) \leq \gamma 2^i\right\}$，其中$\gamma$是稍后将指定的常数。

注意到集合$L_{y, i}$的最小距离是$2^{i-1}$（因为这是$Y_{i-1}$中的最小距离），最大距离是$\gamma 2^{i+1}$，因此其长宽比是有界的。当$S$是一个倍增度量空间时，引理1.2表明$\left|L_{y, i}\right|=O(1)$，其中常数取决于$\gamma$的选择。此外，$Y_k$只包含一个（任意的）点，我们将其记为$y_{\text {top }}$。

现在，给定一个查询点$q \in M$，我们首先设置$y=y_{\text {top }}$，然后从$i=k, k-1,\ldots$开始迭代，找到$L_{y, i} \subseteq Y_{i-1}$中最接近$q$的点，并将$y$设置为该点（用于下一次迭代）。如果在某个阶段我们达到$d\left(q, L_{y, i}\right)>3 \cdot 2^{i-1}$，则停止并输出$y$。否则，输出最后的$Y_0$中的点$y$。

首先，注意到这个算法的运行时间最多为$O(\log \Delta)$，因为在列表$L_{y, i}$中找到最接近$q$的点只需常数时间（列表的大小为$O(1)$）。因此，我们只需要论证输出点$y$是最近邻$a \in S$的3-近似，即$d(q, y) \leq 3 d(q, a)$。

为此，设$j$为满足$d(q, y) \leq 3 \cdot 2^j$但$d\left(q, L_{y, j}\right)>3 \cdot 2^{j-1}$的索引，即在该步骤中距离未能减少一半。首先，我们论证$d\left(a, L_{y, j}\right) \leq 2^{j-1}$。换句话说，最接近$a$的$2^{j-1}$网点包含在$L_{y, j}$中。设$y^* \in Y_{j-1}$使得$d\left(a, y^*\right) \leq 2^{j-1}$。我们需要论证$d\left(y^*, y\right) \leq \gamma 2^j$；在这种情况下，我们将有$y^* \in L_{y, j}$。由于$d(q, y) \leq 3 \cdot 2^j$，因此

$$d\left(y^*, y\right) \leq d\left(y^*, a\right)+d(a, y) \leq 2^{j-1}+d(a, q)+d(q, y) \leq 2^{j-1}+2 \cdot d(q, y) \leq 7 \cdot 2^j$$

因此，选择$\gamma=7$是足够的。

这表明下降过程“跟踪”了最接近$a$的网点。现在很容易看到

$$3 \cdot 2^{j-1}<d\left(q, L_{y, j}\right) \leq d(q, a)+d\left(a, L_{y, j}\right) \leq d(q, a)+2^{j-1}$$

因此$d(q, a)>2^j$。由于$d(q, y) \leq 3 \cdot 2^j$，这表明$y$是一个3-近似最近邻。类似的论证表明，如果我们最终得到$y \in Y_0$，那么$y$实际上是最接近$q$的点。稍后我们将看到，在获得$O(1)$近似最近邻后，可以在$O(1 / \epsilon)^{O(1)}$的时间内解决$(1+\epsilon)$-NNS问题。

前述简单的算法展示了在不同尺度上使用网格来导航度量空间的强大能力，大致上在每一步中将$q$与最近点之间的距离减半。我们数据结构中的所有操作都在这个简单框架内实现。

为了克服上述方案的一些技术限制，第2节中介绍的实际数据结构更加复杂。首先，上述算法必须扩展到$(1+\epsilon)$-NNS，适用于任何$\epsilon>0$。这可以通过大约$\log (1 / \epsilon)$次相同决策过程的更多迭代来实现，但在每次迭代中，我们现在需要处理的不仅仅是一个点$y$。其次，在存在插入和删除操作的情况下，我们不能简化假设$S$中点之间的最小距离为1，因此必须找到一种方法来跟踪“相关的”尺度。最后，由于技术原因，上述数据结构不支持高效的删除操作，并且可能需要$\Omega(n \log \Delta)$的空间。解决这些问题的主要方法是选择$Y_i$作为$Y_{i-1}$中的$2^i$-网，而不是$S$中的$2^i$-网。

# 2. Navigating nets  

在本节中，我们介绍一种数据结构，用于维护度量空间中的点集$S$，以支持邻近查询和对$S$的更新。该数据结构的性能保证（在时间和空间方面）取决于第1节中定义的数据集$S$的维度。然而，该数据结构是确定性的，并且无条件地保证对任何度量空间都是正确的。既不需要提前知道$S$的维度，也不需要知道$\epsilon$（用于$(1+\epsilon)$-NNS）。

我们首先在第2.1节中描述该数据结构，并在第2.2节中分析其空间需求。然后在第2.3和第2.4节中介绍计算邻近查询的过程，并在第2.5和第2.6节中介绍更新$S$的过程。最后，我们在第2.7节中给出了对KR度量的改进界限。

我们将使用第1节中的符号，其中$(M, d)$是宿主度量空间，$S$是由数据结构维护的点集，且$n=|S|$。设$d_{\text {max }}:=\sup \{d(x, y): x, y \in S\}$和$d_{\text {min }}:=\inf \{d(x, y): x, y \in S\}$分别表示$S$中点之间的最大和最小距离，因此$\Delta:=d_{\max } / d_{\min }$是$S$的长宽比。

## 2.1 The data structure

设$\Gamma=\left\{2^i: i \in \mathbb{Z}\right\}$，我们将每个$r \in \Gamma$的值称为一个尺度。为了简化说明，我们考虑无限多的尺度，但显然只有$O(\log \Delta)$个尺度是“相关的”，其余的尺度将是无关紧要的。

对于每个$r \in \Gamma$，令$Y_r$为$Y_{r / 2}$的一个$r$-网。作为基准情况，我们定义所有尺度$r \leq d_{\min}$时$Y_r:=S$。对于每个$r \in \Gamma$和每个$y \in Y_r$，数据结构存储了$y$在$r / 2$-网$Y_{r / 2}$中的附近点的列表。$y$的这个尺度$r$的导航列表定义为：

$L_{y, r}:=\left\{z \in Y_{r / 2}: d(z, y) \leq \gamma \cdot r\right\}$ 

其中$\gamma>0$是一个通用常数。我们将看到$\gamma \geq 4$足以满足所有操作。虽然$Y_r$不必是$S$的$r$-网，但以下引理表明$Y_r$是$S$的$r$-网的一个放松变体。

**引理 2.1.** 对于每个尺度$r$，我们有：

1. **覆盖性**：对于每个$z \in S$，有$d\left(z, Y_r\right)<2r$。
2. **包装性**：对于每个$x, y \in Y_r$，有$d(x, y) \geq r$。

**证明**。覆盖性属性可以通过归纳法得到。基准情况是$r<d_{\min }$，此时$Y_r=S$，所需的性质是显然的。对于归纳步骤，假设该性质对于尺度$r$成立，即存在$y \in Y_r$使得$d(z, y)<2r$。由于$Y_{2r}$是$Y_r$的一个$2r$-网，我们得到$d\left(z, Y_{2r}\right) \leq d(z, y)+d\left(y, Y_{2r}\right)<4r$。

包装性属性直接来自$Y_r$是$Y_{r / 2}$的$r$-网这一事实。

下一个引理给出了任何导航列表的大小的上界。它的证明来自以下观察：列表$L_{y, r}$中的所有点都是$Y_{r / 2} \subseteq S$中的点，因此它们之间的距离至少为$r / 2$，而它们都位于一个半径为$\gamma r$的球内。应用引理1.2可以得到以下结论。

**引理 2.2.** 每个导航列表的大小至多为$2^{O(\operatorname{dim}(S))}$。

### 2.1.1. Implementation  

我们现在讨论该数据结构的实现。首先，网格$Y_r$并不是显式地维护的，而是通过列表$L_{y, r}$隐式地确定，即$Y_r$是所有存在$L_{y, r}$的点$y \in S$的集合。其次，如果$S$非空，根据引理2.1，对于每个尺度$r>d_{\max }$，网格$Y_r$只包含同一个点，我们将其记为$y_{\text{top}}$。数据结构维护这个点$y_{\text{top}}$和截止尺度$r_{\max }:=\min \left\{r \in \Gamma: \forall r^{\prime} \geq r,\left|Y_{r^{\prime}}\right|=1\right\}$，以便为大多数操作提供引导。第三，对于所有尺度$r \leq d_{\min }$，网格$Y_r$等于$S$，因此对于尺度$r \leq d_{\min } / 2$，每个点$x \in S$都有一个简单的列表$L_{x, r}=\{x\}$。这些简单的列表可以通过为每个$x \in S$存储一个尺度$r_x \in \Gamma$（低于此尺度的所有列表都是简单的）来简洁地表示。为了分析，定义$r_{\min }:=\min \left\{r_x: x \in S\right\}$。

我们现在可以对需要为$S$中的任意点$x$存储的导航列表的数量进行上界估计。

**引理 2.3.** $r_{\max }=\Theta\left(d_{\max }\right)$且$r_{\min }=\Theta\left(d_{\min }\right)$，因此每个点有$O(\log \Delta)$个非简单的导航列表。

**证明**。利用引理2.1很容易看出$r_{\max }=\Theta\left(d_{\text {max }}\right)$。根据定义(2.1)可以直接得出，对于每个$x \in S$，$r_x \geq \Omega\left(d_{\min }\right)$。此外，$Y_{r_{\min } / 2}=S$，所以根据引理2.1，$d_{\min } \geq r_{\min } / 2$。我们得出，只有在$\Omega\left(d_{\min }\right) \leq r_x \leq r \leq r_{\max } \leq O\left(d_{\max }\right)$时，才需要存储尺度$r$的列表。引理得证。

结合引理2.2和2.3，对数据结构所需的总空间的上界是$n \cdot 2^{O(\operatorname{dim}(X))} \log \Delta$。我们将在第2.2节中通过更精细的分析改进这个结果，消除$\log \Delta$因子。

在分析数据结构的性能时，我们假设导航列表的存储方式如下。对于每个点$x \in S$，非简单的导航列表使用平衡搜索树存储，这样需要线性空间，并且可以在对数时间内实现查找、插入和删除操作。根据引理2.3，插入一个点的新导航列表可以在$O(\log \log \Delta)$的时间内完成。

**备注**：通过让每个导航列表$L_{x, r}$不仅包含指向$y \in Y_{r / 2}$的指针，还包含指向相应导航列表$L_{y, r / 2}$的指针，可以加快$(1+\epsilon)$-NNS过程。由于后者的导航列表可能是简单的（且未显式存储），我们实际上将存储一个指向导航列表$L_{y, r^{\prime}}$的指针，其中$r^{\prime}$是满足$r^{\prime} \leq r / 2$且$L_{y, r^{\prime}}$非简单的最大尺度。为了在插入和删除的情况下更新这些指针，必须将它们实现为双向指针。

## 2.2 Space requirement  