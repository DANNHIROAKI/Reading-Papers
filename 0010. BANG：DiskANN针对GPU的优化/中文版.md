### BANG: Billion-Scale Approximate Nearest Neighbor Search using a Single GPU  

# 0. Abstract

近似最近邻搜索（ANNS）是信息检索、模式识别、数据挖掘、图像处理等算法中常用的一个子程序。最近的研究表明，==基于图的ANNS算法在大数据集上比文献中提出的其他方法在实际应用中效率更高==。随着数据量和维度的不断增长，设计==可扩展的ANNS==技术变得越来越重要。为此，已有研究探索了利用GPU的高计算能力和能源效率来并行化基于图的ANNS。当前最先进的基于GPU的ANNS算法通常有两种方式：(i) 需要将索引图和数据完全存储在GPU内存中，或 (ii) 将数据划分为多个独立的小片段，每个片段可以放入GPU内存，并在GPU上对这些片段进行搜索。第一种方法由于GPU内存有限，无法处理大规模数据集，而第二种方法在处理大规模数据集时表现不佳，原因是低带宽的PCIe总线导致数据传输量过大。

---

在本文中，我们介绍了BANG，这是首个能够在==无法完全放入GPU内存的==十亿级数据集上高效运行的基于GPU的ANNS方法。BANG通过在GPU上使用压缩数据进行距离计算，同时将图保存在CPU上，从而在有限的GPU内存中实现对大规模图的高效ANNS。BANG结合了高度优化的GPU内核，并以并行的方式在GPU和CPU上分阶段运行，充分利用了它们各自的架构特点。此外，它通过重叠通信与计算，减少了通过PCIe总线在CPU和GPU之间的数据传输。我们在一台NVIDIA Ampere A100 GPU上，使用十个流行的ANN基准数据集对BANG进行了评估。BANG在大多数情况下都优于现有的最先进方法。特别是在十亿级数据集上，我们比竞争对手快得多，达到了比竞争方法高40到200倍的吞吐量，并在高召回率0.9的情况下实现了这一结果。我们将公开所有代码。

# 1. INTRODUCTION  

$k$最近邻搜索问题是要在多维数据集中找到与给定查询点最近的$k$个数据点。随着维度的增加，精确搜索方法变得越来越低效。为了在具有$n$个数据点、$d$维的数据集中评估查询点的精确$k$个最近邻，由于维度灾难的影响，必须检查所有$n$个数据点，这需要$O(n d)$的时间【25】。因此，在寻找最近邻时，通常使用==近似最近邻（ANN）搜索==，以减轻维度灾难，==牺牲少量的准确性来换取速度==。ANN搜索作为子程序被广泛应用于计算机视觉、文档检索和推荐系统等领域。这些应用需要在嵌入于多维空间的巨大数据集中进行搜索，数据集可能包含单词、图像和文档，其搜索查询通常是批处理的，因此需要高吞吐量。与传统的CPU相比，==GPU的大规模并行处理能力可以显著提高ANN搜索算法的吞吐量==。

---

基于图的ANNS算法【18, 26, 33】已被证明在处理大规模数据集时通常在实际应用中更为高效。然而，这些算法的GPU实现【22, 35, 47】要求==将图数据结构存储在GPU内存中==，这限制了它们处理大规模数据集的能力。即使是最新的GPU，如NVIDIA Ampere A100，拥有80GB的设备内存，也无法容纳所有的输入数据（包括图和数据点）。现有的解决方案，如通过GGNN【22】有效实现的==分片方法==，导致了高内存传输成本。此外，哈希和压缩技术，如SONG【47】和FAISS【28】展示的技术，通过降低数据维度或压缩向量，可以在单个GPU上管理大规模数据。然而，这些方法在处理大规模数据集并达到高召回率时可能存在局限性。此外，多GPU配置，如GGNN【22】使用八个GPU的实现，和其他需要多CPU的解决方案【14】，虽然可以提供解决方案，但伴随着巨大的硬件成本。因此，本文探讨了一个重要问题：==我们能否在不降低召回率的情况下，通过使用单个GPU来提高ANN搜索查询的吞吐量？==

---

在本文中，我们介绍了BANG，这是一种新颖的基于GPU的近似最近邻搜索（ANN）方法，==旨在高效处理无法完全放入GPU内存的十亿级数据集==。BANG的独特之处在于通过==在GPU上使用压缩数据加速距离计算==，同时==将图结构和实际数据点保存在CPU上==（？？？？？？），从而实现对大规模图的高效ANNS。在每次搜索迭代中，CPU将邻居信息传输到GPU。BANG分阶段执行，这些阶段可以在CPU和GPU上==并行运行==，从而优化资源利用，缓解CPU与GPU之间的数据传输瓶颈。BANG采用了==产品量化（PQ）==【29】对数据进行压缩，并==基于Vamana图进行操作==，这是一种在DiskANN【26】框架中已被证明成功的技术，该框架是当前==基于CPU==的十亿级ANNS的==最先进工作==【14】。BANG结合了各种优化措施，包括CPU-GPU负载平衡、预取、流水线、用于搜索遍历的布隆(Bloom)过滤器，以及针对距离计算、排序、更新工作列表等任务的高度优化的GPU内核。需要注意的是，我们的重点是基于图索引的ANN搜索，因此我们不构建图，而是==使用DiskANN中的Vamana图==。

---

我们的评估显示，BANG在大多数情况下显著超越了现有最先进的方法，特别是在使用单个NVIDIA A100 GPU处理十个流行的ANN基准数据集时表现突出。尤其是在十亿级数据集上，==我们的吞吐量比竞争方法高出40到200倍==，并且在高召回率0.9的情况下达成这一结果。在百万级数据集上，BANG在大多数情况下也优于现有方法。整体而言，BANG的算术平均吞吐量比当前最先进的技术高出2倍，且能达到相同的召回率。本文的主要贡献包括：

- 我们引入了BANG，一种新颖的基于GPU的ANN方法，能够在仅使用GPU的情况下高效搜索十亿点级的数据集。
- 我们提出了==高度优化的GPU内核==，用于ANN搜索中的距离计算和工作列表更新。
- 我们通过预取和流水线等优化方法，有效利用CPU和GPU，==减少CPU和GPU之间通过PCIe互连的数据流量==，从而在不牺牲召回率的前提下提高整体吞吐量。
- 我们展示了我们的方法在性能上超越了当前最先进的基于GPU的ANNS方法。

# 2. BACKGROUND  

## 2.1. GPU Architecture and Programming Model  

图形处理器（GPU）是加速器，能够提供大规模的多线程处理和高内存带宽【24】。CPU（主机）与GPU（设备）通过诸如PCI-Express等互连进行连接，支持CPU$\rightleftarrows$GPU之间的通信。我们在实现中使用了NVIDIA GPU以及CUDA【4】编程模型。==GPU的主要内存空间被称为全局内存==。GPU包含若干CUDA核心，这些核心被组织成流式多处理器（SM）。每个SM的所有核心==共享片上共享内存==，这是由软件管理的L1数据缓存。GPU上的程序（称为内核）在多个SM上==并行运行==，通过启动一个线程网格来实现。线程被分组为线程块，每个块分配到一个SM上。线程块内的线程通过共享内存进行通信和同步。一个线程块由多个“warp”组成，每个warp包含32个线程，这些线程以单指令多数据（SIMD）的方式执行。内核调用和内存传输通过称为流的任务队列来执行，这些流可以用于任务并行处理。

---

- 说的都是些啥，懵逼

## 2.2. A summary of DiskANN and Vamana Graph  

为了克服基于GPU的ANN搜索中==GPU内存限制==和==CPU-GPU数据传输开销==，我们的工作采用了DiskANN【26】框架中引入的方法。此外，BANG使用DiskANN构建的Vamana图来执行ANN搜索。因此，我们在此介绍其一些关键方面。DiskANN是一种==基于CPU的==图结构ANNS技术，它通过将大型数据集分割为较小的重叠簇，在只有64GB RAM的工作站上高效地构建Vamana搜索图。该技术迭代处理单个簇，并使用RobustPrune和GreedySearch在RAM上构建有向子图，然后将其存储在SSD上。最终的图是通过合并这些子图的边形成的。虽然之前的图（通过SNG属性构建）沿着搜索路径减少了到查询点的距离，Vamana图则==强调减少磁盘访问==，从而实现低搜索延迟（<5ms），这得益于RobustPrune属性。为了进行ANN搜索，DiskANN将Vamana图保存在磁盘上，并在CPU上对PQ压缩向量执行贪婪搜索。==BANG也利用PQ压缩向量在GPU上并行处理ANN查询==。接下来我们会讨论产品量化（PQ）【29】的细节。

## 2.3. Vector Compression  

产品量化（Product Quantization）最早由Jégou等人【29】应用于近似最近邻搜索（ANNS）。它将$d$维数据集划分为$m$个子空间，并在每个子空间中独立进行$k$-均值聚类。每个向量在每个子空间中被分配了簇ID（0到k），形成一个压缩的$m$维向量。这些ID与簇中的唯一质心向量相关联。在预处理步骤中，计算了每个子空间内质心向量与给定查询向量之间的距离。在ANNS搜索期间，通过累加所有子空间中预先计算的簇ID距离，可以快速计算数据点（以压缩形式）与查询向量之间的距离。通过利用簇质心的预计算距离，将操作维度从$d$减少到$m$，该方法加速了距离计算。

---

- 似懂非懂，妈的，啥时候有空看下[原论文](https://inria.hal.science/inria-00514462v2/document) 

# 3. ANN SEARCH ON GPU 

## 3.1.  Challenges in Handling Billion-scale Data  

我们提出了一种==并行实现==的ANN搜索方法，该方法利用Vamana图【26】上的产品量化（PQ）向量化【29】进行==单GPU执行==。它结合了==预取和流水线技术==，以有效管理CPU和GPU的空闲时间，并引入了一种新颖的==并行合并==操作，用于排序和工作列表更新。此外，我们还集成了布隆(Bloom)过滤器来处理大规模图上众多查询的搜索遍历已访问集合。在将ANN搜索并行化到GPU上的过程中，面临两个主要挑战：(1) 在有限的GPU内存限制下管理大规模图结构，(2) 提取足够的并行性以充分利用硬件资源。

### 3.1.1. Limited GPU Memory  

在GPU上处理大规模数据集的==主要挑战之一是其巨大的内存占用==。表1显示了各种十亿级ANN搜索数据集及其相应图结构的大小。值得注意的是，即使是最近的A100 GPU，其最大全局内存容量为80GB，也无法容纳整个输入数据（包括基础数据、图结构和查询）。为了解决这一挑战，最近的工作==GGNN【22】有效地实施了分片==作为解决方案。然而，该方法需要==频繁地在GPU上交换已处理和未处理的分片==，导致较高的内存传输成本。在CPU-GPU混合平台上处理数据的固有挑战是需要==将数据传输到GPU内存进行计算==，然后将处理后的==数据返回主存以进行基于CPU的操作==。此外，将整个数据分片也不可行，因为即使PCIe 4.0互连在其理论峰值传输带宽32GB/s下运行，预计从CPU传输最大数据集（384GB+260GB）到GPU也需要约20秒。这将导致吞吐量显著降低（下限），即使在GPU上并发运行10000个查询，吞吐量也仅能达到大约500 QPS（每秒查询数）。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241020213326219.png" alt="image-20241020213326219" style="zoom: 50%;" /> 

---

虽然采用==多GPU==设置可以有效地将数据分片分布在所有GPU上，从而容纳整个输入数据（如在GGNN【22】中使用八个GPU所展示的那样），但这种方法会带来==显著的硬件成本==。因此，我们的重点是开发一种在==单GPU==上实现高效且成本效益高的并行ANN搜索实现。

---

或者，==哈希和压缩技术==可以在单个GPU上处理大规模数据。哈希有效地==降低了数据的维度==，使其适应GPU内存，如SONG【47】所展示的那样，在MNIST8M数据集中将784字节的向量压缩到了64字节。然而，哈希==更适合处理较小的数据集==，在处理包含数十亿数据点的大规模数据集时，可能==无法实现高召回率==。数据压缩技术，如FAISS【28】所展示的那样，可以加速基于GPU的ANN搜索中的查询处理，但其召回率可能==取决于数据集和查询参数==，存在一定的权衡。

---

为了弥补这些限制，我们提出了一种基于GPU的解决方案，其中我们==在主机上存储图结构并进行ANN搜索==，而==在GPU上使用压缩数据（利用产品量化【29】）进行距离计算==。通过仅将压缩数据传输到GPU，这种方法缓解了CPU-GPU通信瓶颈，并通过按需从CPU向GPU获取节点的邻居信息，避免了将整个图传输到GPU的需求。

---

在DiskANN【26】框架中，已经提出了使用产品量化（PQ）【29】压缩向量在CPU上执行ANN搜索，同时将Vamana图保存在磁盘上的方法。据我们所知，我们的工作是首次将这种方法适用于基于GPU的ANN搜索，以克服GPU内存限制和CPU-GPU通信开销。

### 3.1.2. Optimal Hardware Usage.  

DiskANN方法无法直接应用于CPU-GPU（heterogeneous）系统，因为基于GPU的ANN搜索面临==两个特有的可扩展性挑战==，这些问题不会出现在仅基于CPU的系统中。第一个挑战是如何==在CPU和GPU之间平衡工作负载==（这在DiskANN中不适用于磁盘和CPU）。第二个挑战是基于GPU的ANN搜索==占用的内存显著增加==，因为它可以在GPU上同时处理数千个查询，并且在图遍历的每次迭代或跳跃中，CPU需要为所有这些查询传输所需的信息。

---

由于图结构位于CPU上，而使用压缩向量计算图节点与查询节点之间的距离发生在GPU上，因此必须设计一种高效的==工作分配策略==，确保CPU和GPU同时==持续工作==而不产生空闲状态。这需要根据CPU和GPU各自在内存访问延迟和计算能力方面的优势来分配任务，以优化整体性能。此外，在平衡两者之间的工作分配时，必须考虑需要在CPU和GPU之间传输的数据量，确保性能不受数据传输带宽限制的影响，如GGNN【22】框架中因长时间数据传输所导致的性能瓶颈。

---

例如，对于一组在GPU上并行执行的查询集$(Q)$，在管理用于跟踪已访问节点$(V)$的数据结构及其后续处理时，需要考虑这些结构==应该位于CPU还是GPU上==。设计工作流时需要战略性地考虑多个关键因素，包括与$Q \times V$内存大小相关的计算复杂度、V中的内存访问模式（CPU和GPU的内存带宽不同会导致访问时间差异），以及在CPU和GPU之间通过PCIe互连传输$Q \times V$数据所需的时间，考虑到该带宽的限制。

---

我们提出了一种并行ANN搜索实现——BANG，它通过高效利用硬件（CPU、GPU和PCIe总线）来提取==最大的并行性==。BANG的工作流程能够在CPU和GPU之间有效地负载均衡ANN搜索任务，并通过预取和流水线等优化技术管理CPU和GPU的空闲时间。此外，它集成了布隆(Bloom)过滤器以处理大规模图上的大量查询中的搜索遍历已访问节点集合，并且在GPU上针对距离计算、排序和合并等操作提供了==高度优化的内核==。

## 3.2. BANG Overview  

BANG在CPU-GPU混合系统上执行近似最近邻搜索。图1展示了BANG的完整工作流程。给定一组可用的查询集$Q$，BANG通过有效利用CPU（主机）、GPU（设备）和PCIe（CPU与GPU之间的数据传输通道）并行处理这些查询。我们旨在高效利用CPU和GPU的不同能力，并协同使用多核CPU和单个GPU，即使在十亿级数据上也能实现高吞吐量。

---

BANG使用DiskANN中的Vamana索引图（见第2.2节）作为执行查询查找的搜索结构。由于索引图非常大，无法放入GPU内存，==因此它存储在CPU的RAM中==（让人迷惑的表达）。如图1所示，CPU通过查找图数据结构中的邻接表来检索给定节点的邻居。同样，由于完整的数据集无法完全放入GPU，GPU上的搜索过程使用的是从实际数据向量派生出的压缩向量。BANG技术大致分为三个阶段：(1) 距离表构建，(2) ANN搜索，(3) 重新排序。

### 3.2.1. Distance Table Construction  

考虑图2中所示的一个简单示例，其中基准数据集中有12个点。基准数据集有两个维度，坐标值的数据类型为无符号整数（1字节，即uint8）。压缩技术涉及用一个压缩的$m$字节代码来表示$d$维数据集向量，具体方法是将$d$维度划分为$m$个向量子空间，进行子空间聚类，并应用量化技术【29】来进行压缩，如第2.3节所述。我们将产品量化中的$\mathrm{m}=1$进行指定。因此，在压缩步骤结束时，每个节点（2字节长）将用1字节来表示。

----

在开始搜索之前，我们会在GPU上并行预计算所有簇质心与查询点的欧氏距离平方。需要注意的是，每个子空间中有$c$个簇，它们是通过$k$-均值聚类创建的。该数据结构称为PQ距离表，维度为$c \times m$，并且它会一直保留在GPU上，直到搜索结束。在这个示例中，有12个簇和一个子空间，因此PQ距离表的维度是$12 \times 1$。图2b中展示了查询点与每个簇质心之间的距离。在生成PQ距离表之后，搜索过程开始，首先检索一个节点的邻居，并确定它们所属的PQ簇；近似距离通过将来自PQ距离表的预计算距离（压缩向量）相加来计算得出。

### 3.2.2. ANN Search.  

由于批处理中的所有查询都是独立的，因此可以通过高度并行的方式处理。在图1的CUDA Cores部分所示的每个步骤（第2.1节）都由各自的CUDA线程块独立处理每个查询。Medoid将作为每个查询搜索的起点。在图2所示的示例中，medoid是节点6。一旦提供查询，搜索将从medoid开始，并迭代地向查询点推进。初始时工作列表L为空，因此我们将其初始化为节点6。工作列表中的条目始终按照与查询点的距离升序排列。我们使用PQ距离表（即压缩向量）计算节点6与查询节点的距离。由于工作列表中只有一个元素，节点6被立即选为下一个访问的候选节点，无需进行排序。然后我们将节点ID 6从GPU传输到CPU，并请求节点6的邻居。CPU端实现访问邻接表，检索节点6的邻居，并向GPU提供节点ID 8和2，完成第0次迭代。

---

在第1次迭代中，GPU过滤掉邻居节点中已经访问过的节点，使用PQ距离表计算其与查询点的距离，随后对这些节点进行排序，并将排序列表与工作列表合并。然后选择第一个候选节点，即节点8，作为下一个要访问的节点。GPU使用布隆过滤器数据结构来跟踪已访问的节点，以==避免再次获取相同节点的邻居并重新计算距离==。如图所示，邻居集合逐步接近查询点，当工作列表中的所有节点都被访问完毕时，迭代结束。

### 3.2.3. Re-ranking  

在每次迭代中，识别并发送到CPU的候选节点被存储在一个数据结构中，以便在最后一步中进行重新排序。重新排序过程可以补偿由于使用压缩向量进行距离计算而在搜索过程中产生的误差【26 (DiskANN那篇)】。一旦搜索过程结束，GPU会使用==精确的基准向量==重新计算这些节点与查询点之间的精确距离，随后根据这些精确距离对节点进行排序，从列表中提取最终的k个最近邻节点。在这个示例中，按与查询节点距离排序后的候选节点列表为：10、8、11、9、6、7、5、2，最终选择了前两个最近邻节点，即10和8。需要注意的是，在重新排序步骤中，只有被选中的节点的完整向量会被发送到GPU，这些节点数量较少，因此可以集体适应GPU内存，适用于所有查询。

# 4. BANG: BILLION-SCALE ANN SEARCH ON A SINGLE GPU

## 4.1. Search Algorithm  

在基于图的近似最近邻（ANN）算法中，贪婪搜索（如算法1所示）用于回答搜索查询。该算法从一个固定点𝑠开始，以最优先顺序探索图𝐺，通过评估工作列表L中每个点与查询点𝑞之间的距离，在每次迭代中逐步向𝑞推进，最终报告𝑘个最近邻节点。BANG在DiskANN的Vamana图上执行贪婪搜索（见第2.2节）。

---

算法2描述了BANG在Vamana图上进行==批量查询查找==的方案。由于批处理中的所有查询都是独立的，因此可以通过==高度并行==的方式处理（第1行）。每个查询在一个独立的CUDA线程块中运行（第2.1节），这将导致有多少个查询就有多少个线程块，充分利用GPU提供的大规模并行处理能力，从而最大限度地提高每秒查询数（QPS），即吞吐量。

---

通过将近似最近邻搜索（ANNS）活动系统地划分为不同的步骤，每个步骤具有自己的工作分配和跨度特征，我们==最大化了GPU的并行性==，与其他将整个搜索活动视为单一块的方案【22, 47】形成了对比。例如，我们为以下操作==设计了独立的GPU内核==：(1) 构建PQ距离表，(2) 过滤已访问的邻居，(3) 计算邻居到查询点的距离，(4) 按计算出的距离对邻居进行排序，(5) 将最近的邻居合并到工作列表中，(6) 对候选节点进行重新排序。对于每个独立的内核，我们基于经验优化了每个查询的线程块大小，以确保GPU的最大占用率。一个配置不佳的块可能会显著降低性能。

- 为每一小步都设计独立的GPU内核

---

在接下来的小节中，我们将详细介绍如何并行化和优化前述内核以及算法2中的每个步骤，以实现最佳的GPU性能、最大化占用率，并充分利用硬件资源。

## 4.2. Construction of 𝑃𝑄𝐷𝑖𝑠𝑡𝑇𝑎𝑏𝑙𝑒 in Parallel  

在这里，我们描述了数据结构𝑃𝑄𝐷𝑖𝑠𝑡𝑇𝑎𝑏𝑙𝑒，这是算法2用于执行近似最近邻搜索的==输入==。如前所述，由于整个数据集无法容纳在GPU上，GPU上的搜索使用的是从实际数据向量派生的压缩向量。也就是说，算法在第12行使用压缩向量计算的近似距离。这是通过𝑃𝑄𝐷𝑖𝑠𝑡𝑇𝑎𝑏𝑙𝑒实现的，该结构包含质心向量（通过产品量化获得的）与查询向量之间的距离（参见第3.2节和第2.3节）。

---

对于批处理中的每个查询点，我们计算其与每个通过压缩生成的子空间中的质心之间的欧氏距离平方。这些距离被保存在一个查找数据结构中，我们称之为PQ距离表（𝑃𝑄𝐷𝑖𝑠𝑡𝑇𝑎𝑏𝑙𝑒）。稍后（在第4.5节）我们将描述如何使用这些预计算的距离来高效计算（未压缩）查询点与压缩数据点之间的不对称距离【29】。

---

我们将𝑃𝑄𝐷𝑖𝑠𝑡𝑇𝑎𝑏𝑙𝑒维护为一个连续的线性数组，大小为(𝜌 · 𝑚 · 256)，其中𝜌是查询批次的大小，𝑚是子空间的数量，每个子空间有256个质心。质心的数量与之前使用产品量化的研究$[26,28]$相同。根据可用的GPU全局内存，我们通过经验确定$m=74$（第6节展示了对不同$m$值的消融研究）。和其他内核一样，每个线程块处理一个查询，结果是有$\rho$个并发线程块。此外，对于给定的查询$q \in \mathbb{R}^d$，查询$q_s \in \mathbb{R}^{d / m}$的每个子向量与一个子空间中256个质心的距离可以由线程块中的线程并行独立计算。需要注意的是，对于查询$q$，各子空间中子向量的距离在一个线程中依次计算，以确保每个线程有足够的工作量，同时受线程块大小的限制。

---

**工作-跨度(Span)分析**。算法的工作量是$O\left(\left(m \cdot subspace\_size\right) \cdot 256 \cdot \rho\right)$。由于并行化方案，算法的跨度是$O(m \cdot$ subspace_size $)=O(d)$。

## 4.3. Handling Data Transfer Overheads  

如算法2所述，每当GPU传输节点（第16行）时，CPU将邻居（第6行）传输给在GPU上执行的搜索例程。与GPU强大的处理能力相比，==设备（GPU）和主机（CPU）之间的数据传输是耗时的==（PCIe 4.0总线连接GPU，其最大传输带宽仅为32 GB/s）。因此，BANG只传输==最低限度所需的信息==，以尽量减少数据传输开销。具体而言，从设备到主机的传输包括在算法2收敛后搜索出的最终近似最近邻（ANN）列表，以及每次迭代中每个查询的候选节点（第16行）；而从主机到设备的传输则包括邻居顶点列表（第6行）和候选节点的基础向量（未在算法中显示）。

---

此外，BANG使用高级CUDA功能，通过内核计算来隐藏数据传输延迟。为了同时执行数据传输和内核操作，CUDA提供了==异步内存拷贝API==和==流的概念==【40】；BANG充分利用了这些功能。在搜索过程中，CPU线程为所有𝜌个查询检索给定顶点的邻居（见算法2第5行）。我们利用了图数据的高效结构，因为==顶点的基础向量和其邻居列表在CPU内存中紧挨着排列==，允许顺序内存访问。因此，在每次搜索迭代中将邻居列表传输到GPU后，我们会策略性地异步传输基础向量以备将来使用，这些向量仅在最终的重新排序步骤中需要（即算法2收敛后的GPU操作）。结果是，==GPU的内核执行引擎和拷贝引擎都能保持忙碌==，从而实现更高的吞吐量。因此，在我们的实现中，我们尽量使所有数据传输通过`cudaMemcpyAsync()`异步完成，并在适当位置使用`cudaStreamSynchronize()`来满足数据依赖。

## 4.4. Handling Visited Vertices: Bloom Filter  

由于图$G_{\mathcal{p}}$具有建立远程边的特性【26】，算法2在搜索过程中可能多次遇到相同的顶点，这可能会影响吞吐量和/或召回率。吞吐量下降的问题在于搜索过程中可能会==重复执行某些操作==，特别是在获取邻居节点（第5行）和计算它们与查询点的距离（第12行）时，这些操作都非常计算密集。避免这种冗余是提高吞吐量的机会。召回率下降的问题则在于，如果不过滤已访问的邻居，它们可能会多次被添加到工作列表$\mathcal{L}_i$中，导致搜索可能会过早终止。此外，BANG中==保持对已访问顶点的记录非常重要==，而不是进行重复计算【45】。我们的实验表明，==如果不过滤已访问的顶点，召回率可能会降至原来的十分之一==。考虑到这些因素，我们在遍历过程中跟踪已访问的顶点，以确保每个节点只被访问一次。

---

记录已访问顶点的最常见方法有：i）为每个顶点设置一个位，ii）使用集合、优先队列或哈希表等数据结构。第一种方法速度快，但内存占用取决于图的大小和批处理查询的数量。对于十亿级图和10000个查询的批处理，它将需要$\left(10^9 \times 10^4\right)$位或$\frac{10^{13}}{10^9 \times 8} \mathrm{~GB}=$ 125 GB的内存。这仅这一项就==太大==，无法放入GPU中。第二种方法的内存开销较小，因为它只存储已访问的顶点。然而，集合、优先队列或哈希表等==动态数据结构在GPU上存在维护的困难==，特别是由于动态性和高度不规则的依赖关系。这些固有特性导致了GPU计算带宽的未充分利用，正如之前的研究$[22,47]$所证明的那样，因此不利于GPU的并行化处理。

---

为了在有限内存约束下优化GPU并行性，我们采用了==成熟的布隆过滤器【6】数据结构==，它在近似集合成员测试中的效率高且内存占用小，能够充分利用GPU的并行处理能力。我们为每个查询使用一个布隆过滤器，如算法2第10行所示。我们的布隆过滤器使用一个大小为$z$的布尔数组，其中$z$是根据查询过程中访问的顶点数量的估计值、可容忍的少量假阳性率（即出现假阳性的概率）以及使用的哈希函数数量确定的。我们使用了两个FNV1a哈希函数【20】，这是常用来实现布隆过滤器的轻量级非加密哈希函数。

## 4.5. Parallel Neighbor Distance Computation  

对于批处理中的每个查询，我们在每次迭代中计算其与当前邻居列表的非对称距离（算法2第12行）。这种计算对每个查询都是独立的，因此我们==为每个线程块分配一个查询==。此外，对于一个查询，与其每个邻居的距离也可以独立于其他邻居进行计算。由于之前构建的PQDistTable（第4.2节），我们可以通过汇总PQDistTable中所有子空间的质心部分距离来计算查询点$q_i$与邻居$n_i$的距离，质心信息由$n_i$的压缩向量提供（第2.3节和第3.2节）。==##############==

---

在基于GPU的近似最近邻搜索中，距离计算通常使用Nvidia的CUB【38】库中的归约API来进行求和，如【22, 47】中的线程块级或warp级归约。然而，在我们的方法中，一个大小为$t_b$的线程块被细分为$g$个组，每组由$g_{size}:=\frac{t_b}{g}$个线程组成。这些组协作计算查询点与邻居的距离，每组$g_{size}$个线程汇总$m$个质心的部分距离（每个片段的大小为$\frac{m}{g_{size}}$）。每个线程使用线程本地寄存器顺序计算片段中的值总和，避免同步。为了使用$g_{size}$线程有效地添加片段和的总和，我们探索了两种方法：(i) 使用`atomicAdd()`进行最终结果的原子操作；(ii) 子warp级归约，利用CUB【38】中的`WarpReduce`。通过实验调优，当$m=74, t_b=512$和$g_{size}=8$时，第二种方法（使用子warp级归约）的性能略优于第一种方法。总体来说，我们的分段方法优于标准替代方法，例如CUB的`WarpReduce`（慢$1.2 \times$）和`BlockReduce`（慢$4 \times$）的warp级和线程块级归约。

---

这个内核在十亿级数据集上的总运行时间中占据了相当大的一部分，平均约为38%。内核的效率受限于GPU全局内存访问的延迟，这是由于不规则的图结构导致对多个邻居的压缩向量进行未合并的访问。为了缓解这种不规则性，我们尝试了使用著名的逆Cuthill-McKee（RCM）算法【13, 19】进行图重排序。然而，这种方法并没有显著改善局部性，因此我们放弃了图重排序。

---

**工作-跨度分析**。算法的工作量为$O(N u m N b r s \cdot m \cdot \rho)$。由于并行化方案，算法的跨度为$O(\log m)$。

## 4.6. Prefetching Candidate Nodes  

如算法2所述，每个查询$q_i$的候选节点$u_i^*$只有在GPU更新完工作列表$\mathcal{L}_i$之后（第16行）才会传送给CPU。当CPU收集邻居节点（第5行）并将其传送给GPU（第6行）时，GPU处于空闲状态，等待接收这些信息。为了避免当前迭代开始时的延迟，必须在GPU需要时立即提供邻居ID。为了解决这个问题，我们进行了优化，在上一迭代结束时将候选节点传送到CPU之前，提前预测下一个候选节点，即在邻居距离计算完成后立即进行（即在排序邻居列表并更新工作列表$\mathcal{L}_i$之前）。

---

我们的优化方法是通过提前计算候选节点$u_i^*$，选择新邻居列表$N_i^{\prime}$中的最近邻节点和工作列表$\mathcal{L}_i$中第一个未访问的节点（按与查询点的距离排序），然后从两者中选择最佳节点。提前选择在算法2第13行之前进行。在将提前选出的候选节点传送给CPU后，GPU继续执行本次迭代的剩余任务（对新邻居列表进行排序并更新工作列表）。与此同时，CPU并行收集邻居ID。也就是说，CPU在执行第5行时，与GPU同时执行第13行和第14行任务。当GPU完成剩余任务时，CPU已经传送了邻居ID。这一策略消除了GPU的空闲时间，实验表明吞吐量提高了约10%。

## 4.7. Parallel Merge Sort  

为了在算法2的每次迭代中添加比工作列表$\mathcal{L}_i$中节点更接近$q_i$的新顶点，我们根据它们与$q_i$之间的非对称距离对邻居$N_i^{\prime}$进行排序，并尝试将符合条件的邻居合并到$\mathcal{L}_i$中。我们使用并行自底向上归并排序对邻居进行排序。传统上，一个工作线程合并两组列表（在算法的合并阶段）。因此，随着算法的进展，并行性会下降，因为需要合并的列表减少，而每个线程的工作量增加，因为需要合并的列表规模变大。由于大多数GPU线程在排序完成之前保持空闲，并且每个线程的工作负载较高，这种方案并不适合GPU处理。我们通过并行合并列表来缓解这些问题，使用一个并行合并例程，该例程将在接下来的小节中描述。

---

在我们的使用场景中，我们需要对小列表进行排序，通常每个列表最多有64个邻居。我们为每个查询分配一个线程块，进一步为列表中的每个邻居分配一个线程，通过将线程块大小设置为图中一个节点的最大邻居数来进行排序。我们从每个只有一个元素的排序列表开始，并通过并行合并例程（第4.8节）在每个步骤中将它们的大小加倍。由于这些列表较小，我们可以在整个排序算法过程中将它们保存在GPU的共享内存中。

---

**工作-跨度分析**。使用并行合并例程合并两个大小为$t$的列表的工作量为$O(t \cdot \log (t))$（见第4.8节）。对于包含$n$个元素的数组，总工作量$T(n)$遵循递推关系：$T(n)=2 \cdot T(n / 2)+n \cdot \log (n)$，这得出$T(n)=O\left(n \cdot \log ^2(n)\right)$。因此，算法的工作量为$O\left(N u m N b r s \cdot \log ^2(N u m N b r s) \cdot \rho\right)$。由于并行化方案，算法的跨度为$O\left(\log ^2(N u m N b r s)\right)$。

## 4.8. Parallel Merge  

我们在本节中详细介绍的合并例程用于前述部分中提到的合并排序。此外，它还用于在算法2第14行将已排序的邻居列表$\mathcal{N}_i^{\prime}$与工作列表$\mathcal{L}_i$合并。我们借助现有的适用于GPU的并行列表合并算法【21】来满足我们的特定用例。图3展示了两个大小分别为4的列表的并行合并算法。假设有两个已排序的列表$L_{in1}$和$L_{in2}$，其中包含$m$和$n$个项目。对于列表$L_{in1}$中的一个项目$e$，其位置为$p_1<m$，当插入到$L_{in2}$中时，每个项目都分配了一个线程。根据分配给项目的线程ID，我们可以计算项目在原列表中的位置。我们使用二分查找来确定该项目在另一个列表中应出现的位置。例如，在图3中，列表$L_{in1}$中28这个项目的索引为3，这是通过线程ID确定的。在列表$L_{in2}$中，通过二分查找其索引为4。因此，28在合并列表中的索引为7 $(3+4)$。每个线程分配给一个项目，同时在另一个列表中执行二分查找。我们将列表保存在GPU的共享内存中，以最大限度减少搜索延迟。因此，计算每个元素在合并列表中的位置需要$O(\log(m)+\log(n))$时间。最后，元素被分散到合并列表中的新位置，例如，28被放置在合并列表$L_{\text {out }}$的索引7处。这一步也是并行执行的，每个线程将其项目写入合并列表中的唯一位置。

---

**工作-跨度分析**。合并两个大小为$l$的列表的算法工作量为$O(l \cdot \log (l))$。由于并行化方案，算法的跨度为$O(\log (l))$。

## 4.9. Re-ranking  

在算法2的搜索过程中，我们始终使用近似距离（通过PQ距离计算）。在搜索收敛后，我们采用了最终重新排序步骤【43】来提高整体召回率（见第3.2节）。

---

我们将重新排序实现为一个单独的内核，优化了线程块的大小。重新排序涉及为每个查询向量与其候选节点计算精确的L2距离，随后根据它们与查询向量的距离对这些候选节点进行排序，并报告前$k$个最近邻节点。对于每个查询，计算其与每个候选节点的距离是并行进行的。为了按精确距离排序候选节点，我们使用了第4.8节中描述的并行合并程序。得益于异步数据传输（见第4.3节），当该内核开始时，候选节点已经可用。根据我们的实验，重新排序对于所考察的数据集，召回率提高了10-15%。

---

**工作-跨度分析**。重新排序步骤的工作量为$O\left(\left(d \cdot|C|+|C| \cdot \log ^2(|C|)\right) \cdot \rho\right)$，其中$|C|$是每个查询的候选节点最大数量，$d$是向量维度。由于并行化方案，重新排序的跨度为$O\left(d+\log ^2(|C|)\right)$。

## 5. IMPLEMENTATION  

我们将前面第3和第4节中描述的实现称为BANG基础版本（或简称BANG）。此外，我们提供了专门针对小型和中型数据集（最多一亿个数据点）进行优化的版本，这些数据集可以完全放入GPU内存。以下是这些版本的讨论。

## 5.1. In-memory Version  

当像BANG基础版本那样将图结构存储在CPU时，CPU与GPU之间的链接会保持持续忙碌。这种CPU-GPU的相互依赖性还需要在GPU上仔细管理任务，以避免空闲时间。对于可以完全放入GPU的较小图，继续使用相同的方法是没有必要的。将整个图持久化在GPU上，而不是CPU上，能够减少CPU-GPU通信开销和依赖关系，从而提高吞吐量。这种优化消除了CPU为GPU候选节点从主内存中提取邻居这一内存密集型过程。此外，它还减少了在搜索过程中CPU与GPU之间通过PCIe总线传输数据的频率，从而实现了高达50%的吞吐量提升。我们将这种实现版本称为BANG内存版。该版本的示意框图如图4所示。与图1相比，主要有两个区别：(1) 在搜索开始之前，我们将整个图从CPU传输到GPU (2) 步骤(2)和(3)在GPU上通过合并访问全局内存本地获取邻居，而不是依赖CPU。

## 5.2. Exact-distance Version  

在这个版本中，为了在BANG内存版的基础上进一步优化，我们不再在搜索循环中使用PQ距离并通过后续的重新排序步骤来弥补精度损失，而是直接使用精确的L2距离计算。我们将这个版本称为BANG精确距离版（BANG Exact-distance）。与图4相比，主要的区别包括：(1) 没有PQ距离计算，(2) 进行精确距离计算（代替PQ距离求和），(3) 没有重新排序步骤。

# 6. EXPERIMENTAL SETUP  

## 6.1. Machine Configuration  

**硬件** 我们的实验是在一台配备Intel Xeon Gold 6326 CPU的机器上进行的，该CPU有32个核心（两个插槽，每个插槽16个核心），时钟速度为$2.90 \mathrm{GHz}$，拥有48 MiB的L3缓存和660 GiB的DDR4内存。该机器配备了NVIDIA Ampere A100 GPU，频率为1.41 GHz，拥有80 GB全局内存，峰值带宽为$2039 \mathrm{~GB} / \mathrm{s}$，以及6912个处理单元，分布在108个流式多处理器（SM）上。每个多处理器有164 KB的共享内存。GPU通过PCIe Gen 4.0总线连接主机，峰值传输带宽为$32 \mathrm{~GB} / \mathrm{s}$。我们在所有实验中只使用了一块GPU。

---

**软件** 该机器运行Ubuntu 22.04.01（64位）。我们使用g++ 11.3版本，编译标志为-O3、-std=c++11和-fopenmp来编译C++代码。我们使用OpenMP对C++代码进行并行化处理。编译GPU代码时，我们使用nvcc 11.8版本，编译标志为-O3。我们计划公开所有代码。

## 6.2. Datasets  

我们在来源于各种应用的十个流行的真实数据集上进行了实验，表2总结了测试集中数据集的特性。DEEP1B是一个包含十亿个图像嵌入的集合，压缩为96维。SIFT1B和SIFT1M分别代表十亿和一百万个图像的128维SIFT（尺度不变特征变换）描述符。SPACEV1B数据集【11】对来自Bing的网页文档和网页查询进行了编码，使用的是Microsoft SpaceV Superior模型。该数据集包含超过十亿个数据点，为了与我们测试集中其他十亿级数据集保持一致，我们从数据集中选取了前十亿个点。DEEP100M和SIFT100M分别是从DEEP1B和SIFT1B数据集中提取的前一亿个点生成的。MNIST8M是一个784维的数据集，包含手写数字图像的变形和平移后的嵌入。GIST1M包含一百万个图像的960维GIST（全局图像结构张量）描述符。GloVe200是一个包含1,183,514个200维单词嵌入的集合。NYTimes是一个包含256维单词嵌入和289,761个点的数据集。GloVe200和NYTimes的数据集具有偏态分布的向量，而其他数据集的向量分布较为均匀。除GIST1M和SPACEV1B之外的每个数据集都有10,000个查询。我们像之前的方法【22, 47】一样，在一次批处理中运行所有10,000个查询。由于GIST1M的原始查询集包含1,000个查询点，我们将其重复九次，使其变为10,000个点，以保持对比的一致性。同样，对于SPACEV1B，我们从查询集中选取了前10,000个点（总大小为29,316）。

## 6.3. Parameter Configuration: BANG  

**图构建** 我们在Vamana图上运行BANG搜索算法，该图是使用DiskANN【26】构建的。根据原始来源的推荐，我们指定了$R=64$（最大顶点度数）、$L=200$（构建时使用的工作列表大小）和$\sigma=1.2$（剪枝参数）。表2的第六列显示了每个数据集生成的图的大小。

---

**压缩** 我们为压缩指定了参数（见第2.3节中的PQ），使得生成的压缩向量可以适应GPU全局内存。在我们的设置中，我们通过实验确定$m$的最大值为74。除非另有说明，我们在所有实验中对所有数据集都使用$m=74$。需要注意的是，压缩率因数据集的维度而异。例如，对于128维的SIFT1B数据集，压缩率为$74 / 128=0.57$，而对于384维的DEEP1B数据集，压缩率为$74 / 384=0.19$。表2的最后一列显示了每个数据集的PQ压缩向量的总大小。

---

**搜索** 搜索从构建图时预先确定的medoid节点开始。我们使用标准的$k-r e c a l l @ k$召回率指标【26】，其中$k$是所需的最近邻数量。我们的查询批次大小为10,000。我们使用标准定义的每秒查询数（QPS）来衡量吞吐量。算法2中的搜索参数$t$（表示工作列表$\mathcal{L}$的大小）被调整以生成吞吐量与召回率的关系图。随着$t$值的增加，召回率也会增加。$t$的下限为$k$，上限通过启发式设置为152。布隆过滤器（见第4.4节）被创建以容纳399887个条目。我们调整布隆过滤器的大小至较低值，以生成低于使用$t=k$时的召回值。为了提高统计意义，我们报告的是五次独立运行的平均吞吐量。

# 6.4. Baselines  

我们将BANG与当前支持在GPU上进行十亿规模ANN搜索的开源最先进技术进行了定量比较。

---

**GGNN [22]**：它在GPU上使用分层$k$-NN图进行ANN查询搜索。该方法将大数据集分割成可以在多个GPU上并行处理的小分片，从而实现高吞吐量和高召回率。GGNN提供了可配置的图构建参数，例如：出边数量$(k)$、为子图选择的离散点数量$(s)$、分层图中的层数$(l)$、松弛因子$\left(\tau_b\right)$和精炼迭代次数$(r)$。我们为大多数数据集使用了原始源代码库中推荐的构建参数；对于其他数据集，我们按照作者的私人电子邮件建议进行了设置，以在我们的环境中获得最佳性能。  

SPACEV1B: $\left\{k: 20, s: 32, l: 4, \tau_b: 0.6, r: 2,16\right.$ shards $\}$. 

DEEP100M and SIFT100M: $\left\{k: 24, s: 32, l: 4, \tau_b: 0.5, r: 2\right.$, a single shard $\}$. 

MNIST8M $\{k: 96, s: 64$, the rest are the same as for DEEP100M and SIFT100M $\}$.

---

**SONG [47]**：这是一种基于图的ANN搜索算法，要求图和数据集完全位于GPU的全局内存中。它通过哈希处理较大的数据集。SONG使用一个有界优先队列，其大小可以变化，以控制吞吐量与召回率的权衡。在我们的实验中，我们根据我们的设置，将优先队列的大小在30到600之间调整，以获得最佳结果。

---

**FAISS [28]**：这是一种支持十亿级ANN的算法，提供CPU和GPU实现。我们使用了GPU版本。我们使用FAISS Wiki【27】中指定的三个索引构建参数：OPQ32_128、IVF262144和PQ32。第一个字段表示向量变换技术（预处理），它有助于索引构建和压缩阶段的后续步骤。第二个字段表示使用的IVF索引类型。第三个字段表示使用的编码方案（量化）。

---

我们在各种数据集规模上对BANG与这些基准方法进行了比较。由于SONG和GGNN在百万级数据集上优于FAISS，我们只在十亿级数据集上与FAISS进行比较。由于SONG的图生成在MNIST8M、100M和1B数据集上即使经过48小时也没有完成（多次尝试后仍无明显进展），因此我们未报告SONG在这些数据集上的结果。

# 7. RESULTS  

在本节中，我们对BANG与各种最先进方法在第6节详细描述的不同数据集上的性能和解决方案质量进行了全面比较。

## 7.1. Performance on Billion-scale Datasets  

图5展示了BANG与GGNN和FAISS在十亿级数据集上的对比。BANG在保持相同的10-recall@10召回率的情况下，在所有三个数据集上吞吐量均优于GGNN和FAISS。总体而言，BANG实现了卓越的性能，在召回率为0.9时，其吞吐量为$40 \times-200 \times$。

---

由于通过量化进行的激进压缩，FAISS的召回率较低；更低的压缩率会导致索引过大，无法放入GPU内存，因此无法使用。在单GPU上，GGNN的数据从CPU传输到GPU的开销阻碍了其吞吐量（原始GGNN实现通过利用八个GPU解决了这个问题）。例如，SIFT1B在搜索过程中需要额外将143 GB的数据从CPU传输到GPU，因为数据集和图结构分别为128 GB和95 GB，而GPU的全局内存为80 GB。通过PCIe 4.0互连@32GB/s传输这额外的数据需要接近5秒的时间。通过在GPU上使用压缩向量进行搜索，BANG避免了传输这额外数据的需求。此外，BANG的高吞吐量可以归因于在GPU上实现的大规模并行处理（第4节）。重新排序步骤，加上布隆过滤器用于过滤已访问节点（允许更优节点被包含到工作列表中），显著有助于BANG实现高召回率。

## 7.2. Performance on 100 Million-scale Datasets  

图6a和图6b展示了BANG与GGNN在100M数据集上的性能对比。由于100M数据集的图结构和数据点可以完全放入GPU内存中，我们报告了BANG的三种变体（第5节）的性能。正如图中所示，在DEEP100M和SIFT100M数据集上，BANG内存版和BANG精确距离版的吞吐量分别比BANG基础版高出2倍和3倍，因为没有CPU-GPU通信的开销。如图所示，BANG精确距离版也优于GGNN。我们本可以预期BANG精确距离版与GGNN表现相同，因为两者都将数据结构保存在GPU上，并都计算精确距离，但底层图结构不同。GGNN的k-NN图结构需要更多跳跃或迭代来终止搜索。相比之下，在BANG精确距离版中，Vamana图包含长距离边缘，减少了跳跃次数，使得计算量更少，从而更快收敛。

## 7.3. Performance on Million-scale Datasets  

图7以及图8a到图8d展示了BANG与GGNN和SONG在百万级数据集上的性能对比。由于这些数据点和图结构可以轻松适应GPU内存，我们报告了BANG三种变体的性能。BANG基础版的表现优于SONG，但在除MNIST8M之外的所有五个数据集中（图7），均不及GGNN。BANG基础版由于CPU-GPU数据传输的延迟，使GGNN在这方面占有优势。而对于MNIST8M，由于其高维度性，GGNN的搜索延迟增加；此外，BANG的Vamana图中的长距离边缘减少了跳跃次数，带来了明显的优势。

---

BANG内存版和BANG精确距离版在NYTimes、GIST1M和MNIST8M数据集上均优于GGNN。由于NYTimes和GloVe200数据集的分布偏态且聚类现象严重，所有BANG变体在这些数据集中均难以达到接近1的高召回率，这一点在【47】中也有所观察。SIFT1M和GloVe200是BANG精确距离版略微落后于GGNN的唯一两个数据集。

---

总体而言，BANG精确距离版比BANG内存版提供了更高的吞吐量，因为前者在到达查询点时所需的迭代次数较少，而后者使用压缩向量，这不可避免地引入了距离计算的不准确性（还需要重新排序步骤）。然而，在GIST1M中，由于计算960个维度的精确距离所需的开销较大，BANG精确距离版相比使用压缩向量的版本略有性能下滑。

## 7.4. Effect of Varying Compression Ratio  

正如第2.3节所讨论的，PQ压缩技术会引入召回率损失。为了评估压缩比对BANG整体召回率的影响，我们通过在SIFT1B数据集上改变PQ压缩方案中的压缩因子$m$进行了研究（其他搜索参数与第6节相同）。结果如图9所示。尽管压缩比降低时召回率预期会下降，但我们观察到即使将$m$从74降至32（相应的压缩比为0.57和0.25），吞吐量和召回率值都没有明显变化。我们发现，召回率在压缩比达到0.25之前保持稳定，之后才开始下降。这种表现使BANG能够在GPU内存有限的情况下高效运行，而不会牺牲召回率或吞吐量。例如，在SIFT1B数据集上，我们可以在拥有40 GB内存的A100 GPU上实现与80 GB A100相同的召回率和性能，这突显了BANG在内存受限的GPU环境中的潜力。

---

压缩比降低时，吞吐量应增加，因为它减少了距离计算内核中的计算量。然而，由于距离计算不准确性增加，BANG搜索需要更多迭代，采用更多跳跃和绕道才能到达最终的最近邻节点。这种负面效应导致在较低压缩比下吞吐量没有显著提高。

## 7.5. Efficiency of Parallel Query Processing  

BANG的ANN搜索涉及一个迭代循环（见算法2），总迭代次数代表给定查询的关键路径和总工作量。为了了解每个线程块的工作负载（回顾我们通过为每个查询分配一个线程块来进行并行化），我们研究了SIFT1B数据集上每个查询的迭代次数。我们选择不同的$\mathcal{L}$值，从20到180不等，块数量$m$设置为64，其他参数与第6节相同。结果如图10所示。需要注意的是，迭代总数的下限是工作列表$\mathcal{L}$的大小，因为工作列表中的每个条目都必须被访问。如图所示，对于每个具有特定$\mathcal{L}$值的运行，$95 \%$的查询在$1.1 \mathcal{L}$次迭代内完成，表明在迭代次数方面具有较高的效率。因此，在我们的并行化方案中，我们没有提出特定的线程调度机制，例如工作窃取（workstealing），即完成其分配查询$q_i$处理的线程块可以参与处理另一个查询$q_j$的工作列表$\mathcal{L}_j$中的未处理候选节点。

# 8. RELATED WORK  

ANNS 有多种算法 [42]，可在索引构建时间、召回率和吞吐量等方面进行权衡。 

## 8.1. ANN Search Data Structure.  

传统上，ANNS 采用 KD 树 [8, 12]。基于树的方法（包括 KD 树和覆盖树）采用分支和边界法进行导航。基于位置敏感哈希（LSH）的技术[2, 3]依赖于对附近点具有较高碰撞概率的哈希函数。随着维度和大小的不断增加，这两种方法都面临着可扩展性的挑战。

---

NSW【32】是一种早期基于图的近似最近邻搜索（ANNS）方法，它使用Delaunay图（DG）【17】来识别节点邻居，确保图的近乎完全连通性，但随着数据集维度的增加引入了高阶搜索空间。该图包含短距离边缘以提高准确性，同时包含长距离边缘以改善搜索效率，但这导致由于创建“交通枢纽”而使图的度数不平衡。HNSW【33】通过引入分层结构，将邻居分布在多个层次上并对节点度数设置上限，解决了这个问题。然而，HNSW在处理高维数据和大规模数据集时仍然存在扩展性问题。NSG【18】改进了基于图的方法的效率和扩展性，提出了单调相对邻域图（MRNG），通过减少索引大小和搜索路径长度，扩展到十亿级数据集。DiskANN【26】通过Vamana图在普通硬件上索引数百维的十亿数据集，结合了NSG和NSW的优势。它引入了可调参数$\alpha$，用于在构建过程中进行图度数与直径之间的权衡，并提出了压缩方案以减少内存消耗和搜索过程中的计算成本（见第2.2节）。文献【15】对流行的基于图的ANNS算法进行了广泛研究和比较，解决了扩展性问题，并提出了并行化方法。BANG利用了DiskANN【26】中的Vamana图进行搜索，重点在于基于GPU的ANN搜索，这与上述方法有所不同。

## 8.2. ANN Search on GPU.  

由于近似最近邻搜索（ANNS）的复杂性和对吞吐量的高要求，将这些计算任务转移到像GPU这样的大规模并行加速器上已受到广泛关注。FAISS【28】通过产品量化（Product Quantization）进行降维，并使用倒排索引作为底层数据结构。虽然FAISS在十亿级数据集上实现了高吞吐量，但在高召回率方面表现不佳。【43】扩展了产品量化的概念，提出了两级产品和向量量化树（PQT），其在高维和大规模ANNS中展示了GPU相比CPU的优越性能。然而，PQT存在一些缺点，如编码长度比PQ-code更长，且编码误差较大。为了解决这些问题，文献【9】提出了一种新型的两级树结构，称为向量与产品量化树（V-PQT），它使用两种不同的量化器对数据库向量进行索引，在召回率方面优于PQT。【39】提出了FAISS的高效分布式内存版本，适用于混合CPU-GPU系统，可扩展至256个节点。

---

SONG【47】是一种基于GPU的图结构ANN实现，它通过高效的GPU中心策略在ANNS中相较于最先进的CPU方法实现了显著的$50-180 \times$加速，并在FAISS的基础上取得了显著改进。基于此，GANNS【45】进一步通过GPU友好的数据结构和基于NSW的邻域图构建方案，探讨了增强并行性和提高GPU占用率的方案。然而，SONG和GANNS在扩展到十亿节点图时面临局限性，因为需要将整个图索引放入GPU中。在相关的工作中，文献【41】提出了一种两阶段的图多样化方法，用于图构建和适应各种批量查询规模的GPU友好搜索程序。基于GPU的最近邻搜索（GGNN）【22】是最近的一种实现，其性能超过了SONG。GGNN使用八个GPU进行十亿规模的ANN搜索，通过将大数据集分成较小的分片，使其能够适应GPU内存进行并行图构建，最终结果在CPU上合并。GGNN在高召回率（$>0.95$）下实现了非常高的吞吐量（>100,000）。最近的基于图的ANNS实现CAGRA【35】利用现代GPU功能（例如warp拆分）在百万级数据集上比现有的GPU ANNS方法取得了显著的性能提升。它要求图索引适应GPU内存，因此在处理十亿级数据集时需要多个GPU。与这些方法不同，BANG是一种高召回率、高吞吐量的ANNS方法，它仅在单个GPU上运行。

## 8.3. ANN Search on Other Accelerators.  

研究人员已经探索了使用定制硬件（例如FPGA）来加速ANNS，重点关注基于量化【1, 46】和基于图的【36】方法。然而，这些方法由于CPU内存和设备内存之间的频繁数据移动，导致高能耗和较低的吞吐量。为了解决这些局限性，vStore【31】提出了一种基于SSD的图结构ANNS的存储计算技术，避免了数据移动开销，实现了低搜索延迟和高精度。另一方面，TPU-KNN【10】专注于在TPU上实现ANNS。它使用一个精确的加速器性能模型，考虑了内存和指令瓶颈问题。

# 9. Conclusion

我们提出了BANG，一种新颖的基于GPU的ANNS方法，它能够高效处理超过GPU内存容量的大规模十亿级数据集，尤其是在单个GPU上运行。BANG结合了对压缩数据的计算和优化的GPU并行化技术，以实现大数据集上的高吞吐量。BANG通过GPU与CPU的计算流水线以及通信与计算的重叠，使其能够充分利用GPU和多核CPU的性能，并减少主机与GPU之间通过PCIe互连的数据传输。因此，在十亿级数据集上，BANG显著优于当前最先进的基于GPU的方法，在高召回率情况下实现了$40 \times-200 \times$的吞吐量提升；在较小的数据集上，其速度几乎总是比最先进的方法更快或相当。
