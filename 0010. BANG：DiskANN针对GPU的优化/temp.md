

## 4.3. Handling Data Transfer Overheads  

如算法2所述，每当GPU传输节点（第16行）时，CPU将邻居（第6行）传输给在GPU上执行的搜索例程。与GPU强大的处理能力相比，==设备（GPU）和主机（CPU）之间的数据传输是耗时的==（PCIe 4.0总线连接GPU，其最大传输带宽仅为32 GB/s）。因此，BANG只传输==最低限度所需的信息==，以尽量减少数据传输开销。具体而言，从设备到主机的传输包括在算法2收敛后搜索出的最终近似最近邻（ANN）列表，以及每次迭代中每个查询的候选节点（第16行）；而从主机到设备的传输则包括邻居顶点列表（第6行）和候选节点的基础向量（未在算法中显示）。

---

此外，BANG使用高级CUDA功能，通过内核计算来隐藏数据传输延迟。为了同时执行数据传输和内核操作，CUDA提供了==异步内存拷贝API==和==流的概念==【40】；BANG充分利用了这些功能。在搜索过程中，CPU线程为所有𝜌个查询检索给定顶点的邻居（见算法2第5行）。我们利用了图数据的高效结构，因为==顶点的基础向量和其邻居列表在CPU内存中紧挨着排列==，允许顺序内存访问。因此，在每次搜索迭代中将邻居列表传输到GPU后，我们会策略性地异步传输基础向量以备将来使用，这些向量仅在最终的重新排序步骤中需要（即算法2收敛后的GPU操作）。结果是，==GPU的内核执行引擎和拷贝引擎都能保持忙碌==，从而实现更高的吞吐量。因此，在我们的实现中，我们尽量使所有数据传输通过`cudaMemcpyAsync()`异步完成，并在适当位置使用`cudaStreamSynchronize()`来满足数据依赖。 

## 4.4. Handling Visited Vertices: Bloom Filter  

由于图$G_{\mathcal{p}}$具有建立远程边的特性【26】，算法2在搜索过程中可能多次遇到相同的顶点，这可能会影响吞吐量和/或召回率。吞吐量下降的问题在于搜索过程中可能会==重复执行某些操作==，特别是在获取邻居节点（第5行）和计算它们与查询点的距离（第12行）时，这些操作都非常计算密集。避免这种冗余是提高吞吐量的机会。召回率下降的问题则在于，如果不过滤已访问的邻居，它们可能会多次被添加到工作列表$\mathcal{L}_i$中，导致搜索可能会过早终止。此外，BANG中==保持对已访问顶点的记录非常重要==，而不是进行重复计算【45】。我们的实验表明，==如果不过滤已访问的顶点，召回率可能会降至原来的十分之一==。考虑到这些因素，我们在遍历过程中跟踪已访问的顶点，以确保每个节点只被访问一次。

---

记录已访问顶点的最常见方法有：i）为每个顶点设置一个位，ii）使用集合、优先队列或哈希表等数据结构。第一种方法速度快，但内存占用取决于图的大小和批处理查询的数量。对于十亿级图和10000个查询的批处理，它将需要$\left(10^9 \times 10^4\right)$位或$\frac{10^{13}}{10^9 \times 8} \mathrm{~GB}=$ 125 GB的内存。这仅这一项就==太大==，无法放入GPU中。第二种方法的内存开销较小，因为它只存储已访问的顶点。然而，集合、优先队列或哈希表等==动态数据结构在GPU上存在维护的困难==，特别是由于动态性和高度不规则的依赖关系。这些固有特性导致了GPU计算带宽的未充分利用，正如之前的研究$[22,47]$所证明的那样，因此不利于GPU的并行化处理。

---

为了在有限内存约束下优化GPU并行性，我们采用了==成熟的布隆过滤器【6】数据结构==，它在近似集合成员测试中的效率高且内存占用小，能够充分利用GPU的并行处理能力。我们为每个查询使用一个布隆过滤器，如算法2第10行所示。我们的布隆过滤器使用一个大小为$z$的布尔数组，其中$z$是根据查询过程中访问的顶点数量的估计值、可容忍的少量假阳性率（即出现假阳性的概率）以及使用的哈希函数数量确定的。我们使用了两个FNV1a哈希函数【20】，这是常用来实现布隆过滤器的轻量级非加密哈希函数。

## 4.5. Parallel Neighbor Distance Computation  

对于批处理中的每个查询，我们在每次迭代中计算其与当前邻居列表的非对称距离（算法2第12行）。这种计算对每个查询都是独立的，因此我们==为每个线程块分配一个查询==。此外，对于一个查询，与其每个邻居的距离也可以独立于其他邻居进行计算。由于之前构建的PQDistTable（第4.2节），我们可以通过汇总PQDistTable中所有子空间的质心部分距离来计算查询点$q_i$与邻居$n_i$的距离，质心信息由$n_i$的压缩向量提供（第2.3节和第3.2节）。==##############==

---

在基于GPU的近似最近邻搜索中，距离计算通常使用Nvidia的CUB【38】库中的归约API来进行求和，如【22, 47】中的线程块级或warp级归约。然而，在我们的方法中，一个大小为$t_b$的线程块被细分为$g$个组，每组由$g_{size}:=\frac{t_b}{g}$个线程组成。这些组协作计算查询点与邻居的距离，每组$g_{size}$个线程汇总$m$个质心的部分距离（每个片段的大小为$\frac{m}{g_{size}}$）。每个线程使用线程本地寄存器顺序计算片段中的值总和，避免同步。为了使用$g_{size}$线程有效地添加片段和的总和，我们探索了两种方法：(i) 使用`atomicAdd()`进行最终结果的原子操作；(ii) 子warp级归约，利用CUB【38】中的`WarpReduce`。通过实验调优，当$m=74, t_b=512$和$g_{size}=8$时，第二种方法（使用子warp级归约）的性能略优于第一种方法。总体来说，我们的分段方法优于标准替代方法，例如CUB的`WarpReduce`（慢$1.2 \times$）和`BlockReduce`（慢$4 \times$）的warp级和线程块级归约。

---

这个内核在十亿级数据集上的总运行时间中占据了相当大的一部分，平均约为38%。内核的效率受限于GPU全局内存访问的延迟，这是由于不规则的图结构导致对多个邻居的压缩向量进行未合并的访问。为了缓解这种不规则性，我们尝试了使用著名的逆Cuthill-McKee（RCM）算法【13, 19】进行图重排序。然而，这种方法并没有显著改善局部性，因此我们放弃了图重排序。

---

**工作-跨度分析**。算法的工作量为$O(N u m N b r s \cdot m \cdot \rho)$。由于并行化方案，算法的跨度为$O(\log m)$。

## 4.6. Prefetching Candidate Nodes  

如算法2所述，每个查询$q_i$的候选节点$u_i^*$只有在GPU更新完工作列表$\mathcal{L}_i$之后（第16行）才会传送给CPU。当CPU收集邻居节点（第5行）并将其传送给GPU（第6行）时，GPU处于空闲状态，等待接收这些信息。为了避免当前迭代开始时的延迟，必须在GPU需要时立即提供邻居ID。为了解决这个问题，我们进行了优化，在上一迭代结束时将候选节点传送到CPU之前，提前预测下一个候选节点，即在邻居距离计算完成后立即进行（即在排序邻居列表并更新工作列表$\mathcal{L}_i$之前）。

---

我们的优化方法是通过提前计算候选节点$u_i^*$，选择新邻居列表$N_i^{\prime}$中的最近邻节点和工作列表$\mathcal{L}_i$中第一个未访问的节点（按与查询点的距离排序），然后从两者中选择最佳节点。提前选择在算法2第13行之前进行。在将提前选出的候选节点传送给CPU后，GPU继续执行本次迭代的剩余任务（对新邻居列表进行排序并更新工作列表）。与此同时，CPU并行收集邻居ID。也就是说，CPU在执行第5行时，与GPU同时执行第13行和第14行任务。当GPU完成剩余任务时，CPU已经传送了邻居ID。这一策略消除了GPU的空闲时间，实验表明吞吐量提高了约10%。

## 4.7. Parallel Merge Sort  

为了在算法2的每次迭代中添加比工作列表$\mathcal{L}_i$中节点更接近$q_i$的新顶点，我们根据它们与$q_i$之间的非对称距离对邻居$N_i^{\prime}$进行排序，并尝试将符合条件的邻居合并到$\mathcal{L}_i$中。我们使用并行自底向上归并排序对邻居进行排序。传统上，一个工作线程合并两组列表（在算法的合并阶段）。因此，随着算法的进展，并行性会下降，因为需要合并的列表减少，而每个线程的工作量增加，因为需要合并的列表规模变大。由于大多数GPU线程在排序完成之前保持空闲，并且每个线程的工作负载较高，这种方案并不适合GPU处理。我们通过并行合并列表来缓解这些问题，使用一个并行合并例程，该例程将在接下来的小节中描述。

---

在我们的使用场景中，我们需要对小列表进行排序，通常每个列表最多有64个邻居。我们为每个查询分配一个线程块，进一步为列表中的每个邻居分配一个线程，通过将线程块大小设置为图中一个节点的最大邻居数来进行排序。我们从每个只有一个元素的排序列表开始，并通过并行合并例程（第4.8节）在每个步骤中将它们的大小加倍。由于这些列表较小，我们可以在整个排序算法过程中将它们保存在GPU的共享内存中。

---

**工作-跨度分析**。使用并行合并例程合并两个大小为$t$的列表的工作量为$O(t \cdot \log (t))$（见第4.8节）。对于包含$n$个元素的数组，总工作量$T(n)$遵循递推关系：$T(n)=2 \cdot T(n / 2)+n \cdot \log (n)$，这得出$T(n)=O\left(n \cdot \log ^2(n)\right)$。因此，算法的工作量为$O\left(N u m N b r s \cdot \log ^2(N u m N b r s) \cdot \rho\right)$。由于并行化方案，算法的跨度为$O\left(\log ^2(N u m N b r s)\right)$。

## 4.8. Parallel Merge  

我们在本节中详细介绍的合并例程用于前述部分中提到的合并排序。此外，它还用于在算法2第14行将已排序的邻居列表$\mathcal{N}_i^{\prime}$与工作列表$\mathcal{L}_i$合并。我们借助现有的适用于GPU的并行列表合并算法【21】来满足我们的特定用例。图3展示了两个大小分别为4的列表的并行合并算法。假设有两个已排序的列表$L_{in1}$和$L_{in2}$，其中包含$m$和$n$个项目。对于列表$L_{in1}$中的一个项目$e$，其位置为$p_1<m$，当插入到$L_{in2}$中时，每个项目都分配了一个线程。根据分配给项目的线程ID，我们可以计算项目在原列表中的位置。我们使用二分查找来确定该项目在另一个列表中应出现的位置。例如，在图3中，列表$L_{in1}$中28这个项目的索引为3，这是通过线程ID确定的。在列表$L_{in2}$中，通过二分查找其索引为4。因此，28在合并列表中的索引为7 $(3+4)$。每个线程分配给一个项目，同时在另一个列表中执行二分查找。我们将列表保存在GPU的共享内存中，以最大限度减少搜索延迟。因此，计算每个元素在合并列表中的位置需要$O(\log(m)+\log(n))$时间。最后，元素被分散到合并列表中的新位置，例如，28被放置在合并列表$L_{\text {out }}$的索引7处。这一步也是并行执行的，每个线程将其项目写入合并列表中的唯一位置。

---

**工作-跨度分析**。合并两个大小为$l$的列表的算法工作量为$O(l \cdot \log (l))$。由于并行化方案，算法的跨度为$O(\log (l))$。

## 4.9. Re-ranking  

在算法2的搜索过程中，我们始终使用近似距离（通过PQ距离计算）。在搜索收敛后，我们采用了最终重新排序步骤【43】来提高整体召回率（见第3.2节）。

---

我们将重新排序实现为一个单独的内核，优化了线程块的大小。重新排序涉及为每个查询向量与其候选节点计算精确的L2距离，随后根据它们与查询向量的距离对这些候选节点进行排序，并报告前$k$个最近邻节点。对于每个查询，计算其与每个候选节点的距离是并行进行的。为了按精确距离排序候选节点，我们使用了第4.8节中描述的并行合并程序。得益于异步数据传输（见第4.3节），当该内核开始时，候选节点已经可用。根据我们的实验，重新排序对于所考察的数据集，召回率提高了10-15%。

---

**工作-跨度分析**。重新排序步骤的工作量为$O\left(\left(d \cdot|C|+|C| \cdot \log ^2(|C|)\right) \cdot \rho\right)$，其中$|C|$是每个查询的候选节点最大数量，$d$是向量维度。由于并行化方案，重新排序的跨度为$O\left(d+\log ^2(|C|)\right)$。

## 5. IMPLEMENTATION  

我们将前面第3和第4节中描述的实现称为BANG基础版本（或简称BANG）。此外，我们提供了专门针对小型和中型数据集（最多一亿个数据点）进行优化的版本，这些数据集可以完全放入GPU内存。以下是这些版本的讨论。

## 5.1. In-memory Version  

当像BANG基础版本那样将图结构存储在CPU时，CPU与GPU之间的链接会保持持续忙碌。这种CPU-GPU的相互依赖性还需要在GPU上仔细管理任务，以避免空闲时间。对于可以完全放入GPU的较小图，继续使用相同的方法是没有必要的。将整个图持久化在GPU上，而不是CPU上，能够减少CPU-GPU通信开销和依赖关系，从而提高吞吐量。这种优化消除了CPU为GPU候选节点从主内存中提取邻居这一内存密集型过程。此外，它还减少了在搜索过程中CPU与GPU之间通过PCIe总线传输数据的频率，从而实现了高达50%的吞吐量提升。我们将这种实现版本称为BANG内存版。该版本的示意框图如图4所示。与图1相比，主要有两个区别：(1) 在搜索开始之前，我们将整个图从CPU传输到GPU (2) 步骤(2)和(3)在GPU上通过合并访问全局内存本地获取邻居，而不是依赖CPU。

## 5.2. Exact-distance Version  

在这个版本中，为了在BANG内存版的基础上进一步优化，我们不再在搜索循环中使用PQ距离并通过后续的重新排序步骤来弥补精度损失，而是直接使用精确的L2距离计算。我们将这个版本称为BANG精确距离版（BANG Exact-distance）。与图4相比，主要的区别包括：(1) 没有PQ距离计算，(2) 进行精确距离计算（代替PQ距离求和），(3) 没有重新排序步骤。

# 6. EXPERIMENTAL SETUP  

## 6.1. Machine Configuration  

**硬件** 我们的实验是在一台配备Intel Xeon Gold 6326 CPU的机器上进行的，该CPU有32个核心（两个插槽，每个插槽16个核心），时钟速度为$2.90 \mathrm{GHz}$，拥有48 MiB的L3缓存和660 GiB的DDR4内存。该机器配备了NVIDIA Ampere A100 GPU，频率为1.41 GHz，拥有80 GB全局内存，峰值带宽为$2039 \mathrm{~GB} / \mathrm{s}$，以及6912个处理单元，分布在108个流式多处理器（SM）上。每个多处理器有164 KB的共享内存。GPU通过PCIe Gen 4.0总线连接主机，峰值传输带宽为$32 \mathrm{~GB} / \mathrm{s}$。我们在所有实验中只使用了一块GPU。

---

**软件** 该机器运行Ubuntu 22.04.01（64位）。我们使用g++ 11.3版本，编译标志为-O3、-std=c++11和-fopenmp来编译C++代码。我们使用OpenMP对C++代码进行并行化处理。编译GPU代码时，我们使用nvcc 11.8版本，编译标志为-O3。我们计划公开所有代码。

## 6.2. Datasets  

我们在来源于各种应用的十个流行的真实数据集上进行了实验，表2总结了测试集中数据集的特性。DEEP1B是一个包含十亿个图像嵌入的集合，压缩为96维。SIFT1B和SIFT1M分别代表十亿和一百万个图像的128维SIFT（尺度不变特征变换）描述符。SPACEV1B数据集【11】对来自Bing的网页文档和网页查询进行了编码，使用的是Microsoft SpaceV Superior模型。该数据集包含超过十亿个数据点，为了与我们测试集中其他十亿级数据集保持一致，我们从数据集中选取了前十亿个点。DEEP100M和SIFT100M分别是从DEEP1B和SIFT1B数据集中提取的前一亿个点生成的。MNIST8M是一个784维的数据集，包含手写数字图像的变形和平移后的嵌入。GIST1M包含一百万个图像的960维GIST（全局图像结构张量）描述符。GloVe200是一个包含1,183,514个200维单词嵌入的集合。NYTimes是一个包含256维单词嵌入和289,761个点的数据集。GloVe200和NYTimes的数据集具有偏态分布的向量，而其他数据集的向量分布较为均匀。除GIST1M和SPACEV1B之外的每个数据集都有10,000个查询。我们像之前的方法【22, 47】一样，在一次批处理中运行所有10,000个查询。由于GIST1M的原始查询集包含1,000个查询点，我们将其重复九次，使其变为10,000个点，以保持对比的一致性。同样，对于SPACEV1B，我们从查询集中选取了前10,000个点（总大小为29,316）。

## 6.3. Parameter Configuration: BANG  

**图构建** 我们在Vamana图上运行BANG搜索算法，该图是使用DiskANN【26】构建的。根据原始来源的推荐，我们指定了$R=64$（最大顶点度数）、$L=200$（构建时使用的工作列表大小）和$\sigma=1.2$（剪枝参数）。表2的第六列显示了每个数据集生成的图的大小。

---

**压缩** 我们为压缩指定了参数（见第2.3节中的PQ），使得生成的压缩向量可以适应GPU全局内存。在我们的设置中，我们通过实验确定$m$的最大值为74。除非另有说明，我们在所有实验中对所有数据集都使用$m=74$。需要注意的是，压缩率因数据集的维度而异。例如，对于128维的SIFT1B数据集，压缩率为$74 / 128=0.57$，而对于384维的DEEP1B数据集，压缩率为$74 / 384=0.19$。表2的最后一列显示了每个数据集的PQ压缩向量的总大小。

---

**搜索** 搜索从构建图时预先确定的medoid节点开始。我们使用标准的$k-r e c a l l @ k$召回率指标【26】，其中$k$是所需的最近邻数量。我们的查询批次大小为10,000。我们使用标准定义的每秒查询数（QPS）来衡量吞吐量。算法2中的搜索参数$t$（表示工作列表$\mathcal{L}$的大小）被调整以生成吞吐量与召回率的关系图。随着$t$值的增加，召回率也会增加。$t$的下限为$k$，上限通过启发式设置为152。布隆过滤器（见第4.4节）被创建以容纳399887个条目。我们调整布隆过滤器的大小至较低值，以生成低于使用$t=k$时的召回值。为了提高统计意义，我们报告的是五次独立运行的平均吞吐量。

# 6.4. Baselines  

我们将BANG与当前支持在GPU上进行十亿规模ANN搜索的开源最先进技术进行了定量比较。

---

**GGNN [22]**：它在GPU上使用分层$k$-NN图进行ANN查询搜索。该方法将大数据集分割成可以在多个GPU上并行处理的小分片，从而实现高吞吐量和高召回率。GGNN提供了可配置的图构建参数，例如：出边数量$(k)$、为子图选择的离散点数量$(s)$、分层图中的层数$(l)$、松弛因子$\left(\tau_b\right)$和精炼迭代次数$(r)$。我们为大多数数据集使用了原始源代码库中推荐的构建参数；对于其他数据集，我们按照作者的私人电子邮件建议进行了设置，以在我们的环境中获得最佳性能。  

SPACEV1B: $\left\{k: 20, s: 32, l: 4, \tau_b: 0.6, r: 2,16\right.$ shards $\}$. 

DEEP100M and SIFT100M: $\left\{k: 24, s: 32, l: 4, \tau_b: 0.5, r: 2\right.$, a single shard $\}$. 

MNIST8M $\{k: 96, s: 64$, the rest are the same as for DEEP100M and SIFT100M $\}$.

---

**SONG [47]**：这是一种基于图的ANN搜索算法，要求图和数据集完全位于GPU的全局内存中。它通过哈希处理较大的数据集。SONG使用一个有界优先队列，其大小可以变化，以控制吞吐量与召回率的权衡。在我们的实验中，我们根据我们的设置，将优先队列的大小在30到600之间调整，以获得最佳结果。

---

**FAISS [28]**：这是一种支持十亿级ANN的算法，提供CPU和GPU实现。我们使用了GPU版本。我们使用FAISS Wiki【27】中指定的三个索引构建参数：OPQ32_128、IVF262144和PQ32。第一个字段表示向量变换技术（预处理），它有助于索引构建和压缩阶段的后续步骤。第二个字段表示使用的IVF索引类型。第三个字段表示使用的编码方案（量化）。

---

我们在各种数据集规模上对BANG与这些基准方法进行了比较。由于SONG和GGNN在百万级数据集上优于FAISS，我们只在十亿级数据集上与FAISS进行比较。由于SONG的图生成在MNIST8M、100M和1B数据集上即使经过48小时也没有完成（多次尝试后仍无明显进展），因此我们未报告SONG在这些数据集上的结果。

# 7. RESULTS  

在本节中，我们对BANG与各种最先进方法在第6节详细描述的不同数据集上的性能和解决方案质量进行了全面比较。

## 7.1. Performance on Billion-scale Datasets  

图5展示了BANG与GGNN和FAISS在十亿级数据集上的对比。BANG在保持相同的10-recall@10召回率的情况下，在所有三个数据集上吞吐量均优于GGNN和FAISS。总体而言，BANG实现了卓越的性能，在召回率为0.9时，其吞吐量为$40 \times-200 \times$。

---

由于通过量化进行的激进压缩，FAISS的召回率较低；更低的压缩率会导致索引过大，无法放入GPU内存，因此无法使用。在单GPU上，GGNN的数据从CPU传输到GPU的开销阻碍了其吞吐量（原始GGNN实现通过利用八个GPU解决了这个问题）。例如，SIFT1B在搜索过程中需要额外将143 GB的数据从CPU传输到GPU，因为数据集和图结构分别为128 GB和95 GB，而GPU的全局内存为80 GB。通过PCIe 4.0互连@32GB/s传输这额外的数据需要接近5秒的时间。通过在GPU上使用压缩向量进行搜索，BANG避免了传输这额外数据的需求。此外，BANG的高吞吐量可以归因于在GPU上实现的大规模并行处理（第4节）。重新排序步骤，加上布隆过滤器用于过滤已访问节点（允许更优节点被包含到工作列表中），显著有助于BANG实现高召回率。

## 7.2. Performance on 100 Million-scale Datasets  

图6a和图6b展示了BANG与GGNN在100M数据集上的性能对比。由于100M数据集的图结构和数据点可以完全放入GPU内存中，我们报告了BANG的三种变体（第5节）的性能。正如图中所示，在DEEP100M和SIFT100M数据集上，BANG内存版和BANG精确距离版的吞吐量分别比BANG基础版高出2倍和3倍，因为没有CPU-GPU通信的开销。如图所示，BANG精确距离版也优于GGNN。我们本可以预期BANG精确距离版与GGNN表现相同，因为两者都将数据结构保存在GPU上，并都计算精确距离，但底层图结构不同。GGNN的k-NN图结构需要更多跳跃或迭代来终止搜索。相比之下，在BANG精确距离版中，Vamana图包含长距离边缘，减少了跳跃次数，使得计算量更少，从而更快收敛。

## 7.3. Performance on Million-scale Datasets  

图7以及图8a到图8d展示了BANG与GGNN和SONG在百万级数据集上的性能对比。由于这些数据点和图结构可以轻松适应GPU内存，我们报告了BANG三种变体的性能。BANG基础版的表现优于SONG，但在除MNIST8M之外的所有五个数据集中（图7），均不及GGNN。BANG基础版由于CPU-GPU数据传输的延迟，使GGNN在这方面占有优势。而对于MNIST8M，由于其高维度性，GGNN的搜索延迟增加；此外，BANG的Vamana图中的长距离边缘减少了跳跃次数，带来了明显的优势。

---

BANG内存版和BANG精确距离版在NYTimes、GIST1M和MNIST8M数据集上均优于GGNN。由于NYTimes和GloVe200数据集的分布偏态且聚类现象严重，所有BANG变体在这些数据集中均难以达到接近1的高召回率，这一点在【47】中也有所观察。SIFT1M和GloVe200是BANG精确距离版略微落后于GGNN的唯一两个数据集。

---

总体而言，BANG精确距离版比BANG内存版提供了更高的吞吐量，因为前者在到达查询点时所需的迭代次数较少，而后者使用压缩向量，这不可避免地引入了距离计算的不准确性（还需要重新排序步骤）。然而，在GIST1M中，由于计算960个维度的精确距离所需的开销较大，BANG精确距离版相比使用压缩向量的版本略有性能下滑。

## 7.4. Effect of Varying Compression Ratio  

正如第2.3节所讨论的，PQ压缩技术会引入召回率损失。为了评估压缩比对BANG整体召回率的影响，我们通过在SIFT1B数据集上改变PQ压缩方案中的压缩因子$m$进行了研究（其他搜索参数与第6节相同）。结果如图9所示。尽管压缩比降低时召回率预期会下降，但我们观察到即使将$m$从74降至32（相应的压缩比为0.57和0.25），吞吐量和召回率值都没有明显变化。我们发现，召回率在压缩比达到0.25之前保持稳定，之后才开始下降。这种表现使BANG能够在GPU内存有限的情况下高效运行，而不会牺牲召回率或吞吐量。例如，在SIFT1B数据集上，我们可以在拥有40 GB内存的A100 GPU上实现与80 GB A100相同的召回率和性能，这突显了BANG在内存受限的GPU环境中的潜力。

---

压缩比降低时，吞吐量应增加，因为它减少了距离计算内核中的计算量。然而，由于距离计算不准确性增加，BANG搜索需要更多迭代，采用更多跳跃和绕道才能到达最终的最近邻节点。这种负面效应导致在较低压缩比下吞吐量没有显著提高。

## 7.5. Efficiency of Parallel Query Processing  

BANG的ANN搜索涉及一个迭代循环（见算法2），总迭代次数代表给定查询的关键路径和总工作量。为了了解每个线程块的工作负载（回顾我们通过为每个查询分配一个线程块来进行并行化），我们研究了SIFT1B数据集上每个查询的迭代次数。我们选择不同的$\mathcal{L}$值，从20到180不等，块数量$m$设置为64，其他参数与第6节相同。结果如图10所示。需要注意的是，迭代总数的下限是工作列表$\mathcal{L}$的大小，因为工作列表中的每个条目都必须被访问。如图所示，对于每个具有特定$\mathcal{L}$值的运行，$95 \%$的查询在$1.1 \mathcal{L}$次迭代内完成，表明在迭代次数方面具有较高的效率。因此，在我们的并行化方案中，我们没有提出特定的线程调度机制，例如工作窃取（workstealing），即完成其分配查询$q_i$处理的线程块可以参与处理另一个查询$q_j$的工作列表$\mathcal{L}_j$中的未处理候选节点。

# 8. RELATED WORK  

ANNS 有多种算法 [42]，可在索引构建时间、召回率和吞吐量等方面进行权衡。 

## 8.1. ANN Search Data Structure.  

传统上，ANNS 采用 KD 树 [8, 12]。基于树的方法（包括 KD 树和覆盖树）采用分支和边界法进行导航。基于位置敏感哈希（LSH）的技术[2, 3]依赖于对附近点具有较高碰撞概率的哈希函数。随着维度和大小的不断增加，这两种方法都面临着可扩展性的挑战。

---

NSW【32】是一种早期基于图的近似最近邻搜索（ANNS）方法，它使用Delaunay图（DG）【17】来识别节点邻居，确保图的近乎完全连通性，但随着数据集维度的增加引入了高阶搜索空间。该图包含短距离边缘以提高准确性，同时包含长距离边缘以改善搜索效率，但这导致由于创建“交通枢纽”而使图的度数不平衡。HNSW【33】通过引入分层结构，将邻居分布在多个层次上并对节点度数设置上限，解决了这个问题。然而，HNSW在处理高维数据和大规模数据集时仍然存在扩展性问题。NSG【18】改进了基于图的方法的效率和扩展性，提出了单调相对邻域图（MRNG），通过减少索引大小和搜索路径长度，扩展到十亿级数据集。DiskANN【26】通过Vamana图在普通硬件上索引数百维的十亿数据集，结合了NSG和NSW的优势。它引入了可调参数$\alpha$，用于在构建过程中进行图度数与直径之间的权衡，并提出了压缩方案以减少内存消耗和搜索过程中的计算成本（见第2.2节）。文献【15】对流行的基于图的ANNS算法进行了广泛研究和比较，解决了扩展性问题，并提出了并行化方法。BANG利用了DiskANN【26】中的Vamana图进行搜索，重点在于基于GPU的ANN搜索，这与上述方法有所不同。

## 8.2. ANN Search on GPU.  

由于近似最近邻搜索（ANNS）的复杂性和对吞吐量的高要求，将这些计算任务转移到像GPU这样的大规模并行加速器上已受到广泛关注。FAISS【28】通过产品量化（Product Quantization）进行降维，并使用倒排索引作为底层数据结构。虽然FAISS在十亿级数据集上实现了高吞吐量，但在高召回率方面表现不佳。【43】扩展了产品量化的概念，提出了两级产品和向量量化树（PQT），其在高维和大规模ANNS中展示了GPU相比CPU的优越性能。然而，PQT存在一些缺点，如编码长度比PQ-code更长，且编码误差较大。为了解决这些问题，文献【9】提出了一种新型的两级树结构，称为向量与产品量化树（V-PQT），它使用两种不同的量化器对数据库向量进行索引，在召回率方面优于PQT。【39】提出了FAISS的高效分布式内存版本，适用于混合CPU-GPU系统，可扩展至256个节点。

---

SONG【47】是一种基于GPU的图结构ANN实现，它通过高效的GPU中心策略在ANNS中相较于最先进的CPU方法实现了显著的$50-180 \times$加速，并在FAISS的基础上取得了显著改进。基于此，GANNS【45】进一步通过GPU友好的数据结构和基于NSW的邻域图构建方案，探讨了增强并行性和提高GPU占用率的方案。然而，SONG和GANNS在扩展到十亿节点图时面临局限性，因为需要将整个图索引放入GPU中。在相关的工作中，文献【41】提出了一种两阶段的图多样化方法，用于图构建和适应各种批量查询规模的GPU友好搜索程序。基于GPU的最近邻搜索（GGNN）【22】是最近的一种实现，其性能超过了SONG。GGNN使用八个GPU进行十亿规模的ANN搜索，通过将大数据集分成较小的分片，使其能够适应GPU内存进行并行图构建，最终结果在CPU上合并。GGNN在高召回率（$>0.95$）下实现了非常高的吞吐量（>100,000）。最近的基于图的ANNS实现CAGRA【35】利用现代GPU功能（例如warp拆分）在百万级数据集上比现有的GPU ANNS方法取得了显著的性能提升。它要求图索引适应GPU内存，因此在处理十亿级数据集时需要多个GPU。与这些方法不同，BANG是一种高召回率、高吞吐量的ANNS方法，它仅在单个GPU上运行。

## 8.3. ANN Search on Other Accelerators.  

研究人员已经探索了使用定制硬件（例如FPGA）来加速ANNS，重点关注基于量化【1, 46】和基于图的【36】方法。然而，这些方法由于CPU内存和设备内存之间的频繁数据移动，导致高能耗和较低的吞吐量。为了解决这些局限性，vStore【31】提出了一种基于SSD的图结构ANNS的存储计算技术，避免了数据移动开销，实现了低搜索延迟和高精度。另一方面，TPU-KNN【10】专注于在TPU上实现ANNS。它使用一个精确的加速器性能模型，考虑了内存和指令瓶颈问题。

# 9. Conclusion

我们提出了BANG，一种新颖的基于GPU的ANNS方法，它能够高效处理超过GPU内存容量的大规模十亿级数据集，尤其是在单个GPU上运行。BANG结合了对压缩数据的计算和优化的GPU并行化技术，以实现大数据集上的高吞吐量。BANG通过GPU与CPU的计算流水线以及通信与计算的重叠，使其能够充分利用GPU和多核CPU的性能，并减少主机与GPU之间通过PCIe互连的数据传输。因此，在十亿级数据集上，BANG显著优于当前最先进的基于GPU的方法，在高召回率情况下实现了$40 \times-200 \times$的吞吐量提升；在较小的数据集上，其速度几乎总是比最先进的方法更快或相当。