#### 基于BERT的上下文延迟交互中的学习型Token剪枝（ColBERT）

#### 摘要

基于BERT的排序模型在信息检索任务中作为重排序器已被证明非常有效。为了将这些模型扩展到全排序场景中，最近提出了ColBERT模型，该模型采用了延迟交互机制。这种机制允许提前预计算文档的表示。然而，延迟交互机制导致了较大的索引大小，因为需要为每个文档的每个Token保存一个表示。在本研究中，我们专注于Token剪枝技术，以缓解这一问题。我们测试了四种方法，从较简单的方法到使用单层注意力机制在索引时选择要保留的Token。我们的实验表明，对于MS MARCO-passages数据集，索引可以剪枝到其原始大小的70%，而不会显著降低性能。我们还在MS MARCO-documents数据集和BEIR基准上进行了评估，揭示了所提出机制的一些挑战。

## 1 引言

在最近的信息检索（IR）系统中，预训练语言模型（PLM）[11]已经引领了最新技术的发展。主要开发了两类PLM检索方法：

(1) 基于表示的方法 [13]，其中查询和文档分别编码为单一表示，并通过表示之间的距离进行评分；

(2) 基于交互的方法 [12]，其中查询-文档对由神经网络联合处理以生成评分。

---

一方面，前者非常高效，因为文档的表示可以被索引，在推理时只需要计算查询。另一方面，后者具有更好的性能，因为它能够在查询和文档之间进行更彻底的评分。为了弥补这两类方法之间的差距，ColBERT [9] 方法为每个Token索引一个表示，这允许预计算文档表示，并部分实现交互模型的能力（查询的每个上下文化Token与文档的每个预计算上下文化Token进行交互）。然而，ColBERT的优势带来了索引大小的重要成本，因为每个Token（而不是文档的池化版本）都需要被索引。

---

在本研究中，我们研究了ColBERT模型，重点关注表示的两个特征：归一化和查询扩展。然后，我们通过限制每个文档中要保存的Token数量来关注索引大小，使用了四种方法，基于i）Token的位置，ii）Token的逆文档频率（IDF），iii）特殊Token，以及iv）最后一层的注意力机制。为了实证评估我们的方法，我们在最常用的神经IR基准（MS MARCO的段落和文档任务）下进行了研究，表明我们能够在保持可接受的有效性（在MRR和召回率方面）的同时，显著提高效率（在索引大小和复杂性方面）。

## 2 相关工作

**高效预训练语言模型**：在自然语言处理（NLP）领域，有许多研究致力于提高预训练语言模型的效率。例如，量化和蒸馏技术在这一背景下得到了广泛研究 [14]。与我们的工作最接近的是Power-BERT [6] 和长度自适应Transformer [10]，它们通过消除预训练语言模型层中的Token来减少浮点运算（FLOPS）次数。

---

**信息检索中的索引剪枝**：索引剪枝是信息检索中一种传统的方法，用于减少延迟和内存需求，并且在许多研究中得到了深入探讨（例如 [21] 和 [2]）。据我们所知，这是首次将索引剪枝应用于预训练语言模型的Token级索引。

---

**提高ColBERT的效率**：缓解ColBERT索引大小问题的一种方法是降低维度并对文档Token进行量化。需要注意的是，这一方法已经在默认情况下实施，因为预训练语言模型的输出（通常为768维）会被投影到一个较小的空间（128维）。在原论文中，作者通过降低维度和量化Token展示了非常好的性能 [9]。在这方面，提出了一种用于信息检索的二值化技术 [20]，同时与本工作并行的实验表明，ColBERTToken确实可以进行二值化 [15]。

---

本工作与上述研究方向正交，因为我们旨在通过从索引中移除Token来减少索引大小，而不是减小其尺寸。我们认为这些研究方向的结合对于改进ColBERT模型是必要的，但将其留作未来工作。最后，ColBERT已被扩展用于执行伪相关反馈 [19]，并且为了提高延迟，查询剪枝也已在 [17] 中进行了研究。

## 3 方法论与ColBERT

ColBERT模型基于Transformer [3] 编码器，用于处理文档和查询。每个条目$Q$或$D$被编码为一个表示$\mathbf{R}$，这是一个维度为$(t, d)$的矩阵，其中$t$是条目中的Token数量，$d$是每个Token的编码维度。对于给定查询$(Q)$，文档$(D)$的评分由以下公式给出：

$$
\begin{equation*}
s(D, Q)=\sum_{j=0}^{\left|t_{Q}\right|} \max _{i=0}^{\left|t_{D}\right|}\left(\mathbf{R}_{D} \cdot \mathbf{R}_{Q}\right)_{i, j} \tag{1}
\end{equation*}
$$

---

这种延迟交互模型与完全交互模型相比，提高了计算效率，因为所有文档$(D \in \mathcal{D})$的表示$\mathbf{R}_{D}$可以预先计算。这样，在推理过程中只需要计算查询的表示。然而，这会导致每个文档的表示大小$(d t)$非常大，当文档数量增加时，可能会迅速变得难以处理。由于这些是影响索引大小的唯一因素，因此有两种解决方案：i) 减少每个Token的维度，ii) 减少每个文档的Token数量。虽然针对第一种情况有许多解决方案，但我们尚未发现任何关于第二种情况的前期工作。

---

**归一化和查询扩展**：为了确保每个查询和文档的评分不依赖于它们的长度，每个Token在l2超球面中被归一化。此外，查询被扩展为具有相同的长度（32个Token）。我们通过实验表明，并非所有预训练语言模型（PLM）骨干都需要这样做（参见第4节），这显著降低了推理成本。

### 3.1 限制Token数量

我们研究了四种不同的方法，用于限制每个文档存储的Token数量。这些方法在训练ColBERT模型时被集成${ }^{1}$。换句话说，我们在ColBERT之上添加了一个池化层，以选择每个文档最多$k$个Token，然后使用ColBERT评分公式，但仅限于这$k$个选定的Token。

---

**前$k$个Token**：一个非常简单但非常强的基线是仅保留文档的前$k$个Token。事实上，这样的基线利用了MS MARCO数据集的内在偏差，其中前几个Token被认为是最重要的[8]。这是唯一一种不删除标点符号的方法。

---

**前$k$个IDFToken**：另一种可能性是选择文档中最稀有的Token，即具有最高逆文档频率（IDF）的Token。这应该使我们能够仅保留文档中最具定义性的词。

---

**$k$个未使用Token（UNU）**：我们还测试了在文档开头添加$k$个特殊Token（来自BERT词汇表的“未使用”术语）并仅保留这些Token的可能性。保留的Token对于所有文档始终相同，并且始终位于相同的位置，这增加了模型的一致性，我们认为这有助于优化。然而，这种方法有一些缺点：i) 增加了计算时间，因为它强制文档至少具有$k$个Token；ii) 由于截断时使用的“真实词”Token较少，可能会丢失长文档的上下文。

---

**注意力分数（ATT）**：我们建议使用预训练语言模型（PLM）的最后一层注意力来检测文档中最重要的$k<t$个Token。回想一下，文档的注意力$\left(A_{D}\right)$是一个三维张量$\left(h, t, t^{\prime}\right)$，其中第一个维度是注意力头的数量（不同“注意力”的数量），最后两个维度表示Token$t$对Token$t^{\prime}$的注意力大小，其被归一化，使得对于每个Token$t$，所有$t^{\prime}$的注意力之和为1。我们提出了一种基于每个Token的重要性分数，该分数是Token在所有注意力头中接收到的注意力之和：

$$
\begin{equation*}
i(t)_{D}=\sum_{i=0}^{h} \sum_{j=0}^{\left|t_{D}\right|}\left(A_{D}\right)_{i, t, j} \tag{2}
\end{equation*}
$$

---

因此，最终我们仅保留基于前$k$个分数的$k<t$个Token。需要注意的是，该分数是在文档编码的最后一层计算的，而不是在文档与查询的交互中计算的。换句话说，它与查询无关。

## 4 实验

**实验设置**：在本研究中，我们使用MINILM-12-384 [18] ${ }^{2}$ 训练ColBERT [9] 模型。模型训练150k步，学习率从0线性增加到最终学习率（$2 \mathrm{e}-5$），前10k步为学习率预热阶段，后140k步为线性学习率衰减（从$2 \mathrm{e}-5$到$0$）。我们同时考虑了段落任务和文档任务，并使用原始的三元组文件进行评估。我们在全排序场景下评估模型，使用暴力搜索实现，这与原始ColBERT论文中使用近似最近邻（ANN）搜索的方式不同。通过不使用ANN搜索，我们避免了ANN超参数调优的需要以及不公平比较的风险。尽管如此，我们也尝试了ANN搜索，结果与初始段落中呈现的结果非常接近。最后，我们训练和评估的“段落”模型的最大长度为180个Token（截断更长的序列），而“文档”模型的最大长度为512个Token。

---

**ColBERT建模研究**：首先，我们研究了默认的ColBERT模型，并验证归一化和查询扩展操作是否对所有预训练语言模型（PLM）都有帮助。这一消融实验的结果如表1所示。我们观察到，当使用MINILM代替BERT时，归一化和扩展操作并不是必需的。因此，我们可以跳过这些步骤，从而减少延迟（搜索复杂度与查询中的Token数量线性相关）。在本研究的其余部分，我们仅使用不包含这些操作的模型。

---

**在段落数据集上减少索引大小**：我们现在评估了四种提出的Token剪枝方法（参见第3.1节）的能力。结果如表2所示。我们注意到，当保留前$k=50$个Token时，几乎所有测试的方法都允许我们将Token数量减少$30\%$，同时保持相似的性能（MRR@10下降小于0.01，Recall@1000下降小于$1\%$）。另一方面，当$k=10$时，所有方法的MRR@10性能都有明显下降，其中未使用Token方法表现最佳，这表明该技术对于较小的$k$非常有用，但对于较大的$k$则不然。需要注意的是，在其他数据集上的进一步测试可能有助于验证未使用Token方法与其他方法之间的差异，以及利用MS MARCO数据集对文档开头固有偏差的“前$k$个Token”方法 [8]。此外，Token的百分比可能会因以下因素而变化：段落的大小（$k=50$）；是否使用标点符号（前$k$个Token方法）；重复分数（注意力和IDF方法）；或因为某种方法增加了原始段落的长度（未使用Token方法）。

- 表2：在MS MARCO数据集上不同方法和保留$k$个Token的结果。我们使用MRR@10评估段落数据集，使用MRR@100评估文档数据集。索引大小考虑每个向量2kb（128维，fp16格式）。

---

**在文档数据集上减少索引大小**：在文档任务中，每个文档的平均Token数量高于段落任务。由于这种差异，我们仅测试了$k=50$的情况，这允许我们剪枝$85\%$的文档Token。虽然$k=50$显著减少了索引大小，但这仍然意味着每个文档需要保留的数据量较大$(50 d)$。例如，虽然文档$(k=50)$的索引大小相对减少与段落任务中$k=10$的情况相同，但最终的索引大小仍然是段落任务的两倍。最后，与段落任务中较小Token数量的情况类似，未使用Token方法和前$k$个Token方法表现最佳。

---

**BEIR基准测试结果**：我们现在验证了通过Token剪枝学习的网络在MS MARCO领域之外的泛化能力。为此，我们使用了BEIR基准测试 [16]，并使用了其中13个已准备就绪的数据集子集${ }^{3}$。我们对比的模型包括标准的ColBERT [9]（使用[16]中报告的数据）、我们的基线ColBERT（使用MINILM预训练语言模型且不进行扩展或归一化）以及为段落检索训练的4个模型（$k=50$，即最大序列长度为180个Token）。结果如表3所示。

---

首先，我们注意到所有为本研究训练的模型在Arguana数据集上均优于原始的ColBERT [9]，这给结果带来了一些噪音。我们认为这是因为Arguana的查询实际上是文档，因此[9]由于其查询扩展而表现不佳，但我们没有进一步深入研究以验证这一说法。其次，我们注意到性能损失略高于之前在MS MARCO中观察到的结果（BEIR上的平均nDCG@10损失约为6\%，而MS MARCO上的MRR@10损失为$2\%$），这主要是因为在BEIR数据集中文档的尺寸更大（即压缩率更高）。然而，这并不是性能损失的唯一（甚至可能不是主要）原因，因为与MS MARCO尺寸相似的数据集（例如HotpotQA）也表现出显著的性能下降。最后，与在MS MARCO上的表现相比，注意力方法在BEIR上的性能提升更大，这可能表明在MS MARCO对段落开头的固有偏差之外 [8]，这种方法确实能够表现出色。

---

我们还测试了为文档检索训练的模型，并在表4中展示了结果。与段落检索相比，基线模型在平均性能上表现更好，而一些剪枝模型的平均性能有所下降（注意力分数和IDF方法），另一些则提升了性能（未使用Token方法）或保持了相同的性能（前$k$个Token方法）。注意力和IDF方法的性能损失似乎源于随着Token数量的增加，这些模型倾向于重复单词（参见第5节），而未使用Token方法的性能提升则可能源于网络能够处理更多Token，从而减少了由于增加序列长度而导致的信息损失（因此更少的真实序列被截断）。最后，前$k$个Token方法保持相同的性能并不令人意外，因为它无论训练或最大序列长度如何，都保留了相同的Token。

---

**使用近似最近邻（ANN）的结果**：为了完整性，我们还按照原始ColBERT论文 [9] 中描述的近似最近邻方案进行了段落检索，代码可从以下网址获取：https://github.com/stanford-futuredata/ColBERT/。在第一阶段的Token检索中，我们探测了128个最近的簇，并对每个查询Token的前8k个Token所在的文档进行完整检索。结果如表5所示。

---

值得注意的是，MRR@10的差异可以忽略不计，因为我们评估的7个网络中没有一个的MRR@10差异超过0.001。另一方面，当我们查看Recall@1000时，差异开始显现，基线模型损失了$0.6\%$，$k=50$的模型平均损失了$0.7\%$。然而，对于$k=10$的模型，没有出现这种性能损失。这证实了虽然使用ANN的结果与暴力搜索非常接近，但它们可能会影响结果，特别是如果我们将相同的ANN应用于BEIR数据集，因为不同数据集的Token数量（以及簇的大小）各不相同。

## 5 结果分析

**索引文档的分析**：我们分析了在文档集上通过注意力机制选择的Token。我们观察到一个问题是重复：它选择了同一Token在不同位置的出现。例如，我们展示了两个不同文档的选定Token示例，以演示重复和词干提取效果（dog vs dogs, cake vs \#\#cake）：

---

```
['[CLS]', 'dog', 'nasal', 'poly', 'removal', 'dog', 'nasal', 'poly', 'removal', 'poly', 'grow', 'body', 'parts', 'dogs', 'nasal', 'poly', 'dog', 'being', 'dog', 'nasal', 'poly', 'dog', 'dog', 'nasal', 'poly', 'removal', 'nasal', 'poly', 'dogs', 'poly', 'dog', 'dogs', 'nasal', 'poly', 'dogs', 'hide', "', 'dogs', 'dog', 'nasal', 'growth', 'nasal', 'poly', 'dogs', "', 'dogs', 'nose', 'dogs', 'nose', '[SEP]']

['[CLS]', 'ba', 'mini', 'cup', '\#\#cake', 'cake', 'mix', 'ba', 'mini', ' ' $\quad$ ',', '\#\#cake', 'cake', 'mix', 'cup', '\#\#cake', 'sweet', 'ba', 'cake', 'mini', 'cup', '\#\#cake', 'cup', 'cake', 'mini', 'cup', '\#\#cake', 'oven', 'mini', 'cup', '\#\#cake', 'pan', 'mini', 'cake', 'mini', 'cup', '\#\#cake,' 'mini', 'cup', 'mini', 'cup', '\#\#cake', 'cup', '\#\#cake', 'cup', '\#\#cake', 'cup', '\#\#cake', 'mini', 'cup', '[SEP]']
```

这些结果表明，Token选择方法应考虑重复问题或建模所选Token的多样性，我们将此留作未来工作。

---

**ColBERT的相关性**：需要注意的是，虽然ColBERT在发布时是MS MARCO全排序任务的最先进模型，但它已被具有更好训练过程的稠密 [5, 7] 和稀疏 [4] 孪生模型超越，这些训练方法包括：i) 困难负样本挖掘 [4, 5, 7]，ii) 蒸馏 [4, 7]，以及iii) 预训练 [5]。在本研究进行时，这些技术尚未在ColBERT模型上测试，因此我们未应用它们，但与此同时，这些技术在一篇预印本 [15] 中已被证明是有益的。最后，ColBERT还展示了一些有趣的特性，例如更好的零样本性能 [16] 以及能够与传统IR技术（如伪相关反馈 [19]）结合使用。

## 6 结论

在本研究中，我们研究了ColBERT模型，并测试了不同方法以减少其延迟交互的复杂性和其索引大小的问题。我们首先验证了对于某些预训练语言模型（如MINILM），不需要对Token进行归一化和查询扩展，从而提高了延迟性能。此外，我们还证明了一些非常简单的方法可以从段落索引中移除30\%的Token，而不会导致性能损失。另一方面，MS MARCO文档集揭示了这种机制的挑战，即使剪枝90\%的Token也可能不足以显著减少索引大小。将这种Token剪枝方法与已经研究的嵌入压缩方法结合，可能会进一步改进基于ColBERT的信息检索系统。
