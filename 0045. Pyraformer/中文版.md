# PYRAFORMER: 用于长程时间序列建模和预测的低复杂度金字塔注意力模型

#### 摘要

基于时间序列数据对未来进行准确预测至关重要，因为它为提前决策和风险管理提供了可能。在实际应用中，挑战在于构建一个灵活但简洁的模型，能够捕捉广泛的时序依赖关系。本文中，我们提出了Pyraformer，通过探索时间序列的多分辨率表示来实现这一目标。具体而言，我们引入了金字塔注意力模块（PAM），其中跨尺度树结构总结了不同分辨率的特征，而尺度内的相邻连接则建模了不同范围的时序依赖关系。在温和条件下，Pyraformer中信号传输路径的最大长度与序列长度$L$无关（即$\mathcal{O}(1)$），而其时间和空间复杂度与$L$呈线性关系。大量实验结果表明，Pyraformer通常在单步和长程多步预测任务中实现了最高的预测精度，同时消耗的时间和内存最少，尤其是在序列较长时${ }^{11}$。

## 1 引言

时间序列预测是决策和风险管理等下游任务的基石。例如，对微服务在线流量的可靠预测可以为云系统中的潜在风险提供早期预警。此外，它还为动态资源分配提供了指导，以在不降低性能的情况下最小化成本。除了在线流量，时间序列预测还在其他领域（如疾病传播、能源管理、经济与金融）中得到了广泛应用。

---

时间序列预测的主要挑战在于构建一个强大但简洁的模型，能够紧凑地捕捉不同范围的时序依赖关系。时间序列通常同时表现出短期和长期的重复模式（Lai等，2018），而考虑这些模式是实现准确预测的关键。尤其值得注意的是，处理长程依赖关系是一项更为困难的任务，其特点是时间序列中任意两个位置之间的最长信号传输路径（定义见命题2）的长度（Vaswani等，2017）。路径越短，依赖关系捕捉得越好。此外，为了让模型能够学习这些长期模式，模型的历史输入也应当足够长。因此，低时间和空间复杂度成为优先考虑的目标。

---

遗憾的是，目前最先进的方法未能同时实现这两个目标。一方面，RNN（Salinas等，2020）和CNN（Munir等，2018）实现了与时间序列长度$L$成线性关系的低时间复杂度，但它们的信号传输路径的最大长度为$\mathcal{O}(L)$，这使得它们难以学习远距离位置之间的依赖关系。另一方面，Transformer将最大路径长度显著缩短至$\mathcal{O}(1)$，但代价是将时间复杂度增加到$\mathcal{O}\left(L^{2}\right)$，因此无法处理非常长的序列。为了在模型能力和复杂度之间找到平衡，提出了Transformer的多种变体，例如Longformer（Beltagy等，2020）、Reformer（Kitaev等，2019）和Informer（Zhou等，2021）。然而，很少有方法能够在显著降低时间和空间复杂度的同时，将最大路径长度减少到小于$\mathcal{O}(L)$。

---

在本文中，我们提出了一种基于金字塔注意力的新型Transformer模型（Pyraformer），以在捕捉长程依赖关系和实现低时间与空间复杂度之间架起桥梁。具体而言，我们通过在图1(d)所示的金字塔图中基于注意力传递消息，开发了金字塔注意力机制。该图中的边可以分为两组：跨尺度连接和尺度内连接。跨尺度连接构建了原始序列的多分辨率表示：最精细尺度上的节点对应于原始时间序列中的时间点（例如，每小时观测值），而较粗糙尺度上的节点则表示较低分辨率的特征（例如，每日、每周和每月模式）。这些潜在的较粗糙尺度节点最初是通过较粗糙尺度构建模块引入的。另一方面，尺度内边通过连接相邻节点来捕捉每个分辨率的时序依赖关系。因此，该模型通过在较粗糙的分辨率上捕捉远距离位置之间的长程时序依赖行为，提供了紧凑的表示，从而减少了信号传输路径的长度。此外，通过在不同尺度上使用稀疏的尺度内连接建模不同范围的时序依赖关系，显著降低了计算成本。简而言之，我们的主要贡献包括：

- 我们提出了Pyraformer，以一种紧凑的多分辨率方式同时捕捉不同范围的时序依赖关系。为了将Pyraformer与最先进的方法区分开来，我们在图1中从图的角度总结了所有模型。
- 从理论上，我们证明了通过适当选择参数，可以同时实现$\mathcal{O}(1)$的最大路径长度和$\mathcal{O}(L)$的时间与空间复杂度。为了突出所提出模型的吸引力，我们进一步在表1中比较了不同模型在最大路径和复杂度方面的表现。
- 在实验中，我们表明，在单步和长程多步预测场景下，所提出的Pyraformer在各种真实世界数据集上比原始Transformer及其变体产生了更准确的预测，但时间和内存成本更低。

## 2 相关工作

### 2.1 时间序列预测

时间序列预测方法大致可分为统计方法和基于神经网络的方法。第一类方法包括ARIMA（Box \& Jenkins, 1968）和Prophet（Taylor \& Letham, 2018）。然而，这两种方法都需要分别拟合每个时间序列，并且在长程预测中的表现较差。

---

近年来，深度学习的发展催生了大量基于神经网络的时间序列预测方法，包括CNN（Munir等，2018）、RNN（Salinas等，2020）和Transformer（Li等，2019）。如前一节所述，CNN和RNN具有较低的时间和空间复杂度（即$\mathcal{O}(L)$），但需要$\mathcal{O}(L)$的路径来描述长程依赖关系。我们建议读者参阅附录A以获取关于相关基于RNN模型的更详细综述。相比之下，Transformer（Vaswani等，2017）可以通过$\mathcal{O}(1)$步的路径有效捕捉长程依赖关系，但复杂度从$\mathcal{O}(L)$大幅增加到$\mathcal{O}\left(L^{2}\right)$。为了缓解这一计算负担，提出了LogTrans（Li等，2019）和Informer（Zhou等，2021）：前者限制序列中的每个点只能关注其前$2^{n}$步的点，其中$n=1,2, \cdots$，而后者利用注意力分数的稀疏性，显著降低了复杂度（即$\mathcal{O}(L \log L)$），但代价是引入了更长的最大路径长度。

### 2.2 稀疏Transformer

除了时间序列预测的文献外，自然语言处理（NLP）领域也提出了大量方法来提高Transformer的效率。与CNN类似，Longformer（Beltagy等，2020）在局部滑动窗口或扩张滑动窗口内计算注意力。尽管复杂度降低到$\mathcal{O}(A L)$，其中$A$是局部窗口大小，但有限的窗口大小使得全局信息交换变得困难。其结果是最大路径长度为$\mathcal{O}(L / A)$。作为替代方案，Reformer（Kitaev等，2019）利用局部敏感哈希（LSH）将序列划分为多个桶，然后在每个桶内执行注意力计算。它还采用可逆Transformer进一步减少内存消耗，从而可以处理极长的序列。然而，其最大路径长度与桶的数量成正比，更糟糕的是，为了降低复杂度，需要较大的桶数量。另一方面，ETC（Ainslie等，2020）引入了一组额外的全局标记以实现全局信息交换，导致$\mathcal{O}(G L)$的时间和空间复杂度以及$\mathcal{O}(1)$的最大路径长度，其中$G$是全局标记的数量。然而，$G$通常随$L$增加，因此复杂度仍然是超线性的。与ETC类似，所提出的Pyraformer也引入了全局标记，但以多尺度的方式成功地将复杂度降低到$\mathcal{O}(L)$，同时未像原始Transformer那样增加最大路径长度的阶数。

### 2.3 分层Transformer

最后，我们简要回顾了提升Transformer捕捉自然语言分层结构能力的方法，尽管这些方法从未用于时间序列预测。HIBERT（Miculicich等，2018）首先使用Sent Encoder提取句子的特征，然后将文档中句子的EOS标记形成新序列并输入到Doc Encoder中。然而，它是专门为自然语言设计的，无法推广到其他序列数据。多尺度Transformer（Subramanian等，2020）使用自上而下和自下而上的网络结构学习序列数据的多尺度表示。这种多尺度表示有助于减少原始Transformer的时间和内存成本，但它仍然面临二次复杂度的缺陷。另一种方法，BP-Transformer（Ye等，2019）递归地将整个输入序列划分为两部分，直到每个分区仅包含一个标记。分区的序列随后形成一棵二叉树。在注意力层中，每个上层节点可以关注其子节点，而底层节点可以关注同一尺度内的相邻$A$个节点以及所有较粗糙尺度的节点。需要注意的是，BP-Transformer用零初始化较粗糙尺度的节点，而Pyraformer则通过构建模块以更灵活的方式引入较粗糙尺度的节点。此外，BP-Transformer的图比Pyraformer更密集，因此其复杂度为$\mathcal{O}(L \log L)$。

## 3 方法

时间序列预测问题可以表述为：给定前$L$步的观测值$\boldsymbol{z}_{t-L+1: t}$和相关的协变量$\boldsymbol{x}_{t-L+1: t+M}$（例如，一天中的小时），预测未来$M$步$\boldsymbol{z}_{t+1: t+M}$。为了实现这一目标，我们在本文中提出了Pyraformer，其整体架构如图2所示。如图所示，我们首先分别嵌入观测数据、协变量和位置信息，然后将它们相加，这与Informer（Zhou等，2021）的做法类似。接下来，我们使用较粗糙尺度构建模块（CSCM）构建一个多分辨率的$C$叉树，其中较粗糙尺度的节点总结了相应较精细尺度上$C$个节点的信息。为了进一步捕捉不同范围的时序依赖关系，我们通过金字塔图中的注意力机制传递消息，引入了金字塔注意力模块（PAM）。最后，根据下游任务，我们采用不同的网络结构输出最终预测结果。在下文中，我们将详细阐述所提出模型的每个部分。为了方便说明，本文中所有符号总结在表4中。

### 3.1 金字塔注意力模块（PAM）

我们从PAM的介绍开始，因为它是Pyraformer的核心。如图1(d)所示，我们利用金字塔图以多分辨率的方式描述观测时间序列的时序依赖关系。这种多分辨率结构已被证明是计算机视觉（Sun等，2019；Wang等，2021）和统计信号处理（Choi等，2008；Yu等，2019）领域中长程交互建模的有效且高效的工具。我们可以将金字塔图分解为两部分：跨尺度连接和尺度内连接。跨尺度连接形成了一棵$C$叉树，其中每个父节点有$C$个子节点。例如，如果我们将金字塔图的最精细尺度与原始时间序列的每小时观测值相关联，那么较粗糙尺度上的节点可以被视为时间序列的每日、每周甚至每月的特征。因此，金字塔图提供了原始时间序列的多分辨率表示。此外，通过尺度内连接简单地连接相邻节点，更容易在较粗糙尺度上捕捉长程依赖关系（例如，月度依赖）。换句话说，较粗糙尺度以一种比单一最精细尺度模型更简洁的图形方式描述长程相关性。事实上，原始的单尺度Transformer（见图1(a)）采用了一个全连接图，将最精细尺度上的每两个节点连接起来以建模长程依赖关系，导致了一个计算负担重的模型，其时间和空间复杂度为$\mathcal{O}\left(L^{2}\right)$（Vaswani等，2017）。与之形成鲜明对比的是，如下所示，Pyraformer中的金字塔图将计算成本降低到$\mathcal{O}(L)$，而不会增加信号传输路径最大长度的阶数。

---

在深入探讨PAM之前，我们首先介绍原始的注意力机制。设$\boldsymbol{X}$和$\boldsymbol{Y}$分别表示单个注意力头的输入和输出。注意，可以引入多个头以从不同角度描述时序模式。$\boldsymbol{X}$首先被线性变换为三个不同的矩阵，即查询$\boldsymbol{Q}=\boldsymbol{X} \boldsymbol{W}_{Q}$、键$\boldsymbol{K}=\boldsymbol{X} \boldsymbol{W}_{K}$和值$\boldsymbol{V}=\boldsymbol{X} \boldsymbol{W}_{V}$，其中$\boldsymbol{W}_{Q}, \boldsymbol{W}_{K}, \boldsymbol{W}_{V} \in \mathbb{R}^{L \times D_{K}}$。对于$\boldsymbol{Q}$中的第$i$行$\boldsymbol{q}_{i}$，它可以关注$\boldsymbol{K}$中的任何行（即键）。换句话说，相应的输出$\boldsymbol{y}_{i}$可以表示为：

$$
\begin{equation*}
\boldsymbol{y}_{i}=\sum_{\ell=1}^{L} \frac{\exp \left(\boldsymbol{q}_{i} \boldsymbol{k}_{\ell}^{T} / \sqrt{D_{K}}\right) \boldsymbol{v}_{\ell}}{\sum_{\ell=1}^{L} \exp \left(\boldsymbol{q}_{i} \boldsymbol{k}_{\ell}^{T} / \sqrt{D_{K}}\right)} \tag{1}
\end{equation*}
$$

其中$\boldsymbol{k}_{\ell}^{T}$表示$\boldsymbol{K}$中第$\ell$行的转置。我们强调，需要计算和存储的查询-键点积（Q-K对）的数量决定了注意力机制的时间和空间复杂度。从另一个角度来看，这个数量与图中的边数成正比（见图1(a)）。由于在完整的注意力机制（1）中计算和存储了所有的Q-K对，因此时间和空间复杂度为$\mathcal{O}\left(L^{2}\right)$。

---

与上述完整的注意力机制不同，PAM中的每个节点仅关注一组有限的键，对应于图1(d)中的金字塔图。具体来说，假设$n_{\ell}^{(s)}$表示尺度$s$上的第$\ell$个节点，其中$s=1, \cdots, S$依次表示从底层尺度到顶层尺度。一般来说，图中的每个节点可以关注三个尺度上的一组相邻节点$\mathbb{N}_{\ell}^{(s)}$：同一尺度上的相邻$A$个节点（包括节点本身，记为$\mathbb{A}_{\ell}^{(s)}$）、$C$叉树中的$C$个子节点（记为$\mathbb{C}_{\ell}^{(s)}$）以及$C$叉树中的父节点（记为$\mathbb{P}_{\ell}^{(s)}$），即：

$$
\left\{\begin{array}{l}
\mathbb{N}_{\ell}^{(s)}=\mathbb{A}_{\ell}^{(s)} \cup \mathbb{C}_{\ell}^{(s)} \cup \mathbb{P}_l^{(s)} \\
\mathbb{A}_{\ell}^{(s)}=\left\{n_j^{(s)}:|j-\ell| \leq \frac{A-1}{2}, 1 \leq j \leq \frac{L}{C^{s-1}}\right\} \\
\mathbb{C}_{\ell}^{(s)}=\left\{n_j^{(s-1)}:(\ell-1) C<j \leq \ell C\right\} \quad \text { if } s \geq 2 \text { else } \emptyset \\
\mathbb{P}_{\ell}^{(s)}=\left\{n_j^{(s+1)}: j=\left\lceil\frac{\ell}{C}\right\rceil\right\} \quad \text { if } s \leq S-1 \text { else } \emptyset
\end{array}\right.
$$

因此，节点$n_{\ell}^{(s)}$的注意力可以简化为：

$$
\begin{equation*}
\boldsymbol{y}_{i}=\sum_{\ell \in \mathbb{N}_{\ell}^{(s)}} \frac{\exp \left(\boldsymbol{q}_{i} \boldsymbol{k}_{\ell}^{T} / \sqrt{d_{K}}\right) \boldsymbol{v}_{\ell}}{\sum_{\ell \in \mathbb{N}_{l}^{(s)}} \exp \left(\boldsymbol{q}_{i} \boldsymbol{k}_{\ell}^{T} / \sqrt{d_{K}}\right)}, \tag{3}
\end{equation*}
$$

我们进一步将注意力层的数量记为$N$。不失一般性，我们假设$L$可被$C^{S-1}$整除。然后我们可以得到以下引理（证明见附录B，符号含义见表4）。

---

**引理1**：给定满足方程（4）的$A, C, L, N$和$S$，在$N$个堆叠的注意力层之后，最粗糙尺度上的节点可以获得全局感受野。
$$
\begin{equation*}
\frac{L}{C^{S-1}}-1 \leq \frac{(A-1) N}{2} . \tag{4}
\end{equation*}
$$

此外，当尺度数量$S$固定时，以下两个命题总结了所提出的金字塔注意力机制的时间和空间复杂度以及最大路径长度的阶数。我们建议读者参阅附录C和D以获取证明。

**命题1**：对于给定的$A$和$L$，金字塔注意力机制的时间和空间复杂度为$\mathcal{O}(A L)$，当$A$是关于$L$的常数时，复杂度为$\mathcal{O}(L)$。

**命题2**：令图中两个节点之间的信号传输路径表示连接它们的最短路径。那么对于给定的$A, C, L$和$S$，金字塔图中任意两个节点之间的信号传输路径的最大长度为$\mathcal{O}\left(S+L / C^{S-1} / A\right)$。假设$A$和$S$固定，且$C$满足方程（5），则对于长度为$L$的时间序列，最大路径长度为$\mathcal{O}(1)$。
$$
\begin{equation*}
\sqrt[s-1]{L} \geq C \geq \sqrt[s-1]{\frac{L}{(A-1) N / 2+1}} \tag{5}
\end{equation*}
$$

---

在我们的实验中，我们固定了$S$和$N$，且$A$只能取3或5，与序列长度$L$无关。因此，所提出的PAM实现了$\mathcal{O}(L)$的复杂度，且最大路径长度为$\mathcal{O}(1)$。需要注意的是，在PAM中，一个节点最多可以关注$A+C+1$个节点。遗憾的是，现有的深度学习库（如Pytorch和TensorFlow）并不支持这种稀疏的注意力机制。一种朴素实现PAM的方法是首先计算所有Q-K对的乘积，即$\boldsymbol{q}_{i} \boldsymbol{k}_{\ell}^{T}$，其中$\ell=1, \cdots, L$，然后屏蔽掉$\ell \notin \mathbb{N}_{\ell}^{(s)}$。然而，这种实现的时间和空间复杂度仍然是$\mathcal{O}\left(L^{2}\right)$。相反，我们使用TVM（Chen等，2018）构建了一个专门为PAM定制的CUDA内核，实际减少了计算时间和内存成本，并使所提出的模型适用于长时间序列。更长的历史输入通常有助于提高预测精度，因为提供了更多的信息，尤其是在考虑长程依赖关系时。

### 3.2 较粗糙尺度构建模块（CSCM）

CSCM的目标是初始化金字塔图中较粗糙尺度上的节点，以便后续的PAM能够在这些节点之间交换信息。具体来说，通过对其子节点$\mathbb{C}_{\ell}^{(s)}$进行卷积操作，从下到上逐尺度引入较粗糙尺度节点。如图3所示，在时间维度上依次应用多个卷积核大小为$C$、步长为$C$的卷积层，得到尺度$s$上长度为$L / C^{s}$的序列。不同尺度上生成的序列形成了一棵$C$叉树。在将这些序列输入PAM之前，我们将这些从精细到粗糙的序列拼接在一起。为了减少参数量和计算量，我们在将序列输入堆叠的卷积层之前，通过一个全连接层降低每个节点的维度，并在所有卷积操作之后恢复其维度。这种瓶颈结构显著减少了模块中的参数数量，并可以防止过拟合。

### 3.3 预测模块

对于单步预测，我们在将历史序列$z_{t-L+1: t}$输入嵌入层之前，在其末尾添加一个结束标记（通过设置$z_{t+1}=0$）。在序列被PAM编码后，我们收集金字塔图中所有尺度上最后一个节点的特征，将它们拼接起来，然后输入到一个全连接层中进行预测。

---

对于多步预测，我们提出了两种预测模块。第一种与单步预测模块相同，但将所有尺度上的最后一个节点映射到所有$M$个未来时间步。第二种则采用了一个带有两个完整注意力层的解码器。具体来说，类似于原始Transformer（Vaswani等，2017），我们将未来$M$个时间步的观测值替换为0，并以与历史观测值相同的方式嵌入它们，并将观测值、协变量和位置嵌入的总和称为“预测标记”$\boldsymbol{F}_{p}$。第一个注意力层将预测标记$\boldsymbol{F}_{p}$作为查询，编码器的输出$\boldsymbol{F}_{e}$（即PAM中的所有节点）作为键和值，并生成$\boldsymbol{F}_{d 1}$。第二个层将$\boldsymbol{F}_{d 1}$作为查询，但将拼接的$\boldsymbol{F}_{d 1}$和$\boldsymbol{F}_{e}$作为键和值。历史信息$\boldsymbol{F}_{e}$直接输入到两个注意力层中，因为这种信息对于准确的长程预测至关重要。最终预测结果通过跨通道维度的全连接层获得。同样，我们一次性输出所有未来预测结果，以避免Transformer自回归解码器中的误差累积问题。

---

表2：三个数据集上的单步预测结果。“Q-K对”指的是网络中所有注意力层执行的查询-键点积的数量，它编码了时间和空间复杂度。我们将注意力层的数量记为$N$，注意力头的数量记为$H$，尺度的数量记为$S$，节点的维度记为$D$，键的维度记为$D_{K}$，前馈层的最大维度记为$D_{F}$，卷积步长记为$C$。

## 4 实验

### 4.1 数据集与实验设置

我们在四个真实世界的数据集上展示了所提出的Pyraformer的优势，包括Wind、App Flow、Electricity和ETT。前三个数据集用于单步预测，而后两个用于长程多步预测。我们建议读者参阅附录E和F以获取关于数据描述和实验设置的更多细节。

### 4.2 结果与分析

### 4.2.1 单步预测

我们在三个数据集上进行了单步预测实验：Electricity、Wind和App Flow。历史长度分别为169、192和192，包括结束标记。我们将Pyraformer与其他5种注意力机制进行了基准测试，包括原始的全注意力（Vaswani等，2017）、对数稀疏注意力（即LogTrans）（Li等，2019）、LSH注意力（即Reformer）（Kitaev等，2019）、带有全局节点的滑动窗口注意力（即ETC）（Ainslie等，2020）以及扩张滑动窗口注意力（即Longformer）（Beltagy等，2020）。特别是对于ETC，选择了一些在最精细尺度上等间隔的节点作为全局节点。全局节点可以关注序列中的所有节点，所有节点也可以依次关注它（见图1(e)）。所有模型的训练和测试方案相同。我们进一步研究了预训练策略（见附录G）、加权采样器和硬样本挖掘对所有方法的有用性，并展示了最佳结果。我们采用NRMSE（归一化均方根误差）和ND（归一化偏差）作为评估指标（定义见附录H）。结果总结在表2中。为了公平比较，除了全注意力外，所有注意力机制的总点积数量控制在相同数量级。

---

我们的实验结果表明，Pyraformer在NRMSE和ND方面优于Transformer及其变体，并且查询-键点积（即Q-K对）的数量最少。具体来说，从表2中可以总结出三个主要趋势：（1）所提出的Pyraformer产生了最准确的预测结果，表明金字塔图通过考虑不同范围的依赖关系，能够更好地解释时间序列中的时序交互。有趣的是，对于Wind数据集，稀疏注意力机制，即LogTrans、ETC、Longformer和Pyraformer，优于原始的全注意力Transformer，可能是因为数据中包含大量零值，适当的稀疏性提升有助于避免过拟合。（2）Pyraformer中的Q-K对数量最少。值得注意的是，它比LogTrans减少了65.4%，比全注意力减少了96.6%。值得强调的是，对于更长的时间序列，这种计算增益将继续增加。（3）Pyraformer的参数数量略多于其他模型，这是由于CSCM的存在。然而，该模块非常轻量，与其他模型相比，模型大小仅增加了5%。此外，在实践中，我们可以固定超参数$A, S$和$N$，并确保$C$满足$C>\sqrt[S-1]{L /((A-1) N / 2+1)}$。因此，CSCM引入的额外参数数量仅为$\mathcal{O}\left((S-1) C D_K^2\right) \approx \mathcal{O}\left({L^{\cfrac{1}{S-1}}}\right)$。

### 4.2.2 长程多步预测

我们在三个数据集上评估了Pyraformer在长程预测中的性能，即Electricity、ETTh1和ETTm1。特别是对于ETTh1和ETTm1，我们同时预测了未来的油温和6个电力负荷特征，这是一个多变量时间序列预测问题。我们在所有模型上测试了第3.3节中介绍的两种预测模块，并将更好的结果列在表3中。

---

显然，无论预测长度如何，Pyraformer在所有数据集上仍然以最少的Q-K对数量实现了最佳性能。更具体地说，与Informer（Zhou等，2021）相比，当预测长度为168、336和720时，Pyraformer在ETTh1上的MSE分别减少了24.8%、28.9%和26.2%。这再次巩固了我们的信念，即在描述时间依赖关系时，使用金字塔图更为有益。有趣的是，我们注意到对于Pyraformer，第一个预测模块给出的结果优于第二个预测模块。一个可能的解释是，基于全注意力层的第二个预测模块无法区分不同分辨率的特征，而基于单个全连接层的第一个模块可以以自动化的方式充分利用这些特征。为了更好地阐明Pyraformer在长程预测中的建模能力，我们建议读者参阅附录□，了解关于合成数据的详细示例。

---

图4：全注意力、概率稀疏注意力和TVM实现的金字塔注意力在时间和内存消耗上的比较：（a）计算时间；（b）内存占用。

### 4.2.3 速度与内存消耗

为了检查基于TVM实现的定制CUDA内核的效率，我们在图4中描绘了经验计算时间和内存成本随序列长度$L$的变化。这里我们仅将Pyraformer与Informer（Zhou等，2021）中的全注意力和概率稀疏注意力进行了比较。所有计算均在配备12 GB Titan Xp GPU、Ubuntu 16.04、CUDA 11.0和TVM 0.8.0的机器上进行。图4显示，基于TVM的Pyraformer的时间和内存成本大致是$L$的线性函数，符合预期。此外，TVM实现的时间和内存消耗可以比全注意力和概率稀疏注意力小几个数量级，尤其是对于相对较长的时间序列。事实上，对于12GB的Titan Xp GPU，当序列长度达到5800时，全注意力会遇到内存不足（OOM）问题，而Pyraformer的TVM实现仅占用1GB内存。当序列长度为20000时，甚至Informer也会遇到OOM问题，而Pyraformer的内存成本仅为1.91GB，每批计算时间仅为0.082秒。

### 4.3 消融实验

我们还进行了消融实验，以衡量$A$和$C$、CSCM架构、历史长度以及PAM对Pyraformer预测精度的影响。结果展示在表[7]至表10中。关于结果的详细讨论可以在附录J中找到。这里，我们仅提供主要发现的概述：（1）为了减少预测误差，最好随着$L$的增加而增加$C$，但将$A$固定为一个较小的常数；（2）带有瓶颈的卷积在预测精度和参数数量之间取得了平衡，因此我们将其用作CSCM；（3）更多的历史数据有助于提高预测精度；（4）PAM对于准确预测至关重要。

## 5 结论与展望

在本文中，我们提出了Pyraformer，这是一种基于金字塔注意力的新型模型，能够以低时间和空间复杂度有效描述短期和长期的时序依赖关系。具体来说，我们首先利用CSCM构建一棵$C$叉树，然后设计PAM以跨尺度和尺度内方式传递消息。通过调整$C$并在序列长度$L$增加时固定其他参数，Pyraformer可以实现理论上的$\mathcal{O}(L)$复杂度和$\mathcal{O}(1)$的最大信号传输路径长度。实验结果表明，所提出的模型在单步和长程多步预测任务中均优于最先进的模型，同时计算时间和内存成本更低。目前，我们仅关注在构建金字塔图时固定$A$和$S$，并随着$L$的增加而调整$C$的场景。另一方面，我们在附录I中展示了其他超参数配置可能会进一步提高Pyraformer的性能。在未来的工作中，我们希望能够探索如何从数据中自适应地学习超参数。此外，将Pyraformer扩展到其他领域，包括自然语言处理和计算机视觉，也很有意义。

---

Table 4: Meanings of notations.

$\small\begin{array}{|c|c|l|}\hline{}符号&大小&含义\\\hline{}L&常数&历史序列的长度\\\hline{}G&常数&ETC中全局标记的数量\\\hline{}M&常数&需要预测的未来序列的长度\\\hline{}B&常数&批量大小\\\hline{}D&常数&每个节点的维度\\\hline{}D_K&常数&键的维度\\\hline{}\boldsymbol{X}&B×L×D&单个注意力头的输入\\\hline{}\boldsymbol{Y}&B×L×D&单个注意力头的输出\\\hline{}Q&B×L×D_K&查询\\\hline{}K&B×L×D_K&键\\\hline{}V&B×L×D_K&值\\\hline{}\boldsymbol{W}_Q&D×D_K&查询的权重矩阵\\\hline{}\boldsymbol{W}_K&D×D_K&键的权重矩阵\\\hline{}\boldsymbol{W}_V&D×D_K&值的权重矩阵\\\hline{}S&常数&尺度的数量\\\hline{}A&常数&同一尺度下节点可以关注的相邻节点数量\\\hline{}C&常数&较粗尺度节点可以总结的较细尺度节点数量\\\hline{}N&常数&注意力层的数量\\\hline{}n_l^{(s)}&D&尺度s下的第\ell个节点\\\hline{}\mathbb{N}_{\ell}^{(s)}&\operatorname{len}\left(\mathbb{N}_{\ell}^{(s)}\right)×D&节点n_l^{(s)}的邻居节点集合\\\hline{}\mathbb{A}_{\ell}^{(s)}&\operatorname{len}\left(\mathbb{A}_{\ell}^{(s)}\right)×D&与n_l^{(s)}同一尺度的相邻A节点\\\hline{}\mathbb{C}_{\ell}^{(s)}&\operatorname{len}\left(\mathbb{C}_{\ell}^{(s)}\right)×D&n_l^{(s)}的子节点\\\hline{}\mathbb{P}_{\ell}^{(s)}&\operatorname{len}\left(\mathbb{P}_{\ell}^{(s)}\right)×D&n_l^{(s)}的父节点\\\hline{}\boldsymbol{F}_p&B×M×D&预测标记\\\hline{}\boldsymbol{F}_e&B×L_{\text{tot}}×D&编码器的输出L_{\text{tot}}表示编码器的输出长度\\\hline{}\boldsymbol{F}_{d1}&B×M×D&第一个基于注意力的解码器层的输出\\\hline{}H&常数&注意力头的数量\\\hline{}D_F&常数&前馈层的最大维度\\\hline\end{array}$ 

## A 相关基于RNN模型的简要回顾

在本节中，我们对相关基于RNN的模型进行了简要回顾。HRNN（Costa-jussà \& Fonollosa 2016）和HMRNN（Chung等，2019）成功捕捉了多尺度时间依赖关系。前者需要专家知识将序列划分为不同的分辨率，而后者则从数据中自动学习划分。需要注意的是，这两个模型中信号传输路径的理论最大长度仍然是$\mathcal{O}(L)$。另一类工作旨在通过向LSTM添加残差连接（Kim等，2017）或扩张连接（Chang等，2017）来缩短信号传输路径。然而，它们并未明确考虑多分辨率时间依赖关系。此外，上述所有RNN仅从过去到未来单向传播信息。一种允许双向信息交换的有吸引力的方法是Bi-LSTM（Schuster，1996）。尽管通过两个不同的LSTM实现了前向和后向传播，但仍然存在较长的信号传输路径。与上述基于RNN的模型不同，所提出的Pyraformer实现了双向信息交换，能够更好地描述时间依赖关系，同时提供观测序列的多分辨率表示。我们还注意到，由于RNN的单向性，很难基于RNN实现图1(d)中的金字塔图。

## B 引理1的证明

证明。设$S$表示金字塔图中的尺度数量，$C$表示在较粗糙尺度$s$中一个节点可以总结的较精细尺度$s-1$中的子节点数量，其中$s=2, \cdots, S$，$A$表示每个尺度内一个节点可以关注的相邻节点数量，$N$表示注意力层数，$L$表示输入时间序列的长度。我们将图中任意节点$n_{a}$的“感受野”定义为$n_{a}$可以接收消息的节点集合。我们进一步将图中两个任意节点之间的距离定义为它们之间最短路径的长度（即从一个节点到另一个节点的步数）。需要注意的是，在每个注意力层中，消息只能在图中传输一步。

---

在不失一般性的情况下，我们假设$L$可被$C^{S-1}$整除，那么最粗糙尺度$S$上的节点数量为$L / C^{S-1}$。由于每个节点都连接到同一尺度内最接近的$A$个节点，最粗糙尺度上最左和最右节点之间的距离为$2\left(L / C^{S-1}-1\right) /(A-1)$。因此，在堆叠$N \geq 2\left(L / C^{S-1}-1\right) /(A-1)$层金字塔注意力后，最粗糙尺度上的最左和最右节点彼此在感受野内。此外，由于CSCM的存在，最粗糙尺度上的节点可以被视为较精细尺度上节点的总结。因此，当方程（4）满足时，最粗糙尺度上的所有节点都具有全局感受野，证明完毕。

## C 命题1的证明

证明。假设$L^{(s)}$表示尺度$s$上的节点数量，即：

$$
\begin{equation*}
L^{(s)}=\frac{L}{C^{s-1}}, 1 \leq s \leq S \tag{6}
\end{equation*}
$$

对于金字塔图中的节点$n_{\ell}^{(s)}$，其作为查询的点积数量$P_{\ell}^{(s)}$可以分解为两部分：

$$
\begin{equation*}
P_{\ell}^{(s)}=P_{\ell}^{(s)}{ }_{\text {inter }}+P_{\ell}^{(s)}{ }_{\text {intra }}, \tag{7}
\end{equation*}
$$

其中$P_{\ell}^{(s)}{ }_{\text {intra }}$和$P_{\ell}^{(s)}{ }_{\text {inter }}$分别表示尺度内和尺度间部分。根据金字塔图的结构，我们可以得到以下不等式：

$$
\begin{align*}
& P_{\ell}^{(s)}{ }_{\text {intra }} \leq A  \tag{8}\\
& P_{\ell}^{(s)}{ }_{\text {inter }} \leq C+1 . \tag{9}
\end{align*}
$$

第一个不等式（8）成立，因为一个节点通常关注同一尺度内最接近的$A$个节点，但对于最左和最右节点，其可以关注的尺度内节点数量小于$A$。另一方面，第二个不等式（9）成立，因为一个节点在金字塔图中通常有$C$个子节点和1个父节点，但顶层和底层的节点只能关注少于$C+1$个相邻尺度上的节点。

---

综上所述，尺度$s$上需要计算的点积数量为：

$$
\begin{equation*}
P^{(s)}=\sum_{\ell=1}^{L^{(s)}}\left(P_{\ell}^{(s)}{ }_{\text {intra }}+P_{\ell}^{(s)}{ }_{\text {inter }}\right) \leq L^{(s)}(A+C+1) \tag{10}
\end{equation*}
$$

需要注意的是，对于最精细尺度（即$s=1$），$P^{(1)} \leq L(A+1)$，因为该尺度上的节点没有任何子节点。因此，整个金字塔注意力层需要计算的点积数量为：

$$
P=\sum_{s=1}^{S} P^{(s)}
$$

$$
\begin{align*}
& \leq L(A+1)+L^{(2)}(A+C+1)+\ldots+L^{(S)}(A+C+1) \\
& =L\left(\sum_{s=1}^{S} C^{-(s-1)} A+\sum_{s=2}^{S} C^{-(s-1)}+\sum_{s=1}^{S-1} C^{-(s-1)}+1\right) \\
& <L\left((A+2) \sum_{s=1}^{S} C^{-(s-1)}+1\right) . \tag{11}
\end{align*}
$$

为了确保最粗糙尺度上的节点具有全局感受野，我们选择$C$使得$C \propto \sqrt[s-1]{L}$。因此，所提出的金字塔注意力的复杂度为：

$$
\begin{align*}
\mathcal{O}(P) & \leq \mathcal{O}\left(L\left((A+2) \sum_{s=1}^{S} C^{-(s-1)}+1\right)\right) \\
& =\mathcal{O}\left(L(A+2) \sum_{s=1}^{S} C^{-(s-1)}\right) \\
& =\mathcal{O}\left(\frac{(A+2) L^{\frac{S}{s-1}}-1}{L^{\frac{1}{S-1}}-1}\right) \\
& =\mathcal{O}\left(\frac{A L^{\frac{S}{s-1}}-1}{L^{\frac{1}{S-1}}-1}\right) \tag{12}
\end{align*}
$$

当$L$趋近于无穷大时，上述表达式趋近于$\mathcal{O}(A L)$。由于$A$可以在$L$变化时固定，复杂度可以进一步降低为$\mathcal{O}(L)$。

## D 命题2的证明

证明。设$n_{\ell}^{(s)}$表示第$s$尺度的第$\ell$个节点。显然，$n_{1}^{(1)}$和$n_{L}^{(1)}$之间的距离是金字塔图中所有节点对中最大的。从$n_{1}^{(1)}$到$n_{L}^{(s)}$的最短路径为：

$$
\begin{equation*}
n_{1}^{(1)} \rightarrow n_{1}^{(2)} \rightarrow \cdots \rightarrow n_{1}^{(S)} \rightarrow \cdots \rightarrow n_{L^{(S)}}^{(S)} \rightarrow n_{L^{(S-1)}}^{(S-1)} \rightarrow \cdots \rightarrow n_{L}^{(1)} \tag{13}
\end{equation*}
$$

相应地，图中任意两个节点之间的最大路径长度为：

$$
\begin{equation*}
L_{\max }=2(S-1)+\frac{2\left(L^{(S)}-1\right)}{A-1} \tag{14}
\end{equation*}
$$

当$C$满足方程（5）时，即$L^{(S)}-1 \leq(A-1) N / 2$，我们可以得到：

$$
\begin{align*}
\mathcal{O}\left(L_{\max }\right) & =\mathcal{O}\left(2(S-1)+\frac{2\left(L^{(S)}-1\right)}{A-1}\right) \\
& =\mathcal{O}\left(2(S-1)+\frac{2\left(\frac{L}{C^{S-1}}-1\right)}{A-1}\right) \\
& =\mathcal{O}(2(S-1)+N) \\
& =\mathcal{O}(S+N) \tag{15}
\end{align*}
$$

由于$A, S$和$N$与$L$无关，最大路径长度$L_{\max }$的阶数可以进一步简化为$\mathcal{O}(1)$。

## E 数据集

我们在以下四个数据集上展示了所提出的Pyraformer的优势。前三个数据集用于单步预测，而后两个用于长程多步预测。

---

Wind ${ }^{2}$。该数据集包含1986年至2015年间28个国家每小时对发电厂最大输出功率的能源潜力估计值（以百分比表示）。与其余数据集相比，它更为稀疏，并且周期性出现大量零值。由于该数据集规模较大，训练集和测试集的比例约为$32:1$。

App Flow：该数据集由蚂蚁集团${ }^{3}$收集。它包含部署在16个逻辑数据中心的128个系统的每小时最大流量，总共有1083个不同的时间序列。每个序列的长度超过4个月。每个时间序列被分为两部分分别用于训练和测试，比例为$32:1$。

Electricity ${ }^{4}$（Yu等，2016）：该数据集包含370个用户每15分钟记录一次的电量消耗时间序列。遵循DeepAR（Salinas等，2020），我们将每4条记录聚合为每小时观测值。该数据集用于单步和长程预测。对于单步预测，我们使用2011-01-01至2014-09-01的数据进行训练；对于长程预测，我们使用2011-04-01至2014-04-01的数据进行训练。

ETT $^{5}$（Zhou等，2021）：该数据集包含从2个站点收集的2个电力变压器的2年数据，包括油温和6个电力负荷特征。提供了每小时（即ETTh1）和每15分钟（即ETTm1）的观测值。该数据集通常用于长程预测的模型评估。在这里，我们遵循Informer（Zhou等，2021）的方法，将数据分为12个月和4个月分别用于训练和测试。

## F 实验设置

在所有实验中，我们为Pyraformer设置了$S=4$和$N=4$。当历史长度$L$不能被$C$整除时，我们仅在上一层引入$\lfloor L / C\rfloor$个节点，其中$\lfloor\cdot\rfloor$表示向下取整操作。底层最后的$L-(\lfloor L / C\rfloor-1) C$个节点都连接到上一层的最后一个节点。对于单步预测，我们在所有实验中设置$C=4, A=3$和$H=4$。训练和测试都使用固定长度的历史序列来预测单个未来值的高斯分布的均值和方差。我们选择MSE损失和对数似然（Zuo等，2020）作为损失函数，两者之间的比例设置为100。对于优化，我们使用Adam优化器，学习率从$10^{-5}$开始，每轮减半。我们训练Pyraformer共10轮。为了提高网络的泛化能力，使用了基于每个窗口平均值的加权采样器和硬样本挖掘。另一方面，对于长程预测，我们在每个实验中测试了$A$和$C$的四种组合，并展示了最佳结果。具体来说，当预测长度小于600时，我们测试了$A=3,5$和$C=4,5$；当预测长度大于600时，我们测试了$A=3,5$和$C=5,6$。每个实验的超参数选择列在表5中。此外，损失函数仅为MSE损失。我们仍然使用Adam作为优化器，但学习率从$10^{-4}$开始，每轮减少十分之一。我们将训练轮数设置为5轮。

## G 预训练

对于单步预测，要预测的值通常接近历史数据的最后一个值。由于我们仅使用所有尺度的最后一个节点进行预测，网络往往只关注短期依赖关系。为了迫使网络捕捉长程依赖关系，我们在训练的前几轮中添加了额外的监督。具体来说，在第一轮中，我们将网络构建为自编码器，如图5所示。除了预测未来值外，PAM还被训练以恢复输入值。需要注意的是，我们在所有方法中测试了有无这种预训练策略，并展示了更好的结果在表2中。

## H 评估指标

设目标值为$z_{j, t}$，预测值为$\hat{z}_{j, t}$，其中$j$为样本索引，$t$为时间索引。则NRMSE和ND的计算公式如下：

$$
\begin{align*}
\mathrm{NRMSE} & =\cfrac{\sqrt{\cfrac{1}{N T} \displaystyle\sum_{j=1}^{N} \displaystyle\sum_{t=1}^{T}\left(z_{j, t}-\hat{z}_{j, t}\right)^{2}}}{\cfrac{1}{N T} \displaystyle\sum_{j=1}^{N} \displaystyle\sum_{t=1}^{T}\left|z_{j, t}\right|}  \tag{16}\\
\mathrm{ND} & =\cfrac{\displaystyle\sum_{j=1}^{N} \displaystyle\sum_{t=1}^{T}\left|z_{j, t}-\hat{z}_{j, t}\right|}{\displaystyle\sum_{j=1}^{N} \displaystyle\sum_{t=1}^{T}\left|z_{j, t}\right|} \tag{17}
\end{align*}
$$

## I 合成数据实验

为了进一步评估Pyraformer捕捉不同范围时间依赖关系的能力，我们合成了一个具有多范围依赖关系的每小时数据集，并对其进行了实验。

具体来说，合成数据集中的每个时间序列是三个不同周期（24、168和720）的正弦函数的线性组合，即：

$$
\begin{equation*}
f(t)=\beta_{0}+\beta_{1} \sin \left(\frac{2 \pi}{24} t\right)+\beta_{2} \sin \left(\frac{2 \pi}{168} t\right)+\beta_{3} \sin \left(\frac{2 \pi}{720} t\right) \tag{18}
\end{equation*}
$$

在上述方程中，每个时间序列的三个正弦函数系数$\beta_{1}, \beta_{2}$和$\beta_{3}$从$[5,10]$中均匀采样。$\beta_{0}$是一个高斯过程，其协方差函数为$\displaystyle\displaystyle\sum_{t_{1}, t_{2}}=\left|t_{1}-t_{2}\right|^{-1}$，且$\displaystyle\displaystyle\sum_{t_{1}}=\displaystyle\displaystyle\sum_{t_{2}}=1$，其中$t_{1}$和$t_{2}$表示任意两个时间戳。这种多项式衰减的协方差函数被认为具有长程依赖性，与指数衰减的协方差函数不同（Yu等，2019）。每个时间序列的起始时间$t_{0}$从[0,719]中均匀采样。我们首先生成了60个长度为14400的时间序列，然后将每个时间序列划分为宽度为1440、步长为24的滑动窗口。在我们的实验中，我们使用历史720个时间点来预测未来720个点。由于合成时间序列的确定性和随机性部分都具有长程相关性，模型应很好地捕捉这些依赖关系，以准确预测接下来的720个点。结果总结在表6中。在这里，我们考虑了Pyraformer的两种不同配置：1）金字塔图中所有尺度的$C=6$（记为Pyraformer $_{6,6,6}$）；2）从下到上依次为三层设置$C=12,7$和$4$（记为Pyraformer ${ }_{12,7,4}$）。

---

可以观察到，所有尺度使用相同$C$的Pyraformer ${ }_{6,6,6}$已经显著优于基准方法。特别是，与现有Transformer变体中MSE最小的Reformer相比，Pyraformer的MSE减少了$18.2 \%$。另一方面，通过利用已知周期的信息，Pyraformer ${ }_{12,7,4}$的表现甚至优于Pyraformer $_{6,6,6}$。需要注意的是，在Pyraformer $_{12,7,4}$中，尺度2、3和4上的节点分别表征了较粗糙的时间分辨率，对应半天、半周和半月。我们还测试了Pyraformer ${ }_{24,7,4}$，但在第二层设置$C=24$会降低性能，可能是因为核大小为24的卷积层难以训练。

---

我们进一步在图6中可视化了Pyraformer $_{12,7,4}$的预测结果。蓝色实线和红色虚线分别表示真实时间序列和预测时间序列。通过捕捉不同范围的时间依赖关系，Pyraformer的预测结果与真实值非常接近。

---

另一方面，为了检查Pyraformer是否能够提取具有不同时间分辨率的特征，我们在图7中描绘了金字塔图中每个尺度上随机选择的通道随时间变化的提取特征。显然，较粗糙尺度上的特征可以被视为较精细尺度上特征的低分辨率版本。

## J 消融实验

## J.1 $A$和$C$的影响

我们研究了$A$和$C$对Pyraformer在长程时间序列预测中性能的影响，结果如表7所示。这里，我们重点关注数据集ETTh1。历史长度为336，预测长度为720。从表7中可以得出结论，PAM中最粗糙尺度上节点的感受野在减少Pyraformer的预测误差中起着不可或缺的作用。例如，当$C=2$时，最粗糙尺度上有42个节点。如果没有尺度内连接，每个节点只能接收来自最精细尺度上16个节点的消息。随着每个尺度中相邻连接数$A$的增加，最粗糙尺度节点的感受野也随之扩展，因此预测误差相应减少。然而，只要最上层节点具有全局感受野，进一步增加$A$不会带来显著收益。对于$C=5$，即使$A$增加，性能也没有改善。这些观察结果表明，一旦PAM中最上层节点具有全局感受野，最好将$A$设置为较小的值。在实践中，我们只随着$L$的增加而增加$C$，但保持$A$较小。

## J.2 CSCM架构的影响

除了卷积之外，还存在其他构建$C$叉树的机制，例如最大池化和平均池化。我们研究了不同CSCM架构对数据集ETTh1长程预测性能的影响。历史和预测长度均为168，所有机制的$C=4$。结果列在表8中。从表8可以看出：（1）使用池化层代替卷积通常会降低性能。然而，基于最大池化的Pyraformer的性能仍然优于Informer，展示了PAM相对于Informer中概率稀疏注意力的优势。（2）带瓶颈的卷积的MSE仅比不带瓶颈的卷积大$1.51 \%$，但参数数量减少了近$90 \%$。因此，我们采用更紧凑的带瓶颈卷积模块作为CSCM。

## J.3 历史长度的影响

我们还检查了历史长度对预测精度的影响。数据集为ETTm1，因为其粒度为分钟，并且包含更多的长程依赖关系。我们将预测长度固定为1344，并在表9中将历史长度从84更改为1344。正如预期的那样，更长的历史通常会提高预测精度。另一方面，当引入更多历史不再提供新信息时，这种性能提升开始趋于平稳。如图8所示，长度为672的时间序列几乎包含了所有对预测至关重要的周期性信息，而长度为1344的时间序列则引入了更多噪声。

## J.4 PAM的影响

最后，我们研究了PAM的重要性。我们在数据集ETTm1上比较了Pyraformer有和没有PAM的性能。为了公平比较，两种方法的参数数量控制在相同数量级。更准确地说，我们增加了仅包含CSCM模型的“带瓶颈卷积”的瓶颈维度。结果如表10所示。显然，PAM对于产生准确的预测至关重要。

## K 超参数选择的讨论

我们建议首先根据可用的计算资源确定注意力层数$N$，因为该数量与模型大小直接相关。接下来，尺度数$S$可以根据时间序列的粒度来确定。例如，对于每小时观测值，我们通常假设它可能还具有每日、每周和每月的周期性。因此，我们可以将$S$设置为4。然后我们关注$A$和$C$的选择。根据消融实验，我们通常倾向于选择较小的$A$，例如3和5。最后，为了确保网络具有$L$的感受野，我们可以选择满足方程（5）的$C$。在实践中，我们可以使用验证集从满足（5）的候选值中选择$C$。同样值得检查的是，根据时间序列的粒度为不同尺度选择不同的$C$是否可以进一步提高性能，正如我们在附录I中所做的那样。
