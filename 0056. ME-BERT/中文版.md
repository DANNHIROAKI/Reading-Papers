### 稀疏、密集和注意力表示在文本检索中的应用


#### 摘要

双编码器通过将文档和查询编码为低维密集向量进行检索，并通过文档与查询的内积对每个文档进行评分。我们研究了这种架构相对于稀疏词袋模型和注意力神经网络的容量。通过理论和实证分析，我们建立了编码维度、黄金文档与低排名文档之间的边界以及文档长度之间的联系，表明固定长度编码在支持长文档精确检索方面存在局限性。基于这些见解，我们提出了一个简单的神经模型，该模型结合了双编码器的效率和更昂贵的注意力架构的表达能力，并探索了稀疏-密集混合模型以利用稀疏检索的精确性。这些模型在大规模检索中优于其他强大替代方案。


## 1 引言

检索相关文档是语言技术的核心任务，并且是信息抽取和问答等应用的组成部分（例如，Narasimhan等，2016；Kwok等，2001；Voorhees，2001）。虽然经典的信息检索专注于稀疏词袋表示的启发式权重（Spärck Jones，1972），但最近的工作采用了两个阶段的检索和排序管道，其中使用稀疏高维查询/文档表示检索大量文档，并使用学习的神经模型进一步重新排序（Mitra和Craswell，2018）。这种两阶段方法在IR基准测试中取得了最先进的结果（Nogueira和Cho，2019；Yang等，2019；Nogueira等，2019a），特别是自从有大量标注数据可用于训练深度神经模型以来（Dietz等，2018；Craswell等，2020）。然而，这种管道受到第一阶段检索模型中任何召回错误的严格上限的限制：例如，Yan等（2020）报告的BM25的recall@1000为69.4。

---

一个有希望的替代方案是使用学习的低维密集编码进行第一阶段检索（Huang等，2013；Reimers和Gurevych，2019；Gillick等，2019；Karpukhin等，2020）。双编码器模型通过文档编码与查询编码的内积对每个文档进行评分。与需要大量计算在每个候选文档上的完整注意力架构不同，双编码器可以轻松应用于非常大的文档集合，这得益于内积搜索的高效算法；与未训练的稀疏检索模型不同，它可以利用机器学习在相关术语之间进行泛化。

---

为了评估文档与信息搜索查询的相关性，模型必须同时完成以下两个任务：(i) 检查精确的术语重叠（例如，查询中关键实体的存在）和 (ii) 计算语义相似性，泛化相关概念。稀疏检索模型在第一个子任务中表现出色，而学习的双编码器在第二个子任务中表现更好。自然语言处理（NLP）的近期发展可能表明，学习的密集表示总体上应该总是优于稀疏特征，但这并不一定正确：如图1所示，BM25模型（Robertson等，2009）可以优于基于BERT的双编码器，尤其是在较长的文档和需要精确检测单词重叠的任务中。${ }^{1}$ 这引发了对双编码器局限性的疑问，以及在哪些情况下这些强大模型尚未达到最先进水平的问题。本文通过理论和实证工具探讨这些问题，并提出了一种新架构，该架构利用双编码器的优势，同时避免其弱点。

- 图1：从三百万候选段落中检索包含查询的段落的Recall@1。该图比较了基于微调BERT的双编码器（DE-BERT768）、基于平均池化的现成BERT编码器（BERT-init）和基于稀疏术语的检索（BM25），同时按段落长度分组。

---

我们首先对压缩双编码器（即维度低于词汇量的密集编码）进行理论探讨，并分析其保留稀疏词袋检索模型所做出的区分的能力，我们称之为其保真度。保真度对于检测精确术语重叠的子任务非常重要，并且是容量的可处理代理。利用降维理论，我们将保真度与黄金检索结果与其竞争者之间的归一化边界联系起来，并表明该边界又与集合中文档的长度相关。我们通过实证研究验证了这一理论，使用来自TRECCAR（一个最近的IR基准，Dietz等，2018）的查询和文档，分析了随机投影压缩对稀疏BM25检索的影响。

---

接下来，我们提出了一种多向量编码模型，该模型在计算上像双编码器架构一样可行，但实现了显著更好的质量。一个基于密集和稀疏表示的简单混合模型进一步提升了性能。

---

我们将双编码器、多向量编码器及其稀疏-密集混合模型的性能与经典稀疏检索模型和注意力神经网络进行比较，并在可用的情况下与已发布的最新结果进行比较。我们的评估包括开放检索基准（MS MARCO段落和文档）以及问答段落检索（Natural Questions）。我们证实了先前的研究结果，即完整的注意力架构在重新排序任务中表现出色，但对于大规模检索来说效率不够高。在更高效的替代方案中，混合多向量编码器在每次评估中均处于或接近顶部，在MS MARCO中超越了最先进的检索结果。我们的代码公开在https://github.com/google-research/language/tree/master/language/multivec。

## 2 双编码器保真度分析

查询或文档是从某个词汇表 $\mathcal{V}$ 中抽取的单词序列。在本节中，我们假设查询和文档的表示通常用于稀疏词袋模型：每个查询 $q$ 和文档 $d$ 是 $\mathbb{R}^{v}$ 中的向量，其中 $v$ 是词汇表大小。我们将内积 $\langle q, d\rangle$ 作为文档 $d$ 对查询 $q$ 的相关性评分。该框架涵盖了几种众所周知的排序模型，包括布尔内积、TF-IDF 和 BM25。

---

我们将稀疏检索模型与压缩双编码器进行比较，对于后者，我们用 $f(d)$ 和 $f(q)$ 表示将 $d$ 和 $q$ 压缩到 $\mathbb{R}^{k}$，其中 $k \ll v$，且 $k$ 不随文档长度变化。对于这些模型，相关性评分是内积 $\langle f(q), f(d)\rangle$。（在 $\S 3$ 中，我们考虑适用于Token序列而不是计数向量的编码器。）

---

一个基本问题是双编码器的容量如何随嵌入大小 $k$ 变化。在本节中，我们专注于相关的、更易处理的保真度概念：我们可以在多大程度上压缩输入，同时保持模仿词袋检索性能的能力？我们主要通过随机投影的编码模型探讨这个问题，但也在 § 2.2 中讨论更一般的降维。

### 2.1 随机投影

为了建立压缩双编码器检索保真度的基线，我们现在考虑基于随机投影的编码器（Vempala，2004）。编码器定义为 $f(x)=$ $A x$，其中 $A \in \mathbb{R}^{k \times v}$ 是一个随机矩阵。在 Rademacher 嵌入中，矩阵 $A$ 的每个元素 $a_{i, j}$ 以相等概率从两个可能的值中采样：$\left\{-\frac{1}{\sqrt{k}}, \frac{1}{\sqrt{k}}\right\}$。在高斯嵌入中，每个 $a_{i, j} \sim N\left(0, k^{-1 / 2}\right)$。当 $\left\langle q, d_{1}\right\rangle>$ $\left\langle q, d_{2}\right\rangle$ 但 $\left\langle A q, A d_{1}\right\rangle<\left\langle A q, A d_{2}\right\rangle$ 时，会发生成对排序错误。使用此类随机投影，可以根据嵌入大小限制任何此类成对错误的概率。

定义 2.1。对于查询 $q$ 和文档对 $\left(d_{1}, d_{2}\right)$，使得 $\left\langle q, d_{1}\right\rangle \geq\left\langle q, d_{2}\right\rangle$，归一化边界定义为 $\mu\left(q, d_{1}, d_{2}\right)=$ $\frac{\left\langle q, d_{1}-d_{2}\right\rangle}{\|q\| \times\left\|d_{1}-d_{2}\right\|}$。

---

**引理 1.** 定义一个高斯或 Rademacher 嵌入的矩阵 $A \in \mathbb{R}^{k \times d}$。定义向量 $q, d_{1}, d_{2}$，使得 $\mu\left(q, d_{1}, d_{2}\right)=\epsilon>0$。当 $\left\langle A q, A d_{2}\right\rangle \geq\left\langle A q, A d_{1}\right\rangle$ 时，会发生排序错误。如果 $\beta$ 是此类错误的概率，则：

$$
\begin{equation*}
\beta \leq 4 \exp \left(-\frac{k}{2}\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)\right) \tag{1}
\end{equation*}
$$

该证明基于随机投影的著名结果，详见 § A.1。通过求解 (1) 中的 $k$，我们可以推导出一个嵌入大小，该大小保证成对错误概率的期望上限：

$$
\begin{equation*}
k \geq 2\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)^{-1} \ln \frac{4}{\beta} \tag{2}
\end{equation*}
$$

为了方便，我们可以推导出一个更简单但更宽松的二次边界（证明见 § A.2）：

**推论 1.** 定义向量 $q, d_{1}, d_{2}$，使得 $\epsilon=\mu\left(q, d_{1}, d_{2}\right)>0$。如果 $A \in \mathbb{R}^{k \times v}$ 是一个随机高斯或 Rademacher 嵌入矩阵，且 $k>12 \epsilon^{-2} \ln \frac{4}{\beta}$，则 $\operatorname{Pr}\left(\left\langle A q, A d_{1}\right\rangle \leq\right.$ $\left.\left\langle A q, A d_{2}\right\rangle\right) \leq \beta$。

---

**关于边界的紧密度。** 设 $k^{*}\left(q, d_{1}, d_{2}\right)$ 表示根据引理 1 定义的高斯或 Rademacher 随机投影的最小维度，对于给定的文档对 $\left(d_{1}, d_{2}\right)$ 和查询 $q$，其归一化边界为 $\epsilon$，使得 $\operatorname{Pr}\left(\left\langle A q, A d_{1}\right\rangle<\left\langle A q, A d_{2}\right\rangle\right) \leq \beta$。我们的引理对 $k^{*}$ 给出了一个上限，即 $k^{*}\left(q, d_{1}, d_{2}\right) \leq 2\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)^{-1} \ln \frac{4}{\beta}$。任何 $k \geq k^{*}\left(q, d_{1}, d_{2}\right)$ 都具有足够低的错误概率，但更小的 $k$ 值也可能具有所需的性质。在本节后面，我们通过实证评估来研究边界的紧密度；尽管分布 Johnson-Lindenstrauss 引理的最优性结果（Johnson 和 Lindenstrauss，1984；Jayram 和 Woodruff，2013；Kane 等，2011）表明理论紧密度（在常数因子范围内）是可能的，但这里我们仅通过实证研究该问题。

### 2.1.1 召回率-at-$r$

在检索应用中，将所需结果返回在前 $r$ 个搜索结果中非常重要。对于查询 $q$，定义 $d_{1}$ 为最大化某个内积排序指标的文档。在随机投影后，将 $d_{1}$ 返回在前 $r$ 个结果中的概率可以通过嵌入大小和归一化边界的函数来界定：

**引理 2.** 考虑一个查询 $q$，目标文档 $d_{1}$，以及排除 $d_{1}$ 的文档集合 $\mathcal{D}$，且满足 $\forall d_{2} \in \mathcal{D}, \mu\left(q, d_{1}, d_{2}\right)>0$。定义 $r_{0}$ 为任意整数，满足 $1 \leq r_{0} \leq|\mathcal{D}|$。定义 $\epsilon$ 为任何 $d_{2} \in \mathcal{D}$ 的第 $r_{0}$ 个最小归一化边界 $\mu\left(q, d_{1}, d_{2}\right)$，为简化起见，假设只有一个文档 $d_{2} \in \mathcal{D}$ 满足 $\mu\left(q, d_{1}, d_{2}\right)=\epsilon^{2}$。

---

定义一个高斯或 Rademacher 嵌入的矩阵 $A \in \mathbb{R}^{k \times d}$。定义 $R$ 为一个随机变量，使得 $R=\mid\left\{d_{2} \in \mathcal{D}\right.$ : $\left.\left\langle A q, A d_{1}\right\rangle \leq\left\langle A q, A d_{2}\right\rangle\right\} \mid$，并设 $C=4(|\mathcal{D}|-$ $\left.r_{0}+1\right)$。则

$$
\operatorname{Pr}\left(R \geq r_{0}\right) \leq C \exp \left(-\frac{k}{2}\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)\right)
$$

证明见 § A.3。该引理的一个直接推论是，为了以概率 $\geq 1-\beta$ 实现召回率-at-$r_{0}=1$，对于给定的 $\left(q, d_{1}, \mathcal{D}\right)$ 三元组，只需设置

$$
\begin{equation*}
k \geq \frac{2}{\epsilon^{2} / 2-\epsilon^{3} / 3} \ln \frac{4\left(|\mathcal{D}|-r_{0}+1\right)}{\beta} \tag{3}
\end{equation*}
$$

其中 $\epsilon$ 是第 $r_{0}$ 个最小归一化边界。

---

与引理 1 中成对相关性错误的边界类似，引理 2 暗示了最小随机投影维度 $k^{*}\left(q, d_{1}, \mathcal{D}\right)$ 的上界，该维度以概率 $\geq 1-\beta$ 将 $d_{1}$ 召回在前 $r_{0}$ 个结果中。由于联合界的应用以及对 $\mathcal{D}_{\epsilon}$ 中文档归一化边界的最坏情况假设，该边界可能较为宽松。在本节后面，我们将通过实证研究最大文档长度、归一化边界的分布与 $k^{*}$ 之间的关系。

### 2.1.2 布尔内积的应用

布尔内积是一种检索函数，其中 $d, q \in\{0,1\}^{v}$ 作用于大小为 $v$ 的词汇表，$d_{i}$ 表示术语 $i$ 在文档中的存在（类似地，$q_{i}$ 表示术语 $i$ 在查询中的存在）。相关性评分 $\langle q, d\rangle$ 则是同时出现在 $q$ 和 $d$ 中的术语数量。对于这种简单的检索函数，可以计算出一个嵌入大小，该大小保证在整个文档数据集上的成对错误概率满足预期。

---

**推论 2.** 对于文档集 $\mathcal{D}=\{d \in$ $\left.\{0,1\}^{v}\right\}$ 和查询 $q \in\{0,1\}^{v}$，设 $L_{D}=$ $\max _{d \in \mathcal{D}}\|d\|^{2}$ 和 $L_{Q}=\|q\|^{2}$。设 $A \in \mathbb{R}^{k \times v}$ 是一个随机 Rademacher 或高斯嵌入矩阵，且 $k \geq 24 L_{Q} L_{D} \ln \frac{4}{\beta}$。则对于任何 $d_{1}, d_{2} \in \mathcal{D}$，如果 $\left\langle q, d_{1}\right\rangle>\left\langle q, d_{2}\right\rangle$，则 $\left\langle A q, A d_{1}\right\rangle \leq\left\langle A q, A d_{2}\right\rangle$ 的概率 $\leq \beta$。

证明见 § A.4。该推论表明，对于布尔内积排序，我们可以通过选择一个与最长文档中唯一术语数量 $L_{D}$ 线性增长的嵌入大小 $k$，来保证任何期望的错误边界 $\beta$。

### 2.1.3 TF-IDF 和 BM25 的应用

TF-IDF（Spärck Jones，1972）和 BM25（Robertson 等，2009）都可以表示为文档和查询的词袋表示之间的内积，如本节前面所述。设置查询表示 $\tilde{q}_{i}=q_{i} \times \mathrm{IDF}_{i}$，其中 $q_{i}$ 表示术语在查询中的存在，$\mathrm{IDF}_{i}$ 表示术语 $i$ 的逆文档频率。TF-IDF 评分则为 $\langle\tilde{q}, d\rangle$。对于 BM25，我们定义 $\tilde{d} \in \mathbb{R}^{v}$，其中每个 $\tilde{d}_{i}$ 是计数 $d_{i}$ 和文档长度（以及超参数）的函数；$\operatorname{BM25}(q, d)$ 则为 $\langle\tilde{q}, \tilde{d}\rangle$。由于其在实际检索中的实用性，我们现在重点关注 BM25。

---

**成对准确性。** 我们使用实证数据来测试引理 1 对 BM25 相关性模型的适用性。我们从 TREC-CAR 数据集（Dietz 等，2018）中选择查询-文档三元组 $\left(q, d_{1}, d_{2}\right)$，通过考虑所有可能的 $\left(q, d_{2}\right)$，并选择 $d_{1}=\arg \max _{d} \operatorname{BM} 25(q, d)$。我们按归一化边界 $\epsilon$ 对三元组进行分箱，并计算量 $\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)^{-1}$。根据引理 1，对于归一化边界为 $\epsilon$ 的三元组，随机投影的最小嵌入大小 $k^{*}$ 的误差概率 $\leq \beta$，其上界是该量的线性函数。特别是对于 $\beta=.05$，引理表明 $k^{*} \leq 8.76\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)^{-1}$。在本实验中，我们测量 $k^{*}$ 的实证值以评估边界的紧密度。

---

结果如图 2 的 $x$ 轴所示。对于每个分箱，我们使用 32 到 9472 之间的 40 个可能值作为 $k$ 的网格（显示在 $y$ 轴上），计算在排序 $d_{1}$ 和 $d_{2}$ 时达到 $95\%$ 成对准确率所需的最小嵌入大小。（我们排除了 $\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)^{-1}$ 值高于所示范围的示例，因为它们在探索的 $k$ 范围内未达到 $95\%$ 的准确率。）该图表明，理论边界在常数因子范围内是紧的，并且实现所需保真度的最小嵌入大小与 $\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)^{-1}$ 线性增长。

- 图 2：对于 TREC-CAR 数据集，使用 Rademacher 嵌入近似 BM25 成对排序所需的最小 $k$，误差率 $\beta<.05$。

---

**边界与文档长度。** 对于布尔内积，可以用 $L_{Q}$ 和 $L_{D}$ 分别表示所有查询和文档中唯一术语的最大数量，从而表达最小可能的归一化边界（以及足够的嵌入大小）。然而，对于 TF-IDF 或 BM25，很难通过分析推导出最小归一化边界 $\epsilon$：因为每个术语可能具有唯一的逆文档频率，最小非零边界 $\left\langle q, d_{1}-d_{2}\right\rangle$ 会随着查询中术语数量的增加而减小，因为每个额外的术语都会增加两个文档获得几乎相同分数的可能性。因此，我们通过实证研究归一化边界如何随最大文档长度变化。使用 TREC-CAR 检索数据集，我们按文档长度分箱。对于每个查询，我们计算分箱中 BM25 得分最高的文档与分箱中所有其他文档之间的归一化边界，并查看第 10、100 和 1000 个最小的归一化边界。这些归一化边界的分布如图 3a 所示，显示归一化边界随文档长度减小。在实践中，观察到 BM25 的文档和查询集合的最小归一化边界比布尔内积低得多。例如，对于图 2 中使用的集合，BM25 的最小归一化边界为 $6.8 \mathrm{e}-06$，而布尔内积的最小归一化边界为 0.0169。

- 图 3：在 TREC-CAR 数据集中对 BM25 检索进行随机投影，文档按长度分箱。

---

**文档长度与编码维度。** 图 3b 显示了使用与图 3a 相同的文档分箱，达到期望的 recall-at-10 所需的最小随机投影维度的增长情况。正如预测的那样，所需维度随文档长度增加，而归一化边界则缩小。

### 2.2 通用编码函数的边界

我们在上文推导了随机线性投影所需最小编码的上界，并发现对于 $\left(q, d_{1}, d_{2}\right)$ 三元组的边界在常数因子范围内是实证紧的。更一般的非线性和学习编码器可能更高效。然而，有一些普遍的理论结果表明，对于任何编码器，如果编码不随 $\Omega\left(\epsilon^{-2}\right)$ 增长，则无法保证内积失真 $|\langle f(x), f(y)\rangle-\langle x, y\rangle| \leq \epsilon$（Larsen 和 Nelson，2017；Alon 和 Klartag，2017），其中向量 $x, y$ 的范数 $\leq 1$。这些结果表明，当文档长度增加时，固定长度的双编码器存在更普遍的容量限制。

---

在我们的设置中，BM25、TF-IDF 和布尔内积都可以等价地重新表述为内积，通过 $L_{2}$-归一化每个查询向量，并用 $\sqrt{L_{D}}=$ $\max _{d}\|d\|$ 重新缩放所有文档向量，其中 $\sqrt{L_{D}}$ 是一个随最长文档长度增长的常数因子。现在假设我们希望将未归一化内积的失真限制为某个值 $\leq \tilde{\epsilon}$，这可能会保证所需的性能特征。这对应于将最大归一化内积失真 $\epsilon$ 减小 $\sqrt{L_{D}}$ 倍。根据上一段提到的降维的一般边界，这可能需要将编码大小增加 $L_{D}$ 倍。

---

然而，这一理论论证存在一些注意事项。首先，该理论仅表明存在一些向量集无法被编码为增长速度低于 $\Omega\left(\epsilon^{-2}\right)$ 的表示；如果实际文档和查询是从某些简单的底层随机过程生成的，它们可能更容易编码。其次，我们的构造通过用常数因子重新缩放所有文档向量来实现 $\|d\| \leq 1$，但可能存在其他方法来约束范数，同时更高效地使用嵌入空间。第三，在非线性情况下，可能可以在不实现低内积失真的情况下消除排序错误。最后，从实际角度来看，在评估感兴趣的实际任务时，学习到的双编码器提供的泛化能力可能会超过任何保真度的牺牲。由于缺乏理论工具来解决这些问题，我们在本文的后续部分中提出了一系列实证研究。但首先，我们探索对双编码器的一种轻量级修改，它在有限的额外计算成本下提供了表达能力的提升。

## 3 多向量编码

理论分析表明，如果稀疏高维表示的保真度很重要，那么固定长度的文档向量表示通常需要针对长文档进行较大扩展。交叉注意力架构可以实现更高的保真度，但在大规模检索中不实用（Nogueira 等，2019b；Reimers 和 Gurevych，2019；Humeau 等，2020）。因此，我们提出了一种新架构，将每个文档表示为固定大小的 $m$ 个向量集合。相关性评分通过该集合中的最大内积计算。

---

形式上，设 $\mathrm{x}=\left(x_{1}, \ldots, x_{T}\right)$ 表示一个Token序列，其中 $x_{1}$ 为特殊Token [CLS]，并类似地定义 y。然后 $\left[h_{1}(\mathrm{x}), \ldots, h_{T}(\mathrm{x})\right]$ 表示深度 Transformer 顶层的上下文嵌入序列。我们将查询 x 的单向量表示定义为 $f^{(1)}(\mathrm{x})=h_{1}(\mathrm{x})$，将文档 y 的多向量表示定义为 $f^{(m)}(\mathrm{y})=\left[h_{1}(\mathrm{y}), \ldots, h_{m}(\mathrm{y})\right]$，即 y 中Token序列的前 $m$ 个表示向量，其中 $m<T$。相关性评分定义为 $\max _{j=1 \ldots m}\left\langle f^{(1)}(\mathrm{x}), f_{j}^{(m)}(\mathrm{y})\right\rangle$。

---

尽管此评分函数不是双编码器，但通过向搜索索引数据结构中为每个文档添加多个（$m$）条目，可以使用标准的近似最近邻搜索高效地找到得分最高的文档。如果某个向量 $f_{j}^{(m)}(\mathrm{y})$ 与查询向量 $f^{(1)}(\mathrm{x})$ 的内积最大，则很容易证明相应的文档必须是最大化相关性评分 $\psi^{(m)}(\mathrm{x}, \mathrm{y})$ 的文档。索引的大小必须增加 $m$ 倍，但由于现代近似最近邻和最大内积搜索的效率，时间复杂度可以是指数大小的亚线性（Andoni 等，2019；Guo 等，2016b）。因此，使用 $m$ 个大小为 $k$ 的向量表示文档的模型在运行时比使用单个大小为 $m k$ 的向量的双编码器更高效。

---

这种效率是与 POLY-ENCODER（Humeau 等，2020）的关键区别，后者为每个查询计算固定数量的向量，并通过 softmax 注意力与文档向量进行聚合。Yang 等（2018b）提出了一种类似的架构用于语言建模。由于这些方法中使用了 softmax，无法将相关性评分分解为内积的最大值，因此无法应用快速最近邻搜索。此外，这些工作并未涉及从大规模文档集合中进行检索。

---

**分析。** 为了理解为什么多向量编码可以使每个向量的编码更小，考虑一个理想化的情况：每个文档向量是 $m$ 个正交片段的和，即 $d=\sum_{i=1}^{m} d^{(i)}$，并且每个查询恰好指向黄金文档中的一个片段。${ }^{3}$ 正交分割可以通过将词汇表划分为片段来实现。

---

**定理 1.** 定义向量 $q, d_{1}, d_{2} \in \mathbb{R}^{v}$，使得 $\left\langle q, d_{1}\right\rangle>\left\langle q, d_{2}\right\rangle$，并假设 $d_{1}$ 和 $d_{2}$ 都可以分解为 $m$ 个片段，即 $d_{1}=\sum_{i=1}^{m} d_{1}^{(i)}$，$d_{2}$ 同理；两个文档的所有片段都是正交的。如果存在一个 $i$，使得 $\left\langle q, d_{1}\right\rangle=\left\langle q, d_{1}^{(i)}\right\rangle$ 且 $\left\langle q, d_{2}\right\rangle \geq\left\langle q, d_{2}^{(i)}\right\rangle$，则 $\mu\left(q, d_{1}^{(i)}, d_{2}^{(i)}\right) \geq$ $\mu\left(q, d_{1}, d_{2}\right)$。（证明见 $\S$ A.5。）

---

**备注。** BM25 评分可以从文档和查询的非负表示中计算；如果分割对应于词汇表的划分，则片段也将是非负的，因此条件 $\left\langle q, d_{2}\right\rangle \geq$ $\left\langle q, d_{2}^{(i)}\right\rangle$ 对所有 $i$ 成立。

---

相关的情况是当相同的片段对两个文档都是最大的，即 $\left\langle q, d_{2}^{(i)}\right\rangle=$ $\max _{j}\left\langle q, d_{2}^{(j)}\right\rangle$，这适用于与分割对齐良好的“简单”查询。此时，多向量模型中的归一化边界至少与等效的单向量表示中的边界一样大。与编码大小的关系可以从上一节的理论中得出：定理 1 表明，如果我们设 $f_{i}^{(m)}(\mathrm{y})=A d^{(i)}$（对于适当的 $A$），则归一化边界的增加使得可以使用更小的编码维度 $k$，同时仍支持相同的成对错误率。现在需要评估的“文档”数量增加了 $m$ 倍，但引理 2 表明，对于所需的 recall@ $r$，这只会对编码大小产生对数级的增加。尽管我们希望这一论证具有启发性，但正交片段和查询与片段完美匹配的假设非常强。因此，我们必须依靠实证分析来验证多向量编码在实际应用中的有效性。

---

**交叉注意力。** 交叉注意力架构可以被视为多向量模型的泛化：(1) 设 $m=T_{\text {max }}$（每个Token一个向量）；(2) 为查询中的每个Token计算一个向量；(3) 允许对向量进行比上述简单最大值更丰富的聚合。任何稀疏评分函数（例如 BM25）都可以通过交叉注意力模型来模拟，该模型只需计算单个单词之间的同一性；这可以通过随机投影词嵌入实现，其维度与词汇表大小的 $\log$ 成正比。根据定义，所需的表示也随段落和查询中的Token数量线性增长。与 POLY-ENCODER 一样，交叉注意力模型中的检索无法通过快速最近邻搜索高效地大规模执行。在同期工作中，Khattab 和 Zaharia（2020）提出了一种方法，每个查询使用 $T_{Y}$ 个向量，每个文档使用 $T_{X}$ 个向量，并通过简单的最大和来聚合内积。他们通过重新排序 $T_{Y}$ 个最近邻搜索的结果应用这种方法进行检索。我们的多向量模型则使用固定长度表示，并且每个查询仅执行一次最近邻搜索。

## 4 实验设置

完整的 IR 任务需要同时检测精确的单词重叠和语义泛化。我们的理论结果侧重于第一个方面，并推导了随着文档长度增长，两种类型的线性随机投影在稀疏词袋模型下实现高保真度的充分维度的理论和实证边界。理论设置与真实信息检索场景的建模至少有两个不同之处。

---

首先，经过训练的非线性双编码器可能能够以更低维的编码检测精确的单词重叠，特别是对于具有自然分布的查询和文档，这些分布可能表现出低维子空间结构。其次，对于实际应用，IR 任务的语义泛化方面可能比第一个方面更重要，而我们的理论并未对编码器维度与计算一般语义相似性的能力之间的关系做出预测。

---

我们通过实验研究将理论分析与实际文本检索联系起来，涉及三项任务。第一项任务在 $\S 5$ 中描述，测试模型检索完全包含查询的自然语言文档的能力，并在检测精确单词重叠的任务中评估 BM25 和深度神经双编码器，该任务基于自然分布的文本定义。第二项任务在 $\S 6$ 中描述，是开放域问答版本 Natural Questions（Kwiatkowski 等，2019；Lee 等，2019）的段落检索子问题；该基准反映了捕获相似性分级概念的需求，并具有自然的查询文本分布。对于这两项任务，我们通过改变集合中文档的最大长度进行对照实验，从而评估编码器维度与文档长度之间的关系。

---

为了评估我们最佳模型与大规模检索和排序领域最新工作的性能，在 $\S 7$ 中，我们报告了第三组任务的结果，重点关注段落/文档排序：MS MARCO 段落和文档级检索数据集（Nguyen 等，2016；Craswell 等，2020）。在这里，我们遵循标准的两阶段检索和排序系统：首先从大规模文档集合中进行第一阶段检索，然后使用交叉注意力模型进行重新排序。我们重点关注第一阶段检索模型的影响。

### 4.1 模型

我们的实验比较了压缩和稀疏双编码器、交叉注意力以及混合模型。

**BM25。** 我们使用不区分大小写的词片段分词和 gensim 库中的默认 BM25 参数。我们应用单字（BM25-uni）或单字+双字组合表示（BM25-bi）。

---

**基于 BERT 的双编码器（DE-BERT）。** 我们使用 BERT-base 编码查询和文档，BERT-base 是一个预训练的 Transformer 网络（12 层，768 维）（Devlin 等，2019）。我们将基于 BERT 的双编码器实现为 $\S 3$ 中形式化的多向量模型的特例，文档的向量数量 $m=1$：查询和文档的表示是 [CLS] Token的顶层表示。这种方法广泛用于检索（Lee 等，2019；Reimers 和 Gurevych，2019；Humeau 等，2020；Xiong 等，2020）。${ }^{4}$ 对于低维编码，我们学习从 $d=768$ 到 $k \in 32,64,128,512,{ }^{5}$ 的下投影，实现为单个前馈层，后接层归一化。所有参数都针对检索任务进行了微调。我们将这些模型称为 DE-BERT- $k$。

---

**交叉注意力 BERT。** 我们考虑的最具表达力的模型是交叉注意力 BERT，我们通过将 BERT 编码器应用于查询和文档的连接来实现，$x$ 和 $y$ 之间用特殊的 [SEP] 分隔符分隔。相关性评分是 [CLS] Token编码的学习线性函数。由于计算成本，交叉注意力 BERT 仅用于重新排序，如之前的工作（Nogueira 和 Cho，2019；Yang 等，2019）。这些模型称为 Cross-Attention。

---

**基于 BERT 的多向量编码（ME-BERT）。** 在 § 3 中，我们介绍了一种模型，其中每个文档由恰好 $m$ 个向量表示。我们使用 $m=8$ 作为 $\S 5$ 和 $\S 6$ 中成本与准确性的良好折衷，并在 $\S 7$ 的数据集中发现 $m$ 值为 3 到 4 时更准确。除了直接使用 BERT 输出表示外，我们还考虑使用前馈层实现的下投影表示，维度为 $768 \times k$。具有 $k$ 维嵌入的模型称为 ME-BERT- $k$。

---

**稀疏-密集混合模型（Hybrid）。** 平衡稀疏表示的保真度和学习到的密集表示的泛化能力的一种自然方法是构建混合模型。为此，我们使用单个可训练权重 $\lambda$ 线性结合稀疏和密集系统的评分，该权重在开发集上进行调优。例如，ME-BERT 和 BM25-uni 的混合模型称为 HYBRID-ME-BERT-uni。我们通过重新排序每个系统的 $n$-best 最高评分候选者，实现近似搜索以使用两个系统的线性组合进行检索。先前和同时期的工作也使用了稀疏-密集混合模型（Guo 等，2016a；Seo 等，2019；Karpukhin 等，2020；Ma 等，2020；Gao 等，2020）。我们的贡献是评估稀疏-密集混合模型在文档长度增长时的影响。

### 4.2 学习与推理

对于 $\S 5$ 和 $\S 6$ 中的实验，所有训练模型均从 BERT-base 初始化，所有参数均使用交叉熵损失进行微调，从预计算的 200 个文档列表中采样 7 个负样本，并添加批内负样本（每批共有 1024 个候选者）；预计算的候选者包括来自 BM25 的 100 个最高邻居和 100 个随机样本。这与 Lee 等（2019）的方法类似，但增加了固定候选者，也用于同期工作（Karpukhin 等，2020）。对于以这种方式训练的模型，对于可扩展方法，我们还应用了 Gillick 等（2019）中的硬负样本挖掘，并在有益时使用了一次迭代。同期工作提出了更复杂的负样本选择方法（Xiong 等，2020）。对于使用可扩展模型从大规模文档集合中进行检索，我们使用了 ScaNN：一个高效的近似最近邻搜索库（Guo 等，2020）；在大多数实验中，我们使用精确搜索设置，但在 § 7 中也评估了近似搜索。在 § 7 中，使用了相同的一般方法，但超参数略有不同（详见该部分），以便更直接地与先前工作进行比较。

## 5 包含段落的 ICT 任务

我们首先在检索包含单词序列 $x$ 的维基百科段落 $y$ 的任务上进行实验。我们使用维基百科创建了一个数据集，遵循 Lee 等（2019）的逆完形填空任务定义，但根据我们的研究目标进行了调整。该任务首先将维基百科文本分割为长度最多为 $l$ 的片段。这些片段构成了文档集合 $\mathcal{D}$。查询 $x_{i}$ 通过从文档 $y_{i}$ 中采样子序列生成。我们使用长度在 5 到 25 之间的查询，并且不会从相应的文档 $y_{i}$ 中移除查询 $x_{i}$。

---

我们创建了一个包含一百万个查询的数据集，并针对四个文档集合 $\mathcal{D}_{l}$（$l \in 50,100,200,400$）进行评估。每个 $\mathcal{D}_{l}$ 包含三百万个最大长度为 $l$ 个Token的文档。除了原始的维基百科段落外，每个 $\mathcal{D}_{l}$ 还包含合成的干扰文档，这些文档包含 $x$ 中的大多数单词，但有一两个Token不同。5 K 个查询用于评估，其余的用于训练和验证。尽管检查包含性是一个简单的机器学习任务，但它是评估压缩神经模型保真度的良好测试平台。BM25-bi 在该任务中在多个集合上实现了超过 95 的 MRR@10。

---

图 4（左）显示了重新排序的测试集结果，模型需要从 200 个段落（前 100 个 BM25-bi 和 100 个随机候选）中选择一个。有趣的是，稀疏模型相对于甚至 768 维的 DEBERT 表现如何强大。随着文档长度的增加，稀疏和密集双编码器的性能都会下降；DE-BERT 模型的准确性下降最快，与 BM25 的差距扩大。

- 图 4：包含段落的 ICT 任务结果随最大段落长度变化（50 到 400 个Token）。左：重新排序 200 个候选；右：从三百万个候选中进行检索。具体数字参见表 3。

---

完整的交叉注意力几乎完美，并且不会随文档长度而降低。ME-BERT-768 使用 8 个维度为 768 的向量来表示文档，显著优于最佳的 DE-BERT 模型。即使是 ME-BERT-64，它使用 8 个大小仅为 64 的向量（因此需要与 DE-BERT-512 相同的文档集合大小，并且在推理时更快），也大幅优于 DE-BERT 模型。

---

图 4（右）显示了从三百万个候选中进行检索这一更具挑战性的任务的结果。对于后一种设置，我们仅评估能够从如此大的集合中高效检索最近邻的模型。我们看到与重新排序设置类似的行为，多向量方法在所有长度上都超过了 BM25-uni 的性能，而 DE-BERT 模型表现不如 BM25-uni。混合模型在组合中优于两个组件，对于最长的文档集合，相对于 ME-BERT 的改进最大。

## 6 开放域问答的检索

对于此任务，我们同样使用英文维基百科 ${ }^{6}$ 作为四个不同的文档集合，最大段落长度 $l \in\{50,100,200,400\}$，对应的近似大小分别为 3900 万、2730 万、1610 万和 1020 万文档。这里我们使用 Natural Questions 数据集（Kwiatkowski 等，2019）中包含的真实用户查询。我们遵循 Lee 等（2019）的设置。训练集中有 87,925 个问答对，测试集中有 3,610 个问答对。我们保留一部分训练集用于开发。

---

对于文档检索，如果段落包含与问题标注者提供的简短答案完全匹配的字符串，则该段落对于查询 $x$ 是正确的。我们通过考虑 BM25-uni 的前 100 个结果和 100 个随机样本来形成重新排序任务，并且还考虑了完整的检索设置。这里使用 BM25-uni 而不是 BM25-bi，因为它是该任务的更强模型。

---

我们的理论结果并未直接预测压缩双编码器模型相对于 BM25 在此任务上的性能。但它们告诉我们，随着文档长度的增加，低维压缩双编码器可能无法精确测量加权术语重叠，可能导致任务性能下降。因此，我们预计更高维的双编码器、多向量编码器和混合模型对于较长文档的集合更有用。

---

图 5（左）显示了重新排序任务的保留集结果。为了公平比较处理不同大小段落集合的系统，我们允许每个模型选择大致相同数量的Token（400），并评估是否包含答案。例如，从 $\mathcal{D}_{50}$ 检索的模型返回其前 8 个段落，而从 $\mathcal{D}_{100}$ 检索的模型返回前 4 个段落。图中显示了各模型的 recall@400 Token。由于需要语义泛化，BM25-uni 和 DE-BERT 的相对性能与 ICT 任务中看到的不同。然而，更高维的 DE-BERT 模型通常表现更好，多向量模型提供了进一步的好处，特别是对于较长文档的集合；ME-BERT-768 优于 DE-BERT-768，ME-BERT-64 优于 DE-BERT-512；交叉注意力仍然明显更强。

- 图 5：NQ 段落召回结果随最大段落长度变化（50 到 400 个Token）。左：200 个段落的重新排序；右：整个（英文）维基百科的开放域检索结果。具体数字参见表 3。

---

图 5（右）显示了从维基百科检索四个文档集合 $\mathcal{D}_{l}$ 中每个集合的保留集结果。与重新排序设置不同，只有更高维的 DE-BERT 模型在段落长度超过 50 时优于 BM25。混合模型相对于其组件提供了大幅改进，捕捉了精确的单词重叠和语义相似性。随着集合中文档长度的增加，将 BM25 添加到 ME-BERT 和 DE-BERT 中的增益增加，这与我们基于理论的预期一致。

## 7 大规模监督信息检索

之前的实验部分重点在于理解压缩编码器表示维度与文档长度之间的关系。在这里，我们评估我们新提出的多向量检索模型 ME-BERT、其对应的双编码器基线 DE-BERT 以及稀疏-密集混合模型是否在信息检索基准上的大规模监督检索和排序任务中优于最先进的模型。

---

**数据集。** MS MARCO 段落排序任务专注于从大约 880 万段落的集合中对段落进行排序。提供了大约 532,000 个与相关段落配对的查询用于训练。MS MARCO 文档排序任务则是对完整文档进行排序。完整集合包含大约 300 万篇文档，训练集有大约 367,000 个查询。我们在表 1 中报告了段落和文档开发集的结果，分别包含 6,980 和 5,193 个查询。我们在表 2 中报告了 MS MARCO 和 TREC DL 2019（Craswell 等，2020）测试结果。

- 表 1：MS MARCO 段落（MS-Passage）和 MS MARCO 文档（MSDoc）开发集结果，显示 MRR@10。
- 表 2：在段落和文档 TREC 2019 DL 评估以及 MS MARCO 评估 MRR@10（段落）和 MRR@100（文档）下的测试集首次检索结果，显示 MRR(MS)。

---

**模型设置。** 对于 MS MARCO 段落，我们将模型应用于提供的段落集合。对于 MS MARCO 文档，我们遵循 Yan 等（2020）的方法，将文档分解为一系列重叠的段落，长度最多为 482 个Token，每个段落包括文档 URL 和标题。对于每个任务，我们仅使用该任务的训练数据训练模型。我们使用 BERT-large 初始化检索器和重新排序模型。我们使用来自 BM25 的 1000 个最佳列表中的正负候选者训练密集检索模型，并在有益时使用一次硬负样本挖掘。对于 ME-BERT，我们在段落任务中使用 $m=3$，在文档任务中使用 $m=4$。

---

**结果。** 表 1 比较了我们在两个任务的开发集上的模型。之前的最先进工作遵循两阶段检索和重新排序方法，其中高效的第一阶段系统从文档集合中检索一个（通常很大的）候选列表，第二阶段更昂贵的模型（如交叉注意力 BERT）对候选进行重新排序。

---

我们的重点是改进第一阶段，并在两种设置中与之前的工作进行比较：**检索**，表 1 的上半部分，仅使用第一阶段的高效检索系统；**重新排序**，表的下半部分，使用更昂贵的第二阶段模型对候选进行重新排序。图 6 深入探讨了第一阶段检索系统的影响，因为第二阶段重新排序器可以访问的候选数量显著减少，从而提高了效率。

- 图 6：MS MARCO 在不同检索深度（10 到 1000 个候选）下重新排序时的 MRR@10。

---

我们报告了与以下系统的比较结果：1）**Multi-Stage**（Nogueira 和 Lin，2019），使用 BERT 模型级联对 BM25 候选进行重新排序。2）**DOc2Query**（Nogueira 等，2019b）和 **DocT5Query**（Nogueira 和 Lin，2019），使用神经模型在索引和稀疏检索模型评分之前扩展文档。3）**DEEPCT**（Dai 和 Callan，2020b），学习将 BERT 的上下文文本表示映射到上下文感知的术语权重。4）**HDCT**（Dai 和 Callan，2020a）使用分层方法将段落级术语权重组合成文档级术语权重。5）**IDST**，Yan 等（2020）提出的两阶段级联排序管道，以及 6）**Leader-board**，截至 2020 年 9 月 18 日 MS MARCO 段落排行榜上的最佳分数。${ }^{7}$

---

我们还将我们的模型与我们自己在 § 4.1 中描述的 BM25 实现以及外部公开的稀疏模型实现（用 BM25-E 表示）进行了比较。对于段落任务，BM25-E 是 Anserini（Yang 等，2018a）系统，使用默认参数。对于文档任务，BM25-E 是官方的 IndriQueryLikelihood 基线。我们报告了使用我们自己的 BM25 和外部稀疏系统的密集-稀疏混合模型；后者用后缀 -E 表示。

---

查看表 1 的上半部分，我们可以看到我们的 DE-BERT 模型已经优于或与之前的系统竞争。多向量模型在包含较长文档的数据集（MS MARCO 文档）上带来了更大的改进，而稀疏-密集混合模型在两个数据集上都优于纯密集模型。根据 Wilcoxon 符号秩检验的统计显著性，De-bert、me-bert、De-hybrid-e 和 ME-HYBRID-E 之间的所有差异在两个开发集上均具有统计显著性，$p$ 值 $<.0001$。

---

当可以对大量候选进行重新排序时，第一阶段系统的影响会减小。在表的下半部分，我们看到我们的模型与对 BM25 候选进行重新排序的系统相当。当重新排序大量候选的成本过高时，第一阶段系统的准确性尤为重要。图 6 显示了重新排序较少候选的系统性能。我们看到，当可以使用昂贵的交叉注意力模型对非常少量的候选进行评分时，多向量 ME-BERT 和混合模型在 MS MARCO 任务上相比之前的系统实现了大幅改进。

---

表 2 显示了密集模型、外部稀疏模型基线以及两者的混合模型（不进行重新排序）的测试结果。除了 MS MARCO 段落任务的测试集（eval）结果外，我们还报告了在 TREC DL 2019 手动注释的段落和文档检索测试集上的指标。我们按照 Xiong 等（2020）的方法报告未评分项目的分数为 Holes@10。

---

**时间和空间分析。** 图 7 比较了在 MS MARCO 段落任务上使用 ScaNN（Guo 等，2020）库的 DEBERT 和 ME-BERT 的运行时间/质量权衡曲线，机器配置为 $160 \operatorname{Intel}(\mathrm{R}) \mathrm{Xeon}(\mathrm{R}) \mathrm{CPU}$ @ 2.20 GHz 核心，1.88 TB 内存。两个模型每个查询使用一个大小为 $k=1024$ 的向量；DEBERT 每个文档使用一个向量，ME-BERT 每个文档使用 3 个大小为 $k=1024$ 的向量。DEBERT 的文档索引大小为 34.2 GB，ME-BERT 的索引大小约为其 3 倍。DEBERT 和 ME-BERT 的索引时间分别为 1.52 小时和 3.02 小时。我们使用的 ScaNN 配置为 num_leaves $=5000$，num_leaves_to_search 范围从 25 到 2000（从较少到较多精确搜索），使用所有 160 个核心进行并行推理时测量每个查询的时间。在曲线的较高质量范围内，ME-BERT 在相同的每个查询推理时间下实现了比 DEBERT 显著更高的 MRR。

- 图 7：MS MARCO 段落开发集上 DE-BERT 和 ME-BERT 的质量/运行时间权衡。虚线显示精确搜索的质量。

## 8 相关工作

我们在整篇论文中提到了提高检索模型准确性的研究。这里我们重点关注与我们的核心焦点——密集双编码器表示相对于稀疏词袋模型的容量——相关的工作。

---

在压缩感知中，可以从投影 $A x$ 中恢复词袋向量 $x$，前提是 $A$ 合适。各向同性高斯投影的充分维度的边界（Candes 和 Tao，2005；Arora 等，2018）比 $\S 2$ 中描述的边界更为悲观，但这并不意外，因为从压缩测量中恢复词袋的任务比恢复内积更严格。

---

Subramani 等（2019）探讨了是否可以从预训练的解码器中精确恢复句子（Token序列），使用向量嵌入作为解码器隐藏状态的偏置。由于他们的解码模型比内积检索更具表达性（因此计算量更大），这里研究的理论问题并不适用。然而，Subramani 等通过实证观察到了句子长度和嵌入大小之间的类似依赖关系。Wieting 和 Kiela（2019）将句子表示为随机投影的词袋，发现高维投影（$k=4096$）的表现几乎与训练的编码模型一样好。这些实证结果进一步支持了 Larsen 和 Nelson（2017）提出的假设，即来自真实文本的词袋向量在某种意义上“难以嵌入”。我们的贡献是系统地探索文档长度和编码维度之间的关系，专注于基于精确内积的检索案例。我们将表示学习和近似检索的结合留给未来的工作。

## 9 结论

Transformer 在自然语言处理中的各种问题上表现良好。然而，大规模检索的计算需求迫使我们寻求其他架构：上下文嵌入的交叉注意力太慢，但固定长度向量的双编码可能表达力不足，有时甚至无法与稀疏词袋模型的性能匹敌。我们使用理论和实证技术来表征固定长度双编码器的保真度，重点关注文档长度的作用。基于这些观察，我们提出了在保持可扩展性的同时提供强大性能的混合模型。

## A 证明

## A.1 引理 1

**证明。** 对于两种嵌入分布，平方范数的误差可以以高概率界定（Achlioptas, 2003, 引理 5.1）：

$$
\begin{align*}
& \operatorname{Pr}\left(\|A A x\|^{2}-\|x\|^{2} \mid>\epsilon\|x\|^{2}\right) \\
& <2 \exp \left(-\frac{k}{2}\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)\right) \tag{4}
\end{align*}
$$

这一边界隐含了内积绝对误差的类似边界（Ben-David 等, 2002, 推论 19）：

$$
\begin{align*}
& \operatorname{Pr}\left(|\langle A x, A y\rangle-\langle x, y\rangle| \geq \frac{\epsilon}{2}\left(\|x\|^{2}+\|y\|^{2}\right)\right) \\
& \leq 4 \exp \left(-\frac{k}{2}\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)\right) \tag{5}
\end{align*}
$$

---

设 $\bar{q}=q /\|q\|$ 和 $\bar{d}=\left(d_{1}-d_{2}\right) /\left\|d_{1}-d_{2}\right\|$。则 $\mu\left(q, d_{1}, d_{2}\right)=\langle\bar{q}, \bar{d}\rangle$。当且仅当 $\langle A \bar{q}, A \bar{d}\rangle \leq 0$ 时发生排序错误，这意味着 $|\langle A \bar{q}, A \bar{d}\rangle-\langle\bar{q}, \bar{d}\rangle| \geq \epsilon$。根据构造 $\|\bar{q}\|=\|\bar{d}\|=1$，因此内积失真 $\geq \epsilon$ 的概率由 (5) 的右侧界定。

## A.2 推论 1

**证明。** 根据 Cauchy-Schwarz 不等式，我们有 $\epsilon=\mu\left(q, d_{1}, d_{2}\right)=\langle\bar{q}, \bar{d}\rangle \leq 1$。对于 $\epsilon \leq 1$，有 $\epsilon^{2} / 6 \leq \epsilon^{2} / 2-\epsilon^{3} / 3$。然后我们可以将 (1) 的边界放宽为 $\beta \leq 4 \exp \left(-\frac{k}{2} \frac{\epsilon^{2}}{6}\right)$。取自然对数得到 $\ln \beta \leq \ln 4-\epsilon^{2} k / 12$，可以重新排列为 $k \geq 12 \epsilon^{-2} \ln \frac{4}{\beta}$。

## A.3 引理 2

**证明。** 为方便起见，定义 $\mu\left(d_{2}\right)=$ $\mu\left(q, d_{1}, d_{2}\right)$。定义 $\epsilon$ 如定理所述，$\mathcal{D}_{\epsilon}=\left\{d_{2} \in \mathcal{D}: \mu\left(q, d_{1}, d_{2}\right) \geq \epsilon\right\}$。我们有

$$
\begin{aligned}
& \operatorname{Pr}\left(R \geq r_{0}\right) \leq \operatorname{Pr}\left(\exists d_{2} \in \mathcal{D}_{\epsilon}: A q_{1} \leq A q_{2}\right) \\
& \leq \sum_{d_{2} \in \mathcal{D}_{\epsilon}} 4 \exp \left(-\frac{k}{2}\left(\mu\left(d_{2}\right)^{2} / 2-\mu\left(d_{2}\right)^{3} / 3\right)\right) \\
& \leq 4\left|\mathcal{D}_{\epsilon}\right| \exp \left(-\frac{k}{2}\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)\right)
\end{aligned}
$$

第一个不等式成立是因为事件 $R \geq$ $r_{0}$ 隐含事件 $\exists d_{2} \in \mathcal{D}_{\epsilon}: A q_{1} \leq A q_{2}$。第二个不等式由引理 1 和并集边界组合得出。最后一个不等式成立是因为对于任何 $d_{2} \in \mathcal{D}_{\epsilon}$，$\mu\left(q, d_{1}, d_{2}\right) \geq \epsilon$。定理成立是因为 $\left|\mathcal{D}_{\epsilon}\right|=|\mathcal{D}|-r_{0}+1$。

## A.4 推论 2

**证明。** 对于检索函数 $\max _{d}\langle q, d\rangle$，当 $q$ 和 $d$ 是布尔向量时，最小非零非归一化边界 $\left\langle q, d_{1}\right\rangle-\left\langle q, d_{2}\right\rangle$ 为 1。因此归一化边界的下限为 $\mu\left(q, d_{1}, d_{2}\right) \geq 1 /\left(\|q\| \times\left\|d_{1}-d_{2}\right\|\right)$。对于非负 $d_{1}$ 和 $d_{2}$，我们有 $\left\|d_{1}-d_{2}\right\| \leq$ $\sqrt{\left\|d_{1}\right\|^{2}+\left\|d_{2}\right\|^{2}} \leq \sqrt{2 L_{D}}$。因此，保持归一化边界 $\epsilon=\left(2 L_{Q} L_{D}\right)^{-\frac{1}{2}}$ 足以避免任何成对错误。通过将此值代入推论 1，我们看到设置 $k \geq 24 L_{Q} L_{D} \ln \frac{4}{\beta}$ 确保任何成对错误的概率 $\leq \beta$。

## A.5 定理 1

**证明。** 回顾 $\mu\left(q, d_{1}, d_{2}\right)=\frac{\left\langle q, d_{1}-d_{2}\right\rangle}{\|q\| \times\left\|d_{1}-d_{2}\right\|}$。根据假设，我们有 $\left\langle q, d_{1}^{(i)}\right\rangle=\left\langle q, d_{1}\right\rangle$ 和 $\max _{j}\left\langle q, d_{2}^{(j)}\right\rangle \leq\left\langle q, d_{2}\right\rangle$，这意味着

$$
\begin{equation*}
\left\langle q, d_{1}^{(i)}-d_{2}^{(i)}\right\rangle \geq\left\langle q, d_{1}-d_{2}\right\rangle \tag{6}
\end{equation*}
$$

在分母中，我们展开 $\left\|d_{1}-d_{2}\right\|=$ $\left\|\left(d_{1}^{(i)}-d_{2}^{(i)}\right)+\left(d_{1}^{(\neg i)}-d_{2}^{(\neg i)}\right)\right\|$，其中 $d^{(\neg i)}=$ $\sum_{j \neq i} d^{(j)}$。将其代入平方范数，

$$
\begin{align*}
& \left\|d_{1}-d_{2}\right\|^{2} \\
& =\left\|\left(d_{1}^{(i)}-d_{2}^{(i)}\right)+\left(d_{1}^{(\neg i)}-d_{2}^{(\neg i)}\right)\right\|^{2}  \tag{7}\\
& =\left\|d_{1}^{(i)}-d_{2}^{(i)}\right\|^{2}+\left\|d_{1}^{(\neg i)}-d_{2}^{(\neg i)}\right\|^{2} \\
& \quad+2\left\langle d_{1}^{(i)}-d_{2}^{(i)}, d_{1}^{(\neg i)}-d_{2}^{(\neg i)}\right\rangle  \tag{8}\\
& =\left\|d_{1}^{(i)}-d_{2}^{(i)}\right\|^{2}+\left\|d_{1}^{(\neg i)}-d_{2}^{(\neg i)}\right\|^{2}  \tag{9}\\
& \geq\left\|d_{1}^{(i)}-d_{2}^{(i)}\right\|^{2} . \tag{10}
\end{align*}
$$

内积 $\left\langle d_{1}^{(i)}-d_{2}^{(i)}, d_{1}^{(\neg i)}-d_{2}^{(\neg i)}\right\rangle=0$，因为片段是正交的。(6) 和 (10) 的组合完成了定理的证明。
