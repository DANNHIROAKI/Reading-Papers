### Sparse, Dense, and Attentional Representations for Text Retrieval 


#### Abstract

Dual encoders perform retrieval by encoding documents and queries into dense lowdimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.


## 1 Introduction

Retrieving relevant documents is a core task for language technology, and is a component of applications such as information extraction and question answering (e.g., Narasimhan et al., 2016; Kwok et al., 2001; Voorhees, 2001). While classical information retrieval has focused on heuristic weights for sparse bag-of-words representations (Spärck Jones, 1972), more recent work has adopted a two-stage retrieval and ranking pipeline, where a large number of documents are retrieved using sparse high dimensional query/document representations, and are further reranked with learned neural models (Mitra and Craswell, 2018). This two-stage approach has achieved state-of-theart results on IR benchmarks (Nogueira and Cho, 2019; Yang et al., 2019; Nogueira et al., 2019a),especially since sizable annotated data has become available for training deep neural models (Dietz et al., 2018; Craswell et al., 2020). However, this pipeline suffers from a strict upper bound imposed by any recall errors in the first-stage retrieval model: for example, the recall@ 1000 for BM25 reported by Yan et al. (2020) is 69.4.

---

A promising alternative is to perform first-stage retrieval using learned dense low-dimensional encodings of documents and queries (Huang et al., 2013; Reimers and Gurevych, 2019; Gillick et al., 2019; Karpukhin et al., 2020). The dual encoder model scores each document by the inner product between its encoding and that of the query. Unlike full attentional architectures, which require extensive computation on each candidate document, the dual encoder can be easily applied to very large document collections thanks to efficient algorithms for inner product search; unlike untrained sparse retrieval models, it can exploit machine learning to generalize across related terms.

---

To assess the relevance of a document to an information-seeking query, models must both (i) check for precise term overlap (for example, presence of key entities in the query) and (ii) compute semantic similarity generalizing across related concepts. Sparse retrieval models excel at the first sub-problem, while learned dual encoders can be better at the second. Recent history in NLP might suggest that learned dense representations should always outperform sparse features overall, but this is not necessarily true: as shown in Figure 1 , the BM25 model (Robertson et al., 2009) can outperform a dual encoder based on BERT, particularly on longer documents and on a task that requires precise detection of word overlap. ${ }^{1}$ This raises questions about the limitations of dual encoders, and the circumstances in which these powerful models do not yet reach the state of the art. Here we explore these questions using both theoretical and empirical tools, and propose a new architecture that leverages the strengths of dual encoders while avoiding some of their weaknesses.

- Figure 1: Recall@1 for retrieving passage containing a query from three million candidates. The figure compares a fine-tuned BERT-based dual encoder (DE-BERT768), an off-the-shelf BERT-based encoder with average pooling (BERT-init), and sparse term-based retrieval (BM25), while binning passages by length.

---

We begin with a theoretical investigation of compressive dual encoders - dense encodings whose dimension is below the vocabulary size and analyze their ability to preserve distinctions made by sparse bag-of-words retrieval models, which we term their fidelity. Fidelity is important for the sub-problem of detecting precise term overlap, and is a tractable proxy for capacity. Using the theory of dimensionality reduction, we relate fidelity to the normalized margin between the gold retrieval result and its competitors, and show that this margin is in turn related to the length of documents in the collection. We validate the theory with an empirical investigation of the effects of random projection compression on sparse BM25 retrieval using queries and documents from TRECCAR, a recent IR benchmark (Dietz et al., 2018).

---

Next, we offer a multi-vector encoding model, which is computationally feasible for retrieval like the dual-encoder architecture and achieves significantly better quality. A simple hybrid that interpolates models based on dense and sparse representations leads to further improvements.

---

We compare the performance of dual encoders, multi-vector encoders, and their sparse-dense hybrids with classical sparse retrieval models and attentional neural networks, as well as state-of-the-art published results where available. Our evaluations include open retrieval benchmarks (MS MARCO passage and document), and passage retrieval for question answering (Natural Questions). We confirm prior findings that full attentional architectures excel at reranking tasks, but are not efficient enough for large-scale retrieval. Of the more efficient alternatives, the hybridized multi-vector encoder is at or near the top in every evaluation, outperforming state-of-the-art retrieval results in MS MARCO. Our code is publicly available at https://github.com/google-research/language/tree/master/language/multivec.

## 2 Analyzing dual encoder fidelity

A query or a document is a sequence of words drawn from some vocabulary $\mathcal{V}$. Throughout this section we assume a representation of queries and documents typically used in sparse bag-of-words models: each query $q$ and document $d$ is a vector in $\mathbb{R}^{v}$ where $v$ is the vocabulary size. We take the inner product $\langle q, d\rangle$ to be the relevance score of document $d$ for query $q$. This framework accounts for a several well-known ranking models, including boolean inner product, TF-IDF, and BM25.

---

We will compare sparse retrieval models with compressive dual encoders, for which we write $f(d)$ and $f(q)$ to indicate compression of $d$ and $q$ to $\mathbb{R}^{k}$, with $k \ll v$, and where $k$ does not vary with the document length. For these models, the relevance score is the inner product $\langle f(q), f(d)\rangle$. (In $\S 3$, we consider encoders that apply to sequences of tokens rather than vectors of counts.)

---

A fundamental question is how the capacity of dual encoders varies with the embedding size $k$. In this section we focus on the related, more tractable notion of fidelity: how much can we compress the input while maintaining the ability to mimic the performance of bag-of-words retrieval? We explore this question mainly through the encoding model of random projections, but also discuss more general dimensionality reduction in § 2.2.

### 2.1 Random projections

To establish baselines on the fidelity of compressive dual encoder retrieval, we now consider encoders based on random projections (Vempala, 2004). The encoder is defined as $f(x)=$ $A x$, where $A \in \mathbb{R}^{k \times v}$ is a random matrix. In Rademacher embeddings, each element $a_{i, j}$ of the matrix $A$ is sampled with equal probability from two possible values: $\left\{-\frac{1}{\sqrt{k}}, \frac{1}{\sqrt{k}}\right\}$. In Gaussian embeddings, each $a_{i, j} \sim N\left(0, k^{-1 / 2}\right)$. A pairwise ranking error occurs when $\left\langle q, d_{1}\right\rangle>$ $\left\langle q, d_{2}\right\rangle$ but $\left\langle A q, A d_{1}\right\rangle<\left\langle A q, A d_{2}\right\rangle$. Using such random projections, it is possible to bound the probability of any such pairwise error in terms of the embedding size.

Definition 2.1. For a query $q$ and pair of documents $\left(d_{1}, d_{2}\right)$ such that $\left\langle q, d_{1}\right\rangle \geq\left\langle q, d_{2}\right\rangle$, the normalized margin is defined as, $\mu\left(q, d_{1}, d_{2}\right)=$ $\frac{\left\langle q, d_{1}-d_{2}\right\rangle}{\|q\| \times\left\|d_{1}-d_{2}\right\|}$.

---

Lemma 1. Define a matrix $A \in \mathbb{R}^{k \times d}$ of Gaussian or Rademacher embeddings. Define vectors $q, d_{1}, d_{2}$ such that $\mu\left(q, d_{1}, d_{2}\right)=\epsilon>0$. A ranking error occurs when $\left\langle A q, A d_{2}\right\rangle \geq\left\langle A q, A d_{1}\right\rangle$. If $\beta$ is the probability of such an error then,

$$
\begin{equation*}
\beta \leq 4 \exp \left(-\frac{k}{2}\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)\right) \tag{1}
\end{equation*}
$$

The proof, which builds on well-known results about random projections, is found in § A.1. By solving (1) for $k$, we can derive an embedding size that guarantees a desired upper bound on the pairwise error probability,
$$
\begin{equation*}
k \geq 2\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)^{-1} \ln \frac{4}{\beta} \tag{2}
\end{equation*}
$$

It is convenient to derive a simpler but looser quadratic bound (proved in § A.2):

Corollary 1. Define vectors $q, d_{1}, d_{2}$ such that $\epsilon=\mu\left(q, d_{1}, d_{2}\right)>0$. If $A \in \mathbb{R}^{k \times v}$ is a matrix of random Gaussian or Rademacher embeddings such that $k>12 \epsilon^{-2} \ln \frac{4}{\beta}$, then $\operatorname{Pr}\left(\left\langle A q, A d_{1}\right\rangle \leq\right.$ $\left.\left\langle A q, A d_{2}\right\rangle\right) \leq \beta$.

---

On the tightness of the bound. Let $k^{*}\left(q, d_{1}, d_{2}\right)$ denote the lowest dimension Gaussian or Rademacher random projection following the definition in Lemma 1, for which $\operatorname{Pr}\left(\left\langle A q, A d_{1}\right\rangle<\left\langle A q, A d_{2}\right\rangle\right) \leq \beta$, for a given document pair $\left(d_{1}, d_{2}\right)$ and query $q$ with normalized margin $\epsilon$. Our lemma places an upper bound on $k^{*}$, saying that $k^{*}\left(q, d_{1}, d_{2}\right) \leq 2\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)^{-1} \ln \frac{4}{\beta}$. Any $k \geq k^{*}\left(q, d_{1}, d_{2}\right)$ has sufficiently low probability of error, but lower values of $k$ could potentially also have the desired property. Later in this section we perform empirical evaluation to study the tightness of the bound; although theoretical tightness (up to a constant factor) is suggested by results on the optimality of the distributional Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984; Jayram and Woodruff, 2013; Kane et al., 2011), here we study the question only empirically.

### 2.1.1 Recall-at-$r$

In retrieval applications, it is important to return the desired result within the top $r$ search results. For query $q$, define $d_{1}$ as the document that maximizes some inner product ranking metric. The probability of returning $d_{1}$ in the top $r$ results after random projection can be bounded by a function of the embedding size and normalized margin:

Lemma 2. Consider a query $q$, with target document $d_{1}$, and document collection $\mathcal{D}$ that excludes $d_{1}$, and such that $\forall d_{2} \in \mathcal{D}, \mu\left(q, d_{1}, d_{2}\right)>0$. Define $r_{0}$ to be any integer such that $1 \leq r_{0} \leq|\mathcal{D}|$. Define $\epsilon$ to be the $r_{0}$ 'th smallest normalized margin $\mu\left(q, d_{1}, d_{2}\right)$ for any $d_{2} \in \mathcal{D}$, and for simplicity assume that only a single document $d_{2} \in \mathcal{D}$ has $\mu\left(q, d_{1}, d_{2}\right)=\epsilon^{2}$

---

Define a matrix $A \in \mathbb{R}^{k \times d}$ of Gaussian or Rademacher embeddings. Define $R$ to be a random variable such that $R=\mid\left\{d_{2} \in \mathcal{D}\right.$ : $\left.\left\langle A q, A d_{1}\right\rangle \leq\left\langle A q, A d_{2}\right\rangle\right\} \mid$, and let $C=4(|\mathcal{D}|-$ $\left.r_{0}+1\right)$. Then

$$
\operatorname{Pr}\left(R \geq r_{0}\right) \leq C \exp \left(-\frac{k}{2}\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)\right)
$$

The proof is in § A.3. A direct consequence of the lemma is that to achieve recall-at- $r_{0}=1$ for a given $\left(q, d_{1}, \mathcal{D}\right)$ triple with probability $\geq 1-\beta$, it is sufficient to set

$$
\begin{equation*}
k \geq \frac{2}{\epsilon^{2} / 2-\epsilon^{3} / 3} \ln \frac{4\left(|\mathcal{D}|-r_{0}+1\right)}{\beta} \tag{3}
\end{equation*}
$$

where $\epsilon$ is the $r_{0}$ 'th smallest normalized margin.

---

As with the bound on pairwise relevance errors in Lemma 1, Lemma 2 implies an upper bound on the minimum random projection dimension $k^{*}\left(q, d_{1}, \mathcal{D}\right)$ that recalls $d_{1}$ in the top $r_{0}$ results with probability $\geq 1-\beta$. Due to the application of the union bound and worst-case assumptions about the normalized margins of documents in $\mathcal{D}_{\epsilon}$, this bound is potentially loose. Later in this section we examine the empirical relationship between maximum document length, the distribution of normalized margins, and $k^{*}$.

### 2.1.2 Application to Boolean inner product

Boolean inner product is a retrieval function in which $d, q \in\{0,1\}^{v}$ over a vocabulary of size $v$, with $d_{i}$ indicating the presence of term $i$ in the document (and analogously for $q_{i}$ ). The relevance score $\langle q, d\rangle$ is then the number of terms that appear in both $q$ and $d$. For this simple retrieval function, it is possible to compute an embedding size that guarantees a desired pairwise error probability over an entire dataset of documents.

---

Corollary 2. For a set of documents $\mathcal{D}=\{d \in$ $\left.\{0,1\}^{v}\right\}$ and a query $q \in\{0,1\}^{v}$, let $L_{D}=$ $\max _{d \in \mathcal{D}}\|d\|^{2}$ and $L_{Q}=\|q\|^{2}$. Let $A \in \mathbb{R}^{k \times v}$ be a matrix of random Rademacher or Gaussian embeddings such that $k \geq 24 L_{Q} L_{D} \ln \frac{4}{\beta}$. Then for any $d_{1}, d_{2} \in \mathcal{D}$ such that $\left\langle q, d_{1}\right\rangle>\left\langle q, d_{2}\right\rangle$, the probability that $\left\langle A q, A d_{1}\right\rangle \leq\left\langle A q, A d_{2}\right\rangle$ is $\leq \beta$.

The proof is in § A.4. The corollary shows that for boolean inner product ranking, we can guarantee any desired error bound $\beta$ by choosing an embedding size $k$ that grows linearly in $L_{D}$, the number of unique terms in the longest document.

### 2.1.3 Application to TF-IDF and BM25

Both TF-IDF (Spärck Jones, 1972) and BM25 (Robertson et al., 2009) can be written as inner products between bag-of-words representations of the document and query as described earlier in this section. Set the query representation $\tilde{q}_{i}=q_{i} \times \mathrm{IDF}_{i}$, where $q_{i}$ indicates the presence of the term in the query and $\mathrm{IDF}_{i}$ indicates the inverse document frequency of term $i$. The TF-IDF score is then $\langle\tilde{q}, d\rangle$. For BM25, we define $\tilde{d} \in \mathbb{R}^{v}$, with each $\tilde{d}_{i}$ a function of the count $d_{i}$ and the document length (and hyperparameters); $\operatorname{BM25}(q, d)$ is then $\langle\tilde{q}, \tilde{d}\rangle$. Due to its practical utility in retrieval, we now focus on BM25.

---

Pairwise accuracy. We use empirical data to test the applicability of Lemma 1 to the BM25 relevance model. We select query-document triples $\left(q, d_{1}, d_{2}\right)$ from the TREC-CAR dataset (Dietz et al., 2018) by considering all possible $\left(q, d_{2}\right)$, and selecting $d_{1}=\arg \max _{d} \operatorname{BM} 25(q, d)$. We bin the triples by the normalized margin $\epsilon$, and compute the quantity $\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)^{-1}$. According to Lemma 1 , the minimum embedding size of a random projection $k^{*}$ which has $\leq \beta$ probability of making an error on a triple with normalized mar$\operatorname{gin} \epsilon$ is upper bounded by a linear function of this quantity. In particular, for $\beta=.05$, the Lemma entails that $k^{*} \leq 8.76\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)^{-1}$. In this experiment we measure the empirical value of $k^{*}$ to evaluate the tightness of the bound.

---

The results are shown on the $x$-axis of Figure 2. For each bin we compute the minimum embedding size required to obtain $95 \%$ pairwise accuracy in ranking $d_{1}$ vs $d_{2}$, using a grid of 40 possible values for $k$ between 32 and 9472, shown on the $y$ axis. (We exclude examples that had higher values of $\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)^{-1}$ than the range shown because they did not reach $95 \%$ accuracy for the explored range of $k$.) The figure shows that the theoretical bound is tight up to a constant factor, and that the minimum embedding size that yields desired fidelity grows linearly with $\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)^{-1}$.

- Figure 2: Minimum $k$ sufficient for Rademacher embeddings to approximate BM25 pairwise rankings on TREC-CAR with error rate $\beta<.05$.

---

Margins and document length. For boolean inner product, it was possible to express the minimum possible normalized margin (and therefore a sufficient embedding size) in terms of $L_{Q}$ and $L_{D}$, the maximum number of unique terms across all queries and documents, respectively. Unfortunately, it is difficult to analytically derive a minimum normalized margin $\epsilon$ for either TF-IDF or BM25: because each term may have a unique inverse document frequency, the minimum non-zero margin $\left\langle q, d_{1}-d_{2}\right\rangle$ decreases with the number of terms in the query as each additional term creates more ways in which two documents can receive nearly the same score. We therefore study empirically how normalized margins vary with maximum document length. Using the TREC-CAR retrieval dataset, we bin documents by length. For each query, we compute the normalized margins between the document with best BM25 in the bin and all other documents in the bin, and look at the 10th, 100th, and 1000th smallest normalized margins. The distribution over these normalized margins is shown in Figure 3a, revealing that normalized margins decrease with document length. In practice, the observed minimum normalized margin for a collection of documents and queries is found to be much lower for BM25 compared to boolean inner product. For example, for the collection used in Figure 2, the minimum normalized margin for BM25 is $6.8 \mathrm{e}-06$, while for boolean inner product it is 0.0169 .

- Figure 3: Random projection on BM25 retrieval in TREC-CAR dataset, with documents binned by length. 

---

Document length and encoding dimension. Figure 3b shows the growth in minimum random projection dimension required to reach desired recall-at-10, using the same document bins as in Figure 3a. As predicted, the required dimension increases with the document length, while the normalized margin shrinks.

### 2.2 Bounds on general encoding functions

We derived upper bounds on minimum required encoding for random linear projections above, and found the bounds on $\left(q, d_{1}, d_{2}\right)$ triples to be empirically tight up to a constant factor. More general non-linear and learned encoders could be more efficient. However, there are general theoretical results showing that it is impossible for any encoder to guarantee an inner product distortion $|\langle f(x), f(y)\rangle-\langle x, y\rangle| \leq \epsilon$ with an encoding that does not grow as $\Omega\left(\epsilon^{-2}\right)$ (Larsen and Nelson, 2017; Alon and Klartag, 2017), for vectors $x, y$ with norm $\leq 1$. These results suggest more general capacity limitations for fixed-length dual encoders when document length grows.

---

In our setting, BM25, TF-IDF, and boolean inner product can all be reformulated equivalently as inner products in a space with vectors of norm at most 1 by $L_{2}$-normalizing each query vector and rescaling all document vectors by $\sqrt{L_{D}}=$ $\max _{d}\|d\|$, a constant factor that grows with the length of the longest document. Now suppose we desire to limit the distortion on the unnormalized inner products to some value $\leq \tilde{\epsilon}$, which might guarantee a desired performance characteristic. This corresponds to decreasing the maximum normalized inner product distortion $\epsilon$ by a factor of $\sqrt{L_{D}}$. According to the general bounds on dimensionality reduction mentioned in the previous paragraph, this could necessitate an increase in the encoding size by a factor of $L_{D}$.

---

However, there a number of caveats to this theoretical argument. First, the theory states only that there exist vector sets that cannot be encoded into representations that grow more slowly than $\Omega\left(\epsilon^{-2}\right)$; actual documents and queries might be easier to encode if, for example, they are generated from some simple underlying stochastic process. Second, our construction achieves $\|d\| \leq 1$ by rescaling all document vectors by a constant factor, but there may be other ways to constrain the norms while using the embedding space more efficiently. Third, in the non-linear case it might be possible to eliminate ranking errors without achieving low inner product distortion. Finally, from a practical perspective, the generalization offered by learned dual encoders might overwhelm any sacrifices in fidelity, when evaluated on real tasks of interest. Lacking theoretical tools to settle these questions, we present a set of empirical investigations in later sections of this paper. But first we explore a lightweight modification to the dual encoder, which offers gains in expressivity at limited additional computational cost.

## 3 Multi-Vector Encodings

The theoretical analysis suggests that fixed-length vector representations of documents may in general need to be large for long documents, if fidelity with respect to sparse high-dimensional representations is important. Cross-attentional architectures can achieve higher fidelity, but are impractical for large-scale retrieval (Nogueira et al., 2019b; Reimers and Gurevych, 2019; Humeau et al., 2020). We therefore propose a new architecture that represents each document as a fixed-size set of $m$ vectors. Relevance scores are computed as the maximum inner product over this set.

---

Formally, let $\mathrm{x}=\left(x_{1}, \ldots, x_{T}\right)$ represent a sequence of tokens, with $x_{1}$ equal to the special token [CLS], and define y analogously. Then $\left[h_{1}(\mathrm{x}), \ldots, h_{T}(\mathrm{x})\right]$ represents the sequence of contextualized embeddings at the top level of a deep transformer. We define a single-vector representation of the query x as $f^{(1)}(\mathrm{x})=h_{1}(\mathrm{x})$, and a multi-vector representation of document y as $f^{(m)}(\mathrm{y})=\left[h_{1}(\mathrm{y}), \ldots, h_{m}(\mathrm{y})\right]$, the first $m$ representation vectors for the sequence of tokens in y , with $m<T$. The relevance score is defined as $\max _{j=1 \ldots m}\left\langle f^{(1)}(\mathrm{x}), f_{j}^{(m)}(\mathrm{y})\right\rangle$.

---

Although this scoring function is not a dual encoder, the search for the highest-scoring document can be implemented efficiently with standard approximate nearest-neighbor search by adding multiple ( $m$ ) entries for each document to the search index data structure. If some vector $f_{j}^{(m)}(\mathrm{y})$ yields the largest inner product with the query vector $f^{(1)}(\mathrm{x})$, it is easy to show the corresponding document must be the one that maximizes the relevance score $\psi^{(m)}(\mathrm{x}, \mathrm{y})$. The size of the index must grow by a factor of $m$, but due to the efficiency of contemporary approximate nearest neighbor and maximum inner product search, the time complexity can be sublinear in the size of the index (Andoni et al., 2019; Guo et al., 2016b). Thus, a model using $m$ vectors of size $k$ to represent documents is more efficient at run-time than a dual encoder that uses a single vector of size $m k$.

---

This efficiency is a key difference from the POLY-ENCODER (Humeau et al., 2020), which computes a fixed number of vectors per query, and aggregates them by softmax attention against document vectors. Yang et al. (2018b) propose a similar architecture for language modeling. Because of the use of softmax in these approaches, it is not possible to decompose the relevance score into a max over inner products, and so fast nearestneighbor search cannot be applied. In addition, these works did not address retrieval from a large document collection.

---

Analysis. To see why multi-vector encodings can enable smaller encodings per vector, consider an idealized setting in which each document vector is the sum of $m$ orthogonal segments such that $d=\sum_{i=1}^{m} d^{(i)}$ and each query refers to exactly one segment in the gold document. ${ }^{3}$ An orthogonal segmentation can be obtained by choosing the segments as a partition of the vocabulary.

---

Theorem 1. Define vectors $q, d_{1}, d_{2} \in \mathbb{R}^{v}$ such that $\left\langle q, d_{1}\right\rangle>\left\langle q, d_{2}\right\rangle$, and assume that both $d_{1}$ and $d_{2}$ can be decomposed into $m$ segments such that: $d_{1}=\sum_{i=1}^{m} d_{1}^{(i)}$, and analogously for $d_{2}$; all segments across both documents are orthogonal. If there exists an $i$ such that $\left\langle q, d_{1}\right\rangle=\left\langle q, d_{1}^{(i)}\right\rangle$ and $\left\langle q, d_{2}\right\rangle \geq\left\langle q, d_{2}^{(i)}\right\rangle$, then $\mu\left(q, d_{1}^{(i)}, d_{2}^{(i)}\right) \geq$ $\mu\left(q, d_{1}, d_{2}\right)$. (The proof is in $\S$ A.5.)

---

Remark. The BM25 score can be computed from non-negative representations of the document and query; if the segmentation corresponds to a partition of the vocabulary, then the segments will also be non-negative, and thus the condition $\left\langle q, d_{2}\right\rangle \geq$ $\left\langle q, d_{2}^{(i)}\right\rangle$ holds for all $i$.

---

The relevant case is when the same segment is maximal for both documents, $\left\langle q, d_{2}^{(i)}\right\rangle=$ $\max _{j}\left\langle q, d_{2}^{(j)}\right\rangle$, as will hold for "simple" queries that are well-aligned with the segmentation. Then the normalized margin in the multi-vector model will be at least as large as in the equivalent single vector representation. The relationship to encoding size follows from the theory in the previous section: Theorem 1 implies that if we set $f_{i}^{(m)}(\mathrm{y})=A d^{(i)}$ (for appropriate $A$ ), then an increase in the normalized margin enables the use of a smaller encoding dimension $k$ while still supporting the same pairwise error rate. There are now $m$ times more "documents" to evaluate, but Lemma 2 shows that this exerts only a logarithmic increase on the encoding size for a desired recall@ $r$. But while we hope this argument is illuminating, the assumptions of orthogonal segments and perfect segment match against the query are quite strong. We must therefore rely on empirical analysis to validate the efficacy of multi-vector encoding in realistic applications.

---

Cross-attention. Cross-attentional architectures can be viewed as a generalization of the multivector model: (1) set $m=T_{\text {max }}$ (one vector per token); (2) compute one vector per token in the query; (3) allow more expressive aggregation over vectors than the simple max employed above. Any sparse scoring function (e.g., BM25) can be mimicked by a cross-attention model, which need only compute identity between individual words; this can be achieved by random projection word embeddings whose dimension is proportional to the $\log$ of the vocabulary size. By definition, the required representation also grows linearly with the number of tokens in the passage and query. As with the POLY-ENCODER, retrieval in the crossattention model cannot be performed efficiently at scale using fast nearest-neighbor search. In contemporaneous work, Khattab and Zaharia (2020) propose an approach with $T_{Y}$ vectors per query and $T_{X}$ vectors per document, using a simple sum-of-max for aggregation of the inner products. They apply this approach to retrieval via reranking results of $T_{Y}$ nearest-neighbor searches. Our multi-vector model uses fixed length representations instead, and a single nearest neighbor search per query.

## 4 Experimental Setup

The full IR task requires detection of both precise word overlap and semantic generalization. Our theoretical results focus on the first aspect, and derive theoretical and empirical bounds on the sufficient dimensionality to achieve high fidelity with respect to sparse bag-of-words models as document length grows, for two types of linear random projections. The theoretical setup differs from modeling for realistic information-seeking scenarios in at least two ways.

First, trained non-linear dual encoders might be able to detect precise word overlap with much lower-dimensional encodings, especially for queries and documents with a natural distribution, which may exhibit a low-dimensional subspace structure. Second, the semantic generalization aspect of the IR task may be more important than the first aspect for practical applications, and our theory does not make predictions about how encoder dimensionality relates to such ability to compute general semantic similarity.

---

We relate the theoretical analysis to text retrieval in practice through experimental studies on three tasks. The first task, described in $\S 5$, tests the ability of models to retrieve natural language documents that exactly contain a query and evaluates both BM25 and deep neural dual encoders on a task of detecting precise word overlap, defined over texts with a natural distribution. The second task, described in $\S 6$, is the passage retrieval sub-problem of the open-domain QA version of the Natural Questions (Kwiatkowski et al., 2019; Lee et al., 2019); this benchmark reflects the need to capture graded notions of similarly and has a natural query text distribution. For both of these tasks, we perform controlled experiments varying the maximum length of the documents in the collection, which enables assessing the relationship between encoder dimension and document length.

---

To evaluate the performance of our best models in comparison to state-of-the-art works on large-scale retrieval and ranking, in $\S 7$ we report results on a third group of tasks focusing on passage/document ranking: the passage and document-level MS MARCO retrieval datasets (Nguyen et al., 2016; Craswell et al., 2020). Here we follow the standard two-stage retrieval and ranking system: a first-stage retrieval from a large document collection, followed by reranking with a cross-attention model. We focus on the impact of the first-stage retrieval model.

### 4.1 Models

Our experiments compare compressive and sparse dual encoders, cross attention, and hybrid models.

BM25. We use case-insensitive wordpiece tokenizations of texts and default BM25 parameters from the gensim library. We apply either unigram (BM25-uni) or combined unigram+bigram representations (BM25-bi).

---

Dual encoders from BERT (DE-BERT). We encode queries and documents using BERT-base, which is a pre-trained transformer network (12 layers, 768 dimensions) (Devlin et al., 2019). We implement dual encoders from BERT as a special case of the multi-vector model formalized in $\S 3$, with number of vectors for the document $m=1$ : the representations for queries and documents are the top layer representations at the [CLS] token. This approach is widely used for retrieval (Lee et al., 2019; Reimers and Gurevych, 2019; Humeau et al., 2020; Xiong et al., 2020). ${ }^{4}$ For lower-dimensional encodings, we learn downprojections from $d=768$ to $k \in 32,64,128,512,{ }^{5}$ implemented as a single feed-forward layer, followed by layer normalization. All parameters are fine-tuned for the retrieval tasks. We refer to these models as DE-BERT- $k$.

---

Cross-Attentional bert. The most expressive model we consider is cross-attentional BERT, which we implement by applying the BERT encoder to the concatenation of the query and document, with a special [SEP] separator between $x$ and $y$. The relevance score is a learned linear function of the encoding of the [CLS] token. Due to the computational cost, cross-attentional BERT is applied only in reranking as in prior work (Nogueira and Cho, 2019; Yang et al., 2019). These models are referred to as Cross-Attention.

---

Multi-Vector Encoding from BERT (ME-BERT). In § 3 we introduced a model in which every document is represented by exactly $m$ vectors. We use $m=8$ as a good compromise between cost and accuracy in $\S 5$ and $\S 6$, and find values of 3 to 4 for $m$ more accurate on the datasets in $\S 7$. In addition to using BERT output representations directly, we consider down-projected representations, implemented using a feed-forward layer with dimension $768 \times k$. A model with $k$-dimensional embeddings is referred to as ME-BERT- $k$.

---

Sparse-Dense Hybrids (Hybrid). A natural approach to balancing between the fidelity of sparse representations and the generalization of learned dense ones is to build a hybrid. To do this, we linearly combine a sparse and dense system's scores using a single trainable weight $\lambda$, tuned on a development set. For example, a hybrid model of ME-BERT and BM25-uni is referred to as HYBRID-ME-BERT-uni. We implement approximate search to retrieve using a linear combination of two systems by re-ranking $n$-best top scoring candidates from each system. Prior and concurrent work has also used hybrid sparse-dense models (Guo et al., 2016a; Seo et al., 2019; Karpukhin et al., 2020; Ma et al., 2020; Gao et al., 2020). Our contribution is to assess the impact of sparse-dense hybrids as the document length grows.

### 4.2 Learning and Inference

For the experiments in $\S 5$ and $\S 6$, all trained models are initialized from BERT-base, and all parameters are fine-tuned using a cross-entropy loss with 7 sampled negatives from a pre-computed 200 -document list and additional in-batch negatives (with a total number of 1024 candidates in a batch); the pre-computed candidates include 100 top neighbors from BM25 and 100 random samples. This is similar to the method by Lee et al. (2019), but with additional fixed candidates, also used in concurrent work (Karpukhin et al., 2020). Given a model trained in this way, for the scalable methods, we also applied hard-negative mining as in Gillick et al. (2019) and used one iteration when beneficial. More sophisticated negative selection is proposed in concurrent work (Xiong et al., 2020). For retrieval from large document collections with the scalable models, we used ScaNN: an efficient approximate nearest neighbor search library (Guo et al., 2020); in most experiments, we use exact search settings but also evaluate approximate search in Section § 7. In § 7, the same general approach with slightly different hyperparameters (detailed in that section) was used, to enable more direct comparisons to prior work.

## 5 Containing Passage ICT Task

We begin with experiments on the task of retrieving a Wikipedia passage $y$ containing a sequence of words $x$. We create a dataset using Wikipedia, following the Inverse Cloze Task definition by Lee et al. (2019), but adapted to suit the goals of our study. The task is defined by first breaking Wikipedia texts into segments of length at most $l$. These form the document collection $\mathcal{D}$. Queries $x_{i}$ are generated by sampling sub-sequences from the documents $y_{i}$. We use queries of lengths between 5 and 25 , and do not remove the queries $x_{i}$ from their corresponding documents $y_{i}$.

---

We create a dataset with one million queries and evaluate retrieval against four document collections $\mathcal{D}_{l}$, for $l \in 50,100,200,400$. Each $\mathcal{D}_{l}$ contains three million documents of maximum length $l$ tokens. In addition to original Wikipedia passages, each $\mathcal{D}_{l}$ contains synthetic distractor documents, which contain the large majority of words in $x$ but differ by one or two tokens. 5 K queries are used for evaluation, leaving the rest for training and validation. Although checking containment is a straightforward machine learning task, it is a good testbed for assessing the fidelity of compressive neural models. BM25-bi achieves over 95 MRR@10 across collections for this task.

---

Figure 4 (left) shows test set results on reranking, where models need to select one of 200 passages (top 100 BM25-bi and 100 random candidates). It is interesting to see how strong the sparse models are relative to even a 768-dimensional DEBERT. As the document length increases, the performance of both the sparse and dense dual encoders worsens; the accuracy of the DE-BERT models falls most rapidly, widening the gap to BM25.

- Figure 4: Results on the containing passage ICT task as maximum passage length varies ( 50 to 400 tokens). Left: Reranking 200 candidates; Right: Retrieval from three million candidates. Exact numbers refer to Table 3. 

---

Full cross attention is nearly perfect and does not degrade with document length. ME-BERT-768 which uses 8 vectors of dimension 768 to represent documents strongly outperforms the best DE-BERT model. Even ME-BERT-64, which uses 8 vectors of size only 64 instead (thus requiring the same document collection size as DE-BERT-512 and being faster at inference time), outperforms the DE-BERT models by a large margin.

---

Figure 4 (right) shows results for the much more challenging task of retrieval from three million candidates. For the latter setting, we only evaluate models that can efficiently retrieve nearest neighbors from such a large set. We see similar behavior to the reranking setting, with the multi-vector methods exceeding BM25-uni performance for all lengths and DE-BERT models underperforming BM25-uni. The hybrid model outperforms both components in the combination with largest improvements over ME-BERT for the longest-document collection.

## 6 Retrieval for Open-domain QA

For this task we similarly use English Wikipedia ${ }^{6}$ as four different document collections, of maximum passage length $l \in\{50,100,200,400\}$, and corresponding approximate sizes of 39 million, 27.3 million, 16.1 million, and 10.2 million documents, respectively. Here we use real user queries contained in the Natural Questions dataset (Kwiatkowski et al., 2019). We follow the setup in Lee et al. (2019). There are 87,925 QA pairs in training and 3, 610 QA pairs in the test set. We hold out a subset of training for development.

---

For document retrieval, a passage is correct for a query $x$ if it contains a string that matches exactly an annotator-provided short answer for the question. We form a reranking task by considering the top 100 results from BM25-uni and 100 random samples, and also consider the full retrieval setting. BM25-uni is used here instead of BM25-bi, because it is the stronger model for this task.

---

Our theoretical results do not make direct predictions for performance of compressive dual encoder models relative to BM25 on this task. They do tell us that as the document length grows, lowdimensional compressive dual encoders may not be able to measure weighted term overlap precisely, potentially leading to lower performance on the task. Therefore, we would expect that higher dimensional dual encoders, multi-vector encoders, and hybrid models become more useful for collections with longer documents.

---

Figure 5 (left) shows heldout set results on the reranking task. To fairly compare systems that operate over collections of different-sized passages, we allow each model to select approximately the same number of tokens (400) and evaluate on whether an answer is contained in them. For example, models retrieving from $\mathcal{D}_{50}$ return their top 8 passages, and ones retrieving from $\mathcal{D}_{100}$ retrieve top 4. The figure shows this recall@400 tokens across models. The relative performance of BM25-uni and DE-BERT is different from that seen in the ICT task, due to the semantic generalizations needed. Nevertheless, higher-dimensional DE-BERT models generally perform better, and multi-vector models provide further benefits, especially for longer-document collections; ME-BERT-768 outperforms DE-BERT-768 and ME-BERT-64 outperforms DE-BERT-512; Cross-Attention is still substantially stronger.

- Figure 5: Results on NQ passage recall as maximum passage length varies ( 50 to 400 tokens). Left: Reranking of 200 passages; Right: Open domain retrieval result on all of (English) Wikipedia. Exact numbers refer to Table 3.

---

Figure 5 (right) shows heldout set results for the task of retrieving from Wikipedia for each of the four document collections $\mathcal{D}_{l}$. Unlike the reranking setting, only higher-dimensional DE-BERT models outperform BM25 for passages longer than 50 . The hybrid models offer large improvements over their components, capturing both precise word overlap and semantic similarity. The gain from adding BM25 to ME-BERT and DE-BERT increases as the length of the documents in the collection grows, which is consistent with our expectations based on the theory.

## 7 Large-Scale Supervised IR

The previous experimental sections focused on understanding the relationship between compressive encoder representation dimensionality and document length. Here we evaluate whether our newly proposed multi-vector retrieval model ME-BERT, its corresponding dual encoder baseline DE-BERT, and sparse-dense hybrids compare favorably to state-of-the-art models for large-scale supervised retrieval and ranking on IR benchmarks.

---

Datasets. The MS MARCO passage ranking task focuses on ranking passages from a collection of about 8.8 mln . About 532 k queries paired with relevant passages are provided for training. The MS MARCO document ranking task is on ranking full documents instead. The full collection contains about 3 million documents and the training set has about 367 thousand queries. We report results on the passage and document development sets, comprising 6,980 and 5,193 queries, respectively in Table 1. We report MS MARCO and TREC DL 2019 (Craswell et al., 2020) test results in Table 2.

- Table 1: Development set results on MS MARCOPassage (MS-Passage), MS MARCO-Document (MSDoc) showing MRR@10.
- Table 2: Test set first-pass retrieval results on the passage and document TREC 2019 DL evaluation as well as MS MARCO eval MRR@10 (passage) and MRR@100 (document) under MRR(MS). 

---

Model Settings. For MS MARCO passage we apply models on the provided passage collections. For MS MARCO document, we follow Yan et al. (2020) and break documents into a set of overlapping passages with length up to 482 tokens, each including the document URL and title. For each task, we train the models on that task's training data only. We initialize the retriever and reranker models with BERT-large. We train dense retrieval models on positive and negative candidates from the 1000 -best list of BM25, additionally using one iteration of hard negative mining when beneficial. For ME-BERT, we used $m=3$ for the passage and $m=4$ for the document task.

---

Results. Table 1 comparatively evaluates our models on the dev sets of two tasks. The state of the art prior work follows the two-stage retrieval and reranking approach, where an efficient firststage system retrieves a (usually large) list of candidates from the document collection, and a second stage more expensive model such as crossattention BERT reranks the candidates.

---

Our focus is on improving the first stage, and we compare to prior works in two settings: Retrieval, top part of Table 1, where only first-stage efficient retrieval systems are used and Reranking, bottom part of the table, where more expensive secondstage models are employed to re-rank candidates. Figure 6 delves into the impact of the first-stage retrieval systems as the number of candidates the second stage reranker has access to is substantially reduced, improving efficiency.

- Figure 6: MRR@ 10 when reranking at different retrieval depth (10 to 1000 candidates) for MS MARCO.

---

We report results in comparison to the following systems: 1) Multi-Stage (Nogueira and Lin, 2019), which reranks BM25 candidates with a cascade of BERT models. 2) DOc2Query (Nogueira et al., 2019b) and DocT5Query (Nogueira and Lin, 2019), which use neural models to expand documents before indexing and scoring with sparse retrieval models. 3) DEEPCT (Dai and Callan, 2020b), which learns to map BERT's contextualized text representations to context-aware term weights. 4) HDCT (Dai and Callan, 2020a) uses a hierachical approach that combines passage-level term weights into document level term weights. 5) IDST, a two-stage cascade ranking pipeline by Yan et al. (2020), and 6) Leader-board, which is the best score on the MS MARCOpassage leaderboard as of Sept 18, 2020. ${ }^{7}$

---

We also compare our models to both our own BM25 implementation described in § 4.1, and external publicly available sparse model implementations, denoted with BM25-E. For the passage task, BM25-E is the Anserini (Yang et al., 2018a) system with default parameters. For the document task, BM25-E is the official IndriQueryLikelihood baseline. We report on dense-sparse hybrids using both our own BM25, and the external sparse systems; the latter hybrids are indicated by a suffix -E.

---

Looking at the top part of Table 1, we can see that our DE-BERT model already outperforms or is competitive with prior systems. The multi-vector model brings larger improvement on the dataset containing longer documents (MS MARCO document), and the sparse-dense hybrid models bring improvements over dense-only models on both datasets. According to a Wilcoxon signed rank test for statistical significance, all differences between De-bert, me-bert, De-hybrid-e, and ME-HYBRID-E are statistically significant on both development sets with $p$-value $<.0001$.

---

When a large number of candidates can be reranked, the impact of the first-stage system decreases. In the bottom part of the table we see that our models are comparable to systems reranking BM25 candidates.The accuracy of the first-stage system is particularly important when the cost of reranking a large set of candidates is prohibitive. Figure 6 shows the performance of systems that rerank a smaller number of candidates. We see that, when a very small number of candidates can be scored with expensive cross-attention models, the multi-vector ME-BERT and hybrid models achieve large improvements compared to prior systems on both MS MARCO tasks.

---

Table 2 shows test results for dense models, external sparse model baselines, and hybrids of the two (without reranking). In addition to test set (eval) results on the MS MARCO passage task, we report metrics on the manually annotated passage and document retrieval test set at TREC DL 2019. We report the fraction of unrated items as Holes@10 following Xiong et al. (2020).

---

Time and space analysis. Figure 7 compares the running time/quality trade-off curves for DEBERT and ME-BERT on the MS MARCO passage task using the ScaNN (Guo et al., 2020) library on a $160 \operatorname{Intel}(\mathrm{R}) \mathrm{Xeon}(\mathrm{R}) \mathrm{CPU}$ @ 2.20 GHz cores machine with 1.88 TB memory. Both models use one vector of size $k=1024$ per query; DEBERT uses one and ME-BERT uses 3 vectors of size $k=1024$ per document. The size of the document index for DE-BERT is 34.2 GB and the size of the index for ME-BERT is about 3 times larger. The indexing time was 1.52 h and 3.02 h for DE-BERT and ME-BERT, respectively. The ScaNN configuration we use is num_leaves $=5000$, and num_leaves_to_search ranges from 25 to 2000 (from less to more exact search) and time per query is measured when using parallel inference on all 160 cores. In the higher quality range of the curves, ME-BERT achieves substantially higher MRR than DE-BERT for the same inference time per query.

- Figure 7: Quality/running time tradeoff for DE-BERT and ME-BERT on the MS MARCO passage dev set. Dashed lines show quality with exact search.

## 8 Related work

We have mentioned research on improving the accuracy of retrieval models throughout the paper. Here we focus on work related to our central focus on the capacity of dense dual encoder representations relative to sparse bags-of-words.

---

In compressive sensing it is possible to recover a bag of words vector $x$ from the projection $A x$ for suitable $A$. Bounds for the sufficient dimensionality of isotropic Gaussian projections (Candes and Tao, 2005; Arora et al., 2018) are more pessimistic than the bound described in $\S 2$, but this is unsurprising because the task of recovering bags-of-words from a compressed measurement is strictly harder than recovering inner products.

---

Subramani et al. (2019) ask whether it is possible to exactly recover sentences (token sequences) from pretrained decoders, using vector embeddings that are added as a bias to the decoder hidden state. Because their decoding model is more expressive (and thus more computationally intensive) than inner product retrieval, the theoretical issues examined here do not apply. Nonetheless, Subramani et al. empirically observe a similar dependence between sentence length and embedding size. Wieting and Kiela (2019) represent sentences as bags of random projections, finding that high-dimensional projections ( $k=4096$ ) perform nearly as well as trained encoding models. These empirical results provide further empirical support for the hypothesis that bag-of-words vectors from real text are "hard to embed" in the sense of Larsen and Nelson (2017). Our contribution is to systematically explore the relationship between document length and encoding dimension, focusing on the case of exact inner product-based retrieval. We leave the combination of representation learning and approximate retrieval for future work.

## 9 Conclusion

Transformers perform well on an unreasonable range of problems in natural language processing. Yet the computational demands of large-scale retrieval push us to seek other architectures: crossattention over contextualized embeddings is too slow, but dual encoding into fixed-length vectors may be insufficiently expressive, sometimes failing even to match the performance of sparse bag-of-words competitors. We have used both theoretical and empirical techniques to characterize the fidelity of fixed-length dual encoders, focusing on the role of document length. Based on these observations, we propose hybrid models that yield strong performance while maintaining scalability.


## A Proofs

## A. 1 Lemma 1

Proof. For both distributions of embeddings, the error on the squared norm can be bounded with high probability (Achlioptas, 2003, Lemma 5.1):

$$
\begin{align*}
& \operatorname{Pr}\left(\|A A x\|^{2}-\|x\|^{2} \mid>\epsilon\|x\|^{2}\right) \\
& <2 \exp \left(-\frac{k}{2}\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)\right) \tag{4}
\end{align*}
$$

This bound implies an analogous bound on the absolute error of the inner product (Ben-David et al., 2002, corollary 19),

$$
\begin{align*}
& \operatorname{Pr}\left(|\langle A x, A y\rangle-\langle x, y\rangle| \geq \frac{\epsilon}{2}\left(\|x\|^{2}+\|y\|^{2}\right)\right) \\
& \leq 4 \exp \left(-\frac{k}{2}\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)\right) \tag{5}
\end{align*}
$$

---

Let $\bar{q}=q /\|q\|$ and $\bar{d}=\left(d_{1}-d_{2}\right) /\left\|d_{1}-d_{2}\right\|$. Then $\mu\left(q, d_{1}, d_{2}\right)=\langle\bar{q}, \bar{d}\rangle$. A ranking error occurs if and only if $\langle A \bar{q}, A \bar{d}\rangle \leq 0$, which implies $|\langle A \bar{q}, A \bar{d}\rangle-\langle\bar{q}, \bar{d}\rangle| \geq \epsilon$. By construction $\|\bar{q}\|=\|\bar{d}\|=1$, so the probability of an inner product distortion $\geq \epsilon$ is bounded by the righthand side of (5).

## A. 2 Corollary 1

Proof. We have $\epsilon=\mu\left(q, d_{1}, d_{2}\right)=\langle\bar{q}, \bar{d}\rangle \leq 1$ by the Cauchy-Schwarz inequality. For $\epsilon \leq 1$, we have $\epsilon^{2} / 6 \leq \epsilon^{2} / 2-\epsilon^{3} / 3$. We can then loosen the bound in (1) to $\beta \leq 4 \exp \left(-\frac{k}{2} \frac{\epsilon^{2}}{6}\right)$. Taking the natural $\log$ yields $\ln \beta \leq \ln 4-\epsilon^{2} k / 12$, which can be rearranged into $k \geq 12 \epsilon^{-2} \ln \frac{4}{\beta}$.

## A. 3 Lemma 2

Proof. For convenience define $\mu\left(d_{2}\right)=$ $\mu\left(q, d_{1}, d_{2}\right)$. Define $\epsilon$ as in the theorem statement, and $\mathcal{D}_{\epsilon}=\left\{d_{2} \in \mathcal{D}: \mu\left(q, d_{1}, d_{2}\right) \geq \epsilon\right\}$. We have

$$
\begin{aligned}
& \operatorname{Pr}\left(R \geq r_{0}\right) \leq \operatorname{Pr}\left(\exists d_{2} \in \mathcal{D}_{\epsilon}: A q_{1} \leq A q_{2}\right) \\
& \leq \sum_{d_{2} \in \mathcal{D}_{\epsilon}} 4 \exp \left(-\frac{k}{2}\left(\mu\left(d_{2}\right)^{2} / 2-\mu\left(d_{2}\right)^{3} / 3\right)\right) \\
& \leq 4\left|\mathcal{D}_{\epsilon}\right| \exp \left(-\frac{k}{2}\left(\epsilon^{2} / 2-\epsilon^{3} / 3\right)\right)
\end{aligned}
$$

The first inequality follows because the event $R \geq$ $r_{0}$ implies the event $\exists d_{2} \in \mathcal{D}_{\epsilon}: A q_{1} \leq A q_{2}$. The second inequality follows by a combination of Lemma 1 and the union bound. The final inequality follows because for any $d_{2} \in \mathcal{D}_{\epsilon}$, $\mu\left(q, d_{1}, d_{2}\right) \geq \epsilon$. The theorem follows because $\left|\mathcal{D}_{\epsilon}\right|=|\mathcal{D}|-r_{0}+1$.

## A. 4 Corollary 2

Proof. For the retrieval function $\max _{d}\langle q, d\rangle$, the minimum non-zero unnormalized margin $\left\langle q, d_{1}\right\rangle-\left\langle q, d_{2}\right\rangle$ is 1 when $q$ and $d$ are Boolean vectors. Therefore the normalized margin has lower bound $\mu\left(q, d_{1}, d_{2}\right) \geq 1 /\left(\|q\| \times\left\|d_{1}-d_{2}\right\|\right)$. For non-negative $d_{1}$ and $d_{2}$ we have $\left\|d_{1}-d_{2}\right\| \leq$ $\sqrt{\left\|d_{1}\right\|^{2}+\left\|d_{2}\right\|^{2}} \leq \sqrt{2 L_{D}}$. Preserving a normalized margin of $\epsilon=\left(2 L_{Q} L_{D}\right)^{-\frac{1}{2}}$ is therefore sufficient to avoid any pairwise errors. By plugging this value into Corollary 1, we see that setting $k \geq 24 L_{Q} L_{D} \ln \frac{4}{\beta}$ ensures that the probability of any pairwise error is $\leq \beta$.

## A. 5 Theorem 1

Proof. Recall that $\mu\left(q, d_{1}, d_{2}\right)=\frac{\left\langle q, d_{1}-d_{2}\right\rangle}{\|q\| \times\left\|d_{1}-d_{2}\right\|}$. By assumption we have $\left\langle q, d_{1}^{(i)}\right\rangle=\left\langle q, d_{1}\right\rangle$ and $\max _{j}\left\langle q, d_{2}^{(j)}\right\rangle \leq\left\langle q, d_{2}\right\rangle$, implying that

$$
\begin{equation*}
\left\langle q, d_{1}^{(i)}-d_{2}^{(i)}\right\rangle \geq\left\langle q, d_{1}-d_{2}\right\rangle \tag{6}
\end{equation*}
$$

In the denominator, we expand $\left\|d_{1}-d_{2}\right\|=$ $\left\|\left(d_{1}^{(i)}-d_{2}^{(i)}\right)+\left(d_{1}^{(\neg i)}-d_{2}^{(\neg i)}\right)\right\|$, where $d^{(\neg i)}=$ $\sum_{j \neq i} d^{(j)}$. Plugging this into the squared norm,

$$
\begin{align*}
& \left\|d_{1}-d_{2}\right\|^{2} \\
& =\left\|\left(d_{1}^{(i)}-d_{2}^{(i)}\right)+\left(d_{1}^{(\neg i)}-d_{2}^{(\neg i)}\right)\right\|^{2}  \tag{7}\\
& =\left\|d_{1}^{(i)}-d_{2}^{(i)}\right\|^{2}+\left\|d_{1}^{(\neg i)}-d_{2}^{(\neg i)}\right\|^{2} \\
& \quad+2\left\langle d_{1}^{(i)}-d_{2}^{(i)}, d_{1}^{(\neg i)}-d_{2}^{(\neg i)}\right\rangle  \tag{8}\\
& =\left\|d_{1}^{(i)}-d_{2}^{(i)}\right\|^{2}+\left\|d_{1}^{(\neg i)}-d_{2}^{(\neg i)}\right\|^{2}  \tag{9}\\
& \geq\left\|d_{1}^{(i)}-d_{2}^{(i)}\right\|^{2} . \tag{10}
\end{align*}
$$

The inner product $\left\langle d_{1}^{(i)}-d_{2}^{(i)}, d_{1}^{(\neg i)}-d_{2}^{(\neg i)}\right\rangle=0$ because the segments are orthogonal. The combination of (6) and (10) completes the theorem.
