# Efficient Constant-Space Multi-Vector Retrieval 


#### Abstract

Multi-vector retrieval methods, exemplified by the ColBERT architecture, have shown substantial promise for retrieval by providing strong trade-offs in terms of retrieval latency and effectiveness. However, they come at a high cost in terms of storage since a (potentially compressed) vector needs to be stored for every token in the input collection. To overcome this issue, we propose encoding documents to a fixed number of vectors, which are no longer necessarily tied to the input tokens. Beyond reducing the storage costs, our approach has the advantage that document representations become of a fixed size on disk, allowing for better OS paging management. Through experiments using the MSMARCO passage corpus and BEIR with the ColBERT-v2 architecture, a representative multi-vector ranking model architecture, we find that passages can be effectively encoded into a fixed number of vectors while retaining most of the original effectiveness.

## 1 Introduction

Pre-trained contextualized language models, such as BERT [7], learn semantic embeddings from word contexts, enabling them to better capture the relevance of documents with respect to the queries. Notably, they outperform classical ranking approaches [16]. More specifically, cross-encoders concatenate a query and a document texts, and feed them into BERT to compute the query document similarity scores, while bi-encoders compute compact representations of documents as real-valued vectors, both for queries and documents, and the query-document similarity is computed using the cosine similarity or the inner product between query and document embeddings.

---

Cross-encoder can be computationally expensive for estimating query-document similarities due to the complexity of the underlying transformer neural network [10, 11, 13, 27]. On the other side, bi-encoders are much more efficient from a computational perspective, since all document embeddings can be precomputed offline and store in specialised vector indexes such as FAISS [12]. Instead of relying on a single vector per text as done by bi-encoders, multirepresentation systems such as [13] use a vector per token in a text, being able to capture more semantics than a single embedding. While ColBERT achieves more effective results than single representations, it comes at the cost of higher response times and memory usage [18].

---

ColBERTv2 [21] employs a compression method that leverages centroids to represent passage embeddings more efficiently. This method records the ID of the nearest centroid for each embedding and compresses the residuals - the differences between the original embeddings and the centroids-using. This compression strategy helps reduce the storage demands of multi-vector embeddings, but makes retrieval significantly less efficient. To speed up the search latency of ColBERTv2, PLAID [22] uses a centroid interaction mechanism and centroid pruning to eliminate low-scoring passages early in the search process, thus reducing response times significantly. This approach allows multi-vector retrieval models to maintain retrieval quality while reducing retrieval latency.

---

XTR (ConteXtualized Token Retriever) [15] introduces a streamlined approach to multi-vector retrieval by emphasizing efficient token selection during retrieval. Unlike ColBERT's three-stage process (token retrieval, gathering, scoring), XTR simplifies the retrieval pipeline by training the model to prioritize key document tokens, thus only scoring based on these retrieved tokens. Another recent approach worth mentioning is Static Pruning for Multi-Representation Dense Retrieval [1], which addresses the storage challenges in multi-vector models like ColBERT by pruning embeddings for less impactful tokens. By adapting static pruning techniques traditionally used in sparse indexes to embeddingbased indexes, this method reduces storage requirements while maintaining retrieval effectiveness In addition to pruning techniques, a recent advancement named Token Pooling [4] aims to reduce the storage requirements of multi-vector retrieval models like ColBERT. This approach clusters and pools similar token embeddings at indexing time, substantially lowering vector counts without model modifications or query-time processing.

---

Multi-vector models like ColBERT are also highly effective as reranking techniques [17], where they refine the ranking of a candidate set generated by simpler retrieval methods, such as BM25. In this setting, ColBERT's pre-computed, token-level document embeddings allow for efficient late interaction with the query, balancing computational efficiency with strong retrieval performance. This approach leverages the richness of multi-vector representations while maintaining low latency, as document vectors can be cached across queries.

---

Recently, MUVERA (Multi-Vector Retrieval Algorithm) [8 introduced a mechanism to bridge the gap between single-vector and multi-vector retrieval. MUVERA employs Fixed Dimensional Encodings (FDEs) to approximate multivector similarities, enabling the use of optimized Maximum Inner Product Search (MIPS) solvers. This approach significantly enhances retrieval efficiency compared to methods like PLAID. Although MUVERA achieves a good approximation of PLAID using a single-vector representation and MIPS operations, which makes its retrieval algorithm faster, it is able to do so only by employing large high-dimensional vectors. This results in substantial memory consumption, presenting a trade-off between retrieval speed and storage efficiency.

---

In this paper, we propose a novel approach, ConstBERT, to reduce the storage footprint of multi-vector retrieval by encoding each document with a fixed, smaller set of learned embeddings. Instead of relying on token-level embeddings across all document tokens, we introduce a pooling mechanism that projects these token embeddings into a reduced set of document-level embeddings, each capturing distinct semantic facets. This learned pooling reduces the number of embeddings stored per document, achieving considerable space savings in the index while retaining retrieval effectiveness. The fixed number of vectors per document also eases the use of ConstBERT as a reranking method, simplifying integration with initial retrieval systems and allowing efficient late interaction with pre-computed document representations. This reduction is complementary to other methods, such as dimensionality reduction, and enables efficient memory alignment with OS-level paging, ultimately improving both storage efficiency and query processing speed.

## 2 ConstBERT: Multi-Vector Compression

Given a query $q$, our task is to retrieve relevant documents $d$ from a corpus $D$ by ranking them with a relevance scoring function $s(q, d)$. Queries and documents are sequences of tokens from a given vocabulary. Each document $d$ comprises $M$ tokens, and each query $q$ comprises $N$ tokens, with padding/masking tokens if necessary. In a multi-representation dense IR system, any token is represented by a $k$-dimensional real-valued vector, called embedding. Let $q_{1}, \ldots, q_{N}$ denote the token embeddings for the query $q$, and $d_{1}, \ldots, d_{M}$ denote the token embeddings for the document $d$. The relevance score $s(q, d)$ between the query $q$ and the document $d$ is computed with a late interaction mechanism:

$$
s(q, d)=\sum_{i=1}^{N} \max _{j=1, \ldots, M} q_{i}^{T} d_{j}
$$

---

This late interaction mechanism sums up the contributions of the most relevant document token for each query token. For each query token, the max operator can be interpreted as an heuristic pooling mechanism over the token embeddings of the document. Instead of relying on this heuristic pooling across all document tokens, in this paper we propose a new learned pooling, where instead of using the document embeddings $d_{1}, \ldots, d_{M}$, i.e., an embedding per document token, we use $C<M$ new embeddings $\delta_{1}, \ldots, \delta_{C}$, that are learned with an additional projection layer with parameters $W \in \mathbb{R}^{M k \times C k}$ :
$$
\left[\delta_{1}|\cdots| \delta_{C}\right]=W^{T}\left[d_{1}|\cdots| d_{M}\right]
$$

---

This layer is learned end-to-end during training takes as input the token embeddings of a document computed by the multi-representation dense IR system, and with a linear transformation projects them in a fixed number of embeddings, of the same dimensions. In doing so, the new embeddings can be seen as different single document-level embeddings, each one encoding some semantic facet of the
document, given its token embeddings. The relevance score $s(q, d)$ between the query $q$ and the document $d$ is computed now as:
$$
s(q, d)=\sum_{i=1}^{N} \max _{j=1, \ldots, C} q_{i}^{T} \delta_{j}
$$

As a result, the total number of embeddings per document to store in the embedding index decreases by a factor $M / C$. This reduces the space required to store the index on disk and in main memory, as well as the query processing time. This space reduction is orthogonal to any further space reduction obtained, for example, by reducing the number of dimensions $k$ per embedding. Both reductions can be further exploited to align the space occupancy per document to the memory page size, so to exploit more efficiently the underlying memory management mechansisms provided by the operating system.

## 3 Experimental Results

In this section, we evaluate the performance and efficiency of our proposed fixedvector model, denoted as ConstBERT ${ }_{C}$, where $C$ represents the number of fixed embeddings per document. We compare it against the baseline, CoIBERT, which uses token-level embeddings for each document token.

### 3.1 Experimental Setup

Datasets $\mathfrak{G}$ Queries. Our experimental framework utilizes the MSMARCO v1 passage corpus [2, which consists of approximately 8.8 million passages. To assess both the effectiveness and efficiency of query processing, we benchmark our approach against established methodologies using the MSMARCO Dev Queries, as well as datasets from the TREC Deep Learning Tracks of 2019 and 2020 [5, 6]. Furthermore, we conduct evaluations on an additional 13 collections drawn from the BEIR benchmark [23], allowing for a comprehensive analysis across datasets.

---

Metrics. To evaluate effectiveness, we employ the official metrics designated for each query set: Mean Reciprocal Rank at cutoff 10 (MRR@10) for MSMARCO Dev queries and Normalized Discounted Cumulative Gain at cutoff 10 (NDCG@10) for both TREC queries and the BEIR benchmark. Additionally, we report recall across varying cutoff thresholds for the MSMARCO experiments. For efficiency analysis, we compute the Mean Response Time (MRT) for both the Dev and TREC queries, measured in milliseconds. Additionally, we examine the index sizes to demonstrate the substantial storage efficiency gains achieved by our method.

---

Implementations. ConstBERT has been trained following the approach proposed by Santhanam et al. [21. ColBERT $S_{P P}$ refers to the modification of CoIBERT proposed by Acquavia et al. [1], where token embeddings are statically pruned at indexing time. RetroMAE [26] is a single-representation dense retrieval model for which we have used the official checkpoint $t^{4}$.ã€

---

Platform. All experiments were carried out in memory on a Linux system, using a single processing thread. The hardware configuration included dual 2.8 GHz Intel Xeon CPUs and 256 GiB of RAM. For end-to-end retrieval experiments, we used the official PLAID [17, 22] codebas ${ }^{5}$. In our two-stage retrieval experiments, we used BMP [20] and efficient SPLADE [14] for candidate generation and then we performed reranking using our ConstBERT 32 model. We tested other learned sparse models as first-stage retrieval methods, such as DeeperImpact [3, 19], and obtained similar results; however, we did not include them in the main results section due to limited space. Our code is written in Python and is available at https://github.com/pisa-engine/ConstBERT.

### 3.2 Overall

Table 1 presents the results on MSMARCO with different configurations of ConstBERT $C_{C}$ (varying the number of fixed embeddings per document $C$ ) and the baseline CoIBERT. As expected, ConstBERT's performance improves with larger token-level configurations, but at the cost of substantial increases in index size. Our proposed ConstBERT 32 model achieves comparable MRR on the development set and NDCG@10 on both TREC 2019 and TREC 2020 benchmarks, while using a fixed number of vectors per document, which allows for more efficient storage. ConstBERT $C_{C}$ has a variety of tradeoffs compared to the existing static pruning approach ColBERT ${ }_{S P}$. The performance of ColBERT ${ }_{S P}$ lines on the storage-effectiveness Pareto frontier set by various settings of $C$. On the one hand, ConstBERT $C_{C}$ offers advantages in flexibility since it can be tuned directly to a target (and constant-space) representation. On the other hand, it requires re-training learn the weights $W$, while CoIBERT $S_{S P}$ does not require retraining.

---

To evaluate the robustness of ConstBERT $C_{C}$ across different retrieval tasks, we further assess it on the BEIR benchmark (Table 2). The results highlight that our model performs competitively with CoIBERTon most tasks, achieving comparable or even superior NDCG@10 scores while requiring much less storage.

---

A major advantage of our fixed-vector approach lies in its reduced storage footprint.Unlike CoIBERT, which scales linearly with the number of token embeddings per document, ConstBERT $C_{C}$ maintains a consistent index size by using a fixed number of embeddings. This efficiency extends across the BEIR datasets, with our approach consistently reducing index sizes by over $50 \%$ compared to CoIBERTat equivalent effectiveness.

---

The reduced storage requirement of ConstBERT $C_{C}$ also translates into lower memory usage and faster retrieval times, as fewer embeddings need to be processed per query-document pair. This efficiency gain is particularly advantageous when using ConstBERT $C_{C}$ as a reranking method, where computational speed is crucial.

### 3.3 Reranking

In Table 3, we evaluate the performance of using ConstBERT as a reranking model instead of employing it in an end-to-end retrieval system. Specifically, we compare PLAID with a two-stage retrieval process incorporating ESPLADE as the model used for candidate generation and our ConstBERT ${ }_{32}$. The results show the retrieval effectiveness in terms of MRR and nDCG@10, and the average computational efficiency represented by MRT. Combining ESPLADE with our lightweight CoIBERT 32 version balances both performance and efficiency. This combination improves nDCG@10 close to that of the standalone CoIBERT while maintaining MRT below 6 ms , showcasing a practical trade-off. Const$\mathrm{BERT}_{32}$ is particularly advantageous not only due to its smaller index size but also because all documents are embedded with the same number of vectors. This uniformity simplifies implementation and allows for optimized memory usage through aligned memory reads, thereby leveraging the underlying memory management mechanisms provided by the operating system more efficiently.

## 4 Conclusion

Our experimental results demonstrate that the fixed-vector approach, ConstBERT, effectively balances retrieval effectiveness and storage efficiency. By encoding each document with a fixed, smaller set of learned embeddings, our proposal achieves competitive performance across TREC and BEIR benchmarks, while substantially reducing index size and computational demands. This makes it a scalable and practical solution for real-world information retrieval applications, where both storage efficiency and retrieval speed are essential.

---

There are various opportunities for future work in this space. For instance, several studies related to the interpretability of late interaction models have been conducted, given their alignment between tokens and their corresponding representations (e.g., [9, 18, 25]). With our approach, this direct vector-token alignment is no longer present. However, there still may be ways to interpret the interactions, so it may be worth revisiting these studies. Another interesting direction is the application of Pseudo-Relevance Feedback (PRF) with late interaction models (e.g., [24]). Future studies could explore whether our approach is complementary to PRF.
