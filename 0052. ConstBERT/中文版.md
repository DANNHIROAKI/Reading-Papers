# 高效恒定空间多向量检索

#### 摘要

以 ColBERT 架构为代表的多向量检索方法在检索延迟和效果之间提供了良好的权衡，展现了巨大的潜力。然而，由于需要为输入集合中的每个Token存储一个（可能经过压缩的）向量，它们在存储方面成本较高。为了解决这一问题，我们提出将文档编码为固定数量的向量，这些向量不再必然与输入Token绑定。除了降低存储成本外，我们的方法还具有一个优势，即文档表示在磁盘上具有固定大小，从而能够更好地利用操作系统分页管理。通过在 MSMARCO 段落语料库和 BEIR 上使用 ColBERT-v2 架构（一种代表性的多向量排序模型架构）进行实验，我们发现段落可以有效地编码为固定数量的向量，同时保留大部分原始效果。

## 1 引言

预训练的上下文语言模型，如 BERT [7]，能够从词语上下文中学习语义嵌入，从而更好地捕捉文档与查询的相关性。值得注意的是，它们的表现优于传统的排序方法 [16]。具体来说，交叉编码器将查询和文档文本拼接在一起，并将其输入 BERT 以计算查询与文档的相似度得分，而双编码器则计算文档的紧凑表示作为实值向量，同时为查询和文档生成嵌入，查询与文档的相似度通过余弦相似度或查询与文档嵌入的内积来计算。

---

由于底层 Transformer 神经网络的复杂性 [10, 11, 13, 27]，交叉编码器在估计查询与文档相似度时可能计算成本较高。另一方面，双编码器在计算上更为高效，因为所有文档嵌入都可以离线预计算并存储在专门的向量索引中，如 FAISS [12]。与双编码器使用单个向量表示文本不同，多表示系统（如 [13]）使用文本中每个Token的向量，能够比单个嵌入捕捉更多的语义。虽然 ColBERT 比单一表示方法取得了更有效的效果，但它也带来了更高的响应时间和内存使用成本 [18]。

---

ColBERTv2 [21] 采用了一种压缩方法，利用质心来更高效地表示段落嵌入。该方法记录每个嵌入的最近质心 ID，并压缩残差（即原始嵌入与质心之间的差异）。这种压缩策略有助于减少多向量嵌入的存储需求，但也显著降低了检索效率。为了加快 ColBERTv2 的搜索延迟，PLAID [22] 使用质心交互机制和质心剪枝技术，在搜索过程中尽早消除低分段落，从而显著减少响应时间。这种方法使多向量检索模型能够在保持检索质量的同时降低检索延迟。

---

XTR（上下文Token检索器）[15] 提出了一种简化的多向量检索方法，通过在检索过程中高效选择Token来优化流程。与 ColBERT 的三阶段过程（Token检索、收集、评分）不同，XTR 通过训练模型优先选择关键文档Token，从而简化了检索流程，仅基于这些检索到的Token进行评分。另一个值得提及的方法是《多表示稠密检索的静态剪枝》[1]，它通过剪枝对影响较小的Token的嵌入，解决了像 ColBERT 这样的多向量模型中的存储挑战。通过将传统用于稀疏索引的静态剪枝技术应用于基于嵌入的索引，该方法在保持检索效果的同时减少了存储需求。此外，一项名为《Token池化》[4] 的最新进展旨在减少像 ColBERT 这样的多向量检索模型的存储需求。该方法在索引时对相似的Token嵌入进行聚类和池化，从而在不修改模型或进行查询时处理的情况下大幅减少向量数量。

---

像 ColBERT 这样的多向量模型在重排序技术中也表现出色 [17]，它们能够优化由更简单检索方法（如 BM25）生成的候选集排序。在这种情况下，ColBERT 预计算的Token级文档嵌入允许与查询进行高效的延迟交互，在计算效率和强大检索性能之间取得平衡。这种方法充分利用了多向量表示的丰富性，同时保持了低延迟，因为文档向量可以在多个查询之间缓存。

---

最近，MUVERA（多向量检索算法）[8] 引入了一种机制，以弥合单向量和多向量检索之间的差距。MUVERA 使用固定维度编码（FDEs）来近似多向量相似性，从而能够使用优化的最大内积搜索（MIPS）求解器。与 PLAID 等方法相比，这种方法显著提高了检索效率。尽管 MUVERA 通过使用单向量表示和 MIPS 操作实现了对 PLAID 的良好近似，使其检索算法更快，但这是通过使用大型高维向量实现的。这导致了大量的内存消耗，在检索速度和存储效率之间形成了权衡。

---

在本文中，我们提出了一种新颖的方法 ConstBERT，通过使用固定数量的学习嵌入对每个文档进行编码，以减少多向量检索的存储占用。与依赖所有文档Token的Token级嵌入不同，我们引入了一种池化机制，将这些Token嵌入投影为一组减少的文档级嵌入，每个嵌入捕捉不同的语义方面。这种学习池化减少了每个文档存储的嵌入数量，在保持检索效果的同时实现了显著的索引空间节省。每个文档的固定向量数量也简化了 ConstBERT 作为重排序方法的使用，便于与初始检索系统集成，并允许与预计算文档表示进行高效的延迟交互。这种减少与其他方法（如降维）互补，并能够与操作系统级分页进行高效的内存对齐，最终提高存储效率和查询处理速度。

## 2 ConstBERT: 多向量压缩

给定一个查询 $q$，我们的任务是从语料库 $D$ 中检索相关文档 $d$，并通过一个相关性评分函数 $s(q, d)$ 对它们进行排序。查询和文档都是由给定词汇表中的Token序列组成的。每个文档 $d$ 包含 $M$ 个Token，每个查询 $q$ 包含 $N$ 个Token，必要时可以填充/掩码Token。在多表示稠密信息检索系统中，每个Token都由一个 $k$ 维的实值向量表示，称为嵌入。令 $q_{1}, \ldots, q_{N}$ 表示查询 $q$ 的Token嵌入，$d_{1}, \ldots, d_{M}$ 表示文档 $d$ 的Token嵌入。查询 $q$ 和文档 $d$ 之间的相关性评分 $s(q, d)$ 通过一种延迟交互机制计算：

$$
s(q, d)=\sum_{i=1}^{N} \max _{j=1, \ldots, M} q_{i}^{T} d_{j}
$$

---

这种延迟交互机制将对每个查询Token最相关的文档Token的贡献进行求和。对于每个查询Token，max 操作符可以解释为对文档Token嵌入的一种启发式池化机制。在本文中，我们提出了一种新的学习池化方法，不再依赖于对所有文档Token的启发式池化，而是使用 $C<M$ 个新的嵌入 $\delta_{1}, \ldots, \delta_{C}$，这些嵌入通过一个附加的投影层进行学习，该投影层的参数为 $W \in \mathbb{R}^{M k \times C k}$：
$$
\left[\delta_{1}|\cdots| \delta_{C}\right]=W^{T}\left[d_{1}|\cdots| d_{M}\right]
$$

---

该层在训练过程中端到端地学习，输入是由多表示稠密信息检索系统计算的文档Token嵌入，并通过线性变换将它们投影到固定数量的嵌入中，这些嵌入的维度相同。通过这种方式，新的嵌入可以被视为不同的单文档级嵌入，每个嵌入都编码了文档的某些语义方面，基于其Token嵌入。现在，查询 $q$ 和文档 $d$ 之间的相关性评分 $s(q, d)$ 计算如下：
$$
s(q, d)=\sum_{i=1}^{N} \max _{j=1, \ldots, C} q_{i}^{T} \delta_{j}
$$

因此，每个文档在嵌入索引中存储的嵌入总数减少了 $M / C$ 倍。这减少了在磁盘和主内存中存储索引所需的空间，以及查询处理时间。这种空间减少与通过减少每个嵌入的维度 $k$ 所获得的任何进一步空间减少是正交的。这两种减少都可以进一步利用，以将每个文档的空间占用与内存页大小对齐，从而更有效地利用操作系统提供的内存管理机制。

## 3 实验结果

在本节中，我们评估了所提出的固定向量模型（称为 ConstBERT ${ }_{C}$，其中 $C$ 表示每个文档的固定嵌入数量）的性能和效率，并将其与使用每个文档Token的Token级嵌入的基线模型 ColBERT 进行比较。

### 3.1 实验设置

**数据集与查询**。我们的实验框架使用了 MSMARCO v1 段落语料库 [2]，该语料库包含约 880 万条段落。为了评估查询处理的效果和效率，我们使用 MSMARCO 开发查询以及 2019 年和 2020 年 TREC 深度学习赛道的数据集 [5, 6] 对方法进行基准测试。此外，我们还使用 BEIR 基准 [23] 中的 13 个数据集进行了评估，以便在不同数据集上进行全面分析。

---

**评估指标**。为了评估效果，我们采用了每个查询集的官方指标：MSMARCO 开发查询的截断 10 的平均倒数排名（MRR@10），以及 TREC 查询和 BEIR 基准的截断 10 的归一化折扣累积增益（NDCG@10）。此外，我们还报告了 MSMARCO 实验中不同截断阈值下的召回率。对于效率分析，我们计算了开发查询和 TREC 查询的平均响应时间（MRT），单位为毫秒。此外，我们还检查了索引大小，以展示我们方法在存储效率方面的显著提升。

---

**实现细节**。ConstBERT 的训练遵循了 Santhanam 等人 [21] 提出的方法。ColBERT $S_{P P}$ 指的是 Acquavia 等人 [1] 提出的对 ColBERT 的修改，即在索引时对Token嵌入进行静态剪枝。RetroMAE [26] 是一种单表示稠密检索模型，我们使用了其官方检查点 $t^{4}$。

---

**实验平台**。所有实验均在 Linux 系统的内存中运行，使用单线程处理。硬件配置包括双路 2.8 GHz Intel Xeon CPU 和 256 GiB 内存。对于端到端检索实验，我们使用了官方的 PLAID [17, 22] 代码库 ${ }^{5}$。在我们的两阶段检索实验中，我们使用 BMP [20] 和高效的 SPLADE [14] 进行候选生成，然后使用我们的 ConstBERT 32 模型进行重排序。我们还测试了其他学习稀疏模型作为第一阶段的检索方法，例如 DeeperImpact [3, 19]，并获得了类似的结果；但由于篇幅有限，我们未将其纳入主要结果部分。我们的代码使用 Python 编写，可在 https://github.com/pisa-engine/ConstBERT 获取。

### 3.2 总体结果

表 1 展示了不同配置的 ConstBERT $C_{C}$（改变每个文档的固定嵌入数量 $C$）与基线模型 ColBERT 在 MSMARCO 上的结果。正如预期的那样，ConstBERT 的性能随着更大Token级配置的提升而提高，但代价是索引大小的显著增加。我们提出的 ConstBERT 32 模型在开发集上的 MRR 以及 TREC 2019 和 TREC 2020 基准上的 NDCG@10 表现与 ColBERT 相当，同时每个文档使用固定数量的向量，从而实现了更高效的存储。与现有的静态剪枝方法 ColBERT ${ }_{S P}$ 相比，ConstBERT $C_{C}$ 具有多种权衡。ColBERT ${ }_{S P}$ 的性能落在由不同 $C$ 设置所定义的存储-效果帕累托前沿上。一方面，ConstBERT $C_{C}$ 在灵活性方面具有优势，因为它可以直接调整为目标（且恒定空间）表示。另一方面，它需要重新训练以学习权重 $W$，而 ColBERT $S_{S P}$ 则不需要重新训练。

---

为了评估 ConstBERT $C_{C}$ 在不同检索任务中的鲁棒性，我们进一步在 BEIR 基准上进行了评估（表 2）。结果表明，我们的模型在大多数任务中与 ColBERT 表现相当，甚至在某些任务上实现了更高的 NDCG@10 分数，同时所需的存储空间大大减少。

---

我们固定向量方法的一个主要优势在于其减少的存储占用。与 ColBERT 不同，后者随每个文档的Token嵌入数量线性扩展，而 ConstBERT $C_{C}$ 通过使用固定数量的嵌入保持了恒定的索引大小。这种效率在 BEIR 数据集中得到了体现，与 ColBERT 相比，我们的方法在同等效果下始终将索引大小减少了超过 $50\%$。

---

ConstBERT $C_{C}$ 减少的存储需求还转化为更低的内存使用和更快的检索时间，因为每个查询-文档对需要处理的嵌入数量更少。这种效率提升在使用 ConstBERT $C_{C}$ 作为重排序方法时尤为有利，因为计算速度至关重要。

### 3.3 重排序

在表 3 中，我们评估了将 ConstBERT 用作重排序模型而非端到端检索系统的性能。具体来说，我们将 PLAID 与一个两阶段检索过程进行比较，该过程使用 ESPLADE 作为候选生成模型，并结合我们的 ConstBERT ${ }_{32}$。结果展示了以 MRR 和 nDCG@10 表示的检索效果，以及以 MRT 表示的平均计算效率。将 ESPLADE 与我们轻量级的 ConstBERT 32 版本结合，在性能和效率之间取得了平衡。这种组合将 nDCG@10 提升至接近独立 ColBERT 的水平，同时将 MRT 保持在 6 毫秒以下，展示了一种实用的权衡。ConstBERT ${ }_{32}$ 的优势不仅在于其较小的索引大小，还在于所有文档都使用相同数量的向量进行嵌入。这种一致性简化了实现，并通过对齐的内存读取优化了内存使用，从而更高效地利用了操作系统提供的内存管理机制。

## 4 结论

我们的实验结果表明，固定向量方法 ConstBERT 有效地平衡了检索效果和存储效率。通过使用固定数量的学习嵌入对每个文档进行编码，我们的提案在 TREC 和 BEIR 基准测试中实现了具有竞争力的性能，同时显著减少了索引大小和计算需求。这使得它成为现实世界信息检索应用中的可扩展且实用的解决方案，其中存储效率和检索速度都至关重要。

---

在这一领域还有许多未来工作的机会。例如，鉴于延迟交互模型在Token及其对应表示之间的一致性，已经进行了几项关于其可解释性的研究（例如 [9, 18, 25]）。在我们的方法中，这种直接的向量-Token对齐不再存在。然而，可能仍有方法可以解释这些交互，因此值得重新审视这些研究。另一个有趣的方向是将伪相关性反馈（PRF）与延迟交互模型结合应用（例如 [24]）。未来的研究可以探索我们的方法是否与 PRF 互补。
