# 0. Abstract

我们介绍了一种新的语言表示模型，称为 BERT（Bidirectional Encoder Representations from Transformers，即双向编码器表示模型）。与近期的语言表示模型（Peters 等，2018a；Radford 等，2018）不同，BERT 被设计为通过在所有层中==联合考虑左侧和右侧上下文==，从无标注文本中预训练深度双向表示。因此，预训练好的 BERT 模型仅需通过添加一个额外的输出层进行微调，就能为广泛的任务（例如问答和语言推理）创建最先进的模型，而无需进行复杂的任务特定架构修改。

BERT 在概念上简单且在实践中非常强大。它在十一项自然语言处理任务中取得了新的最先进成果，包括将 GLUE 的评分提升至 80.5%（绝对提升 7.7 个百分点）、MultiNLI 的准确率提升至 86.7%（绝对提升 4.6 个百分点）、SQuAD v1.1 问答测试的 F1 分数提升至 93.2（绝对提升 1.5 个百分点），以及 SQuAD v2.0 测试的 F1 分数提升至 83.1（绝对提升 5.1 个百分点）。

# 1. Introduction  

语言模型的预训练已被证明能有效提升许多自然语言处理任务的表现（Dai 和 Le，2015；Peters 等，2018a；Radford 等，2018；Howard 和 Ruder，2018）。这些任务包括句子级别的任务，如自然语言推理（Bowman 等，2015；Williams 等，2018）和释义（Dolan 和 Brockett，2005），其目标是通过整体分析句子来预测句子之间的关系，以及词级别的任务，如命名实体识别和问答（Tjong Kim Sang 和 De Meulder，2003；Rajpurkar 等，2016），这些任务要求模型在词级别生成细粒度的输出。

目前有两种将预训练语言表示应用于下游任务的策略：==基于特征的方法==和==微调方法==。基于特征的方法，例如 ELMo（Peters 等，2018a），使用==任务特定的架构==，并将预训练的表示作为额外特征加入其中。而微调方法，例如生成式预训练 Transformer（OpenAI GPT）（Radford 等，2018），仅引入极少的任务特定参数，通过微调所有预训练参数来完成下游任务。这两种方法在预训练期间共享相同的目标函数，使用单向语言模型学习通用的语言表示。

我们认为当前的技术限制了预训练表示的能力，特别是对于微调方法。主要的限制在于，标准的语言模型是单向的，这限制了在预训练中可以使用的架构选择。例如，在 OpenAI GPT 中，作者使用了一种从左到右的架构，在 Transformer 的自注意力层中，每个词元只能关注先前的词元。这种限制对句子级别任务来说并不理想，而在应用基于微调的方法到诸如问答这样的词级别任务时可能会造成很大的问题，因为在这些任务中，整合来自两个方向的上下文是至关重要的。  

在本文中，我们通过提出 BERT（Bidirectional Encoder Representations from Transformers，即双向编码器表示模型）改进了基于微调的方法。BERT 通过使用“掩码语言模型”（Masked Language Model，MLM）的预训练目标，解决了前述的单向性限制，这一目标灵感来源于 Cloze 任务（Taylor，1953）。掩码语言模型会随机遮掩输入中的一些词元，其目标是仅基于上下文预测被遮掩词的原始词汇 ID。与从左到右的语言模型预训练不同，MLM 目标使表示能够融合左侧和右侧的上下文，从而允许我们预训练深度双向 Transformer。除了掩码语言模型，我们还使用了一个“下一句预测”（Next Sentence Prediction）任务，用以联合预训练文本对的表示。

本文的主要贡献如下：

- 我们证明了双向预训练在语言表示中的重要性。与 Radford 等（2018）使用单向语言模型进行预训练不同，BERT 使用掩码语言模型以实现预训练的深度双向表示。这也不同于 Peters 等（2018a），后者是对独立训练的从左到右和从右到左的语言模型进行浅层拼接。
- 我们表明预训练的表示减少了对许多高度工程化的任务特定架构的需求。BERT 是第一个基于微调的表示模型，在大量句子级别和词级别任务中取得了最先进的性能，超越了许多任务特定的架构。
- BERT 在十一项自然语言处理任务中提升了技术水平。代码和预训练模型可在 https://github.com/google-research/bert 获取。

# 2. Related Work  

通用语言表示预训练有着悠久的历史，本节我们将简要回顾最广泛使用的方法。

## 2.1. 无监督的基于特征的方法

几十年来，研究广泛适用的词表示一直是一个活跃的领域，包括非神经网络方法（Brown 等，1992；Ando 和 Zhang，2005；Blitzer 等，2006）和神经网络方法（Mikolov 等，2013；Pennington 等，2014）。预训练的词嵌入是现代自然语言处理系统的核心部分，与从零开始学习的嵌入相比，预训练词嵌入可以显著提高性能（Turian 等，2010）。在预训练词嵌入向量时，研究者使用了从左到右语言建模目标（Mnih 和 Hinton，2009），以及根据左侧和右侧上下文来区分正确词和错误词的目标（Mikolov 等，2013）。

这些方法被推广到更粗粒度的层面，例如句子嵌入（Kiros 等，2015；Logeswaran 和 Lee，2018）或段落嵌入（Le 和 Mikolov，2014）。为了训练句子表示，已有的工作使用了排序候选下一句的目标（Jernite 等，2017；Logeswaran 和 Lee，2018），基于前一句表示生成下一句词的目标（Kiros 等，2015），或基于去噪自编码器的目标（Hill 等，2016）。

ELMo 及其前身（Peters 等，2017，2018a）从不同的维度扩展了传统词嵌入研究。它们从从左到右和从右到左的语言模型中提取上下文敏感的特征。每个词元的上下文表示是从左到右和从右到左表示的拼接。当将上下文词嵌入与现有任务特定架构相结合时，ELMo 推进了几个主要 NLP 基准的技术水平（Peters 等，2018a），包括问答（Rajpurkar 等，2016）、情感分析（Socher 等，2013）和命名实体识别（Tjong Kim Sang 和 De Meulder，2003）。Melamud 等（2016）提出通过一个基于 LSTM 的任务从左右上下文预测单个词以学习上下文表示。与 ELMo 类似，他们的模型是基于特征的，而不是深度双向的。Fedus 等（2018）表明，填空任务可以用来提高文本生成模型的鲁棒性。

## 2.2. 无监督的微调方法

与基于特征的方法类似，此方向的最初工作仅从无标注文本中预训练词嵌入参数（Collobert 和 Weston，2008）。

近年来，能够生成上下文化词元表示的句子或文档编码器被从无标注文本中预训练，并用于监督的下游任务微调（Dai 和 Le，2015；Howard 和 Ruder，2018；Radford 等，2018）。这些方法的优点是需要从零开始学习的参数很少。至少部分因为这一优点，OpenAI GPT（Radford 等，2018）在 GLUE 基准（Wang 等，2018a）的许多句子级别任务中达到了之前的最先进水平。这些模型的预训练使用了从左到右的语言建模和自编码器目标（Howard 和 Ruder，2018；Radford 等，2018；Dai 和 Le，2015）。

## 2.3. 从监督数据迁移学习

也有研究表明，从具有大规模数据集的监督任务中进行迁移是有效的，例如自然语言推理（Conneau 等，2017）和机器翻译（McCann 等，2017）。计算机视觉研究也展示了从大规模预训练模型进行迁移学习的重要性，其中一个有效的策略是微调基于 ImageNet 预训练的模型（Deng 等，2009；Yosinski 等，2014）。

# 3. BERT

我们在本节中介绍了 BERT 及其详细实现。我们的框架包括两个步骤：预训练和微调。在预训练阶段，模型在不同的预训练任务上使用无标注数据进行训练。在微调阶段，BERT 模型首先用预训练的参数进行初始化，随后使用下游任务中的有标注数据对所有参数进行微调。尽管所有下游任务的模型都由相同的预训练参数初始化，但每个下游任务会有单独的微调模型。图1中的问答示例将作为本节的贯穿示例。

BERT 的一个显著特点是其在不同任务中使用统一的架构。预训练架构与最终下游任务架构之间的差异极小。

### 3.0.1. Model Architecture  

BERT 的模型架构是一个基于 Vaswani 等人（2017）描述的原始实现并在 tensor2tensor 库中发布的多层双向 Transformer 编码器。 由于 Transformer 的使用已经变得普遍且我们的实现与原始版本几乎完全相同，因此我们将省略对模型架构的详尽背景描述，并建议读者参考 Vaswani 等人（2017）以及诸如《The Annotated Transformer》等优秀的指南。

在本文中，我们将层数（即 Transformer 块的数量）记为 $L$，隐藏层大小记为 $H$，自注意力头的数量记为 $A$。我们主要报告两种模型规模的结果：BERT-BASE（$L=12$, $H=768$, $A=12$, 总参数量为 110M）和 BERT-LARGE（$L=24$, $H=1024$, $A=16$, 总参数量为 340M）。

选择 BERT-BASE 的模型规模是为了与 OpenAI GPT 进行对比。然而，关键区别在于 BERT 的 Transformer 使用双向自注意力，而 GPT 的 Transformer 使用限制性自注意力，其中每个标记只能关注其左侧的上下文。

### 3.0.2. Input/Output Representations  

为了使 BERT 能够处理多种下游任务，我们的输入表示法能够在一个标记序列中明确表示单个句子或句子对（例如，<问题，答案>）。在本文中，“句子”可以是任意长度的连续文本片段，而不一定是实际的语言学意义上的句子。“序列”是指 BERT 的输入标记序列，这个序列可以是一个单独的句子，也可以是两个打包在一起的句子。

我们使用 WordPiece 嵌入（Wu 等人，2016）和一个包含 30,000 个标记的词汇表。每个序列的第一个标记始终是一个特殊的分类标记（[CLS]）。对应于此标记的最终隐藏状态用于分类任务中的整体序列表示。句子对会被打包到一个单一的序列中。我们通过两种方式区分这些句子。首先，我们用一个特殊标记（[SEP]）将它们分隔开。其次，我们为每个标记添加一个学习到的嵌入，指示它属于句子 A 还是句子 B。如图 1 所示，我们将输入嵌入表示为 $E$，特殊 [CLS] 标记的最终隐藏向量表示为 $C \in \mathbb{R}^H$，第 $i$ 个输入标记的最终隐藏向量表示为 $T_i \in \mathbb{R}^H$。

对于一个给定的标记，其输入表示通过将相应的词嵌入、段嵌入和位置嵌入相加构建而成。如图 2 所示，这一构建过程的可视化示意也有展示。

## 3.1. Pre-training BERT  

与 Peters 等人（2018a）和 Radford 等人（2018）不同，我们没有使用传统的从左到右或从右到左的语言模型来预训练 BERT。相反，我们使用两种无监督任务对 BERT 进行预训练，这将在本节中进行描述。这一过程在图 1 的左侧部分展示。

### 3.1.1. Task #1: Masked LM  

直观上，可以合理地认为一个深层的双向模型比单纯的从左到右模型或左到右与右到左模型的浅层拼接更强大。不幸的是，标准的条件语言模型只能从左到右或从右到左进行训练，因为双向条件会使每个词间接“看到自己”，从而导致模型可以轻易地在多层上下文中预测目标词。

为了训练深层的双向表示，我们简单地随机屏蔽一部分输入标记，然后预测这些被屏蔽的标记。我们将此过程称为“屏蔽语言模型”（Masked LM, MLM），尽管在文献中它通常被称为填空任务（Cloze task）（Taylor, 1953）。在这种情况下，与标准语言模型一样，与被屏蔽标记对应的最终隐藏向量会被输入到一个针对词汇表的 softmax 输出层中。在我们的所有实验中，我们随机屏蔽每个序列中 15% 的 WordPiece 标记。与去噪自编码器（Vincent et al., 2008）不同，我们只预测被屏蔽的词，而不是重构整个输入。

尽管这使我们能够获得一个双向预训练模型，但缺点是我们在预训练和微调之间引入了不匹配，因为 [MASK] 标记在微调过程中并不会出现。为了缓解这一问题，我们并不总是将“被屏蔽”的词替换为实际的 [MASK] 标记。训练数据生成器随机选择 15% 的标记位置进行预测。如果第 ii 个标记被选中，我们会以下方式替换它：

1. 80% 的概率用 [MASK] 标记替换；
2. 10% 的概率用一个随机标记替换；
3. 10% 的概率保持原标记不变。 然后，使用交叉熵损失（cross-entropy loss）来预测原始标记。我们在附录 C.2 中比较了这种方法的不同变体。

### 3.1.2. Task #2: Next Sentence Prediction (NSP)  

许多重要的下游任务（如问答（QA）和自然语言推断（NLI））都基于理解两个句子之间的关系，而这一点并不能通过语言建模直接捕获。为了训练能够理解句子关系的模型，我们预训练了一个二值化的下一句预测任务（Next Sentence Prediction, NSP），该任务可以从任何单语语料库中轻松生成。具体来说，在为每个预训练样本选择句子 A 和 B 时，有 50% 的概率 B 是实际紧跟在 A 后的句子（标记为 IsNext），另 50% 的概率 B 是从语料库中随机抽取的句子（标记为 NotNext）。如图 1 所示，任务 C 被用于下一句预测（NSP）。尽管这一任务非常简单，但我们在第 5.1 节中证明了针对该任务的预训练对 QA 和 NLI 两个任务都有很大帮助。

NSP 任务与 Jernite 等人（2017）和 Logeswaran 与 Lee（2018）使用的表示学习目标密切相关。然而，在之前的工作中，仅将句子嵌入传递到下游任务中，而 BERT 则将所有参数传递到下游任务模型中，用于初始化最终任务模型的参数。  

### 3.1.3. Pre-training data  

预训练过程在很大程度上遵循了现有关于语言模型预训练的文献。对于预训练语料库，我们使用了 BooksCorpus（8 亿词）（Zhu 等人，2015）和英文维基百科（25 亿词）。对于维基百科，我们仅提取文本段落，忽略列表、表格和标题。使用文档级语料库而非句子级打乱语料库（如 Billion Word Benchmark（Chelba 等人，2013））至关重要，因为这可以提取较长的连续序列。

## 3.2. Fine-tuning BERT  

微调过程非常简单，因为 Transformer 中的自注意力机制使 BERT 能够通过替换适当的输入和输出来处理许多下游任务——无论这些任务涉及单文本还是文本对。在涉及文本对的应用中，常见的模式是先独立编码文本对，然后应用双向交叉注意力机制，例如 Parikh 等人（2016）；Seo 等人（2017）。BERT 则利用自注意力机制将这两个阶段统一起来，因为通过自注意力对连接的文本对进行编码，本质上等同于在两个句子之间实现了双向交叉注意力。

对于每个任务，我们只需将任务特定的输入和输出直接接入 BERT，并端到端微调所有参数。在输入方面，预训练中的句子 A 和句子 B 类似于：(1) 释义中的句子对，(2) 蕴含任务中的假设-前提对，(3) 问答任务中的问题-段落对，以及 (4) 文本分类或序列标注中的退化文本-空集对。在输出方面，标记表示会被送入输出层以处理标记级任务，例如序列标注或问答；而 [CLS] 表示会被送入输出层以处理分类任务，例如蕴含任务或情感分析。

与预训练相比，微调的成本相对较低。论文中的所有结果从同一个预训练模型开始，最多在单个 Cloud TPU 上运行 1 小时，或者在 GPU 上运行数小时即可复现。我们在第 4 节的相应小节中描述了任务的具体细节，更多细节可以在附录 A.5 中找到。

# 4. Experiments  

In this section, we present BERT fine-tuning results on 11 NLP tasks.  

## 4.1. GLUE  

通用语言理解评估 (GLUE) 基准 (Wang 等人, 2018a) 是一个包含多种自然语言理解任务的集合。GLUE 数据集的详细描述见附录 B.1。

在 GLUE 上进行微调时，我们按照第 3 节的描述表示输入序列（单句或句子对），并使用与第一个输入标记 ([CLS]) 对应的最终隐藏向量 $C \in \mathbb{R}^H$ 作为整体表示。微调过程中唯一引入的新参数是分类层的权重 $W \in \mathbb{R}^{K \times H}$，其中 $K$ 是标签的数量。我们利用 $C$ 和 $W$ 计算标准分类损失，即 $\log \left(\operatorname{softmax}\left(C W^T\right)\right)$。

我们使用 32 的批量大小，并对所有 GLUE 任务的数据微调 3 个 epoch。对于每个任务，我们在验证集上选择了最佳的微调学习率（从 5e-5、4e-5、3e-5 和 2e-5 中选择）。此外，对于 BERT-LARGE，我们发现小数据集上的微调有时不稳定，因此我们运行了多次随机重启，并在验证集上选择了最佳模型。随机重启时，我们使用相同的预训练检查点，但对微调数据进行不同的洗牌，并对分类器层进行不同的初始化。

结果见表 1。BERT-BASE 和 BERT-LARGE 在所有任务上都显著超越了所有系统，分别比先前的最新技术水平平均提高了 4.5% 和 7.0% 的准确率。在最大且报告最广泛的 GLUE 任务 MNLI 上，BERT 取得了 4.6% 的绝对准确率提升。在官方 GLUE 排行榜上，截至撰写本文时，BERT-LARGE 的得分为 80.5，而 OpenAI GPT 为 72.8。

我们发现 BERT-LARGE 在所有任务上显著优于 BERT-BASE，尤其是在训练数据非常少的任务上。模型大小的影响将在第 5.2 节中更深入地探讨。

## 4.2. SQuAD v1.1  

斯坦福问答数据集（SQuAD v1.1）是一个包含 10 万对众包问答对的集合 (Rajpurkar 等人, 2016)。在该任务中，给定一个问题和来自 Wikipedia 的包含答案的段落，目标是预测段落中的答案文本片段。

如图 1 所示，在问答任务中，我们将输入问题和段落表示为一个单一的打包序列，其中问题使用 A 嵌入，段落使用 B 嵌入。在微调过程中，我们仅引入了一个起始向量 $S \in \mathbb{R}^H$ 和一个结束向量 $E \in \mathbb{R}^H$。单词 $i$ 是答案片段起始位置的概率通过 $T_i$ 和 $S$ 的点积计算，随后对段落中的所有单词进行 softmax 处理：
$$
Pi=\frac{e^{S \cdot T_i}}{\sum_j e^{S \cdot T_j}}
$$


结束位置的概率公式与起始位置类似。从位置 $i$ 到位置 $j$ 的候选片段得分被定义为 $S \cdot T_i + E \cdot T_j$，并选取 $j \geq i$ 且得分最高的片段作为预测。训练目标是正确的起始和结束位置的对数似然的总和。我们以学习率 $5 \mathrm{e}-5$ 和批量大小为 32 的设置微调 3 个 epoch。

表 2 展示了排行榜上的顶尖条目以及已发表系统的结果 (Seo 等人, 2017；Clark 和 Gardner, 2018；Peters 等人, 2018a；Hu 等人, 2018)。排行榜中的顶尖结果没有最新的公开系统描述，并允许在训练系统时使用任何公共数据。因此，我们通过先在 TriviaQA (Joshi 等人, 2017) 上微调，然后在 SQuAD 上微调，对系统进行了适度的数据增强。

我们表现最佳的系统在集成模型中超越排行榜顶尖系统 +1.5 F1，单模型也超越了 +1.3 F1。事实上，我们的单个 BERT 模型在 F1 分数上甚至优于顶尖的集成系统。如果不使用 TriviaQA 微调数据，F1 分数仅下降 0.1-0.4，但仍以较大优势超过所有现有系统。

## 4.3. SQuAD v2.0   

SQuAD 2.0 任务通过允许提供的段落中可能不存在短答案，扩展了 SQuAD 1.1 的问题定义，从而使问题更加贴近实际情况。

我们使用了一种简单的方法，将 SQuAD v1.1 的 BERT 模型扩展到这一任务。我们将没有答案的问题视为一个起始和结束位置都在 [CLS] token 的答案片段。在答案片段起始和结束位置的概率空间中，加入了 [CLS] token 的位置。在预测时，我们比较无答案片段的得分
$$
s_{null}= S \cdot C + E \cdot C
$$
与最佳非空片段的得分
$$
\widehat{s_{i, j}} = \max _{j \geq i} S \cdot T_i + E \cdot T_j
$$
当 $\widehat{s_{i, j}} > s_{\text{null}} + \tau$ 时，我们预测非空答案，其中阈值 $\tau$ 是在开发集上选择的，以最大化 F1 分数。此模型未使用 TriviaQA 数据进行训练。我们以学习率 5e-5 和批量大小 48 的设置微调了 2 个 epoch。

表 3 展示了与之前排行榜条目和顶尖发表工作的结果比较（Sun 等人，2018；Wang 等人，2018b），排除了使用 BERT 作为其组件的系统。我们观察到相较于之前最佳系统有 +5.1 的 F1 分数提升。

## 4.4. SWAG

**情景对抗生成 (SWAG) 数据集**包含 113k 个句子对完成示例，用于评估基于现实的常识推理（Zellers 等人，2018）。在给定一个句子的情况下，任务是从四个选项中选择最可能的延续。

在 SWAG 数据集上进行微调时，我们构造了四个输入序列，每个序列包含给定句子（句子 A）和一个可能延续（句子 B）的拼接。唯一引入的任务特定参数是一个向量，其与 [CLS] 标记表示 CC 的点积表示每个选项的得分，并通过 softmax 层进行归一化。

我们以学习率 2e-5 和批量大小 16 训练 3 个 epoch。结果如表 4 所示。BERT-LARGE 相较于作者的基线 ESIM+ELMo 系统有 +27.1% 的提升，相较于 OpenAI GPT 有 8.3% 的提升。

# 5. Ablation Studies  

在本节中，我们对 BERT 的多个方面进行了消融实验，以更好地理解它们的相对重要性。更多的消融研究可以在附录 C 中找到。

## 5.1. Effect of Pre-training Tasks  

我们通过对两个预训练目标的评估，展示了 BERT 深度双向性的重要性。这两个目标在完全相同的预训练数据、微调方案和超参数（与 BERT-BASE 一致）下进行对比：

**No NSP**：一种仅通过“掩码语言模型”（MLM）进行训练的双向模型，未使用“下一句预测”（NSP）任务。

**LTR & No NSP**：一种仅使用左上下文的模型，通过标准的从左到右（LTR）语言模型进行训练，而不是 MLM。同时，该模型在微调时也采用左上下文限制，因为移除该限制会导致预训练与微调不一致，从而降低下游任务性能。此外，该模型未进行 NSP 任务的预训练。这个模型与 OpenAI 的 GPT 直接可比，但使用的是更大的训练数据集、我们的输入表示法和微调方案。

我们首先研究 NSP 任务的影响。表 5 显示，移除 NSP 会显著降低 QNLI、MNLI 和 SQuAD 1.1 的性能。接下来，我们通过比较“No NSP”和“LTR & No NSP”来评估训练双向表示的影响。LTR 模型在所有任务上的表现都比 MLM 模型差，在 MRPC 和 SQuAD 上的性能下降尤其显著。

对于 SQuAD，LTR 模型在标记预测任务中的表现较差是直观可理解的，因为标记级别的隐藏状态无法使用右侧上下文。为了尽可能增强 LTR 系统的性能，我们在其顶部添加了一个随机初始化的双向 LSTM（BiLSTM）。尽管这确实显著改善了 SQuAD 的结果，但其性能仍然远低于预训练的双向模型。此外，在 GLUE 任务上，BiLSTM 还降低了性能。

我们认识到，也可以分别训练 LTR 和 RTL（从右到左）模型，并将两者的表示拼接起来表示每个标记，就像 ELMo 那样。然而：（a）这比单一的双向模型消耗双倍资源；（b）对于像 QA（问答）这样的任务，这种方法不直观，因为 RTL 模型无法将答案与问题关联起来；（c）这种方法在能力上严格弱于深度双向模型，因为后者可以在每一层同时利用左右上下文。

## 5.2. Effect of Model Size  

在本节中，我们探讨了模型规模对微调任务准确性的影响。我们训练了多个 BERT 模型，这些模型的层数、隐藏单元和注意力头数各不相同，但其他超参数和训练流程与之前描述的一致。

表 6 展示了部分 GLUE 任务的结果。表中报告了微调过程中 5 次随机重启的开发集平均准确率。从中可以看出，更大的模型在所有四个数据集上都带来了显著的准确率提升，即使是像 MRPC 这样的任务，其仅有 3,600 个带标签的训练样本，与预训练任务有很大不同。这表明，即便是相对已有文献而言已非常庞大的模型，通过进一步扩大规模仍能显著提升性能。例如，Vaswani 等人（2017）研究中最大的 Transformer 是 (L=6, H=1024, A=16)，其编码器包含 1 亿参数；文献中我们能找到的最大的 Transformer 是 (L=64, H=512, A=2)，包含 2.35 亿参数（Al-Rfou 等，2018）。相比之下，BERT-BASE 包含 1.1 亿参数，而 BERT-LARGE 则包含 3.4 亿参数。

长期以来，人们已经知道增大模型规模会持续提升大规模任务（如机器翻译和语言建模）的性能，这通过表 6 中展示的留出训练数据的语言模型困惑度（perplexity）得到验证。然而，我们认为这是首次明确展示，将模型规模扩展到极限也能显著提升小规模任务的表现，前提是模型经过了充分的预训练。Peters 等人（2018b）在将预训练双向语言模型从两层扩展到四层时，在下游任务的影响上得出了混合结果；而 Melamud 等人（2016）则提到，将隐藏层维度从 200 增加到 600 有帮助，但进一步增加到 1,000 并未带来额外改进。这些早期工作均采用了特征工程的方法——我们假设，当模型直接在下游任务上微调，并仅使用少量随机初始化的额外参数时，即使下游任务数据规模非常小，任务特定模型仍能从更大、更具表现力的预训练表示中获益。

## 5.3.  Feature-based Approach with BERT  

到目前为止，所有展示的 BERT 结果都采用了微调方法：在预训练模型上添加一个简单的分类层，并在下游任务上联合微调所有参数。然而，基于特征的方法（即从预训练模型中提取固定特征）也有一定的优势。首先，并非所有任务都能轻松表示为 Transformer 编码器架构，因此需要添加任务特定的模型架构。其次，预先计算训练数据的高成本表示一次，然后在此表示的基础上使用低成本模型运行多个实验，可以带来显著的计算优势。

在本节中，我们通过将 BERT 应用于 CoNLL-2003 命名实体识别（NER）任务（Tjong Kim Sang 和 De Meulder，2003）来比较这两种方法。在输入 BERT 时，我们使用保留大小写的 WordPiece 模型，并包括数据提供的最大文档上下文。按照标准实践，我们将其表述为标注任务，但在输出中不使用 CRF 层。我们将第一个子标记的表示用作对 NER 标签集进行标记级分类的输入。

为了研究微调方法的效果，我们通过提取一个或多个层的激活值并不微调任何 BERT 参数来实现基于特征的方法。这些上下文嵌入作为输入传递给随机初始化的两层 768 维 BiLSTM，之后再连接分类层。

表 7 展示了实验结果。BERT-LARGE 的表现与当前最先进方法具有竞争力。表现最好的方法是将预训练 Transformer 的顶部四个隐藏层的标记表示拼接起来，其 F1 分数仅比对整个模型进行微调低 0.3。这表明 BERT 在微调和基于特征的方法中都非常有效。

# 6. Conclusion  

最近，由于使用语言模型进行迁移学习所带来的经验性改进表明，丰富的无监督预训练是许多语言理解系统的重要组成部分。特别是，这些成果使得即使是低资源任务也能从深度单向架构中受益。我们的主要贡献是将这些发现进一步推广到深度双向架构，使得同一个预训练模型能够成功应对广泛的自然语言处理任务。

# A. Additional Details for BERT  

## A1. Illustration of the Pre-training Tasks  

我们在下文中提供了预训练任务的示例说明：

**掩码语言模型（Masked LM）及掩码过程**
 假设未标注的句子是 *my dog is hairy*，在随机掩码过程中，我们选择了第 4 个单词（即 *hairy*）。掩码过程可以进一步描述为：

- **80% 的情况下**：将单词替换为 [MASK] 标记，例如：
   *my dog is hairy* → *my dog is [MASK]*
- **10% 的情况下**：将单词替换为一个随机单词，例如：
   *my dog is hairy* → *my dog is apple*
- **10% 的情况下**：保持单词不变，例如：
   *my dog is hairy* → *my dog is hairy*。
   这样做的目的是使表示偏向于实际观察到的单词。

这一过程的优势在于，Transformer 编码器无法预先知道哪些单词会被要求预测，或者哪些单词已被随机替换，因此它必须为每个输入标记保持分布式的上下文表示。此外，由于随机替换仅发生在所有标记的 1.5% 上（即 15% 的 10%），这并不会显著损害模型的语言理解能力。在附录 C.2 中，我们评估了这一过程的影响。

与标准语言模型训练相比，掩码语言模型（MLM）仅对每个批次中的 15% 的标记进行预测，这表明模型可能需要更多的预训练步骤才能收敛。在附录 C.1 中，我们展示了 MLM 的收敛速度确实比从左到右模型（LTR，预测每个标记）稍慢，但 MLM 模型在实际效果上的改进远远超过了额外的训练成本。

**下一句预测（Next Sentence Prediction）**
 下一句预测任务可以通过以下示例说明：

- 输入 = [CLS] *the man went to [MASK] store* [SEP]
   *he bought a gallon [MASK] milk* [SEP]
   标签 = *IsNext*
- 输入 = [CLS] *the man [MASK] |o the store* [SEP]
   *penguin [MASK] are flight##less birds* [SEP]
   标签 = *NotNext*

## A.2. Pre-training Procedure  

为了生成每个训练输入序列，我们从语料库中采样两个文本片段，称之为“句子”，尽管它们通常比单个句子长得多（但也可能更短）。第一个句子使用 A 嵌入，第二个句子使用 B 嵌入。在 50% 的情况下，B 是实际紧随 A 的下一句；在另外 50% 的情况下，B 是随机句子。这种采样方法是为了完成“下一句预测”（Next Sentence Prediction）任务。采样时要求组合后的总长度不超过 512 个标记。LM 掩码在 WordPiece 分词之后应用，使用统一的掩码率 15%，且对部分单词片段不作特殊处理。

训练时，我们使用批量大小为 256 的序列（256 个序列 × 512 个标记 = 每批 128,000 个标记），训练 1,000,000 步，相当于在包含 33 亿词语的语料库上约 40 个轮次（epochs）。我们使用 Adam 优化器，学习率为 $1 \times 10^{-4}$，$\beta_1=0.9$，$\beta_2=0.999$，权重衰减为 0.01，前 10,000 步采用学习率预热，并对学习率进行线性衰减。所有层的 dropout 概率为 0.1。激活函数使用 gelu（Hendrycks 和 Gimpel，2016），而非标准的 relu，与 OpenAI GPT 保持一致。训练损失是平均掩码语言模型（Masked LM）似然和平均下一句预测（Next Sentence Prediction）似然之和。

BERT-BASE 的训练在 4 个 Cloud TPU（Pod 配置，共 16 个 TPU 芯片）上完成。BERT-LARGE 的训练在 16 个 Cloud TPU（共 64 个 TPU 芯片）上完成。每次预训练耗时约 4 天。

较长的序列计算成本较高，因为注意力机制的计算复杂度与序列长度的平方成正比。为了加速实验中的预训练，我们在 90% 的训练步骤中使用序列长度为 128 的输入进行预训练。然后，在剩下的 10% 训练步骤中使用长度为 512 的输入，以学习位置嵌入（Positional Embeddings）。

## A.3. Fine-tuning Procedure  

在微调过程中，大多数模型的超参数与预训练时相同，除了批量大小、学习率和训练轮次之外。Dropout 概率始终保持为 0.1。最佳的超参数值因任务而异，但我们发现以下范围的参数在所有任务中表现良好：

- 批量大小：16、32
- 学习率（Adam）：5e-5、3e-5、2e-5
- 训练轮次：2、3、4

我们还观察到，对于大规模数据集（例如，10 万以上的带标签训练样本），模型对超参数选择的敏感性远低于小规模数据集。由于微调通常非常快速，因此完全可以对上述参数进行穷举搜索，然后选择在开发集上表现最佳的模型。

## A.4. Comparison of BERT, ELMo ,and OpenAI GPT

我们研究了最近流行的表示学习模型之间的差异，包括 ELMo、OpenAI GPT 和 BERT。模型架构之间的对比在图 3 中以可视化形式展示。需要注意的是，除了架构上的差异，BERT 和 OpenAI GPT 是微调方法，而 ELMo 是基于特征的方法。

与 BERT 最具可比性的现有预训练方法是 OpenAI GPT，它在一个大规模文本语料库上训练了一个从左到右的 Transformer 语言模型。事实上，BERT 的许多设计决策都是有意为之，以尽量与 GPT 保持一致，从而使两种方法能够进行最小化的对比。本文的核心论点是，双向性以及第 3.1 节中提出的两种预训练任务构成了大多数经验性改进的来源，但我们也注意到 BERT 和 GPT 在训练方式上存在其他一些差异：

- GPT 在 BooksCorpus（8 亿词）上训练；BERT 在 BooksCorpus（8 亿词）和 Wikipedia（25 亿词）上训练。
- GPT 在微调时才引入句子分隔符（[SEP]）和分类标记（[CLS]）；BERT 在预训练期间就学习了 [SEP]、[CLS] 和句子 A/B 嵌入。
- GPT 使用批量大小为 32,000 词，训练了 100 万步；BERT 使用批量大小为 128,000 词，训练了 100 万步。
- GPT 在所有微调实验中使用了固定的学习率 $5 \times 10^{-5}$；BERT 为每个任务选择在开发集上表现最好的任务特定微调学习率。

为了分离这些差异的影响，我们在第 5.1 节中进行了消融实验，结果表明，大多数改进确实来自于两种预训练任务以及它们所实现的双向性。

## A.5. Illustrations of Fine-tuning on Different Tasks

图 4 展示了在不同任务上微调 BERT 的示意图。我们的任务特定模型通过在 BERT 上添加一个额外的输出层形成，因此只需从头学习极少量的参数。在这些任务中，(a) 和 (b) 是序列级任务，而 (c) 和 (d) 是标记级任务。图中，$E$ 表示输入嵌入，$T_i$ 表示标记 $i$ 的上下文表示，[CLS] 是用于分类输出的特殊符号，[SEP] 是用于分隔非连续标记序列的特殊符号。

# B. Detailed Experimental Setup  

## B.1. Detailed Descriptions for the GLUE Benchmark Experiments.

GLUE 基准测试包含以下数据集，其描述最初由 Wang 等人（2018a）总结：

**MNLI**
 Multi-Genre Natural Language Inference 是一个大规模的众包蕴涵分类任务（Williams 等，2018）。给定一对句子，目标是预测第二句相对于第一句是蕴涵（entailment）、矛盾（contradiction）还是中性（neutral）。

**QQP**
 Quora Question Pairs 是一个二分类任务，目标是判断 Quora 上的两条问题是否语义等价（Chen 等，2018）。

**QNLI**
 Question Natural Language Inference 是 Stanford Question Answering Dataset（Rajpurkar 等，2016）的一个版本，该版本被转换为二分类任务（Wang 等，2018a）。正样本是包含正确答案的（问题，句子）对，负样本是来自同一段落但不包含答案的（问题，句子）对。

**SST-2**
 The Stanford Sentiment Treebank 是一个二分类的单句分类任务，由电影评论中提取的句子组成，带有人工标注的情感标签（Socher 等，2013）。

**CoLA**
 The Corpus of Linguistic Acceptability 是一个二分类的单句分类任务，目标是预测一个英语句子在语言学上是否“可接受”（Warstadt 等，2018）。

**STS-B**
 The Semantic Textual Similarity Benchmark 是一个从新闻标题和其他来源中抽取的句子对集合（Cer 等，2017）。这些句子对被人工标注了一个从 1 到 5 的分数，用于表示两句在语义上的相似程度。

**MRPC**
 Microsoft Research Paraphrase Corpus 包含从在线新闻来源中自动提取的句子对，并带有人类标注，标注该句子对的句子是否语义等价（Dolan 和 Brockett，2005）。

**RTE**
 Recognizing Textual Entailment 是一个与 MNLI 类似的二分类蕴涵任务，但训练数据量少得多（Bentivogli 等，2009）。

**WNLI**
 Winograd NLI 是一个小型自然语言推理数据集（Levesque 等，2011）。GLUE 网站指出，该数据集在构建上存在问题，每一个提交到 GLUE 的训练系统的表现都比预测多数类的 65.1% 基线准确率更差。因此，为了公平对比 OpenAI GPT，我们排除了该数据集。在我们的 GLUE 提交中，我们始终预测多数类。

# C. Additional Ablation Studies  

## C.1. Effect of Number of Training Steps  

图 5 展示了从经过 kk 步预训练的检查点微调后的 MNLI 开发集准确率。这帮助我们回答以下问题：

#### 问答 1

**问题**：BERT 是否真的需要如此大量的预训练（128,000 词/批 * 1,000,000 步）才能达到较高的微调准确率？

**回答**：是的。与训练 50 万步相比，经过 100 万步训练的 BERT-BASE 在 MNLI 上的准确率提高了近 1.0%。

#### 问答 2

**问题**：MLM 预训练是否比 LTR 预训练收敛得更慢，因为每批次中仅预测 15% 的单词，而不是每个单词？

**回答**：MLM 模型的确比 LTR 模型收敛略慢。然而，就绝对准确率而言，MLM 模型几乎立即开始优于 LTR 模型。

## C.2. Ablation for Different Masking Procedures  

在第 3.1 节中，我们提到 BERT 在使用掩码语言模型（MLM）目标进行预训练时采用了一种混合策略来掩码目标标记。以下是对不同掩码策略效果的消融研究。

需要注意的是，掩码策略的目的是减少预训练和微调之间的不匹配，因为 [MASK] 符号在微调阶段从未出现过。我们报告了 MNLI 和 NER 的开发集结果。对于 NER，我们同时报告了微调方法和基于特征的方法的结果，因为我们预计对于基于特征的方法，由于模型没有机会调整表示，不匹配问题会更加明显。

结果如表 8 所示。表中，MASK 表示在 MLM 中将目标标记替换为 [MASK] 符号；SAME 表示保持目标标记不变；RND 表示将目标标记替换为另一个随机标记。

表格左侧的数字表示 MLM 预训练中使用特定策略的概率（BERT 使用 80%、10%、10%）。表格右侧为开发集结果。对于基于特征的方法，我们将 BERT 的最后 4 层拼接作为特征，如第 5.3 节中所示，这被证明是最佳方法。

从表中可以看出，微调对不同的掩码策略具有令人惊讶的鲁棒性。然而，正如预期的那样，当将仅使用 MASK 策略应用于基于特征的方法进行 NER 时，效果较差。有趣的是，仅使用 RND 策略的表现也比我们的混合策略差得多。