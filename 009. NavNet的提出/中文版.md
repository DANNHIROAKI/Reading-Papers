# Navigating nets: Simple algorithms for proximity search  



## 0. Abstract

我们提出了一种简单的确定性数据结构，用于在一般度量空间中维护点集 $S$，同时支持邻近搜索（最近邻和范围查询）以及对 $S$ 的更新（插入和删除）。我们的数据结构由一系列逐步精细化的 $S$ 的 $\epsilon$-网组成，并且这些网之间通过指针相互连接，使我们能够轻松地从一个尺度导航到下一个尺度。

---

我们从“抽象维度”的角度分析了该数据结构的最坏情况下的复杂性。对于有界维度的度量空间，该数据结构非常高效，并且在某种距离计算模型中基本达到了最优。最后，作为一个特例，我们的方法优于最近由 Karger 和 Ruhl 提出的一个方法 [KR02]。



## 1. Introduction  

最近邻搜索（NNS）问题是对位于巨大的（可能是无限的）度量空间 $(M, d)$ 中的一组 $n$ 个点集 $S$ 进行预处理，以便在给定查询点 $q \in M$ 时，能够有效地找到 $S$ 中距离 $q$ 最近的点（假设距离函数 $d$ 是非负且对称的，并满足三角不等式）。高效地计算这样的最近邻是一个经典且基本的问题，具有众多实际应用。这些应用包括数据压缩、数据库查询、机器学习、计算生物学、数据挖掘、模式识别和自组织网络。许多这些例子中的一个共同特点是，比较两个元素的代价很高，因此应尽可能减少距离计算的次数。

---

一般的最近邻搜索（NNS）问题有多种变体。例如，所处的度量空间 $(M, d)$ 可能是特定应用相关的（例如，在拼写检查器或基因组数据中，$d$ 可能是加权编辑距离），它可能是无限的（例如，欧几里得空间），甚至可能结构非常松散，以至于几乎未知（如在对等网络中，表示节点间的延迟）。预处理阶段的空间和时间可能会受到限制（例如，限制为多项式或线性于 $n$）。当前的应用程序可能要求数据结构是动态的，即它应能有效支持从 $S$ 中插入和删除点。此外，可能希望数据结构在数据点之间分布，在这种情况下，诸如通信和负载等其他成本指标可能变得非常重要。最后，可能只需要近似解。设 $q$ 为查询点，$a \in S$ 为 $S$ 中离 $q$ 最近的点。那么 $(1+\epsilon)$-NNS 算法是指，给定 $q$，返回某个 $s \in S$ 使得 $d(q, s) \leq (1+\epsilon) d(q, a)$。

---

以往的研究大多集中在一个重要的特殊情况，即 $M=\mathbb{R}^d$，并且距离根据某种 $\ell_p$ 范数计算。虽然许多类型的数据可以自然地以这种形式表示，但对许多应用来说，这显然并不成立，因此在一般度量空间中处理最近邻搜索（NNS）是非常必要的。一方面，针对一般度量的数据结构可能会以 $\Omega(n)$ 的时间执行最近邻查询，而这种性能在实际应用中是不可接受的。即使仅需要近似解，这种依赖性也是固有的。一个众所周知的例子是，当 $S$ 形成均匀度量时，$S$ 中的点间距离都是相等的，基本上不提供任何有用的信息。（模型的具体说明和额外的下界请见第3节。）另一方面，实际应用中出现的度量往往具有更好的结构，可以被算法所利用。

---

鉴于这种情况，越来越多的近期研究关注如何通过度量的隐式结构来理解NNS的复杂性。在欧几里得空间中，度量复杂性的一个自然且常见的度量标准是欧几里得宿主空间的维度。事实上，大多数算法的效率都依赖于该维度。实际上，许多算法的运行时间随维度呈指数增长，这种现象被称为“维度灾难”（著名的例外包括 [IM98, KOR98]）。因此，尝试为抽象度量空间定义一个类似的维度概念是很自然的想法。

---

我们的第一个贡献是概念性的——我们提出了一种抽象维度的自然概念（在最近邻搜索的背景下），它基于Assouad [Ass83] 的工作（来自度量空间分析理论）。这一贡献得到了技术上的支持——我们为一般低维度度量空间中的最近邻搜索提供了一个高效且动态的数据结构。在一般度量空间的最近邻搜索上下文中，也有其他维度性概念被提出并研究过 [Cla99, KR02]，我们的方案与它们相比表现优越。此外，尽管我们的方法是通用的，特别是不依赖于欧几里得几何，但它与已知的最优低维欧几里得空间方案（具有线性预处理空间）[AMN+98] 相当！更详细的说明请参见第1.3节。

### 1.1. Abstract dimension

度量空间 $(X, d)$ 的倍增维度，在本文中记为 $\operatorname{dim}(X)$，是使得 $X$ 中的每个集合都可以被 $2^\rho$ 个直径为原集合一半的集合覆盖的最小值 $\rho$。（集合 $S \subseteq X$ 的直径为 $\sup \{d(x, y): x, y \in S\}$。）按照惯例，定义在 $S \subseteq X$ 中以 $x$ 为中心、半径为 $r$ 的（闭）球为 $B_S(x, r) = \{y \in S: d(x, y) \leq r\}$；当上下文清晰时，我们可以省略下标 $S$。不难看出，使得 $X$ 中的每个球可以被 $2^\rho$ 个半径为原球一半的球覆盖的最小值 $\rho$ 与上述定义相差不超过一个因子 2——这也是我们在整个讨论中使用的值。

---

如果度量空间的维度是 $O(1)$，则称该度量空间为倍增空间。例如，$k$ 个点上的均匀度量的维度是 $\log k$。一个近似的逆命题也成立（见下面的引理 1.2），这表明小的倍增维度量化了缺乏大的近似均匀度量。换句话说，这种维度概念衡量的是 $X$ 的“体积增长”。我们注意到 Clarkson [Cla99] 在最近邻搜索的背景下使用了类似的概念（但称谓不同），不过他的结果似乎没有充分发挥这个概念的全部潜力。（详见第 1.3 节。）

---

倍增维度具有几个自然且吸引人的特性。设 $(X, d)$ 为任意度量空间，以下是一些比较简单的性质（见例如 [Hei01]）：

1. 对于带有任意范数的 $X = \mathbb{R}^k$，$\operatorname{dim}(X) = \Theta(k)$。
2. 如果 $X$ 是 $Y$ 的子空间，则 $\operatorname{dim}(X) \leq \operatorname{dim}(Y)$。
3. $\operatorname{dim}\left(X_1 \cup \cdots \cup X_m\right) \leq \max_i \operatorname{dim}\left(X_i\right) + \log m$。

---

在 [KR02] 中指出，倍增度量（那些具有统一有界维度的度量）在实际应用中自然出现，如对等网络。在特征识别问题中，数据集 $S$ 通常包含在一些非常高维空间 $\mathbb{R}^L$ 中的 $m$ 个低维流形的并集中，所使用的距离函数是 $\mathbb{R}^L$ 的某种范数（通常是 $\ell_1$ 或 $\ell_2$ 范数）。例如，一个流形可以表示对应于同一物体不同视角的特征向量，因此 $m$ 个物体会产生 $m$ 个流形的并集。在这种情况下，仅使用高维宿主空间的结构来执行最近邻搜索会非常昂贵。实际情况更加复杂，因为测量中产生的噪声和误差实际上导致的是 $m$ 个集合的并集，每个集合都仅接近一个流形。幸运的是，倍增维度对这种小的扰动相当不敏感。

### 1.2. Our results  

我们为一般度量空间提供了一个 $(1+\epsilon)$-NNS 的简单方案。该数据结构在第2节中进行了描述，它是确定性的，因此可以保证正确回答 $(1+\epsilon)$-NNS 查询。我们的数据结构是动态的，支持对 $S$ 进行插入和删除操作。它还支持范围查询：给定查询点 $q \in M$ 和范围 $t$，目标是返回所有满足 $d(q, s) \leq t$ 的点 $s \in S$。

---

这些操作的复杂性取决于 $S$ 的维度和 $(S, d)$ 的纵横比，记作 $\Delta=\Delta_S$，它定义为 $S$ 中最大和最小点间距离的比值。在大多数应用中，$\Delta=\operatorname{poly}(|S|)$，因此 $\log \Delta=O(\log n)$。

---

如果 $S$ 是倍增空间，则该数据结构使用 $O(n)$ 空间；它在 $O(\log \Delta) + (1 / \epsilon)^{O(1)}$ 的时间内回答 $(1+\epsilon)$-NNS 查询；并且在 $O(\log \Delta \log \log \Delta)$ 时间内实现插入和删除操作。运行时间指数依赖于 $\operatorname{dim}(S)$（见第2节），但在距离预言机模型中，我们只能通过查询距离函数 $d$ 来访问 $M$，因此这种依赖是必要的，正如我们在第3节所展示的那样。

---

在 [KR02] 中定义了一个不同的维度概念（隐式地），我们记作 $\operatorname{dim}_{\mathrm{KR}}(X)$。这是使得对每个点 $x \in X$ 和任意 $r > 0$，都有 $\left|B_X(x, 2r)\right| \leq 2^k\left|B_X(x, r)\right|$ 的最小值 $k$。在 [GKL03] 中证明了以下简单的引理；为了完整性，我们在附录中重复了该证明。

---

引理 1.1 ([GKL03]) 每个有限度量空间 $(X, d)$ 都满足 $\operatorname{dim}(X) \leq 4 \operatorname{dim}_{\mathrm{KR}}(X)$。

---

反过来的方向（即 $\operatorname{dim}_{\mathrm{KR}}(X)$ 关于 $\operatorname{dim}(X)$ 的界限）并不成立。换句话说，倍增度量（具有有界倍增维度的度量）构成了一个严格更大的类，超出了那些具有有界 KR-维度的度量。接下来的部分对这两种概念进行了进一步比较。

---

当由数据集 $S$ 和查询点 $q$（即 $S \cup \{q\}$）形成的度量属于具有有界 KR 维度的度量类时，通过对我们的查询过程进行轻微修改（但不是数据结构），我们能够返回精确的最近邻，并匹配或改善 [KR02] 的界限；详见第 1.3 节。

### 1.3. Related work  

许多工作（例如 [Bri95]）表明，度量空间中点之间距离的直方图（集中度）可以（凭经验）指示其维度。Chávez 等人 [CNBYM01] 建议将度量的维度定义为 $\rho=\mu^2 / 2 \sigma^2$，其中 $\mu$ 和 $\sigma^2$ 分别是直方图的均值和方差，并且他们展示了在随机情况下，这个维度影响某些最近邻搜索算法的效率。Faloutsos 和 Kamel [FK97] 建议测量数据集的分形维度，但这个概念仅适用于欧几里得空间。

---

Clarkson [Cla99] 设计了两个用于满足某种球堆积性质（等价于具有 $O(1)$ 倍增维度）的度量空间的 NNS 数据结构。（见表 1。）然而，他的数据结构是随机的，非动态的，查询时间是超对数的，并且假设数据集 $S$ 和查询点 $q$ 是从相同的（未知的）概率分布中选取的。我们的算法在这些方面相对于 [Cla99] 有所改进，尽管它们只保证 $(1+\epsilon)$-NNS。

---

最近，Karger 和 Ruhl [KR02] 提出了上一节讨论的维度概念。该概念的一个理由是，$l_1$ 度量下的 $k$ 维网格的 KR 维度为 $\Theta(k)$。Karger 和 Ruhl [KR02] 展示了一个针对具有有界 KR 维度的度量空间的高效 NNS 数据结构。（见表 1）

---

引理 1.1 表明，每个度量空间的倍增维度（本质上）不大于其 KR 维度。因此，我们所有关于倍增度量的结果可以立即适用于具有有界 KR 维度的度量空间。此外，我们可以证明，当在这类度量空间上运行时，我们的算法可以（稍微）修改，从而以类似于 [KR02] 的运行时间找到精确的 NNS，但使用线性（而非近线性）空间。我们的结果在多个方面优于 [KR02]。除了扩展到更大的度量空间家族之外，我们的算法是确定性的且不需要任何参数，而 [KR02] 的算法是随机的（Las Vegas 算法），其性能依赖于正确设置维度参数。

---

我们稍作讨论 KR 维度概念的脆弱性。特别是，正如 [KR02] 中指出的，具有有界 KR 维度的度量空间的子集并不总是具有有界维度。特别地，实线的某些子集具有无界的 KR 维度（尽管显然 $\mathbb{R}$ 的每个子集都具有有界的倍增维度）。其原因很简单：[KR02] 的算法使用随机采样来寻找最近邻，从而在度量空间中的点布局上强加了某种均匀性。因此，不清楚他们的方法是否可以扩展到更大的倍增度量类。要看这个不稳定性的一个简单例子，考虑离散圆环 $S=\{x \in \mathbb{Z}: 2r > |x| > r\}$。不难看出 $\operatorname{dim}_{\mathrm{KR}}(S)=O(1)$（对于任意 $r>0$，都是一致的）。然而，$\operatorname{dim}_{\mathrm{KR}}(S \cup \{0\})=\Omega(\log r)$，因此向 $S$ 中添加一个点就可以使 KR 维度任意增长。

---

或许最有趣的是，我们的结果与 $\left[\mathrm{AMN}^{+} 98\right]$ 对于有界维度欧几里得度量的结果相当（当然，它们也具有有界的倍增维度）；见表 1。令人惊讶的是，仅通过对点集的体积增长进行约束，而不依赖于欧几里得空间的几何结构，也能够实现类似的运行时间。

|          |                 Dimension                  |               Space               |          NNS or $(1+\epsilon)$-NNS          |           Insert/Delete           |
| :------: | :----------------------------------------: | :-------------------------------: | :-----------------------------------------: | :-------------------------------: |
| [Cla99]* |   $\operatorname{dim}(S)=O(1)^{\dagger}$   |        $O(n \log \Delta)$         | $O\left(\log ^4 n \cdot \log \Delta\right)$ |                 -                 |
| [Cla99]* |  $\operatorname{dim}(S)=O(1)^{\ddagger}$   | $n(\log n)^{O(\log \log \Delta)}$ |      $(\log n)^{O(\log \log \Delta)}$       |                 -                 |
|   本文   |        $\operatorname{dim}(S)=O(1)$        |              $O(n)$               |   $O(\log \Delta)+(1 / \epsilon)^{O(1)}$    | $O(\log \Delta \log \log \Delta)$ |
| [KR02]*  | $\operatorname{dim}_{\mathrm{KR}}(S)=O(1)$ |           $O(n \log n)$           |                 $O(\log n)$                 |      $O(\log n \log \log n)$      |
|   本文   | $\operatorname{dim}_{\mathrm{KR}}(S)=O(1)$ |              $O(n)$               |                 $O(\log n)$                 |      $O(\log n \log \log n)$      |
| [AMN98]  |              $O(1)$ Euclidean              |              $O(n)$               |       $(1 / \epsilon)^{O(1)} \log n$        |            $O(\log n)$            |

$\star$ 随机化（Las Vegas）数据结构。

$\dagger$ 假设有用于查询的训练集。

$\ddagger$ 假设查询来自与数据集相同的（未知）分布。

Table 1: 一般度量空间中 NNS 方案的比较。

### 1.4. Techniques  

设 $(X, d)$ 为一个度量空间。我们称子集 $Y \subseteq X$ 为 $X$ 的一个 $\epsilon$-网，如果它满足以下两个条件：(1) 对于每个 $x, y \in Y$，$d(x, y) \geq \epsilon$，(2) $X \subseteq \bigcup_{y \in Y} B(y, \epsilon)$。对于任意 $\epsilon > 0$，这样的网总是存在的。对于有限度量空间，它们可以通过贪心算法构造。对于任意度量空间，它们的存在性证明是佐恩引理的一个简单应用。这种经典的 $\epsilon$-网概念，是度量空间研究中的一个标准工具（见例如 [Hei01]），不应与计算几何中最近引入的（同名的）概念混淆。

---

以下引理是我们分析的关键。

引理 1.2. 设 $(S, d)$ 为一个度量空间，且 $Y \subseteq S$。如果 $Y$ 上诱导的度量的纵横比至多为 $\alpha$ 且 $\alpha \geq 2$，那么 $|Y| \leq \alpha^{O(\operatorname{dim}(S))}$。

证明。设 $d_{\min} = \inf \{d(x, y): x, y \in Y\}$ 和 $d_{\max} = \sup \{d(x, y): x, y \in Y\}$ 分别为 $Y$ 中的最小和最大点间距离，且假设 $\alpha = \frac{d_{\max}}{d_{\min}} < \infty$。注意，$Y$ 包含在 $S$ 中一个半径为 $2 d_{\max} \leq 2 \alpha d_{\min}$ 的球内（以 $Y$ 的任意一点为中心）。通过多次迭代应用倍增维度的定义，我们得到这个球，特别是 $Y$，可以被 $2^{\operatorname{dim}(S) \cdot O(\log \alpha)}$ 个半径为 $d_{\min} / 3$ 的球覆盖。根据 $d_{\min}$ 的定义，每个球至多覆盖 $Y$ 的一个点，因此 $|Y| \leq 2^{\operatorname{dim}(S) \cdot O(\log \alpha)} \leq \alpha^{O(\operatorname{dim}(S))}$

#### 1.4.1. A simplified 3-NNS algorithm  

简化的 $3$-NNS 算法。这里是我们数据结构的一个简化版本。设 $(S, d)$ 为待查询的度量空间，为了方便讨论，假设 $S$ 中的最小点间距离为 $\min \{d(x, y): x, y \in S\} = 1$。在这种情况下，纵横比 $\Delta$ 就是 $S$ 的直径。接下来，对于任意子集 $R \subseteq S$ 和点 $x \in M$，我们定义 $d(x, R) = \inf _{y \in R} d(x, y)$。

---

设 $k = \log \Delta$，对于 $i=0,1, \ldots, k$，令 $Y_i$ 为 $S$ 的一个 $2^i$-网。现在，对于每个点 $y \in Y_i$，假设我们有集合 $L_{y, i} = \left\{z \in Y_{i-1}: d(y, z) \leq \gamma 2^i\right\}$，其中 $\gamma$ 是待确定的常数。

---

注意到集合 $L_{y, i}$ 的最小距离为 $2^{i-1}$（因为这是 $Y_{i-1}$ 中的最小距离），最大距离为 $\gamma 2^{i+1}$，因此它的纵横比是有界的。当 $S$ 是一个倍增度量空间时，引理 1.2 表明 $\left|L_{y, i}\right| = O(1)$，其中常数取决于 $\gamma$ 的选择。此外，$Y_k$ 只包含一个（任意的）点，记作 $y_{\text{top}}$。

---

现在，给定一个查询点 $q \in M$，我们首先设定 $y = y_{\text{top}}$，然后从 $i = k, k-1, \ldots$ 逐步迭代，找到 $L_{y, i} \subseteq Y_{i-1}$ 中距离 $q$ 最近的点，并将 $y$ 设置为该点（用于下一次迭代）。如果在某个阶段，我们达到了 $d(q, L_{y, i}) > 3 \cdot 2^{i-1}$，则停止并输出 $y$。否则，输出最后的值 $y \in Y_0$。

---

首先，注意到该算法的运行时间最多为 $O(\log \Delta)$，因为在列表 $L_{y, i}$ 中找到距离 $q$ 最近的点只需要常数时间（这些列表的大小是 $O(1)$）。因此，我们只需要证明输出点 $y$ 是最近邻 $a \in S$ 的 3-近似解，即 $d(q, y) \leq 3 d(q, a)$。

---

为此，设 $j$ 满足 $d(q, y) \leq 3 \cdot 2^j$，但 $d(q, L_{y, j}) > 3 \cdot 2^{j-1}$，即在该步中距离没有按2的因子减少。首先，我们要证明 $d(a, L_{y, j}) \leq 2^{j-1}$。换句话说，距离 $a$ 最近的 $2^{j-1}$ 网格点包含在 $L_{y, j}$ 中。设 $y^* \in Y_{j-1}$ 使得 $d(a, y^*) \leq 2^{j-1}$。我们需要证明 $d(y^*, y) \leq \gamma 2^j$；在这种情况下，$y^* \in L_{y, j}$。由于 $d(q, y) \leq 3 \cdot 2^j$，我们有

$d(y^*, y) \leq d(y^*, a) + d(a, y) \leq 2^{j-1} + d(a, q) + d(q, y) \leq 2^{j-1} + 2 \cdot d(q, y) \leq 7 \cdot 2^j$ 

因此，选择 $\gamma = 7$ 即可满足条件。

---

这表明下降过程“追踪”了距离 $a$ 最近的网格点。现在很容易看到 

$3 \cdot 2^{j-1} < d(q, L_{y, j}) \leq d(q, a) + d(a, L_{y, j}) \leq d(q, a) + 2^{j-1}$

因此 $d(q, a) > 2^j$。由于 $d(q, y) \leq 3 \cdot 2^j$，因此 $y$ 是 $q$ 的 3-近似最近邻。类似的论证表明，如果最终得到的是 $y \in Y_0$，那么 $y$ 实际上是距离 $q$ 最近的点。稍后我们会看到，在获得 $O(1)$ 近似最近邻之后，$(1+\epsilon)$-NNS 问题可以在时间 $O(1/\epsilon)^{O(1)}$ 内解决。

---

上述简单算法展示了使用不同尺度的网格来在度量空间中导航的强大功能，基本上在每一步都将 $q$ 与最近点之间的距离减半。我们数据结构中的所有操作都在这个简单框架内实现。

---

为了克服上述方案的一些技术限制，第 2 节中展示的实际数据结构更加复杂。首先，上述算法必须扩展为适用于任意 $\epsilon > 0$ 的 $(1+\epsilon)$-NNS。这可以通过大约 $\log(1/\epsilon)$ 次更多的迭代决策过程来实现，但在每次迭代中，我们现在必须处理多个点 $y$，而不仅仅是一个点。其次，在存在插入和删除的情况下，我们不能简化假设 $S$ 中的最小点间距离为 1，因此我们必须找到一种方法来跟踪“相关”的尺度。最后，由于技术原因，上述数据结构不支持高效的删除操作，且可能需要空间 $\Omega(n \log \Delta)$。解决这些问题的主要方法是选择 $Y_i$ 为 $Y_{i-1}$ 中的 $2^i$-网格，而不是 $S$ 中的。







## References  

[AMN+98] S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu. An optimal algorithm for approximate nearest neighbor searching in fixed dimensions. J. ACM, 45(6):891–923, 1998  

[Ass83] P. Assouad. Plongements lipschitziens dans Rn. Bull. Soc. Math. France, 111(4):429–448, 1983.  

[Bri95] S. Brin. Near neighbor search in large metric spaces. In 21st International Conference on Very Large Data Bases, pages 574–584, 1995.  

[Cla99] K. L. Clarkson. Nearest neighbor queries in metric spaces. Discrete Comput. Geom., 22(1):63–93, 1999.  

[CNBYM01] E. Ch‘avez, G. Navarro, R. Baeza-Yates, and J. L. Marroqu´ın. Proximity searching in metric spaces. ACM Computing Surveys, 33(3):273–321, September 2001.  

[FK97] C. Faloutsos and I. Kamel. Relaxing the uniformity and independence assumptions using the concept of fractal dimension. J. Comput. System Sci., 55(2):229– 240, 1997.  

[GKL03] A. Gupta, R. Krauthgamer, and J. R. Lee. Bounded geometries, fractals, and low–distortion embeddings. Accepted to 43rd Symposium on Foundations of Computer Science, 2003.  

[Hei01] J. Heinonen. Lectures on analysis on metric spaces. Universitext. Springer-Verlag, New York, 2001.  

[IM98] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In 30th Annual ACM Symposium on Theory of Computing, pages 604–613, May 1998.  

[KOR98] E. Kushilevitz, R. Ostrovsky, and Y. Rabani. Efficient search for approximate nearest neighbor in highdimensional spaces. In 30th Annual ACM Symposium on Theory of Computing, pages 614–623. ACM, 1998.  

[KR02] D. Karger and M. Ruhl. Finding nearest neighbors in growth-restricted metrics. In 34th Annual ACM Symposium on the Theory of Computing, pages 63–66, 2002.  

## Appendix  A

证明。[引理 1.1] 设 $K$ 是 $X$ 的 KR-常数，并固定某个球 $B(x, 2r)$。我们将证明 $B(x, 2r)$ 可以被 $K^4$ 个半径为 $r$ 的球覆盖。这将表明 $\operatorname{dim}(X) \leq 4 \log_2 K = 4 \cdot \operatorname{dim}_{\mathrm{KR}}(X)$。

设 $Y$ 是 $B(x, 2r)$ 的一个 $r$-网，则$\displaystyle{}B(x, 2r) \subset \bigcup_{y \in Y} B(y, r) \subset B(x, 4r)$  

此外，对于每个 $y \in Y$，有 $|B(x, 4r)| \leq |B(y, 8r)| \leq K^4\left|B\left(y, \frac{r}{2}\right)\right|$。由于当 $y \neq y' \in Y$ 时，$B\left(y, \frac{r}{2}\right)$ 和 $B\left(y', \frac{r}{2}\right)$ 是不相交的，因此 $|Y| \leq K^4$。由此我们得出，$K^4$ 个球 $\{B(y, r): y \in Y\}$ 覆盖了 $B(x, 2r)$。