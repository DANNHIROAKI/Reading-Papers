## Rethinking the Role of Token Retrieval in Multi-Vector Retrieval 


### Abstract

Multi-vector retrieval models such as ColBERT Khattab and Zaharia, 2020] allow token-level interactions between queries and documents, and hence achieve state of the art on many information retrieval benchmarks. However, their nonlinear scoring function cannot be scaled to millions of documents, necessitating a three-stage process for inference: retrieving initial candidates via token retrieval, accessing all token vectors, and scoring the initial candidate documents. The non-linear scoring function is applied over all token vectors of each candidate document, making the inference process complicated and slow. In this paper, we aim to simplify the multi-vector retrieval by rethinking the role of token retrieval. We present XTR, ConteXtualized Token Retriever, which introduces a simple, yet novel, objective function that encourages the model to retrieve the most important document tokens first. The improvement to token retrieval allows XTR to rank candidates only using the retrieved tokens rather than all tokens in the document, and enables a newly designed scoring stage that is two-to-three orders of magnitude cheaper than that of ColBERT. On the popular BEIR benchmark, XTR advances the state-of-the-art by $2.8 \mathrm{nDCG} @ 10$ without any distillation. Detailed analysis confirms our decision to revisit the token retrieval stage, as XTR demonstrates much better recall of the token retrieval stage compared to ColBERT.


# 1 Introduction

The performance of a dense retrieval model is largely affected by how it defines expressive representations over queries and documents, and whether it can efficiently retrieve and score a document using these vector representations. For example, dual encoder models [Yih et al., 2011, Lee et al. 2019. Karpukhin et al., 2020, Ni et al. 2021] encode queries and documents into single vectors and compute query-document similarities using dot products. While these models are very efficient for retrieval, their expressivity is limited due to the absence of token-level modeling for scoring. In contrast, multi-vector models such as ColBERT [Khattab and Zaharia, 2020, Santhanam et al. 2022b are directly designed to capture token-level interactions. By utilizing a (non-linear) scoring function over all query and document token representations, multi-vector models enjoy much better model expressivity and often achieve superior results across various benchmarks [Thakur et al., 2021].

---

The enhanced model expressivity, however, comes at a great cost of inference complexity. Unlike the case in dual encoders, the non-linear scoring function in multi-vector retrieval models prohibits the use of efficient Maximum Inner Product Search (MIPS) [Ram and Gray, 2012, Shrivastava and Li, 2014, 2015, Shen et al., 2015] for finding the maximum scoring documents. As a result, models such as ColBERT adopt an intricate and resource-intensive inference pipeline, which typically consists of three stages: 1) token retrieval: using each query token to retrieve document tokens, with their source documents becoming candidates; 2) gathering: collecting all the token embeddings from each candidate document, including those that are not retrieved in the first stage (most document tokens are not retrieved); and 3) scoring: ranking candidates using a non-linear function based on all the token embeddings per document.

---

This procedure leads to two major issues. First, compared to the token retrieval stage, gathering all document token embeddings and re-scoring the documents can introduce orders of magnitude additional data loading and floating operation cost, making multi-vector models extremely expensive to deploy. Secondly, while the candidate documents are decided in the token retrieval stage, previous training objectives are designed for the scoring stage. This creates a significant training-inference gap causing multi-vector models achieve sub-optimal (and often poor) recall performance. Clearly, the three-stage pipeline has largely limited the potential of multi-vector models, raising an interesting research question - can the token retrieval stage alone be sufficient for great performance?

---

We present XTR, ContXextualized Token Retriever: a simplified and efficient method for multivector retrieval, through re-thinking the role of token retrieval. The key insight of XTR is that the token retrieval in multi-vector models should be trained to retrieve the most salient and informative document tokens, so that the score between a query and document can be computed using only the retrieved information, just like how single-vector retrieval models work. By doing so, the gathering step can be completely eliminated, and the cost of scoring is significantly reduced as only a fraction of the tokens need to be considered and the dot products from the token retrieval can be reused. To improve the quality of the token retrieval, XTR proposes a novel, yet simple, training objective, which dramatically improves retrieval accuracy, doubling the chances of a gold token being retrieved in the top- $k$ results. Furthermore, despite the improved token retrieval, some relevant tokens may still be missed (i.e., not retrieved). To address this issue, we propose a simple method, called missing similarity imputation, which accounts for the contribution of the missing tokens to the overall score.

---

XTR streamlines the inference process, bringing it closer to the straightforward procedure of dual encoders, while maintaining and enhancing the expressive scoring function of multi-vector retrieval models. On the BEIR [Thakur et al., 2021] and LoTTE [Santhanam et al., 2022b] benchmarks, XTR attains state-of-the-art performance, requiring neither distillation nor hard negatiave mining. Notably, our model surpasses state-of-the-art dual-encoder GTR [Ni et al., 2021] by 3.6 nDCG@ 10 on BEIR without any additional training data. On the EntityQuestions benchmark [Sciavolino et al., 2021], XTR outperforms the previous state-of-the-art by 4.1 points on top- 20 retrieval accuracy. XTR also does not require any secondary pre-training for retrieval and greatly outperforms mContriever [Izacard et al., 2022] on MIRACL, which contains multilingual retrieval tasks in 18 languages [Zhang et al. 2022b]. Our analysis supports that XTR indeed benefits from retrieving more contextualized tokens in relevant contexts, while making the scoring stage two-to-three orders of magnitude cheaper.

# 2 Background

## 2.1 Multi-vector Retrieval

Single-vector retrieval models, also known as dual encoders, encode an input text sequence as a single dense embedding and define the similarity of a query and a document based on the dot product |Lee et al., 2019, Karpukhin et al., 2020]. Multi-vector retrieval models, on the other hand, make use of multiple dense embeddings for each query and document, typically leveraging all contextualized word representations of the input to gain improved model expressivity.

---

Consider a query $Q=\left\{\mathbf{q}_{i}\right\}_{i=1}^{n}$ and a document $D=\left\{\mathbf{d}_{j}\right\}_{j=1}^{m}$ where $\mathbf{q}_{i}$ and $\mathbf{d}_{j}$ denote the $d$ dimensional query token vector and the document token vector, respectively. Multi-vector retrieval models compute the query-document similarity as follows: $f(Q, D)=\sum_{i=1}^{n} \sum_{j=1}^{m} \mathbf{A}_{i j} \mathbf{P}_{i j}$ where $\mathbf{P}_{i j}=\mathbf{q}_{i}^{\top} \mathbf{d}_{j}$ and $\mathbf{A} \in\{0,1\}^{n \times m}$ denotes the alignment matrix with $\mathbf{A}_{i j}$ being the token-level alignment between the query token vector $\mathbf{q}_{i}$ and the document token vector $\mathbf{d}_{j}$. The sum-of-max operator of ColBERT [Khattab and Zaharia, 2020] sets $\mathbf{A}_{i j}=\mathbb{1}_{\left[j=\operatorname{argmax}_{j^{\prime}}\left(\mathbf{P}_{i j^{\prime}}\right)\right]}$ where the argmax operator is over $1 \leq j^{\prime} \leq m$ (i.e., tokens from a single document $D$ ) and $\mathbb{1}_{[*]}$ is an indicator function. Then, $f_{\text {ColBERT }}(Q, D)$ is defined as follows:

$$
\begin{equation*}
f_{\text {ColBERT }}(Q, D)=\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{m} \mathbf{A}_{i j} \mathbf{P}_{i j}=\frac{1}{n} \sum_{i=1}^{n} \max _{1 \leq j \leq m} \mathbf{q}_{i}^{\top} \mathbf{d}_{j} . \tag{1}
\end{equation*}
$$

---


Here, we include the normalizer $n$, which was not included in the original sum-of-max, as it stabilizes training while not affecting the ranking during inference. After computing the query-document similarity, multi-vector retrieval models are typically trained with a cross-entropy loss over in-batch negatives [Santhanam et al. 2022b, Qian et al. 2022]. Specifically, given a positive document $D^{+}$ for $Q$ and a set of mini-batch documents $D_{1: B}=\left[D_{1}, \ldots, D_{B}\right]$ where $D^{+} \in D_{1: B}$, they minimize the cross-entropy loss defined as: $\mathcal{L}_{\mathrm{CE}}=-\log \frac{\exp f\left(Q, D^{+}\right)}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)}$.

## 2.2 Three-stage inference of Multi-vector Retrieval

Unlike dual encoder models, finding the maximum scoring document-the document that maximizes eq. (1)-cannot be directly handled by MIPS as the scoring function uses a non-linear, sum-of-max operation. Instead, a multi-vector retrieval model typically takes the following steps for the inference. 1) Token Retrieval: for each of the $n$ query token vectors, it first retrieves $k^{\prime}$ document token vectors, which is simply used to form initial candidate document set by taking the union of source documents of retrieved tokens. The total number of candidate documents is up to $n k^{\prime}$ if each token is coming from a unique document $\left.\right|^{2}$ 2) Gathering: since the scoring function eq. $(1)$ requires the computation over all document tokens, multi-vector models need to load all of the token vectors of the candidate documents. To optimize the loading process, a RAM-based index is often employed. 3) Scoring: to provide final ranks of candidate documents, multi-vector models score all the candidate documents with eq. (1). This stage is also called refinement. Note that the training of typical multi-vector models only takes care of the scoring stage with mini-batch documents. Finally, top- $k$ documents are returned based on the computed scores. The three-stage inference is illustrated in the top of Figure 1

- Figure 1: Overview of XTR. ColBERT has the three-stage inference combining (a) the token retrieval, (b) the gathering and (c) the scoring stages ( $\$ 2.2$. XTR leverages the token retrieval for both training and inference. XTR efficiently obtains the score of each candidate document by applying $f_{\mathrm{XTR}}$ (or $f_{\mathrm{XTR}^{\prime}}$ ) on the retrieved tokens, completely removing the gathering stage ( $\$ 3.2$ ).

# 3 XTR: Contextualized Token Retriever

Unlike existing multi-vector models that follow the retrieve-gather-score stages, XTR directly scores documents utilizing the tokens retrieved from the token retrieval stage. In this section, we start by showing why the existing cross entropy loss with the sum-of-max scoring function would fail on the first-stage token retrieval. Then, we introduce simple but important modifications for XTR. Given a positive document $D^{+}$and a set of negative documents $D_{1: r}^{-}=\left[D_{1}^{-}, \ldots, D_{r}^{-}\right]$for a query $Q$, the first-stage token retrieval needs to retrieve the tokens of $D^{+}$, but not the tokens of negative documents. However, the following example shows that the sum-of-max operator used by ColBERT is not specifically designed to retrieve tokens of relevant documents.

---

Failure case. Assume that $f_{\text {ColBERT }}\left(Q, D^{+}\right)=0.8$ where all the individual max token similarity (i.e., $\mathbf{q}_{i}^{\top} \mathbf{d}_{j}^{+}$ where $\mathbf{A}_{i j}=1$ ) is 0.8 . On the other hand, assume $f_{\text {ColBERT }}\left(Q, D^{-}\right)=0.2$ for all $D^{-} \in D_{1: r}^{-}$where each $D^{-}$ has a highly peaked token similarity greater than 0.8 but others close to zero (i.e., there exists $\mathbf{q}_{i}^{\top} \mathbf{d}_{j}^{-}>0.8$ where $\mathbf{A}_{i j}=1$ while other $\mathbf{q}_{i}^{\top} \mathbf{d}_{j}^{-} \rightarrow 0$ ). Since the sum-of-max operator only cares about the document-level scores, the cross entropy loss would be close to zero during training ${ }^{3}$ However, for each of $n$ query tokens, if there exists at least one negative document token that has a high token similarity greater than 0.8 , the token retrieval with top $-k^{\prime}=1$ would fail to retrieve any tokens of $D^{+}$. As a result, multivector retrieval model with the sum-of-max operator will not be able to lower the high scores of some negative tokens. Figure 2 shows that the sum-of-max training causes many document tokens to have unreasonably high scores regardless of their actual relevance to the query tokens.

- Figure 2: Density histogram of 4,000 token retrieval scores (cosine similarity). Training with $f_{\text {ColBERT }}$ (T5-ColBERT; §(4) causes many document tokens to have extremely high scores regardless of their actual relevance with respect to the input query tokens. XTR mitigates this problem with a better training objective.

## 3.1 In-Batch Token Retrieval

To train multi-vector retrieval models to directly retrieve tokens of relevant documents, we simulate the token retrieval stage during training. This can be simply achieved by employing a different alignment strategy $\hat{\mathbf{A}}$. Specifically, we set the alignment $\hat{\mathbf{A}}_{i j}=\mathbb{1}_{\left[j \in \text { top- } k_{j^{\prime}}\left(\mathbf{P}_{i j^{\prime}}\right)\right]}$ where the top- $k$ operator is applied over $1 \leq j^{\prime} \leq m B$ (i.e., tokens from $B$ mini-batch documents) returning the indices of $k$ largest values. During training, we use a hyperparameter $k_{\text {train }}$ for the top- $k$ operator. Then, we simply modify eq. (1) as follows:

$$
\begin{equation*}
f_{\mathrm{XTR}}(Q, D)=\frac{1}{Z} \sum_{i=1}^{n} \max _{1 \leq j \leq m} \hat{\mathbf{A}}_{i j} \mathbf{q}_{i}^{\top} \mathbf{d}_{j} \tag{2}
\end{equation*}
$$

The intuition is that we consider the token similarities within $D$ only when they are high enough to be retrieved within top- $k_{\text {train }}$ from a mini-batch. Here, we use a normalizer $Z=\mid\left\{i \mid \exists j\right.$, s.t. $\left.\hat{\mathbf{A}}_{i j}>0\right\} \mid$, which is essentially the number of query tokens that retrieved at least one document token of $D \bigsqcup^{4}$ If all $\hat{\mathbf{A}}_{i j}=0$, we clip $Z$ to a small number and $f_{\mathrm{XTR}}(Q, D)$ becomes 0 . As a result, our model cannot assign a high token similarity to negative documents as it blocks tokens of positive documents to be retrieved. With the previous failure case where $f_{\text {CoIBERT }}$ assigned a high score on $D^{+}$even though it cannot be retrieved, our similarity function incurs a high loss as $f_{\mathrm{XTR}}\left(Q, D^{+}\right)=0$ during training (since tokens of $D^{+}$were not retrieved). For training, we use the same cross entropy loss defined in $\S 2.1$ with our new scoring function. Note that the training data only contains document-level annotations, but XTR encourages important tokens from positive documents to be retrieved.

## 3.2 Scoring Documents using Retrieved Tokens

During inference, multi-vector retrieval models first have a set of candidate documents $\hat{D}_{1: C}$ from the token retrieval stage:

$$
\begin{equation*}
\hat{D}_{1: C}=\left\{\hat{D} \mid d_{j} \in \hat{D} \wedge d_{j} \in \operatorname{top}-k^{\prime}\left(\mathbf{q}_{*}\right)\right\} . \tag{3}
\end{equation*}
$$

Here, top $-k^{\prime}\left(\mathbf{q}_{*}\right)$ is a union of top- $k^{\prime}$ document tokens (from the entire corpus) based on the inner product scores with each query vector (i.e., $\mathbf{q}^{\top} \mathbf{d}$ ). Given the $n$ query token vectors, there are $C$ $\left(\leq n k^{\prime}\right)$ candidate documents. Previous methods load the entire token vectors of each document and compute eq. 1 for every query and candidate document pair, which takes $\mathcal{O}\left(n^{2} k^{\prime} \bar{m} d\right)$ computation per query ( $\bar{m}=$ average document length). Instead, we propose to score the documents solely using the retrieved token similarity. This significantly reduces the computational cost for the scoring stage since re-using the token retrieval scores removes computing redundant inner products and unnecessary (non-max) inner products. Furthermore, the expensive gathering stage (which requires loading all the document token vectors for computing eq. (1)) can be removed completely. Unlike previous work [Macdonald and Tonellotto, 2021] that leverages token retrieval to sort first-stage candidate documents before the scoring stage, we aim to directly provide the final scores of documents.

---

Missing similarity imputation. During inference, we retrieve $k^{\prime}$ document tokens for each of $n$ query tokens. Assume that each document token belongs to a unique document, providing $C=n k^{\prime}$ candidate documents in total. This leaves us with a single token similarity to score each document in the absence of the gathering stage. However, during training - either with eq. (1) or eq. (2) -each positive document has up to $n$ (max) token similarities to average, which mostly converges to $n$ as training proceeds. Hence, during inference, we impute the missing similarity for each query token treating each of candidate documents as if it were positive with $n$ token similarities.

---

For every candidate document $\hat{D}$, we first define the following scoring function for the inference:

$$
\begin{equation*}
f_{\mathrm{XTR}^{\prime}}(Q, \hat{D})=\frac{1}{n} \sum_{i=1}^{n} \max _{1 \leq j \leq m}\left[\hat{\mathbf{A}}_{i j} \mathbf{q}_{i}^{\top} \mathbf{d}_{j}+\left(1-\hat{\mathbf{A}}_{i j}\right) m_{i}\right] . \tag{4}
\end{equation*}
$$

This is similar to eq. (2), but introduces $m_{i} \in \mathbb{R}$, which estimates the missing similarity for each $q_{i}$. $\hat{\mathbf{A}}$ is defined similar to the one described in eq. (2) except that it uses $k^{\prime}$ for the top- $k$ operator. Each $q_{i}$ would take the missing similarity $m_{i}$ as the maximum value if $\hat{\mathbf{A}}_{i *}=0$ and $m_{i} \geq 0$. Importantly, $f_{\mathrm{XTR}^{\prime}}$ removes the need of recomputing any $\mathbf{q}_{i}^{\top} \mathbf{d}_{j}$ since when $\hat{\mathbf{A}}_{i j}=1$ we already know the retrieval score from the token retrieval stage, and when $\hat{\mathbf{A}}_{i j}=0$ we simply don't need to compute it as $\hat{\mathbf{A}}_{i j} \mathbf{q}_{i}^{\top} \mathbf{d}_{j}=0$. Note that when every $\hat{\mathbf{A}}_{i j}=1$, the equation becomes the sum-of-max operator. On the other hand, when no document tokens of $\hat{D}$ were retrieved for $q_{i}$ (i.e., $\hat{\mathbf{A}}_{i *}=0$ ), we fall back to the imputed score $m_{i}$, which provides an approximated sum-of-max result.

---

In fact, we can find the upper bound of the missing similarity. For every token retrieval with $\mathbf{q}_{i}$, the missing similarity of the query token for $\hat{D}$ will be upper bounded by its last top- $k^{\prime}$ score. Specifically, for each query token $q_{i}$, we have the following top $-k^{\prime}$ token similarity during inference: $\left[\mathbf{q}_{i}^{\top} \mathbf{d}_{(1)}, \ldots \mathbf{q}_{i}^{\top} \mathbf{d}_{\left(k^{\prime}\right)}\right]$. Here, each $\mathbf{d}_{(*)}$ could come from a different document. Since the missing similarity would have a score less than equal to the score of the last retrieved token, we know that $m_{i} \leq \mathbf{q}_{i}^{\top} \mathbf{d}_{\left(k^{\prime}\right)}$. With a larger $k^{\prime}$, the upper bound becomes tighter. In our experiments, we show that simply choosing $m_{i}=\mathbf{q}_{i}^{\top} \mathbf{d}_{\left(k^{\prime}\right)}$ works well especially when a model is trained with $f_{\mathrm{XTR}}{ }^{5}$ While we also tried more complicated imputation methods based on regression, our method was competitive enough despite its simplicity. The imputation process is illustrated in Figure 3 .

- Figure 3: Comparison of $f_{\text {ColBERT }}$ in eq. (1) and $f_{\mathrm{XTR}^{\prime}}$ in eq. (4). Assume that $D_{a}$ and $D_{b}$ were selected as initial candidate documents from the token retrieval stage. $f_{\text {ColBERT }}$ loads all token vectors of $D_{a}$ and $D_{b}$ and exhaustively recomputes pairwise token similarity to obtain the max values (red boxes). On the other hand, $f_{\mathrm{XTR}^{\prime}}$ does not load any token vectors and reuses retrieval scores from the first-stage token retrieval. Assume that, with the top-2 token retrieval results, the first query token retrieved each max score of $D_{a}$ and $D_{b}$, but the second query token retrieved two tokens only from $D_{a}$ but not $D_{b}$. We impute the missing similarity $m$ for $D_{b}$ (denoted as yellow dashed box) by finding its upper bound using the top-2 score (denoted as $s_{2}$ ) of the second query token (i.e., $m \leq s_{2} \leq s_{1}$ ).

---

Table 1: FLOPs comparison of ColBERT and XTR for the scoring stage. XTR only adds minimal complexity for scoring each candidate document. The setting is derived from MS MARCO.

- Table 1 shows the estimated FLOPs of ColBERT and XTR (see Appendix Bfor more details). Due to the differences in hardware and infrastructure, we mainly compared the theoretical FLOPs. XTR reduces the FLOPs at the scoring stage by $4000 \times$ making multi-vector retrieval more efficient.

# 4 Experiments

Experimental Setting. Following Ni et al. [2021], we fine-tune XTR on MS MARCO with a fixed set of hard negatives from RocketQA [Qu et al., 2021]. Then, we test XTR on MS MARCO (MS; in-domain) and zero-shot IR datasets. For the zero-shot evaluation, we use 13 datasets from BEIR [Thakur et al. 2021] (see Appendix Cfor acronyms), 12 datasets from LoTTE [Santhanam et al., 2022b], and 4 datasets on open-domain QA passage retrieval (EQ: EntityQuestions [Sciavolino et al., 2021], NQ, TQA: TriviaQA, SQD: SQuAD). We also train multilingual XTR (mXTR) and evaluate it on MIRACL [Zhang et al., 2022b], which contains retrieval tasks in 18 languages. The performance gap between T5-ColBERT [Qian et al. 2022] and XTR shows the improvement with our methods on a multi-vector retrieval model. For implementation details and baselines, see Appendix C For the relationship between hyperparameters (e.g., $k_{\text {train }}$ and $k^{\prime}$ ), see $\S 5.3$.


## 4.1 In-domain Document Retrieval

MS MARCO. The first column of Table 2 (top) shows nDCG@ 10 on MS MARCO (see Table D. 1 for recall@100). XTR outperforms most models and remains competitive with T5-ColBERT. This is encouraging since XTR significantly reduces the cost of the gathering-scoring stage. Note that MS MARCO may fail to reflect the actual improvement of state-of-the-art Arabzadeh et al. 2022.

- Table 2: (top) nDCG@ 10 on MS MARCO (in-domain) and BEIR (zero-shot). The last column shows the average over 13 BEIR datasets. (bottom) Top-5 retrieval accuracy on LoTTE datasets (zero-shot).

## 4.2 Zero-shot Document Retrieval

BEIR & LoTTE. Table 2 (top; except the first columns) shows nDCG@10 on BEIR (see Table D. 1 for recall@100). $\mathrm{XTR}_{\mathrm{xx1}}$ achieves the new state-of-the-art performances significantly outperforming both per-domain models and single model state-of-the-art. Simply scaling XTR removes the needs of designing distillation or hard negative mining pipelines [Santhanam et al. 2022b Formal et al., 2021]. Results on LoTTE (Table 2 bottom) also show that XTR base is better than ColBERT and competitive with distillation-based models while $\mathrm{XTR}_{\mathrm{xx} 1}$ advances the state-of-the-art.

---

Passage retrieval for open-domain QA. Table 3 shows results on four open-domain QA datasets. While previous work often includes sparse retrievers (e.g., BM25) [Chen et al. 2021] or contrastive pre-training [Ram et al., 2022, Sachan et al., 2022ab] to achieve better performances on EntityQuestions, XTR simply fine-tuned on MS MARCO achieves the state-of-the-art performance.

- Table 3: Zero-shot passage retrieval accuracy on open-domain question answering datasets. In-domain performances are underlined and all the other performances are based on the zero-shot evaluation. For EntityQuestions, we report macro-averaged performances over different relations.


### 4.3 Multilingual Document Retrieval

MIRACL. Since XTR does not need any secondary pre-training, we expect it to be better at multilingual retrieval by better utilizing the multilingual language models. We train a multilingual version of XTR with mT5 [Xue et al. 2021] and test it on multilingual retrieval tasks in 18 languages. Table 4 shows that mXTR greatly outperforms mContriever that uses expensive contrastive pretraining, as well as the hybrid model, BM25 + mDPR.

- Table 4: nDCG@10 on 18 multilingual retrieval tasks from MIRACL. Each row shows the performance of a single multilingual retrieval model. The last two surprise languages (de and yo) are not included in the training dataset of MIRACL. The last column shows the average over $18 \overline{\text { languages. }}$


## 5 Analysis

### 5.1 Towards Better Token Retrieval

Gold token retrieval. If the tokens of gold documents are not retrieved at all, multi-vector retrieval models would fail to retrieve the gold documents. Hence, a better token retrieval would contain these gold tokens more often in their top results. In Figure4(top), we show the probability of a token at the rank $k$ coming from the gold documents of a query. To compute the probability for the rank $k$, we simply count the number of an event where a token at rank $k$ belongs to the gold document and divide it by the number of tokens at rank $k$. While this is measuring the precision of the token retrieval, we observed a similar trend for the recall of gold tokens. Compared to T5-ColBERT, XTR retrieves gold tokens with higher probability, even on MS MARCO. This shows that the training objective of XTR encourages it to retrieve tokens from more relevant context.

- Figure 4: (top) Gold token retrieval performances of T5-ColBERT and XTR. We plot the probability of each retrieved document token at rank $k$ coming from the gold document. (bottom) Lexical token retrieval performances of T5-ColBERT and XTR. We plot the probability of each retrieved document token at rank $k$ being lexically identical to its query token.

---

Lexical token retrieval. In Figure 4 (bottom), we show the probability of a token at the rank $k$ being the same as its query token (e.g., 'insulin' retrieving 'insulin's). T5-ColBERT has very high probability of retrieving the same token across different ranks and datasets. However, it is unclear to what extent the token retrieval stage should behave as sparse retrieval, as it might suffer from the vocabulary mismatch problem. XTR effectively lowers the reliance on the lexical matching while preserving a good amount of lexical precision so that it would achieve a high retrieval accuracy on the entity-centric dataset ( $\$ 4.2$ ). In fact, Table 6 in Appendix shows that having lower lexical matching doesn't necessarily mean a lower retrieval quality, but often means better contextualization.

## 5.2 Efficient Scoring

In Table 5, we show how we can employ the efficient scoring function $f_{\mathrm{XTR}^{\prime}}$ in XTR with minimal performance losses. We apply $f_{\mathrm{XTR}^{\prime}}$ on both T5-ColBERT and XTR, and show their performances on MS MARCO. With T5-ColBERT, even if we use the top- $k$ ' score for the imputation, the performance is much worse than the original sum-of-max scoring. With XTR, the performance greatly improves as it has better token retrieval. Figure 5 shows how Recall@ 100 improves with larger $k^{\prime}$ 's as it provides more exact upper bound for the missing similarity imputation. Table D. 2 shows that even if we use smaller $k^{\prime}$, XTR still maintains high performances on BEIR.

- Table 5: Impact of training objectives and imputation methods comparing T5-ColBERT and XTR. For both models, we apply $f_{\text {XTR }^{\prime}}$ during inference. We report MRR@10 and Recall@ 1000 on the MS MARCO development set.

- Figure 5: Recall@100 of XTR and T5-ColBERT with different $k^{\prime}$. For T5-ColBERT, we use either $f_{\mathrm{XTR}^{\prime}}$ or $f_{\text {ColBERT }}$.


### 5.3 Relationship between Hyperparameters

$\boldsymbol{k}_{\text {train }}$ vs. $\boldsymbol{k}^{\prime} \quad$ In Figure 6 , we show MRR@ 10 of XTR trained with different $k_{\text {train }}$ and evaluated with different $k^{\prime}$ on the MS MARCO development set. While all variants of XTR prefer larger $k^{\prime}$, ones trained with smaller $k_{\text {train }}$ show higher performances than others under small $k^{\prime}$ settings. XTR with larger $k_{\text {train }}$ exhibits better performances than ones with smaller $k_{\text {train }}$ as $k^{\prime}$ becomes larger.

- Figure 6: MRR@ 10 of XTR with different $k_{\text {train }}$ and $k^{\prime}$. For T5-ColBERT, we also use $f_{\mathrm{XTR}^{\prime}}$ with the top- $k^{\prime}$ score imputation method for the inference.

---

Training batch size vs. $\boldsymbol{k}_{\text {train }}$ In Figure 7, we show the relationship between the training batch size and $k_{\text {train }}$ during training XTR. In this experiment, we use $k^{\prime}=40,000$. While it is evident that XTR mostly favors large training batch sizes, the optimal top $-k_{\text {train }}$ can be different for different datasets. While most datasets including MS MARCO favored a large enough $k_{\text {train }}$, ArguAna prefers smaller $k_{\text {train }}$. We hypothesize that this is due to the longer query length in ArguAna, which makes multi-vector models fall short compared to dual-encoders (see GTR vs. T5-ColBERT in Table 2).

- Figure 7: Effect of training XTR with different batch sizes and $k_{\text {train }}$. For each point of the graph, we train $\mathrm{XTR}_{\text {base }}$ with the specified training batch size $(128,256,320)$ and $k_{\text {train }}(32$, $64,128,256$ ) and evaluate on each dataset (MS MARCO and ArguAna). nDCG@10 of each model is reported.

## 5.4 Qualitative Analysis

Table 6 shows a prediction sample from MS MARCO. For T5-ColBERT, all of the top retrieved tokens are exact lexical matches. Surprisingly, none of the retrieved passages are about the query, demonstrating T5-ColBERT's failure to retrieve tokens from the correct context. In contrast, XTR retrieves fewer exact lexically matching tokens, but the contexts of the retrieved tokens are much more related to the query. This example explains the lower lexical token retrieval probability of XTR compared to T5-ColBERT in Figure 4 (bottom), but higher gold token retrieval performance in Figure4(top). For more qualitative examples, please see Appendix E

- Table 6: Token retrieval example from MS MARCO. Among the top 100 retrieved tokens, $100 \%$ of T5-ColBERT tokens are lexically identical as the query token usual while only $8 \%$ of XTR tokens are lexically identical. XTR retrieves the relevant passage by retrieving average for usual.

# 6 Related Work

One of the main limitations of dense retrieval models is that encoding the query and document into a single vector constrains the representational power of the models. Polyencoder Humeau et al. 2020], MEBERT [Luan et al. 2021], and MVR [Zhang et al. 2022a] propose to use multiple embeddings, instead of one, to represent the query or the document. A more recent approach is token-level multi-vector retrieval, which stores and retrieves with every token embedding. ColBERT [Khattab and Zaharia, 2020] is probably the most renowned model in this family. ALIGNER (i.e. T5ColBERT) [Qian et al., 2022] extends ColBERT by scaling up the backbone langauge model and studying various strategies for aggregating the token-level alignment scores. These token-level retrieval models show strong effectiveness and out-of-domain generalization ability.

---

Efforts for reducing serving costs of multi-vector models have been mostly focused on the token-level retrieval stage. COIL [Gao et al. 2021] accelerates token-level retrieval by confining retrieval within exact match tokens, sharing the spirit of classic inverted indexing. CITADEL [Li et al., 2022] relaxes COIL with a lexical routing mechanism where a query token vector only retrieves from a subset of document token vectors routed to the same key. PLAID [Santhanam et al., 2022a] optimizes the speed of ColBERT by pruning weaker candidates in the earlier stages of retrieval and using better vector quantization. ColBERT-v2 [Santhanam et al., 2022b] further adopts residual representations with cluster centroids to improve the efficiency of ColBERT. On the other hand, how to accelerate the scoring stage remains under-explored. To the best of our knowledge, XTR is the first work to simplify the scoring stage and remove the gathering stage in multi-vector retrieval.

# 7 Conclusion

Multi-vector retrieval leverages query and document token representations for effective information retrieval. In this paper, we propose XTR that simplifies the existing three-stage inference of multivector models by improving the initial token retrieval stage. Specifically, XTR scores documents solely based on the retrieved tokens, which is also optimized during training with in-batch document tokens. As a result, XTR achieves state-of-the-art performances on zero-shot information retrieval benchmarks while greatly reducing the FLOPs of the scoring stage. We further show that our objective function indeed encourages better token retrieval, retrieving more tokens from gold documents, whose contexts are better aligned with the query.

# 8 Limitations

In most of our experiments, XTR was trained on MS MARCO, a large-scale retrieval dataset in English. While our experiments were conducted in a fair setting where most baseline models also utilize MS MARCO, future use cases might need to remove its dependency on MS MARCO due to the license or language-specific issue. We believe that LLM-based retrieval dataset generation [Dai] et al., 2022] would be able to mitigate the problem in the future.


# A Derivatives w.r.t. Similarity Scores

Sum-of-max. Here, we use a cross-entropy loss $\mathcal{L}_{\text {CE }}$ with the sum-of-max operator $f_{\text {ColBERT }}$ and analyze the derivatives with respect to the token similarity scores.

$$
\begin{align*}
& \mathcal{L}_{\mathrm{CE}}=-\log \frac{\exp f\left(Q, D^{+}\right)}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)}=-f_{\mathrm{CoIBERT}}\left(Q, D^{+}\right)+\log \sum_{b=1}^{B} \exp f_{\mathrm{CoIBERT}}\left(Q, D_{b}\right)  \tag{5}\\
& f_{\mathrm{CoIBERT}}(Q, D)=\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{m} \mathbf{A}_{i j} \mathbf{P}_{i j}=\frac{1}{n} \sum_{i=1}^{n} \mathbf{P}_{i \hat{j}} \tag{6}
\end{align*}
$$

---

Here, we denote $\hat{j}$ as the index of the row-wise maximum value, dependent on each $i$ (i.e., $\mathbf{A}_{i j}=1$ ). Given the cross-entropy loss with the sum-of-max operator, we compute the gradient with respect to one of the maximum token similarities $\mathbf{P}_{i \hat{j}}^{+}$for a positive document $D^{+} \in D_{1: B}$ :

$$
\begin{aligned}
\frac{\partial \mathcal{L}_{\mathrm{CE}}}{\partial \mathbf{P}_{i \hat{j}}^{+}} & =-\frac{f\left(Q, D^{+}\right)}{\partial \mathbf{P}_{i \hat{j}}^{+}}+\frac{1}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)} \frac{\partial}{\partial \mathbf{P}_{i \hat{j}}^{+}} \sum_{b=1}^{B} \exp f\left(Q, D_{b}\right) \\
& =-\frac{\partial}{\partial \mathbf{P}_{i \hat{j}}^{+}} \frac{1}{n} \sum_{i=1}^{n} \max _{1 \leq j \leq m} \mathbf{P}_{i j}^{+}+\frac{1}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)} \sum_{b=1}^{B} \frac{\partial}{\partial \mathbf{P}_{i \hat{j}}^{+}} \exp f\left(Q, D_{b}\right) \\
& =-\frac{1}{n}+\frac{1}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)} \sum_{b=1}^{B} \exp f\left(Q, D_{b}\right) \frac{\partial f\left(Q, D_{b}\right)}{\partial \mathbf{P}_{i \hat{j}}^{+}} \\
& =-\frac{1}{n}+\frac{1}{n} \frac{\exp f\left(Q, D^{+}\right)}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)}=-\frac{1}{n}\left[1-P\left(D^{+} \mid Q, D_{1: B}\right)\right]
\end{aligned}
$$

---

Similarly, the gradient w.r.t. a maximum token similarity $\mathbf{P}_{i \hat{j}}^{-}$for a negative document $D^{-} \in D_{1: B}$ is computed as follows:

$$
\begin{aligned}
\frac{\partial \mathcal{L}_{\mathrm{CE}}}{\partial \mathbf{P}_{i \hat{j}}^{-}} & =-\frac{f\left(Q, D^{+}\right)}{\partial \mathbf{P}_{i \hat{j}}^{-}}+\frac{1}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)} \frac{\partial}{\partial \mathbf{P}_{i \hat{j}}^{-}} \sum_{b=1}^{B} \exp f\left(Q, D_{b}\right) \\
& =\frac{1}{n} \frac{\exp f\left(Q, D^{-}\right)}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)}=\frac{1}{n} P\left(D^{-} \mid Q, D_{1: B}\right) .
\end{aligned}
$$

---

Hence, the positive token-level score $\mathbf{P}_{i \hat{j}}^{+}$will gradually increase until $P\left(D^{+} \mid Q, D_{1: B}\right) \rightarrow 1$ and the negative token-level score $\mathbf{P}_{i \hat{j}}^{-}$will decrease until $P\left(D^{-} \mid Q, D_{1: B}\right) \rightarrow 0$. This shows that the tokenlevel scores are trained based on the document-level scores, which might stagnate the token-level scores. For instance, even if $\mathbf{P}_{i \hat{j}}^{-}$is very high-later causing $\mathbf{d}_{\hat{j}}^{-}$to be retrieved instead of ones from positive documents-it will not be penalized as long as $P\left(D^{-} \mid Q, D_{1: B}\right)$ is low enough.

---

In-batch token retrieval. Compared to the sum-of-max operator, our in-batch sum-of-max $f_{\mathrm{XTR}}$ considers the max values only when they are retrieved over other negative tokens in the mini-batch.

$$
f_{\mathrm{XTR}}\left(Q, D_{1: B}\right)=\frac{1}{Z} \sum_{i=1}^{n} \sum_{j=1}^{m} \mathbf{A}_{i j} \hat{\mathbf{A}}_{i j} \mathbf{P}_{i j}=\frac{1}{Z} \sum_{i=1}^{n} \mathbf{P}_{i \bar{j}}
$$

Here, we denote $\bar{j}$ as the index of the row-wise maximum value that is also within the mini-batch top- $k_{\text {train }}$ given $q_{i}$ (i.e., satisfies both $\mathbf{A}_{i j}=1$ and $\hat{\mathbf{A}}_{i j}=1$ ). If there is no such $\bar{j}$, we simply use $\mathbf{P}_{i \bar{j}}=0$. We also use a normalizer $Z$, which is the number of non-zero $\mathbf{P}_{i \bar{j}}$. In this analysis, we assume $Z>0$ since if every $\mathbf{P}_{i \bar{j}}$ is zero, the gradient is undefined.

---

The gradient w.r.t. the maximum token similarity $\mathbf{P}_{i \bar{j}}^{+}$(non-zero) for a positive document $D^{+} \in D_{1: B}$ is computed as follows:

$$
\begin{aligned}
\frac{\partial \mathcal{L}_{\mathrm{CE}}}{\partial \mathbf{P}_{i \bar{j}}^{+}} & =-\frac{f\left(Q, D^{+}\right)}{\partial \mathbf{P}_{i \bar{j}}^{+}}+\frac{1}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)} \frac{\partial}{\partial \mathbf{P}_{i \bar{j}}^{+}} \sum_{b=1}^{B} \exp f\left(Q, D_{b}\right) \\
& =-\frac{1}{Z^{+}}\left[1-\frac{\exp f\left(Q, D^{+}\right)}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)}\right] \\
& =-\frac{1}{Z^{+}}\left[1-P\left(D^{+} \mid Q, D_{1: B}\right)\right]
\end{aligned}
$$

This is a very similar result compared to the sum-of-max operator except that 1 ) the gradient is defined only when $\mathbf{P}_{i \bar{j}}^{+}$is non-zero (i.e. retrieved) and 2 ) it is dependent on $Z^{+}$, which means that the gradient will be large whenever there is a small number of retrieved tokens from the positive document. If only a handful of tokens are retrieved for $D^{+}$, our objective function increases $\mathbf{P}_{i \bar{j}}^{+}$.

---

For negative similarity score $\mathbf{P}_{i \bar{j}}^{-}$, we have the following:

$$
\begin{aligned}
\frac{\partial \mathcal{L}_{\mathrm{CE}}}{\partial \mathbf{P}_{i \bar{j}}^{-}} & =-\frac{f\left(Q, D^{+}\right)}{\partial \mathbf{P}_{i \bar{j}}^{-}}+\frac{1}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)} \frac{\partial}{\partial \mathbf{P}_{i \bar{j}}^{-}} \sum_{b=1}^{B} \exp f\left(Q, D_{b}\right) \\
& =-\frac{1}{Z^{-}}\left[-\frac{\exp f\left(Q, D^{-}\right)}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)}\right] \\
& =\frac{1}{Z^{-}} P\left(D^{-} \mid Q, D_{1: B}\right)
\end{aligned}
$$

Again, it is similar to the sum-of-max result, but it depends on $Z^{-}$. In this case, even when $P\left(D^{-} \mid Q, D_{1: B}\right)$ is low, if there is a small number of retrieved tokens from $D^{-}$(i.e., small $Z^{-}$), $\mathbf{P}_{i \hat{j}}^{-}$ will be decreased significantly. Note that when $Z^{-}$is large, $Z^{+}$naturally becomes smaller as they compete for in-batch token retrieval, which causes positive tokens to have higher scores.

# B Inference Complexity

We compare the complexity of ColBERT and XTR during the scoring stage in terms of FLOPs. We do not measure the complexity for the online query encoding and maximum inner product search (MIPS), which have been extensively studied for both dual encoders and multi-vector retrieval [Santhanam et al. 2022abb, Guo et al. 2020].

---

For the scoring stage, both ColBERT and XTR have $\mathcal{O}\left(n k^{\prime}\right)$ candidate documents. Here, we assume the worst case $n k^{\prime}$ where each document token comes from a unique document. For each candidate document, ColBERT loads a set of document vectors of $\bar{m} d$ floating points ( $\bar{m}=$ average document length) and computes eq. (1) with the query vectors of $n d$ floating points. Computing eq. (1) per candidate document requires $2 n \bar{m} d$ FLOPs for token-level inner products, $n \bar{m}$ for finding the rowwise max, and $n$ for the final average. In total, ColBERT requires $n^{2} k^{\prime}(2 \bar{m} d+\bar{m}+1)$ FLOPs for the scoring stage. Note that this does not include the latency of loading the $\mathcal{O}\left(n k^{\prime} \bar{m} d\right)$ floating points onto the memory, which amounts up to 450 MB per query when $n=16, k^{\prime}=1000, \bar{m}=55, d=128$.

---

On the other hand, XTR first imputes the missing similarity, which is simply done by caching the $k^{\prime}$-th token retrieval score for each query token. Then, each of $n k^{\prime}$ candidate documents requires $n \bar{r}$ FLOPs for finding row-wise max and $n$ for the average where $\bar{r}$ is the average number of retrieved tokens per each candidate document. In total, we have $n^{2} k^{\prime}(\bar{r}+1)$ FLOPs. Table 1 shows the estimated FLOPs of the two models. XTR reduces the FLOPs at the scoring stage by $4000 \times$ making multi-vector retrieval more efficient and practical.

## C Implementation Details

XTR uses $k_{\text {train }}$ for retrieving in-batch document tokens. Since we retrieve over mini-batches, the size of mini-batch affects the performance for different $k_{\text {train }}$, which is shown in $\$ 5.3$. In our experiments, we tried $k_{\text {train }}=\{32,64,128,256,320\}$ for each batch size and choose the best model based on their performance on the MS MARCO development set. For inference, XTR uses $k^{\prime}$ for the token retrieval. We use $k^{\prime}=40,000$, which is possible due to the efficient scoring stage of XTR ${ }^{6}$ We analyze the effect of using different $k^{\prime}$ 's as well as its relationship to $k_{\text {train }}$ in $\$ 5.3$. We initialize XTR from the base and xxl versions of the T5 encoder Raffel et al., 2020] and provide XTR base and $\mathrm{XTR}_{\mathrm{xxl}}$. For multilingual XTR, we initialize XTR from mT5 [Xue et al., 2021]. We fine-tune XTR for 50,000 iterations with the learning rate to $1 \mathrm{e}-3$. Up to 256 chips of TPU v3 accelerator were used depending on the size of the model. We use ScaNN [Guo et al. 2020] for the MIPS during the token retrieval stage. For BEIR, we use 13 datasets (AR: ArguAna. TO: Touché-2020. FE: Fever. CF: Climate-Fever. SF: Scifact. CV: TREC-COVID. NF: NFCorpus. NQ: Natural Questions. HQ: HotpotQA. FQ: FiQA-2018. SD: SCIDOCS. DB: DBPedia. QU: Quora).

---

Baselines. There are two main paradigms on training retriever models for the out-of-domain evaluation. The first group trains a single retriever for each dataset (or domain) by generating queries for each out-of-domain corpus. Typically, this approach generates $N$ datasets to train $N$ independent models for $N$ different domains. For this one-retriever-per-domain approaches, we include GenQ [Thakur et al., 2021], GPL [Wang et al., 2022], and Promptagator [Dai et al., 2022]. The second group builds a single retriever-typically trained on a large-scale IR dataset such as MS MARCO-and directly applies it on the out-of-domain corpora and queries. For this one-retriever-for-all approaches, we present results of state-of-the-art retrievers including Splade ${ }_{\mathrm{v} 2}$ [Formal et al. 2021], ColBERT ${ }_{\mathrm{v} 2}$ [Santhanam et al., 2022b], and $\mathrm{GTR}_{\mathrm{xx}}$ [Ni et al., 2021]. We also show the results of T5-ColBERT ${ }_{\mathrm{xxl}}$ Qian et al.| 2022], which is a T5-initialized ColBERT model and shares the same backbone LM and training dataset with XTR. Note that T5-ColBERT uses the heavy scoring stage based on the original sum-of-max. All of our one-retriever-for-all baselines, as well as XTR, are trained on English MS MARCO, unless otherwise stated.


# D Additional Results

In Table D.1, we show Recall@ 100 on BEIR.

Table D.1: Recall@ 100 on MS-MARCO and BEIR. The last column shows the average over 13 BEIR benchmarks. Compared to GTR, T5-ColBERT only marginally improves the recall. On the other hand, XTR greatly improves the recall showing the importance of having a better token retrieval.

---

In Table D.2, we shownDCG@ 10 and Recall@ 100 on BEIR with different $k^{\prime}$.

Table D.2: nDCG@10 and Recall@100 of XTR ${ }_{\text {base }}$ on MS-MARCO and BEIR with different $k^{\prime}$. The last column shows the average over 13 BEIR benchmarks.

# E Qualitative Analysis

In Table E.5, we show token retrieval results from T5-ColBERT and XTR.

Table E.1: Token retrieval example from MS MARCO for the token "la" in the query "lauren london age". Among the top 100 retrieved tokens, $100 \%$ of T5-ColBERT tokens are lexically identical as the query token la and $100 \%$ of XTR tokens are also lexically identical. However, top retrieved results from XTR contain the correct entity (Lauren London) while those from T5-ColBERT are about wrong entities (Laura Bush, Laura Branigan, etc.).

---

Table E.2: Token retrieval example from MS MARCO for the token "temple" in the query "temple university student population?". Among the top 100 retrieved tokens, $100 \%$ of T5-ColBERT tokens are lexically identical as the query token temple and $100 \%$ of XTR tokens are also lexically identical However, top retrieved results from XTR are of the correct context (student population) while those from T5-ColBERT are off-topic (e.g., tuition, salary, etc.).

---

Table E.3: Token retrieval example from MS MARCO for the token "aire" in the query "aire is expressed in some skin tumors". Among the top 100 retrieved tokens, $77 \%$ of T5-ColBERT tokens are lexically identical as the query token aire and $77 \%$ of XTR tokens are also lexically identical. Top retrieved results from XTR are relevant to the query (about cancer, tumor, skin, and melanocyte), while those from T5-ColBERT are off-topic.

---

Table E.4: Token retrieval example from Scifact for the token "later" in the query "women with a higher birth weight are more likely to develop breast cancer later in life". Among the top 100 retrieved tokens, $72 \%$ of T5-ColBERT tokens are lexically identical as the query token later while only $33 \%$ of XTR tokens are lexically identical. Top retrieved results from XTR can retrieves synonyms (sebsequent) from relevant context, while those from T5-ColBERT are off-topic.

---

Table E.5: Token retrieval example from Scifact for the token "thinner" in the query "vanules have a thinner or absent smooth later compared to arterioles". Among the top 100 retrieved tokens, only $1 \%$ of T5-ColBERT tokens are lexically identical as the query token thinner and only $1 \%$ of XTR tokens are also lexically identical.