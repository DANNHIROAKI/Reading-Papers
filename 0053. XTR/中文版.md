## 重新思考Token检索在多向量检索中的作用

### 摘要

诸如ColBERT [Khattab和Zaharia, 2020]之类的多向量检索模型允许查询和文档之间的Token级别交互，从而在许多信息检索基准上取得了最先进的性能。然而，它们的非线性评分函数无法扩展到数百万文档，因此需要一个三阶段的推理过程：通过Token检索获取初始候选文档，访问所有Token向量，并对初始候选文档进行评分。非线性评分函数应用于每个候选文档的所有Token向量，使得推理过程复杂且缓慢。在本文中，我们旨在通过重新思考Token检索的作用来简化多向量检索。我们提出了XTR（ConteXtualized Token Retriever），它引入了一个简单但新颖的目标函数，鼓励模型首先检索最重要的文档Token。Token检索的改进使得XTR能够仅使用检索到的Token而非文档中的所有Token对候选文档进行排序，并实现了一个新设计的评分阶段，其成本比ColBERT低两到三个数量级。在流行的BEIR基准上，XTR在没有使用任何蒸馏技术的情况下，将最先进水平提高了$2.8 \mathrm{nDCG} @ 10$。详细分析证实了我们重新审视Token检索阶段的决定，因为与ColBERT相比，XTR在Token检索阶段的召回率显著提高。

# 1 引言

密集检索模型的性能在很大程度上取决于它如何定义查询和文档的表达性表示，以及它是否能够使用这些向量表示高效地检索和评分文档。例如，双编码器模型[Yih等, 2011, Lee等, 2019, Karpukhin等, 2020, Ni等, 2021]将查询和文档编码为单个向量，并使用点积计算查询-文档相似度。虽然这些模型在检索方面非常高效，但由于缺乏用于评分的Token级别建模，它们的表达能力受到限制。相比之下，诸如ColBERT [Khattab和Zaharia, 2020, Santhanam等, 2022b]之类的多向量模型直接设计用于捕捉Token级别的交互。通过利用所有查询和文档Token表示的非线性评分函数，多向量模型具有更好的模型表达能力，并在各种基准上取得了优异的结果[Thakur等, 2021]。

---

然而，增强的模型表达能力伴随着极高的推理复杂性代价。与双编码器不同，多向量检索模型中的非线性评分函数禁止使用高效的最大内积搜索（Maximum Inner Product Search, MIPS）[Ram和Gray, 2012, Shrivastava和Li, 2014, 2015, Shen等, 2015]来找到评分最高的文档。因此，诸如ColBERT之类的模型采用了一个复杂且资源密集的推理管道，通常包括三个阶段：1）Token检索：使用每个查询Token检索文档Token，其来源文档成为候选文档；2）收集：从每个候选文档中收集所有Token嵌入，包括第一阶段未检索到的Token（大多数文档Token未被检索）；3）评分：使用基于每个文档所有Token嵌入的非线性函数对候选文档进行排序。

---

这一过程导致了两个主要问题。首先，与Token检索阶段相比，收集所有文档Token嵌入并重新评分文档会引入数量级额外的数据加载和浮点运算成本，使得多向量模型的部署极为昂贵。其次，尽管候选文档在Token检索阶段确定，但之前的训练目标是为评分阶段设计的。这造成了显著的训练-推理差距，导致多向量模型的召回性能次优（通常较差）。显然，三阶段管道在很大程度上限制了多向量模型的潜力，引发了一个有趣的研究问题——仅凭Token检索阶段是否足以实现卓越性能？

---

我们提出了XTR（ConteXtualized Token Retriever）：一种通过重新思考Token检索作用而实现的简化且高效的多向量检索方法。XTR的核心见解是，多向量模型中的Token检索应训练为检索最显著且信息丰富的文档Token，以便查询和文档之间的评分可以仅使用检索到的信息计算，就像单向量检索模型的工作方式一样。通过这种方式，收集步骤可以完全消除，并且评分成本显著降低，因为只需考虑一小部分Token，并且可以重用Token检索中的点积结果。为了提高Token检索的质量，XTR提出了一种新颖但简单的训练目标，显著提高了检索准确性，将黄金Token在top-$k$结果中被检索到的几率翻倍。此外，尽管改进了Token检索，一些相关Token仍可能被遗漏（即未被检索到）。为了解决这一问题，我们提出了一种称为“缺失相似度插值”的简单方法，该方法考虑了缺失Token对整体评分的贡献。

---

XTR简化了推理过程，使其更接近双编码器的直接流程，同时保持并增强了多向量检索模型的表达性评分函数。在BEIR [Thakur等, 2021]和LoTTE [Santhanam等, 2022b]基准测试中，XTR取得了最先进的性能，且无需蒸馏或困难负样本挖掘。值得注意的是，我们的模型在没有额外训练数据的情况下，在BEIR上超越了最先进的双编码器GTR [Ni等, 2021]，提高了3.6 nDCG@10。在EntityQuestions基准测试 [Sciavolino等, 2021]中，XTR在top-20检索准确率上比之前的最先进模型高出4.1分。XTR也不需要任何用于检索的二次预训练，在包含18种语言的多语言检索任务MIRACL上大幅优于mContriever [Izacard等, 2022] [Zhang等, 2022b]。我们的分析表明，XTR确实受益于在相关上下文中检索更多上下文化的Token，同时使评分阶段的成本降低了两到三个数量级。

# 2 背景

## 2.1 多向量检索

单向量检索模型，也称为双编码器，将输入文本序列编码为单个稠密嵌入，并基于点积定义查询和文档的相似性 [Lee等, 2019, Karpukhin等, 2020]。另一方面，多向量检索模型为每个查询和文档使用多个稠密嵌入，通常利用输入的所有上下文化词表示来提高模型的表达能力。

---

考虑一个查询 $Q=\left\{\mathbf{q}_{i}\right\}_{i=1}^{n}$ 和一个文档 $D=\left\{\mathbf{d}_{j}\right\}_{j=1}^{m}$，其中 $\mathbf{q}_{i}$ 和 $\mathbf{d}_{j}$ 分别表示$d$维的查询Token向量和文档Token向量。多向量检索模型计算查询-文档相似性如下：$f(Q, D)=\sum_{i=1}^{n} \sum_{j=1}^{m} \mathbf{A}_{i j} \mathbf{P}_{i j}$，其中 $\mathbf{P}_{i j}=\mathbf{q}_{i}^{\top} \mathbf{d}_{j}$，$\mathbf{A} \in\{0,1\}^{n \times m}$ 表示对齐矩阵，$\mathbf{A}_{i j}$ 表示查询Token向量 $\mathbf{q}_{i}$ 和文档Token向量 $\mathbf{d}_{j}$ 之间的Token级别对齐。ColBERT [Khattab和Zaharia, 2020]的max-of-sum运算符将 $\mathbf{A}_{i j}=\mathbb{1}_{\left[j=\operatorname{argmax}_{j^{\prime}}\left(\mathbf{P}_{i j^{\prime}}\right)\right]}$，其中argmax运算符作用于 $1 \leq j^{\prime} \leq m$（即来自单个文档 $D$ 的Token），$\mathbb{1}_{[*]}$ 是指示函数。然后，$f_{\text {ColBERT }}(Q, D)$ 定义如下：

$$
\begin{equation*}
f_{\text {ColBERT }}(Q, D)=\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{m} \mathbf{A}_{i j} \mathbf{P}_{i j}=\frac{1}{n} \sum_{i=1}^{n} \max _{1 \leq j \leq m} \mathbf{q}_{i}^{\top} \mathbf{d}_{j} . \tag{1}
\end{equation*}
$$

---

在这里，我们加入了归一化因子 $n$，这在原始max-of-sum中并未包含，因为它可以稳定训练，同时不影响推理期间的排序。在计算查询-文档相似性后，多向量检索模型通常使用基于批内负样本的交叉熵损失进行训练 [Santhanam等, 2022b, Qian等, 2022]。具体来说，给定查询 $Q$ 的正文档 $D^{+}$ 和一组小批量文档 $D_{1: B}=\left[D_{1}, \ldots, D_{B}\right]$，其中 $D^{+} \in D_{1: B}$，它们最小化交叉熵损失，定义为：$\mathcal{L}_{\mathrm{CE}}=-\log \frac{\exp f\left(Q, D^{+}\right)}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)}$

## 2.2 多向量检索的三阶段推理

与双编码器模型不同，找到评分最高的文档——即最大化公式（1）的文档——无法直接通过最大内积搜索（MIPS）处理，因为评分函数使用了非线性的max-of-sum操作。相反，多向量检索模型通常采用以下步骤进行推理：  
1) **Token检索**：对于每个$n$查询Token向量，首先检索$k^{\prime}$个文档Token向量，这些Token的源文档的并集形成初始候选文档集。如果每个Token来自不同的文档，候选文档的总数最多为$n k^{\prime}$。  
2) **收集**：由于评分函数公式（1）需要基于所有文档Token进行计算，多向量模型需要加载候选文档的所有Token向量。为了优化加载过程，通常使用基于RAM的索引。  
3) **评分**：为了提供候选文档的最终排名，多向量模型使用公式（1）对所有候选文档进行评分。这一阶段也称为精炼。需要注意的是，典型多向量模型的训练仅针对评分阶段，使用小批量文档进行优化。最后，基于计算出的评分返回top-$k$文档。三阶段推理过程如图1顶部所示。

**图1**：XTR概述。ColBERT采用三阶段推理，包括(a) Token检索、(b) 收集和(c) 评分阶段（见2.2节）。XTR在训练和推理中均利用Token检索。XTR通过在检索到的Token上应用$f_{\mathrm{XTR}}$（或$f_{\mathrm{XTR}^{\prime}}$）高效计算每个候选文档的评分，完全消除了收集阶段（见3.2节）。

# 3 XTR：上下文化Token检索器

与现有多向量模型的“检索-收集-评分”三阶段不同，XTR直接利用Token检索阶段检索到的Token对文档进行评分。在本节中，我们首先说明为什么现有的交叉熵损失与max-of-sum评分函数在Token检索阶段会失败。然后，我们介绍XTR的简单但重要的改进。给定一个正文档$D^{+}$和一组负文档$D_{1: r}^{-}=\left[D_{1}^{-}, \ldots, D_{r}^{-}\right]$，Token检索阶段需要检索$D^{+}$的Token，而不是负文档的Token。然而，以下示例表明，ColBERT使用的max-of-sum运算符并未专门设计用于检索相关文档的Token。

---

**失败案例**：假设$f_{\text {ColBERT }}\left(Q, D^{+}\right)=0.8$，其中所有单个最大Token相似度（即$\mathbf{q}_{i}^{\top} \mathbf{d}_{j}^{+}$，其中$\mathbf{A}_{i j}=1$）均为0.8。另一方面，假设对于所有$D^{-} \in D_{1: r}^{-}$，$f_{\text {ColBERT }}\left(Q, D^{-}\right)=0.2$，其中每个$D^{-}$都有一个高度集中的Token相似度大于0.8，而其他Token相似度接近零（即存在$\mathbf{q}_{i}^{\top} \mathbf{d}_{j}^{-}>0.8$，其中$\mathbf{A}_{i j}=1$，而其他$\mathbf{q}_{i}^{\top} \mathbf{d}_{j}^{-} \rightarrow 0$）。由于max-of-sum运算符仅关注文档级别的评分，训练期间的交叉熵损失将接近零。然而，对于每个$n$查询Token，如果至少存在一个负文档Token的相似度大于0.8，那么在top-$k^{\prime}=1$的Token检索中将无法检索到$D^{+}$的任何Token。因此，使用max-of-sum运算符的多向量检索模型无法降低某些负Token的高评分。图2显示，max-of-sum训练导致许多文档Token的评分异常高，而无论它们与查询Token的实际相关性如何。

- **图2**：4,000个Token检索评分（余弦相似度）的密度直方图。使用$f_{\text {ColBERT }}$（T5-ColBERT；见4节）训练导致许多文档Token的评分极高，而无论它们与输入查询Token的实际相关性如何。XTR通过更好的训练目标缓解了这一问题。

## 3.1 批内Token检索

为了训练多向量检索模型直接检索相关文档的Token，我们在训练期间模拟Token检索阶段。这可以通过采用不同的对齐策略$\hat{\mathbf{A}}$简单实现。具体来说，我们设置对齐$\hat{\mathbf{A}}_{i j}=\mathbb{1}_{\left[j \in \text { top- } k_{j^{\prime}}\left(\mathbf{P}_{i j^{\prime}}\right)\right]}$，其中top-$k$运算符作用于$1 \leq j^{\prime} \leq m B$（即来自$B$个小批量文档的Token），返回$k$个最大值的索引。在训练期间，我们使用超参数$k_{\text {train }}$作为top-$k$运算符的参数。然后，我们简单地将公式（1）修改如下：

$$
\begin{equation*}
f_{\mathrm{XTR}}(Q, D)=\frac{1}{Z} \sum_{i=1}^{n} \max _{1 \leq j \leq m} \hat{\mathbf{A}}_{i j} \mathbf{q}_{i}^{\top} \mathbf{d}_{j} \tag{2}
\end{equation*}
$$

其核心思想是，我们仅当$D$中的Token相似度足够高，能够在小批量中检索到top-$k_{\text {train }}$时，才考虑这些Token相似度。这里，我们使用归一化因子$Z=\mid\left\{i \mid \exists j\right.$, s.t. $\left.\hat{\mathbf{A}}_{i j}>0\right\} \mid$，即检索到至少一个$D$的Token的查询Token数量。如果所有$\hat{\mathbf{A}}_{i j}=0$，我们将$Z$裁剪为一个较小的数，此时$f_{\mathrm{XTR}}(Q, D)$变为0。因此，我们的模型无法为负文档分配高Token相似度，因为它阻止了正文档的Token被检索到。在前述失败案例中，$f_{\text {CoIBERT }}$即使无法检索到$D^{+}$的Token，仍为其分配了高分，而我们的相似度函数在训练期间会导致高损失，因为$f_{\mathrm{XTR}}\left(Q, D^{+}\right)=0$（$D^{+}$的Token未被检索到）。在训练中，我们使用与$\S 2.1$中定义的相同交叉熵损失，但采用新的评分函数。需要注意的是，训练数据仅包含文档级别的标注，但XTR鼓励从正文档中检索重要的Token。

## 3.2 使用检索到的Token对文档评分

在推理过程中，多向量检索模型首先从Token检索阶段获得一组候选文档$\hat{D}_{1: C}$：

$$
\begin{equation*}
\hat{D}_{1: C}=\left\{\hat{D} \mid d_{j} \in \hat{D} \wedge d_{j} \in \operatorname{top}-k^{\prime}\left(\mathbf{q}_{*}\right)\right\} . \tag{3}
\end{equation*}
$$

这里，top$-k^{\prime}\left(\mathbf{q}_{*}\right)$是基于每个查询向量的内积分数（即$\mathbf{q}^{\top} \mathbf{d}$）从整个语料库中检索到的top-$k^{\prime}$文档Token的并集。给定$n$个查询Token向量，共有$C$（$\leq n k^{\prime}$）个候选文档。以往的方法会加载每个文档的所有Token向量，并为每个查询和候选文档对计算公式（1），这需要每查询$\mathcal{O}\left(n^{2} k^{\prime} \bar{m} d\right)$的计算量（$\bar{m}=$平均文档长度）。相反，我们建议仅使用检索到的Token相似度对文档进行评分。这显著降低了评分阶段的计算成本，因为重用Token检索分数避免了计算冗余的内积和不必要的（非最大）内积。此外，昂贵的收集阶段（需要加载所有文档Token向量以计算公式（1））可以完全移除。与以往工作[Macdonald和Tonellotto, 2021]不同，后者利用Token检索在评分阶段之前对第一阶段候选文档进行排序，我们的目标是直接提供文档的最终分数。

---

**缺失相似度插值**：在推理过程中，我们为每个$n$查询Token检索$k^{\prime}$个文档Token。假设每个文档Token属于一个唯一的文档，总共提供$C=n k^{\prime}$个候选文档。在没有收集阶段的情况下，这使我们只能使用单个Token相似度对每个文档进行评分。然而，在训练期间——无论是使用公式（1）还是公式（2）——每个正文档最多有$n$个（最大）Token相似度进行平均，随着训练的进行，这些相似度大多收敛到$n$。因此，在推理过程中，我们为每个查询Token插值缺失的相似度，将每个候选文档视为正文档，假设它有$n$个Token相似度。

---

对于每个候选文档$\hat{D}$，我们首先为推理定义以下评分函数：

$$
\begin{equation*}
f_{\mathrm{XTR}^{\prime}}(Q, \hat{D})=\frac{1}{n} \sum_{i=1}^{n} \max _{1 \leq j \leq m}\left[\hat{\mathbf{A}}_{i j} \mathbf{q}_{i}^{\top} \mathbf{d}_{j}+\left(1-\hat{\mathbf{A}}_{i j}\right) m_{i}\right] . \tag{4}
\end{equation*}
$$

这与公式（2）类似，但引入了$m_{i} \in \mathbb{R}$，用于估计每个$q_{i}$的缺失相似度。$\hat{\mathbf{A}}$的定义与公式（2）中描述的类似，只是它使用$k^{\prime}$作为top-$k$运算符的参数。如果$\hat{\mathbf{A}}_{i *}=0$且$m_{i} \geq 0$，则每个$q_{i}$将缺失相似度$m_{i}$作为最大值。重要的是，$f_{\mathrm{XTR}^{\prime}}$消除了重新计算任何$\mathbf{q}_{i}^{\top} \mathbf{d}_{j}$的需要，因为当$\hat{\mathbf{A}}_{i j}=1$时，我们已经从Token检索阶段知道了检索分数，而当$\hat{\mathbf{A}}_{i j}=0$时，我们根本不需要计算它，因为$\hat{\mathbf{A}}_{i j} \mathbf{q}_{i}^{\top} \mathbf{d}_{j}=0$。需要注意的是，当所有$\hat{\mathbf{A}}_{i j}=1$时，该公式变为max-of-sum运算符。另一方面，当没有为$q_{i}$检索到$\hat{D}$的文档Token时（即$\hat{\mathbf{A}}_{i *}=0$），我们回退到插值分数$m_{i}$，这提供了一个近似的max-of-sum结果。

---

事实上，我们可以找到缺失相似度的上界。对于每个使用$\mathbf{q}_{i}$的Token检索，$\hat{D}$的查询Token的缺失相似度将受其最后一个top-$k^{\prime}$分数的上界限制。具体来说，对于每个查询Token$q_{i}$，在推理过程中我们有以下top-$k^{\prime}$Token相似度：$\left[\mathbf{q}_{i}^{\top} \mathbf{d}_{(1)}, \ldots \mathbf{q}_{i}^{\top} \mathbf{d}_{\left(k^{\prime}\right)}\right]$。这里，每个$\mathbf{d}_{(*)}$可能来自不同的文档。由于缺失相似度的分数将小于或等于最后一个检索到的Token的分数，因此我们知道$m_{i} \leq \mathbf{q}_{i}^{\top} \mathbf{d}_{\left(k^{\prime}\right)}$。随着$k^{\prime}$的增大，上界变得更紧。在我们的实验中，我们展示了简单地选择$m_{i}=\mathbf{q}_{i}^{\top} \mathbf{d}_{\left(k^{\prime}\right)}$效果良好，尤其是在使用$f_{\mathrm{XTR}}{ }^{5}$训练模型时。虽然我们也尝试了基于回归的更复杂的插值方法，但我们的方法尽管简单，却足够具有竞争力。插值过程如图3所示。

- **图3**：公式（1）中的$f_{\text {ColBERT }}$与公式（4）中的$f_{\mathrm{XTR}^{\prime}}$的比较。假设$D_{a}$和$D_{b}$从Token检索阶段被选为初始候选文档。$f_{\text {ColBERT }}$加载$D_{a}$和$D_{b}$的所有Token向量，并重新计算成对的Token相似度以获取最大值（红色框）。另一方面，$f_{\mathrm{XTR}^{\prime}}$不加载任何Token向量，而是重用第一阶段Token检索的检索分数。假设在top-2 Token检索结果中，第一个查询Token检索到$D_{a}$和$D_{b}$的最大分数，但第二个查询Token仅检索到来自$D_{a}$的两个Token，而不是$D_{b}$。我们通过使用第二个查询Token的top-2分数（表示为$s_{2}$）找到其上界（即$m \leq s_{2} \leq s_{1}$），为$D_{b}$插值缺失相似度$m$（表示为黄色虚线框）。

---

**表1**：ColBERT和XTR在评分阶段的FLOPs比较。XTR仅为每个候选文档的评分增加了最小的复杂度。该设置基于MS MARCO。

- **表1**显示了ColBERT和XTR的估计FLOPs（更多细节见附录B）。由于硬件和基础设施的差异，我们主要比较了理论上的FLOPs。XTR将评分阶段的FLOPs减少了$4000 \times$，使多向量检索更加高效。

# 4 实验

**实验设置**：按照Ni等人[2021]的方法，我们在MS MARCO上使用RocketQA [Qu等人, 2021]提供的固定困难负样本集对XTR进行微调。然后，我们在MS MARCO（MS；领域内）和零样本信息检索（IR）数据集上测试XTR。对于零样本评估，我们使用了来自BEIR [Thakur等人, 2021]的13个数据集（详见附录C中的缩写）、来自LoTTE [Santhanam等人, 2022b]的12个数据集，以及4个开放域问答（QA）段落检索数据集（EQ: EntityQuestions [Sciavolino等人, 2021]、NQ、TQA: TriviaQA、SQD: SQuAD）。我们还训练了多语言XTR（mXTR），并在包含18种语言检索任务的MIRACL [Zhang等人, 2022b]上进行了评估。T5-ColBERT [Qian等人, 2022]与XTR之间的性能差距展示了我们在多向量检索模型上的改进。有关实现细节和基线，请参见附录C。关于超参数（如$k_{\text {train }}$和$k^{\prime}$）之间的关系，请参见$\S 5.3$。

## 4.1 领域内文档检索

**MS MARCO**：表2（顶部）的第一列显示了MS MARCO上的nDCG@10（召回率@100见表D.1）。XTR优于大多数模型，并与T5-ColBERT保持竞争力。这令人鼓舞，因为XTR显著降低了收集-评分阶段的成本。需要注意的是，MS MARCO可能无法完全反映最新技术Arabzadeh等人[2022]的实际改进。

- **表2**：（顶部）MS MARCO（领域内）和BEIR（零样本）上的nDCG@10。最后一列显示了13个BEIR数据集的平均值。（底部）LoTTE数据集（零样本）上的Top-5检索准确率。

## 4.2 零样本文档检索

**BEIR & LoTTE**：表2（顶部；除第一列外）显示了BEIR上的nDCG@10（召回率@100见表D.1）。$\mathrm{XTR}_{\mathrm{xx1}}$取得了新的最先进性能，显著优于各领域模型和单模型的最先进技术。简单地扩展XTR消除了设计蒸馏或困难负样本挖掘管道的需求[Santhanam等人, 2022b, Formal等人, 2021]。LoTTE上的结果（表2底部）也表明，XTR基础模型优于ColBERT，并与基于蒸馏的模型竞争，而$\mathrm{XTR}_{\mathrm{xx} 1}$则进一步推动了最先进技术。

---

**开放域问答的段落检索**：表3显示了四个开放域问答数据集上的结果。尽管以往的工作通常包括稀疏检索器（如BM25）[Chen等人, 2021]或对比预训练[Ram等人, 2022, Sachan等人, 2022ab]以在EntityQuestions上取得更好的性能，但XTR仅在MS MARCO上微调就达到了最先进的性能。

- **表3**：开放域问答数据集上的零样本段落检索准确率。领域内性能用下划线标出，所有其他性能基于零样本评估。对于EntityQuestions，我们报告了不同关系上的宏观平均性能。

## 4.3 多语言文档检索

**MIRACL**：由于XTR不需要任何二次预训练，我们预计它通过更好地利用多语言语言模型在多语言检索中表现更优。我们使用mT5 [Xue等人, 2021]训练了一个多语言版本的XTR，并在18种语言的多语言检索任务上进行了测试。表4显示，mXTR显著优于使用昂贵的对比预训练的mContriever，以及混合模型BM25 + mDPR。

- **表4**：MIRACL中18种多语言检索任务上的nDCG@10。每行显示一个多语言检索模型的性能。最后两种意外语言（德语和约鲁巴语）未包含在MIRACL的训练数据集中。最后一列显示了18种语言的平均值。

# 5 分析

## 5.1 迈向更好的Token检索

**黄金Token检索**：如果黄金文档的Token完全没有被检索到，多向量检索模型将无法检索到黄金文档。因此，更好的Token检索会更多地将这些黄金Token包含在其top结果中。在图4（顶部）中，我们展示了排名$k$的Token来自查询的黄金文档的概率。为了计算排名$k$的概率，我们简单地统计排名$k$的Token属于黄金文档的事件数量，并将其除以排名$k$的Token总数。虽然这是衡量Token检索的精确率，但我们观察到黄金Token的召回率也有类似的趋势。与T5-ColBERT相比，XTR以更高的概率检索黄金Token，即使在MS MARCO上也是如此。这表明XTR的训练目标鼓励其从更相关的上下文中检索Token。

- **图4**：（顶部）T5-ColBERT和XTR的黄金Token检索性能。我们绘制了每个检索到的文档Token在排名$k$时来自黄金文档的概率。（底部）T5-ColBERT和XTR的词汇Token检索性能。我们绘制了每个检索到的文档Token在排名$k$时与查询Token词汇相同的概率。

---

**词汇Token检索**：在图4（底部）中，我们展示了排名$k$的Token与查询Token相同的概率（例如，“insulin”检索“insulin”）。T5-ColBERT在不同排名和数据集中检索相同Token的概率非常高。然而，目前尚不清楚Token检索阶段应在多大程度上表现为稀疏检索，因为它可能受到词汇不匹配问题的影响。XTR有效地降低了对词汇匹配的依赖，同时保留了相当高的词汇精确率，从而在以实体为中心的数据集上实现了高检索准确率（见$\S 4.2$）。事实上，附录中的表6显示，较低的词汇匹配并不一定意味着较低的检索质量，而通常意味着更好的上下文化。

## 5.2 高效评分

在表5中，我们展示了如何在XTR中使用高效的评分函数$f_{\mathrm{XTR}^{\prime}}$，同时将性能损失降至最低。我们在T5-ColBERT和XTR上应用$f_{\mathrm{XTR}^{\prime}}$，并展示了它们在MS MARCO上的性能。对于T5-ColBERT，即使我们使用top-$k$分数进行插值，其性能也比原始的max-of-sum评分差得多。而对于XTR，由于其Token检索性能更好，性能显著提升。图5展示了随着$k^{\prime}$的增大，Recall@100如何提升，因为它为缺失相似度插值提供了更精确的上界。表D.2显示，即使使用较小的$k^{\prime}$，XTR在BEIR上仍能保持较高的性能。

- **表5**：训练目标和插值方法对T5-ColBERT和XTR的影响比较。对于两个模型，我们在推理期间应用$f_{\text {XTR }^{\prime}}$。我们报告了MS MARCO开发集上的MRR@10和Recall@1000。

- **图5**：不同$k^{\prime}$下XTR和T5-ColBERT的Recall@100。对于T5-ColBERT，我们使用$f_{\mathrm{XTR}^{\prime}}$或$f_{\text {ColBERT }}$。

## 5.3 超参数之间的关系

**$\boldsymbol{k}_{\text {train }}$与$\boldsymbol{k}^{\prime}$**：在图6中，我们展示了使用不同$k_{\text {train }}$训练的XTR，在不同$k^{\prime}$下在MS MARCO开发集上的MRR@10。虽然所有XTR变体都倾向于更大的$k^{\prime}$，但在较小的$k^{\prime}$设置下，使用较小$k_{\text {train }}$训练的模型表现优于其他模型。随着$k^{\prime}$的增大，使用较大$k_{\text {train }}$的XTR表现优于使用较小$k_{\text {train }}$的模型。

- **图6**：不同$k_{\text {train }}$和$k^{\prime}$下XTR的MRR@10。对于T5-ColBERT，我们在推理时也使用$f_{\mathrm{XTR}^{\prime}}$，并采用top-$k^{\prime}$分数插值方法。

---

**训练批量大小与$\boldsymbol{k}_{\text {train }}$**：在图7中，我们展示了训练XTR时训练批量大小与$k_{\text {train }}$之间的关系。在本实验中，我们使用$k^{\prime}=40,000$。虽然XTR明显更倾向于较大的训练批量大小，但对于不同的数据集，最佳的top-$k_{\text {train }}$可能不同。虽然包括MS MARCO在内的大多数数据集倾向于足够大的$k_{\text {train }}$，但ArguAna更倾向于较小的$k_{\text {train }}$。我们假设这是由于ArguAna中较长的查询长度，使得多向量模型相比双编码器表现不足（见表2中GTR与T5-ColBERT的比较）。

- **图7**：使用不同批量大小和$k_{\text {train }}$训练XTR的效果。对于图中的每个点，我们使用指定的训练批量大小$(128,256,320)$和$k_{\text {train }}(32$, $64,128,256$ )训练$\mathrm{XTR}_{\text {base }}$，并在每个数据集（MS MARCO和ArguAna）上进行评估。报告了每个模型的nDCG@10。

## 5.4 定性分析

表6展示了MS MARCO中的一个预测示例。对于T5-ColBERT，所有top检索到的Token都是完全词汇匹配的。令人惊讶的是，检索到的段落都与查询无关，这表明T5-ColBERT未能从正确的上下文中检索Token。相比之下，XTR检索到的完全词汇匹配Token较少，但检索到的Token的上下文与查询的相关性更高。这个例子解释了XTR在图4（底部）中较低的词汇Token检索概率，但在图4（顶部）中较高的黄金Token检索性能。更多定性示例请参见附录E。

- **表6**：MS MARCO中的Token检索示例。在top 100检索到的Token中，T5-ColBERT的Token中有$100\%$与查询Token“usual”完全词汇匹配，而XTR的Token中只有$8\%$完全词汇匹配。XTR通过检索“average”来检索相关段落。

# 6 相关工作

密集检索模型的主要局限性之一是将查询和文档编码为单个向量，限制了模型的表示能力。Polyencoder [Humeau等人, 2020]、MEBERT [Luan等人, 2021]和MVR [Zhang等人, 2022a]提出使用多个嵌入而非单个嵌入来表示查询或文档。最近的一种方法是Token级别的多向量检索，即存储和检索每个Token的嵌入。ColBERT [Khattab和Zaharia, 2020]可能是这一类别中最著名的模型。ALIGNER（即T5-ColBERT）[Qian等人, 2022]通过扩展骨干语言模型并研究各种Token级别对齐分数的聚合策略，进一步扩展了ColBERT。这些Token级别的检索模型显示出强大的有效性和跨域泛化能力。

---

减少多向量模型服务成本的努力主要集中在Token级别检索阶段。COIL [Gao等人, 2021]通过将检索限制在完全匹配的Token上加速Token级别检索，与经典倒排索引的理念一致。CITADEL [Li等人, 2022]通过词汇路由机制放宽了COIL的限制，即查询Token向量仅从路由到相同键的文档Token向量子集中检索。PLAID [Santhanam等人, 2022a]通过在检索早期阶段修剪较弱的候选者并使用更好的向量量化来优化ColBERT的速度。ColBERT-v2 [Santhanam等人, 2022b]进一步采用残差表示与聚类质心以提高ColBERT的效率。另一方面，如何加速评分阶段仍未被充分探索。据我们所知，XTR是第一个简化评分阶段并移除多向量检索中收集阶段的工作。

# 7 结论

多向量检索利用查询和文档的Token表示来实现高效的信息检索。在本文中，我们提出了XTR，通过改进初始Token检索阶段，简化了现有多向量模型的三阶段推理过程。具体来说，XTR仅基于检索到的Token对文档进行评分，并在训练期间通过批内文档Token进行优化。因此，XTR在零样本信息检索基准上实现了最先进的性能，同时大大降低了评分阶段的浮点运算量（FLOPs）。我们进一步表明，我们的目标函数确实鼓励了更好的Token检索，能够从黄金文档中检索更多Token，其上下文与查询更一致。

# 8 局限性

在我们的大多数实验中，XTR是在MS MARCO（一个大规模的英文检索数据集）上进行训练的。虽然我们的实验是在公平的环境下进行的，大多数基线模型也使用了MS MARCO，但未来的用例可能需要由于许可证或语言特定问题而减少对MS MARCO的依赖。我们相信，基于LLM的检索数据集生成 [Dai等人, 2022] 将能够在未来缓解这一问题。

# A 相似度分数的导数

**max-of-sum**：这里我们使用交叉熵损失$\mathcal{L}_{\text {CE }}$和max-of-sum运算符$f_{\text {ColBERT }}$，并分析关于Token相似度分数的导数。

$$
\begin{align*}
& \mathcal{L}_{\mathrm{CE}}=-\log \frac{\exp f\left(Q, D^{+}\right)}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)}=-f_{\mathrm{CoIBERT}}\left(Q, D^{+}\right)+\log \sum_{b=1}^{B} \exp f_{\mathrm{CoIBERT}}\left(Q, D_{b}\right)  \tag{5}\\
& f_{\mathrm{CoIBERT}}(Q, D)=\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{m} \mathbf{A}_{i j} \mathbf{P}_{i j}=\frac{1}{n} \sum_{i=1}^{n} \mathbf{P}_{i \hat{j}} \tag{6}
\end{align*}
$$

---

这里，我们将$\hat{j}$表示为每行最大值的索引，依赖于每个$i$（即$\mathbf{A}_{i j}=1$）。给定使用max-of-sum运算符的交叉熵损失，我们计算关于正文档$D^{+} \in D_{1: B}$的最大Token相似度$\mathbf{P}_{i \hat{j}}^{+}$的梯度：

$$
\begin{aligned}
\frac{\partial \mathcal{L}_{\mathrm{CE}}}{\partial \mathbf{P}_{i \hat{j}}^{+}} & =-\frac{f\left(Q, D^{+}\right)}{\partial \mathbf{P}_{i \hat{j}}^{+}}+\frac{1}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)} \frac{\partial}{\partial \mathbf{P}_{i \hat{j}}^{+}} \sum_{b=1}^{B} \exp f\left(Q, D_{b}\right) \\
& =-\frac{\partial}{\partial \mathbf{P}_{i \hat{j}}^{+}} \frac{1}{n} \sum_{i=1}^{n} \max _{1 \leq j \leq m} \mathbf{P}_{i j}^{+}+\frac{1}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)} \sum_{b=1}^{B} \frac{\partial}{\partial \mathbf{P}_{i \hat{j}}^{+}} \exp f\left(Q, D_{b}\right) \\
& =-\frac{1}{n}+\frac{1}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)} \sum_{b=1}^{B} \exp f\left(Q, D_{b}\right) \frac{\partial f\left(Q, D_{b}\right)}{\partial \mathbf{P}_{i \hat{j}}^{+}} \\
& =-\frac{1}{n}+\frac{1}{n} \frac{\exp f\left(Q, D^{+}\right)}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)}=-\frac{1}{n}\left[1-P\left(D^{+} \mid Q, D_{1: B}\right)\right]
\end{aligned}
$$

---

类似地，关于负文档$D^{-} \in D_{1: B}$的最大Token相似度$\mathbf{P}_{i \hat{j}}^{-}$的梯度计算如下：

$$
\begin{aligned}
\frac{\partial \mathcal{L}_{\mathrm{CE}}}{\partial \mathbf{P}_{i \hat{j}}^{-}} & =-\frac{f\left(Q, D^{+}\right)}{\partial \mathbf{P}_{i \hat{j}}^{-}}+\frac{1}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)} \frac{\partial}{\partial \mathbf{P}_{i \hat{j}}^{-}} \sum_{b=1}^{B} \exp f\left(Q, D_{b}\right) \\
& =\frac{1}{n} \frac{\exp f\left(Q, D^{-}\right)}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)}=\frac{1}{n} P\left(D^{-} \mid Q, D_{1: B}\right) .
\end{aligned}
$$

---

因此，正Token分数$\mathbf{P}_{i \hat{j}}^{+}$将逐渐增加，直到$P\left(D^{+} \mid Q, D_{1: B}\right) \rightarrow 1$，而负Token分数$\mathbf{P}_{i \hat{j}}^{-}$将逐渐减少，直到$P\left(D^{-} \mid Q, D_{1: B}\right) \rightarrow 0$。这表明Token分数是基于文档分数进行训练的，这可能导致Token分数的停滞。例如，即使$\mathbf{P}_{i \hat{j}}^{-}$非常高——最终导致$\mathbf{d}_{\hat{j}}^{-}$被检索而不是正文档的Token——只要$P\left(D^{-} \mid Q, D_{1: B}\right)$足够低，它就不会受到惩罚。

---

**批内Token检索**：与max-of-sum运算符相比，我们的批内max-of-sum $f_{\mathrm{XTR}}$仅在最大值在小批量中检索到其他负Token时才考虑这些最大值。

$$
f_{\mathrm{XTR}}\left(Q, D_{1: B}\right)=\frac{1}{Z} \sum_{i=1}^{n} \sum_{j=1}^{m} \mathbf{A}_{i j} \hat{\mathbf{A}}_{i j} \mathbf{P}_{i j}=\frac{1}{Z} \sum_{i=1}^{n} \mathbf{P}_{i \bar{j}}
$$

这里，我们将$\bar{j}$表示为每行最大值的索引，并且该值在小批量top-$k_{\text {train }}$内（即同时满足$\mathbf{A}_{i j}=1$和$\hat{\mathbf{A}}_{i j}=1$）。如果没有这样的$\bar{j}$，我们直接使用$\mathbf{P}_{i \bar{j}}=0$。我们还使用归一化因子$Z$，它是非零$\mathbf{P}_{i \bar{j}}$的数量。在本分析中，我们假设$Z>0$，因为如果所有$\mathbf{P}_{i \bar{j}}$都为零，则梯度未定义。

---

关于正文档$D^{+} \in D_{1: B}$的最大Token相似度$\mathbf{P}_{i \bar{j}}^{+}$（非零）的梯度计算如下：

$$
\begin{aligned}
\frac{\partial \mathcal{L}_{\mathrm{CE}}}{\partial \mathbf{P}_{i \bar{j}}^{+}} & =-\frac{f\left(Q, D^{+}\right)}{\partial \mathbf{P}_{i \bar{j}}^{+}}+\frac{1}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)} \frac{\partial}{\partial \mathbf{P}_{i \bar{j}}^{+}} \sum_{b=1}^{B} \exp f\left(Q, D_{b}\right) \\
& =-\frac{1}{Z^{+}}\left[1-\frac{\exp f\left(Q, D^{+}\right)}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)}\right] \\
& =-\frac{1}{Z^{+}}\left[1-P\left(D^{+} \mid Q, D_{1: B}\right)\right]
\end{aligned}
$$

这与max-of-sum运算符的结果非常相似，除了1）梯度仅在$\mathbf{P}_{i \bar{j}}^{+}$非零（即被检索到）时定义，2）它依赖于$Z^{+}$，这意味着当正文档中检索到的Token数量较少时，梯度会较大。如果$D^{+}$中只有少量Token被检索到，我们的目标函数会增加$\mathbf{P}_{i \bar{j}}^{+}$。

---

对于负相似度分数$\mathbf{P}_{i \bar{j}}^{-}$，我们有：

$$
\begin{aligned}
\frac{\partial \mathcal{L}_{\mathrm{CE}}}{\partial \mathbf{P}_{i \bar{j}}^{-}} & =-\frac{f\left(Q, D^{+}\right)}{\partial \mathbf{P}_{i \bar{j}}^{-}}+\frac{1}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)} \frac{\partial}{\partial \mathbf{P}_{i \bar{j}}^{-}} \sum_{b=1}^{B} \exp f\left(Q, D_{b}\right) \\
& =-\frac{1}{Z^{-}}\left[-\frac{\exp f\left(Q, D^{-}\right)}{\sum_{b=1}^{B} \exp f\left(Q, D_{b}\right)}\right] \\
& =\frac{1}{Z^{-}} P\left(D^{-} \mid Q, D_{1: B}\right)
\end{aligned}
$$

同样，这与max-of-sum的结果相似，但它依赖于$Z^{-}$。在这种情况下，即使$P\left(D^{-} \mid Q, D_{1: B}\right)$很低，如果$D^{-}$中检索到的Token数量较少（即$Z^{-}$较小），$\mathbf{P}_{i \hat{j}}^{-}$将显著减少。需要注意的是，当$Z^{-}$较大时，$Z^{+}$自然会变小，因为它们在批内Token检索中竞争，这会导致正Token的分数更高。

# B 推理复杂度

我们比较了ColBERT和XTR在评分阶段的复杂度，以浮点运算（FLOPs）为单位。我们未测量在线查询编码和最大内积搜索（MIPS）的复杂度，这些已在双编码器和多向量检索中得到了广泛研究 [Santhanam等人, 2022abb, Guo等人, 2020]。

---

在评分阶段，ColBERT和XTR都有$\mathcal{O}\left(n k^{\prime}\right)$个候选文档。这里，我们假设最坏情况为$n k^{\prime}$，即每个文档Token来自不同的文档。对于每个候选文档，ColBERT加载一组$\bar{m} d$浮点数的文档向量（$\bar{m}=$平均文档长度），并使用$n d$浮点数的查询向量计算公式（1）。计算每个候选文档的公式（1）需要$2 n \bar{m} d$ FLOPs用于Token级别的内积，$n \bar{m}$用于查找每行的最大值，以及$n$用于最终的平均值。总计，ColBERT在评分阶段需要$n^{2} k^{\prime}(2 \bar{m} d+\bar{m}+1)$ FLOPs。需要注意的是，这未包括将$\mathcal{O}\left(n k^{\prime} \bar{m} d\right)$浮点数加载到内存的延迟，当$n=16, k^{\prime}=1000, \bar{m}=55, d=128$时，每查询的延迟可达450 MB。

---

另一方面，XTR首先插值缺失的相似度，这通过缓存每个查询Token的第$k^{\prime}$个Token检索分数简单实现。然后，每个$n k^{\prime}$候选文档需要$n \bar{r}$ FLOPs用于查找每行的最大值，以及$n$用于平均值，其中$\bar{r}$是每个候选文档检索到的Token的平均数量。总计，我们有$n^{2} k^{\prime}(\bar{r}+1)$ FLOPs。表1显示了两个模型的估计FLOPs。XTR将评分阶段的FLOPs减少了$4000 \times$，使多向量检索更加高效和实用。

## C 实现细节

XTR使用$k_{\text {train }}$来检索批内文档Token。由于我们是在小批量上进行检索，小批量的大小会影响不同$k_{\text {train }}$的性能，如$\S 5.3$所示。在我们的实验中，我们为每个批量大小尝试了$k_{\text {train }}=\{32,64,128,256,320\}$，并根据它们在MS MARCO开发集上的表现选择最佳模型。在推理时，XTR使用$k^{\prime}$进行Token检索。我们使用$k^{\prime}=40,000$，这得益于XTR的高效评分阶段${ }^{6}$。我们在$\S 5.3$中分析了使用不同$k^{\prime}$的效果及其与$k_{\text {train }}$的关系。我们从T5编码器的base和xxl版本 [Raffel等人, 2020] 初始化XTR，并提供了XTR base和$\mathrm{XTR}_{\mathrm{xxl}}$。对于多语言XTR，我们从mT5 [Xue等人, 2021] 初始化XTR。我们以$1 \mathrm{e}-3$的学习率对XTR进行了50,000次微调。根据模型的大小，我们使用了最多256个TPU v3加速器芯片。在Token检索阶段，我们使用ScaNN [Guo等人, 2020] 进行MIPS。对于BEIR，我们使用了13个数据集（AR: ArguAna, TO: Touché-2020, FE: Fever, CF: Climate-Fever, SF: Scifact, CV: TREC-COVID, NF: NFCorpus, NQ: Natural Questions, HQ: HotpotQA, FQ: FiQA-2018, SD: SCIDOCS, DB: DBPedia, QU: Quora）。

---

**基线**：在训练用于跨域评估的检索模型时，主要有两种范式。第一种是为每个数据集（或领域）训练一个独立的检索器，通过为每个跨域语料库生成查询。通常，这种方法生成$N$个数据集，为$N$个不同领域训练$N$个独立模型。对于这种“每个领域一个检索器”的方法，我们包括了GenQ [Thakur等人, 2021]、GPL [Wang等人, 2022] 和Promptagator [Dai等人, 2022]。第二种是构建一个单一的检索器——通常是在大规模IR数据集（如MS MARCO）上训练的——并直接将其应用于跨域语料库和查询。对于这种“一个检索器适用于所有”的方法，我们展示了包括Splade ${ }_{\mathrm{v} 2}$ [Formal等人, 2021]、ColBERT ${ }_{\mathrm{v} 2}$ [Santhanam等人, 2022b] 和$\mathrm{GTR}_{\mathrm{xx}}$ [Ni等人, 2021] 在内的最先进检索器的结果。我们还展示了T5-ColBERT ${ }_{\mathrm{xxl}}$ [Qian等人, 2022] 的结果，这是一个基于T5初始化的ColBERT模型，并与XTR共享相同的骨干语言模型和训练数据集。需要注意的是，T5-ColBERT使用了基于原始max-of-sum的繁重评分阶段。我们所有的“一个检索器适用于所有”基线以及XTR，除非另有说明，都是在英文MS MARCO上训练的。

# D 附加结果

在表D.1中，我们展示了BEIR上的Recall@100。

**表D.1**：MS-MARCO和BEIR上的Recall@100。最后一列显示了13个BEIR基准的平均值。与GTR相比，T5-ColBERT仅略微提高了召回率。另一方面，XTR显著提高了召回率，显示了更好Token检索的重要性。

---

在表D.2中，我们展示了不同$k^{\prime}$下BEIR上的nDCG@10和Recall@100。

**表D.2**：不同$k^{\prime}$下XTR ${ }_{\text {base }}$在MS-MARCO和BEIR上的nDCG@10和Recall@100。最后一列显示了13个BEIR基准的平均值。

# E 定性分析

在表E.5中，我们展示了T5-ColBERT和XTR的Token检索结果。

**表E.1**：MS MARCO中查询“lauren london age”中Token“la”的Token检索示例。在top 100检索到的Token中，$100\%$的T5-ColBERT Token与查询Token“la”词汇相同，$100\%$的XTR Token也词汇相同。然而，XTR的top检索结果包含正确的实体（Lauren London），而T5-ColBERT的检索结果涉及错误的实体（Laura Bush、Laura Branigan等）。

---

**表E.2**：MS MARCO中查询“temple university student population?”中Token“temple”的Token检索示例。在top 100检索到的Token中，$100\%$的T5-ColBERT Token与查询Token“temple”词汇相同，$100\%$的XTR Token也词汇相同。然而，XTR的top检索结果与正确上下文（学生人数）相关，而T5-ColBERT的检索结果偏离主题（例如，学费、工资等）。

---

**表E.3**：MS MARCO中查询“aire is expressed in some skin tumors”中Token“aire”的Token检索示例。在top 100检索到的Token中，$77\%$的T5-ColBERT Token与查询Token“aire”词汇相同，$77\%$的XTR Token也词汇相同。XTR的top检索结果与查询相关（关于癌症、肿瘤、皮肤和黑色素细胞），而T5-ColBERT的检索结果偏离主题。

---

**表E.4**：Scifact中查询“women with a higher birth weight are more likely to develop breast cancer later in life”中Token“later”的Token检索示例。在top 100检索到的Token中，$72\%$的T5-ColBERT Token与查询Token“later”词汇相同，而XTR Token中只有$33\%$词汇相同。XTR的top检索结果可以从相关上下文中检索到同义词（subsequent），而T5-ColBERT的检索结果偏离主题。

---

**表E.5**：Scifact中查询“vanules have a thinner or absent smooth later compared to arterioles”中Token“thinner”的Token检索示例。在top 100检索到的Token中，只有$1\%$的T5-ColBERT Token与查询Token“thinner”词汇相同，XTR Token中也只有$1\%$词汇相同。
