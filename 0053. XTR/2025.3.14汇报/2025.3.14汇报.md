# $\textbf{0. PLAID}$概述

> :one:$\text{ColBERTv2}$的流程
>
> 1. 离线：对段落向量集$\{p_j\}$进行聚类得到$n$个质心，然后对$p_j$进行残差压缩，即离$p_j$最近的质心索引$\text{+}$与质心间残差的二进制编码
>
> 2. 在线：初排阶段如下
>
>    <img src="https://i-blog.csdnimg.cn/direct/c2e23f4fdd8243f8951b358eb61094f5.png" alt="image-20241212155844854" width=400 /> 
>
>    - 查询嵌入：让查询$Q$进行嵌入得到其多向量表示
>    - 候选生成：
>      - 为每个$q_i$查找其最近的若干质心$\{p^C\}$，再由质心$\{p^C\}$回溯到其簇内所段落向量$\{p_j^C\}$的索引
>      - 收集所有与$\{p_j^C\}$有关的段落，构成候选集
>    - 索引查找：传递$\{p_j^C\}$中向量的压缩表示
>    - 残差解压：得到$\{p_j^C\}$中所有段落向量近似的全精度表示
>    - 计算评分：让所有$q_i$对解压后的向量计算$\text{MaxSim}$，由于段落向量可能会残缺不全，所以得到的$\text{MaxSim}$其实是近似下界
>
> 3. 在线：根据初排的结果传递全精度段落多向量，再进行重排
>
> :two:$\text{PLAID}$的优化：
>
> 1. 优化思路：
>
>    - 质心交互：在初排阶段直接用质心$\{p^C\}$代替全精点$\{P_j^C\}$，不进行任何内存传输以及解压缩
>    - 质心剪枝：对于某一个查询而言$\text{95\%}$左右的质心都是无意义的，可直接剪枝并不参与后续操作
>
> 2. 改进后的初排流程
>
>    - 查询嵌入：让查询$Q$进行嵌入得到其多向量表示
>
>    - 候选生成：按同样方式生成段落候选集，同时建立质心到段落的索引
>
>      ```txt
>      ColBERTv2做法:
>      c1 -> {Doc1-Token1, Doc3-Token2, Doc3-Token3}
>      PLAID做法:
>      c1 -> {Doc1, Doc3}
>      ```
>
>    - 质心剪枝：对段落$P$先通过段落索引获取与之有关的质心，让每个质心对查询$\{q_1,...,q_n\}$进行$\text{MaxSim}$并去掉低于阈值着
>
>      ```txt
>      索引：P -> {c1, c2, c3, c4}
>      剪枝：P -> {c1, c2, c3}  
>      ```
>
>    - 质心交互：将剪枝后的质心作为$P$的一种表示，让每个查询$q_i$对剪枝后的质心进行$\text{MaxSim}$以得到近似距离

# $\textbf{1. }$如何解释分桶单向量的$\textbf{Idea}$

> :one:构建一个相似度矩阵以以便于理解
>
> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20250314030528843.png" alt="image-20250314030528843" width=750 />  
>
> 1. 每一格代表一个子相似度(相似度)$\left\langle{q_i,p^{(k)}_j}\right\rangle$ 
> 2. <span style="color: #012660;">**深色**</span>表示该子相似度被选中，<span style="color: #00B0F0;">**浅色**</span>表示未被选中
>
> :two:$\text{ColBERT}$的做法：基于$\text{MaxSim}$操作，对子相似度进行选中
>
> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20250314032545700.png" alt="image-20250314032545700" width=750 />   
>
> 1. 对于每个$P^{(k)}$，为每个$q_i$找到最大的内积($\text{MaxSim}$操作)，即一行中与$q_i$内积最大的一个元素
> 2. 最后再输出每个$P^{(k)}$中被选中相似度的平均，是为$\text{Sim}\left(Q,P^{(k)}\right)$ 
> 3. 鉴于$\text{MaxSim}$操作本质上是非线性的，无法直接执行$\text{MIPS}$搜索，所以为适应大规模文档提出如下方法
>
> :three:$\text{ColBERT}$改进的做法：基于$\text{Top-K}$最邻居查询，对子相似度进行选中
>
> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20250314033230469.png" alt="image-20250314033230469" width=750 /> 
>
> 1. 让每个$q_i$对所有段落$\text{Token}$构成的集合$\left\{p_j^{(k)}\right\}$执行$\text{MIPS}$搜索，找到其$\text{Top-K}$邻近的段落向量，如图中执行的为$\text{Top-3}$搜索
> 2. 认为被选中子相似度多的段落，对查询来讲更为重要，如图中例子为$P^{(2)}\text{>}P^{(1)}\text{>}P^{(3)}\text{>}P^{(4)}$作为一个初排结果
>    - 先考虑将这种排序标准量化，即将每个$P^{(k)}$被选中的子相似度数==视作$P^{(k)}$的一种评分==，并除以$|P^{(k)}|$进行归一化
>    - $\text{Sim}(Q,P)\text{=}\cfrac{\#\text{Selected}}{|P|}$ 
> 3. 重排阶段抛弃没有任何一个子相似度被选中的段落(如图中$P^{(4)}$)，收集段落的完整向量计算精确距离$\text{Sim}\left(Q,P^{(k)}\right)$以重排
>
> :four:分桶单向量的做法：将$\text{SimHash}$过程中被分到同一桶视作一种对$\text{Top-K}$最邻居查询的近似
>
> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20250314042753860.png" alt="image-20250314034906833" width=750 /> 
>
> 1. 对所有的$\{q_i\}$集合和$\left\{p_j^{(k)}\right\}$进行分桶，对于任意一个$q_i$如果$p_{j}^{(k)}$与其同桶，则选中子相似度$\left\langle{q_i,p^{(k)}_j}\right\rangle$ 
>
> 2. 同上述做法，认为认为被选中子相似度多的段落对查询来讲更为重要，同样的于是有$P^{(2)}\text{=}P^{(1)}\text{>}P^{(3)}\text{>}P^{(4)}$ 
>
> 3. 将得到$\text{Sim}(Q,P)$的过程转化为内积计算过程，由此将文档初排的操作变成了$\text{Top-K}$的$\text{MIPS}$搜索
>
>    - 预处理阶段：先将$N$个段落的一共$N\text{×}m$个段落子向量全部进行分桶，记录以下分桶数量矩阵
>
>      | 段落$\backslash$桶 |           $\text{Bucket-1}$            |           $\text{Bucket-2}$            | $\ldots$ |           $\text{Bucket-M}$            |
>      | :----------------: | :------------------------------------: | :------------------------------------: | :------: | :------------------------------------: |
>      |     $P^{(1)}$      | $\text{N}\left(P^{(1)},B^{(1)}\right)$ | $\text{N}\left(P^{(1)},B^{(2)}\right)$ | $\ldots$ | $\text{N}\left(P^{(1)},B^{(M)}\right)$ |
>      |     $P^{(2)}$      | $\text{N}\left(P^{(2)},B^{(1)}\right)$ | $\text{N}\left(P^{(2)},B^{(2)}\right)$ | $\ldots$ | $\text{N}\left(P^{(2)},B^{(M)}\right)$ |
>      |      $\ldots$      |                $\ldots$                |                $\ldots$                | $\ldots$ |                                        |
>      |     $P^{(N)}$      | $\text{N}\left(P^{(N)},B^{(1)}\right)$ | $\text{N}\left(P^{(N)},B^{(2)}\right)$ | $\ldots$ | $\text{N}\left(P^{(N)},B^{(M)}\right)$ |
>
>      - 其中$\text{N}\left(P^{(i)},B^{(j)}\right)$表示$P^{(i)}$中有$\text{N}$个子向量落入了桶$B^{(j)}$中
>      - 相当于生成了向量集$\{\vec{P}^{(1)},...,\vec{P}^{(N)}\}$
>
>    - 查询阶段：再将单个查询向量$Q$一共$n$个查询子向量进行分桶，记录以下分桶权重向量
>
>      | 查询$\backslash$桶 |        $\text{Bucket-1}$         |        $\text{Bucket-2}$         | $\ldots$ |        $\text{Bucket-M}$         |
>      | :----------------: | :------------------------------: | :------------------------------: | :------: | :------------------------------: |
>      |        $Q$         | $\text{W}\left(Q,B^{(1)}\right)$ | $\text{W}\left(Q,B^{(2)}\right)$ | $\ldots$ | $\text{W}\left(Q,B^{(M)}\right)$ |
>
>      - 其中$\text{W}\left(Q,B^{(j)}\right)$表示$Q$中有$\text{W}$个子向量落入了桶$B^{(j)}$中
>      - 生成每个文档的相关性评分，$\text{Score}\left(Q,P^{(i)}\right)\text{=}\cfrac{1}{|P^{(i)}|}\displaystyle{}\sum_{j\text{=}M}^{n}\text{W}\left(Q,B^{(j)}\right)\text{×}\text{N}\left(P^{(i)},B^{(j)}\right)\text{=}\cfrac{1}{|P^{(i)}|}\langle{\vec{Q},\vec{P^{(i)}}}\rangle$ 
>      - 对文档初排的操作由此变成了让$\vec{Q}$在$\{\vec{P}^{(1)},...,\vec{P}^{(N)}\}$中进行$\text{Top-K}$的$\text{MIPS}$搜索
>
> 4. 一些实验结果：以精确$\text{ColBERT}$后期交互的排序结果为基准，集中初排方法的结果
>
>    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20250314053654467.png" alt="image-20250314053654467" width=600 />   

# $\textbf{2. }$存在什么问题$\textbf{?}$ 

> :one:可以提高模型的颗粒度
>
> 1. 原始的模型中，任何段落子向量$p_j^{(k)}$对于向量$\vec{P}^{(k)}$的某一分量的贡献都是$1$，不论该$\text{Token}$重要与否
>
> 2. 改进思路：不管查询与具体场景，识别段落$\text{Token}$中比较稀有(重要)的$\text{Token}$
>
>    - 这基于一个假设，就是大部分情况下，对查询而言其答案往往集中在段落中少数且稀有的几个$\text{Token}$，比如下面的例子
>
>      ```txt
>      - Q: What is the capital of France
>      - P: France is a country in Western Europe. It shares borders with Germany, Belgium, and Spain. The country’s largest city and **capital, Paris**, is known for its art, fashion, and culture.
>      ```
>
>    - 基于匹配的方法：直接将$\text{IDF}(p_j)$作为一个重要性权重，令$p_j^{(k)}$对向量$\vec{P}^{(k)}$的某一分量的贡献为$\text{IDF}(p_j)$
>
>    - 基于神经的方法：$\text{BERT}$在执行推理过程中，会赋予每个$p_j^{(k)}$一个注意力张量
>
>      - 可不进行任何微调，直接提取$\text{BERT}$最后一层的注意力张量即可 
>
>      - 注意力张量：$P\text{=}\{p_1,p_2,...,p_m\}$的注意力为三维张量$A(h,i,j)$，表示在$h$头注意力机制中$p_i$与$p_j$二者的注意力相关性
>
>        <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20250312200743542.png" alt="image-20250312200743542" width=530 /> 
>
>      - 每个$\text{Token}$的注意力评分，以$p_i$为例，为每个注意力头中与$p_i$有关行的总和，即$a(q_i)\text{=}\displaystyle{}\sum_{h=0}^{h_\max}\sum_{j=0}^{m}A(h,i,j)$
>
>      - $a\left(q_i^{(k)}\right)$也可视作$q_i$的重要性权重，故可令$p_j^{(k)}$对向量$\vec{P}^{(k)}$的某一分量的贡献为$a\left(q_i^{(k)}\right)$ 
>
>    - 另一种基于神经的方法：$\text{Aligner}$中所谓的显著性参数，也可视为一个重要性衡量的权重
>
>      - 以段落子向量$p_i$为例，其显著性定义为$u_{i}^{p}\text{=}\lambda_{i}^{p}\text{×ReLU}\left(\textbf{W}^{p} {p}_{i}\text{+}b^{p}\right)$ 
>      - 神经网络部分：$\text{ReLU}\left(\textbf{W}^{p}p_i \text{+}b^{p}\right)$为一预训练的前馈网络，$\textbf{W}^{p}$和$b^{p}$是可学习参数，最终给出${p}_{i}$一个显著性得分
>      - 稀疏性门控变量：$\lambda_{i}^{p}\text{∈}[0,1]$用于控制该${p}_{i}$被激活的程度，当$\lambda_{i}^{p}\text{=}0$时与${p}_{i}$有关的相似度全被屏蔽 
>
> 3. 实验结果：
>
>    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20250314060208865.png" alt="image-20250314055727525" width=600 />  
>
> :two:$\text{ColBERT}$初排方法的固有偏差
>
> 1. 分桶单向量的做法本质上是对于$\text{ColBERT}$初排的一种近似，所以回到$\text{ColBERT}$初排方法的固有偏差
> 2. $\text{ColBERT}$的训练目标，以及$\text{ColBERT}$初排的推理目标出现了差异，举例说明
>    - 假设正段落$P^+$所有$\text{Token}$与$q_i$的相似度均为$0.8$，则最终段落$P^+$评分为$0.8$ 
>    - 假设负段落$P^-$大多$\text{Token}$与$q_i$的相似度均为$0$，但少数几个$\text{Token}$与$q_i$相似度超过$0.8$，导致最终文档得分仍然有$0.2$ 
>    - 训练时：认为得分越高者为越好，即使得模型倾向于选择$P^+$
>    - 推理时：推理的任务是检索与$q_i$最相似的段落$\text{Token}$，如果进行$\text{Top-1}$检索则最终会选择$P^-$ 
> 3. 改进的点：$\text{NeurIPS'23}$的工作，提出了$\text{XTR}$，改变了$\text{ColBERT}$的训练策略
>    - 原有策略：鼓励模型在后期交互中找到$P^{+}$
>    - $\text{XTR}$策略：鼓励模型在初排阶段，就能通过找到最临近的$p_j$从而找到$P^+$ 



