#### FEDformer: 用于长期序列预测的频率增强分解Transformer

#### 摘要

尽管基于Transformer的方法在长期序列预测方面显著提升了最新技术水平，但它们不仅计算成本高昂，更重要的是，无法捕捉时间序列的全局视图（例如整体趋势）。为了解决这些问题，我们提出将Transformer与季节性-趋势分解方法相结合，其中分解方法捕捉时间序列的全局轮廓，而Transformer捕捉更详细的结构。为了进一步提升Transformer在长期预测中的性能，我们利用了一个事实，即大多数时间序列在诸如傅里叶变换等著名基中往往具有稀疏表示，并开发了一种频率增强的Transformer。除了更有效之外，所提出的方法（称为频率增强分解Transformer，简称FEDformer）比标准Transformer更高效，其复杂度与序列长度呈线性关系。我们在六个基准数据集上的实证研究表明，与最新方法相比，FEDformer能够将多变量和单变量时间序列的预测误差分别减少14.8%和22.6%。代码已在https://github.com/MAZiqing/FEDformer公开提供。

## 1. 引言

长期时间序列预测是各种应用领域（如能源、天气、交通、经济）中长期存在的挑战。尽管RNN类型的方法（Rangapuram等，2018；Flunkert等，2017）取得了令人印象深刻的成果，但它们常常面临梯度消失或爆炸的问题（Pascanu等，2013），这显著限制了它们的性能。随着Transformer在NLP和CV领域的成功（Vaswani等，2017；Devlin等，2019；Dosovitskiy等，2021；Rao等，2021），Transformer（Vaswani等，2017）被引入以捕捉时间序列预测中的长期依赖性，并显示出有希望的结果（Zhou等，2021；Wu等，2021）。由于高计算复杂性和内存需求使得Transformer难以应用于长序列建模，许多研究致力于降低Transformer的计算成本（Li等，2019；Kitaev等，2020；Zhou等，2021；Wang等，2020；Xiong等，2021；Ma等，2021）。关于这一系列工作的详细概述可以在附录A中找到。

---

尽管基于Transformer的方法在时间序列预测方面取得了进展，但在某些情况下，它们往往无法捕捉时间序列的整体特征/分布。在图1中，我们比较了真实时间序列与vanilla Transformer方法（Vaswani等，2017）在真实世界ETTm1数据集（Zhou等，2021）中的预测结果。显然，预测的时间序列与真实值的分布不同。真实值与预测之间的差异可以解释为Transformer中的逐点注意力和预测。由于每个时间步的预测是单独且独立进行的，模型很可能无法保持时间序列作为整体的全局属性和统计特性。为了解决这个问题，我们在本工作中提出了两个想法。第一个想法是将季节性-趋势分解方法（Cleveland等，1990；Wen等，2019）引入到基于Transformer的方法中。尽管这一想法之前已被探索过（Oreshkin等，2019；Wu等，2021），但我们提出了一种网络设计，能够有效地使预测的分布接近真实值的分布，根据Kologrov-Smirnov分布检验。我们的第二个想法是将傅里叶分析与基于Transformer的方法相结合。我们不是在时域中应用Transformer，而是在频域中应用它，这有助于Transformer更好地捕捉时间序列的全局特性。结合这两个想法，我们提出了一种频率增强分解Transformer，简称FEDformer，用于长期时间序列预测。

---

FEDformer的一个关键问题是，傅里叶分析中应该使用哪些频率子集来表示时间序列。一个常见的做法是保留低频成分并丢弃高频成分。这对于时间序列预测可能并不合适，因为时间序列中的一些趋势变化与重要事件相关，如果简单地移除所有高频成分，这些信息可能会丢失。我们通过有效利用时间序列在傅里叶基等基上具有（未知）稀疏表示的事实来解决这个问题。根据我们的理论分析，随机选择包括低频和高频成分的频率子集将为时间序列提供更好的表示，这一结论通过大量实证研究得到了进一步验证。除了在长期预测中更有效外，将Transformer与频率分析相结合，使我们能够将Transformer的计算成本从二次复杂度降低到线性复杂度。我们注意到，这与之前加速Transformer的努力不同，后者通常会导致性能下降。

---

简而言之，我们将本工作的关键贡献总结如下：

1. 我们提出了一种频率增强分解Transformer架构，结合专家混合模型进行季节性-趋势分解，以更好地捕捉时间序列的全局特性。
2. 我们在Transformer结构中提出了傅里叶增强块和小波增强块，通过频域映射捕捉时间序列中的重要结构。它们分别替代了自注意力块和交叉注意力块。
3. 通过随机选择固定数量的傅里叶成分，所提出的模型实现了线性计算复杂度和内存成本。这种选择方法的有效性在理论和实证上都得到了验证。
4. 我们在多个领域（能源、交通、经济、天气和疾病）的6个基准数据集上进行了广泛实验。我们的实证研究表明，所提出的模型将多变量和单变量预测的最新方法性能分别提高了14.8%和22.6%。

---

图1. 在真实世界ETTm1数据集中，真实值与vanilla Transformer预测输出之间的不同分布。左图：频率模式和趋势变化。右图：趋势变化。

## 2. 时间序列在频域中的紧凑表示

众所周知，时间序列数据可以从时域和频域进行建模。我们工作的一个关键贡献，也是与其他长期预测算法的区别，是在神经网络中进行频域操作。傅里叶分析是深入频域的常用工具，但如何恰当地使用傅里叶分析表示时间序列中的信息至关重要。简单地保留所有频率成分可能会导致较差的表示，因为时间序列中的许多高频变化是由噪声输入引起的。另一方面，仅保留低频成分对于序列预测也可能不合适，因为时间序列中的一些趋势变化代表了重要事件。相反，使用少量选定的傅里叶成分来保持时间序列的紧凑表示，将有助于Transformer的高效计算，这对于建模长序列至关重要。我们建议通过随机选择包括高频和低频成分的固定数量的傅里叶成分来表示时间序列。以下从理论上分析了随机选择的合理性。实证验证可以在实验部分找到。

---

假设我们有 $ m $ 个时间序列，记为 $ X_{1}(t), \ldots, X_{m}(t) $。通过对每个时间序列应用傅里叶变换，我们将每个 $ X_{i}(t) $ 转换为向量 $ a_{i}=\left(a_{i, 1}, \ldots, a_{i, d}\right)^{\top} \in \mathbb{R}^{d} $。将所有傅里叶变换向量放入矩阵中，我们得到 $ A=\left(a_{1}, a_{2}, \ldots, a_{m}\right)^{\top} \in \mathbb{R}^{m \times d} $，其中每一行对应不同的时间序列，每一列对应不同的傅里叶成分。尽管使用所有傅里叶成分可以最好地保留时间序列中的历史信息，但这可能会导致对历史数据的过拟合，从而对未来的信号预测效果不佳。因此，我们需要选择一个傅里叶成分的子集，一方面它应该足够小以避免过拟合问题，另一方面它应该能够保留大部分历史信息。在这里，我们建议从 $ d $ 个傅里叶成分中均匀随机选择 $ s $ 个成分（$ s<d $）。更具体地说，我们记 $ i_{1}<i_{2}<\ldots<i_{s} $ 为随机选择的成分。我们构建矩阵 $ S \in\{0,1\}^{s \times d} $，其中如果 $ i=i_{k} $，则 $ S_{i, k}=1 $，否则 $ S_{i, k}=0 $。然后，我们对多元时间序列的表示变为 $ A^{\prime}=A S^{\top} \in \mathbb{R}^{m \times s} $。下面我们将证明，尽管傅里叶基是随机选择的，但在一个温和的条件下，$ A^{\prime} $ 能够保留 $ A $ 中的大部分信息。

---

为了衡量 $ A^{\prime} $ 能够多大程度地保留 $ A $ 中的信息，我们将 $ A $ 的每一列向量投影到 $ A^{\prime} $ 的列向量所张成的子空间中。我们记 $ P_{A^{\prime}}(A) $ 为投影后的结果矩阵，其中 $ P_{A^{\prime}}(\cdot) $ 表示投影算子。如果 $ A^{\prime} $ 保留了 $ A $ 中的大部分信息，我们期望 $ A $ 和 $ P_{A^{\prime}}(A) $ 之间的误差较小，即 $ \left|A-P_{A^{\prime}}(A)\right| $。令 $ A_{k} $ 表示通过其前 $ k $ 个最大奇异值分解对 $ A $ 的近似。以下定理表明，如果随机采样的傅里叶成分数量 $ s $ 在 $ k^{2} $ 的数量级上，则 $ \left|A-P_{A^{\prime}}(A)\right| $ 接近 $ \left|A-A_{k}\right| $。

---

**定理 1.** 假设矩阵 $ A $ 的相干性度量 $ \mu(A) $ 为 $ \Omega(k / n) $。那么，以高概率，我们有

$$
\left|A-P_{A^{\prime}}(A)\right| \leq(1+\epsilon)\left|A-A_{k}\right|
$$

如果 $ s=O\left(k^{2} / \epsilon^{2}\right) $。详细分析见附录 C。

---

对于现实世界中的多元时间序列，通过傅里叶变换得到的矩阵 $ A $ 通常表现出低秩特性，因为多元时间序列中的单变量不仅依赖于其过去的值，而且彼此之间也存在依赖关系，同时还共享相似的频率成分。因此，如定理 1 所示，随机选择傅里叶成分的子集使我们能够恰当地表示傅里叶矩阵 $ A $ 中的信息。

---

类似地，小波正交多项式（如勒让德多项式）也满足受限等距性质（RIP），可以用于捕捉时间序列中的信息。与傅里叶基相比，基于小波的表示在捕捉时间序列中的局部结构方面更为有效，因此在某些预测任务中可能更有效。我们将在附录 B 中讨论基于小波的表示。在下一节中，我们将介绍将傅里叶变换融入Transformer的频率增强分解Transformer架构设计。

## 3. 模型结构

在本节中，我们将介绍 (1) FEDformer 的整体结构，如图 2 所示，(2) 用于信号处理的两种子版本结构：一种使用傅里叶基，另一种使用小波基，(3) 用于季节-趋势分解的专家混合机制，以及 (4) 所提出模型的复杂度分析。

- 图 2. FEDformer 结构。FEDformer 由 $N$ 个编码器和 $M$ 个解码器组成。频率增强块（FEB，绿色块）和频率增强注意力机制（FEA，红色块）用于在频域中进行表示学习。FEB 或 FEA 都有两个子版本（FEB-f 和 FEB-w 或 FEA-f 和 FEA-w），其中 '-f' 表示使用傅里叶基，'-w' 表示使用小波基。专家混合分解块（MOEDecomp，黄色块）用于从输入数据中提取季节-趋势模式。

### 3.1. FEDformer 框架

**预备知识**。长期时间序列预测是一个序列到序列的问题。我们将输入长度记为 $I$，输出长度记为 $O$。我们将 $D$ 作为序列的隐藏状态。编码器的输入是一个 $I \times D$ 的矩阵，而解码器的输入为 $(I / 2+O) \times D$。

---

**FEDformer 结构**。受第 1 节中讨论的季节-趋势分解和分布分析的启发，我们将 Transformer 改造为一种深度分解架构，如图 2 所示，包括频率增强块（FEB）、连接编码器和解码器的频率增强注意力机制（FEA），以及专家混合分解块（MOEDecomp）。FEB、FEA 和 MOEDecomp 块的详细描述将分别在接下来的第 3.2、3.3 和 3.4 节中给出。

---

编码器采用多层结构，形式为：$\mathcal{X}_{\text {en }}^{l}=$ $\operatorname{Encoder}\left(\mathcal{X}_{\text {en }}^{l-1}\right)$，其中 $l \in\{1, \cdots, N\}$ 表示第 $l$ 层编码器的输出，$\mathcal{X}_{\text {en }}^{0} \in \mathbb{R}^{I \times D}$ 是嵌入的历史序列。编码器 $(\cdot)$ 的形式化表示为：

$$
\begin{align*}
\mathcal{S}_{\mathrm{en}}^{l, 1},- & =\operatorname{MOEDecomp}\left(\operatorname{FEB}\left(\mathcal{X}_{\mathrm{en}}^{l-1}\right)+\mathcal{X}_{\mathrm{en}}^{l-1}\right) \\
\mathcal{S}_{\mathrm{en}}^{l, 2},- & =\operatorname{MOEDecomp}\left(\text { FeedForward }\left(\mathcal{S}_{\mathrm{en}}^{l, 1}\right)+\mathcal{S}_{\mathrm{en}}^{l, 1}\right),  \tag{1}\\
\mathcal{X}_{\mathrm{en}}^{l} & =\mathcal{S}_{\mathrm{en}}^{l, 2}
\end{align*}
$$

---

其中 $\mathcal{S}_{\mathrm{en}}^{l, i}, i \in\{1,2\}$ 分别表示第 $l$ 层中第 $i$ 个分解块后的季节分量。对于 FEB 模块，它有两个不同的版本（FEB-f 和 FEB-w），分别通过离散傅里叶变换（DFT）和离散小波变换（DWT）机制实现，并且可以无缝替换自注意力块。

---

解码器同样采用多层结构，形式为：$\mathcal{X}_{\mathrm{de}}^{l}, \mathcal{T}_{\mathrm{de}}^{l}=\operatorname{Decoder}\left(\mathcal{X}_{\mathrm{de}}^{l-1}, \mathcal{T}_{\mathrm{de}}^{l-1}\right)$，其中 $l \in\{1, \cdots, M\}$ 表示第 $l$ 层解码器的输出。解码器 $(\cdot)$ 的形式化表示为：

$$
\begin{align*}
\mathcal{S}_{\mathrm{de}}^{l, 1}, \mathcal{T}_{\mathrm{de}}^{l, 1} & =\operatorname{MOEDecomp}\left(\operatorname{FEB}\left(\mathcal{X}_{\mathrm{de}}^{l-1}\right)+\mathcal{X}_{\mathrm{de}}^{l-1}\right), \\
\mathcal{S}_{\mathrm{de}}^{l, 2}, \mathcal{T}_{\mathrm{de}}^{l, 2} & =\operatorname{MOEDecomp}\left(\operatorname{FEA}\left(\mathcal{S}_{\mathrm{de}}^{l, 1}, \mathcal{X}_{\mathrm{en}}^{N}\right)+\mathcal{S}_{\mathrm{de}}^{l, 1}\right), \\
\mathcal{S}_{\mathrm{de}}^{l, 3}, \mathcal{T}_{\mathrm{de}}^{l, 3} & =\operatorname{MOEDecomp}\left(\text { FeedForward }\left(\mathcal{S}_{\mathrm{de}}^{l, 2}\right)+\mathcal{S}_{\mathrm{de}}^{l, 2}\right), \\
\mathcal{X}_{\mathrm{de}}^{l} & =\mathcal{S}_{\mathrm{de}}^{l, 3}, \\
\mathcal{T}_{\mathrm{de}^{l}} & =\mathcal{T}_{\mathrm{de}}^{l-1}+\mathcal{W}_{l, 1} \cdot \mathcal{T}_{\mathrm{de}}^{l, 1}+\mathcal{W}_{l, 2} \cdot \mathcal{T}_{\mathrm{de}}^{l, 2}+\mathcal{W}_{l, 3} \cdot \mathcal{T}_{\mathrm{de}}^{l, 3}, \tag{2}
\end{align*}
$$

---

其中 $\mathcal{S}_{\mathrm{de}}^{l, i}, \mathcal{T}_{\mathrm{de}}^{l, i}, i \in\{1,2,3\}$ 分别表示第 $l$ 层中第 $i$ 个分解块后的季节分量和趋势分量。$\mathcal{W}_{l, i}, i \in\{1,2,3\}$ 表示第 $i$ 个提取趋势 $\mathcal{T}_{\text {de }}^{l, i}$ 的投影器。与 FEB 类似，FEA 也有两个不同的版本（FEA-f 和 FEA-w），分别通过 DFT 和 DWT 投影结合注意力机制实现，并且可以替换交叉注意力块。FEA $(\cdot)$ 的详细描述将在第 3.3 节中给出。

---

最终的预测是两个精炼分解分量的和，即 $\mathcal{W}_{\mathcal{S}} \cdot \mathcal{X}_{\mathrm{de}}^{M}+\mathcal{T}_{\mathrm{de}}^{M}$，其中 $\mathcal{W}_{\mathcal{S}}$ 用于将深度变换后的季节分量 $\mathcal{X}_{\text {de }}^{M}$ 投影到目标维度。

### 3.2. 傅里叶增强结构

**离散傅里叶变换（DFT）**。提出的傅里叶增强结构使用离散傅里叶变换（DFT）。设 $\mathcal{F}$ 表示傅里叶变换，$\mathcal{F}^{-1}$ 表示逆傅里叶变换。给定时域中的实数序列 $x_{n}$，其中 $n=1,2 \ldots N$。DFT 定义为 $X_{l}=\sum_{n=0}^{N-1} x_{n} e^{-i \omega l n}$，其中 $i$ 是虚数单位，$X_{l}, l=1,2 \ldots L$ 是频域中的复数序列。类似地，逆 DFT 定义为 $x_{n}=\sum_{l=0}^{L-1} X_{l} e^{i \omega l n}$。DFT 的复杂度为 $O\left(N^{2}\right)$。通过快速傅里叶变换（FFT），计算复杂度可以降低到 $O(N \log N)$。这里使用了傅里叶基的随机子集，且子集的规模由一个标量限制。当我们在 DFT 和逆 DFT 操作之前选择模式索引时，计算复杂度可以进一步降低到 $O(N)$。

---

##### 基于傅里叶变换的频率增强块（FEB-f）

FEB-f 在编码器和解码器中均被使用，如图 2 所示。FEB-f 块的输入（$\boldsymbol{x} \in \mathbb{R}^{N \times D}$）首先通过 $\boldsymbol{w} \in \mathbb{R}^{D \times D}$ 进行线性投影，得到 $\boldsymbol{q}=\boldsymbol{x} \cdot \boldsymbol{w}$。然后将 $\boldsymbol{q}$ 从时域转换到频域。$\boldsymbol{q}$ 的傅里叶变换记为 $\boldsymbol{Q} \in \mathbb{C}^{N \times D}$。在频域中，仅保留随机选择的 $M$ 个模式，因此我们使用选择操作符：

$$
\begin{equation*}
\tilde{\boldsymbol{Q}}=\operatorname{Select}(\boldsymbol{Q})=\operatorname{Select}(\mathcal{F}(\boldsymbol{q})), \tag{3}
\end{equation*}
$$

其中 $\tilde{\boldsymbol{Q}} \in \mathbb{C}^{M \times D}$ 且 $M \ll N$。然后，FEB-f 定义为：

$$
\begin{equation*}
\operatorname{FEB}-\mathrm{f}(\boldsymbol{q})=\mathcal{F}^{-1}(\operatorname{Padding}(\tilde{\boldsymbol{Q}} \odot \boldsymbol{R})) \tag{4}
\end{equation*}
$$

其中 $\boldsymbol{R} \in \mathbb{C}^{D \times D \times M}$ 是一个随机初始化的参数化核。设 $\boldsymbol{Y}=\boldsymbol{Q} \odot \boldsymbol{C}$，其中 $\boldsymbol{Y} \in \mathbb{C}^{M \times D}$。乘积操作符 $\odot$ 定义为：$Y_{m, d_{o}}=\sum_{d_{i}=0}^{D} Q_{m, d_{i}} \cdot R_{d_{i}, d_{o}, m}$，其中 $d_{i}=1,2 \ldots D$ 是输入通道，$d_{o}=1,2 \ldots D$ 是输出通道。$\boldsymbol{Q} \odot \boldsymbol{R}$ 的结果在执行逆傅里叶变换之前被零填充到 $\mathbb{C}^{N \times D}$。该结构如图 3 所示。

---

##### 基于傅里叶变换的频率增强注意力机制（FEA-f）

我们使用经典 Transformer 的表达形式。输入：查询、键、值分别记为 $\boldsymbol{q} \in \mathbb{R}^{L \times D}, \boldsymbol{k} \in \mathbb{R}^{L \times D}, \boldsymbol{v} \in \mathbb{R}^{L \times D}$。在交叉注意力中，查询来自解码器，可以通过 $\boldsymbol{q}=\boldsymbol{x}_{e n} \cdot \boldsymbol{w}_{q}$ 获得，其中 $\boldsymbol{w}_{q} \in \mathbb{R}^{D \times D}$。键和值来自编码器，可以通过 $\boldsymbol{k}=\boldsymbol{x}_{d e} \cdot \boldsymbol{w}_{k}$ 和 $\boldsymbol{v}=\boldsymbol{x}_{d e} \cdot \boldsymbol{w}_{v}$ 获得，其中 $\boldsymbol{w}_{k}, \boldsymbol{w}_{v} \in \mathbb{R}^{D \times D}$。形式上，经典注意力机制可以表示为：

$$
\begin{equation*}
\operatorname{Atten}(\boldsymbol{q}, \boldsymbol{k}, \boldsymbol{v})=\operatorname{Softmax}\left(\frac{\boldsymbol{q} \boldsymbol{k}^{\top}}{\sqrt{d_{q}}}\right) \boldsymbol{v} \tag{5}
\end{equation*}
$$

---

在 FEA-f 中，我们将查询、键和值通过傅里叶变换转换到频域，并在频域中通过随机选择 $M$ 个模式执行类似的注意力机制。我们将傅里叶变换后的选择版本记为 $\tilde{\boldsymbol{Q}} \in \mathbb{C}^{M \times D}, \tilde{\boldsymbol{K}} \in \mathbb{C}^{M \times D}, \tilde{\boldsymbol{V}} \in \mathbb{C}^{M \times D}$。FEA-f 定义为：

$$
\begin{align*}
& \tilde{\boldsymbol{Q}}=\operatorname{Select}(\mathcal{F}(\boldsymbol{q})) \\
& \tilde{\boldsymbol{K}}=\operatorname{Select}(\mathcal{F}(\boldsymbol{k}))  \tag{6}\\
& \tilde{\boldsymbol{V}}=\operatorname{Select}(\mathcal{F}(\boldsymbol{v})) \\
& \operatorname{FEA}-\mathrm{f}(\boldsymbol{q}, \boldsymbol{k}, \boldsymbol{v})=\mathcal{F}^{-1}\left(\operatorname{Padding}\left(\sigma\left(\tilde{\boldsymbol{Q}} \cdot \tilde{\boldsymbol{K}}^{\top}\right) \cdot \tilde{\boldsymbol{V}}\right)\right), \tag{7}
\end{align*}
$$

---

其中 $\sigma$ 是激活函数。我们使用 softmax 或 tanh 作为激活函数，因为它们的收敛性能在不同数据集中有所不同。设 $\boldsymbol{Y}=\sigma\left(\tilde{\boldsymbol{Q}} \cdot \tilde{\boldsymbol{K}}^{\top}\right) \cdot \tilde{\boldsymbol{V}}$，且 $\boldsymbol{Y} \in \mathbb{C}^{M \times D}$ 在执行逆傅里叶变换之前需要被零填充到 $\mathbb{C}^{L \times D}$。FEA-f 的结构如图 4 所示。 

### 3.3. 小波增强结构

**离散小波变换（DWT）**  

虽然傅里叶变换在频域中创建了信号的表示，但小波变换在频域和时域中同时创建了表示，从而能够高效地访问信号的局部信息。多小波变换结合了正交多项式和小波的优点。对于给定的 $f(x)$，尺度 $n$ 下的多小波系数可以分别定义为 $\mathbf{s}_{l}^{n}=\left[\left\langle f, \phi_{i l}^{n}\right\rangle_{\mu_{n}}\right]_{i=0}^{k-1}$ 和 $\mathbf{d}_{l}^{n}=\left[\left\langle f, \psi_{i l}^{n}\right\rangle_{\mu_{n}}\right]_{i=0}^{k-1}$，其中 $\mu_{n}$ 是测度，$\mathbf{s}_{l}^{n}, \mathbf{d}_{l}^{n} \in \mathbb{R}^{k \times 2^{n}}$。$\phi_{i l}^{n}$ 是分段多项式的小波正交基。跨尺度的分解/重建定义为：
$$
\begin{align*}
\mathbf{s}_{l}^{n} & =H^{(0)} \mathbf{s}_{2 l}^{n+1}+H^{(1)} \mathbf{s}_{2 l+1}^{n+1}, \\
\mathbf{s}_{2 l}^{n+1} & =\Sigma^{(0)}\left(H^{(0) T} \mathbf{s}_{l}^{n}+G^{(0) T} \mathbf{d}_{l}^{n}\right), \\
\mathbf{d}_{l}^{n} & =G^{(0)} \mathbf{s}_{2 l}^{n+1}+H^{(1)} \mathbf{s}_{2 l+1}^{n+1},  \tag{8}\\
\mathbf{s}_{2 l+1}^{n+1} & =\Sigma^{(1)}\left(H^{(1) T} \mathbf{s}_{l}^{n}+G^{(1) T} \mathbf{d}_{l}^{n}\right),
\end{align*}
$$

其中 $\left(H^{(0)}, H^{(1)}, G^{(0)}, G^{(1)}\right)$ 是多小波分解滤波器的线性系数。它们是用于小波分解的固定矩阵。

---

信号的多小波表示可以通过多尺度和多小波基的张量积获得。需要注意的是，不同尺度的基通过张量积耦合在一起，因此我们需要将其解耦。受 (Gupta et al., 2021) 的启发，我们采用非标准小波表示来降低模型复杂度。对于映射函数 $F(x)=x^{\prime}$，多小波域下的映射可以写为：

$$
\begin{equation*}
U_{d l}^{n}=A_{n} d_{l}^{n}+B_{n} s_{l}^{n}, \quad U_{s k l}^{n}=C_{n} d_{l}^{n}, \quad U_{s l}^{L}=\bar{F} s_{l}^{L}, \tag{9}
\end{equation*}
$$

其中 $\left(U_{s l}^{n}, U_{d l}^{n}, s_{l}^{n}, d_{l}^{n}\right)$ 是多尺度和多小波系数，$L$ 是递归分解下的最粗尺度，$A_{n}, B_{n}, C_{n}$ 是三个独立的 FEB-f 块模块，用于在分解和重建过程中处理不同的信号。$\bar{F}$ 是一个单层感知器，用于处理在 $L$ 步分解后剩余的粗信号。更多设计细节见附录 D。

---

##### 基于小波变换的频率增强块（FEB-w）  
FEB-w 的整体架构如图 5 所示。它与 FEB-f 的区别在于递归机制：输入被递归分解为 3 部分并分别处理。对于小波分解部分，我们实现了固定的 Legendre 小波基分解矩阵。三个 FEB-f 模块分别用于处理小波分解后得到的高频部分、低频部分和剩余部分。对于每个循环 $L$，它生成一个处理后的高频张量 $U d(L)$、一个处理后的低频张量 $U s(L)$ 以及原始的低频张量 $X(L+1)$。这是一种阶梯式下降的方法，分解阶段通过 $1 / 2$ 的因子对信号进行抽取，最多运行 $L$ 个循环，其中对于大小为 $M$ 的输入序列，$L<\log _{2}(M)$。在实践中，$L$ 被设置为一个固定的参数。三个 FEB-f 块组在不同的分解循环 $L$ 中共享。对于小波重建部分，我们也递归地构建输出张量。对于每个循环 $L$，我们结合分解部分生成的 $X(L+1)$、$U s(L)$ 和 $U d(L)$，并生成 $X(L)$ 用于下一个重建循环。在每个循环中，信号张量的长度维度增加 2 倍。

- 图 5. 左上：小波频率增强块分解阶段。右上：FEB-w 和 FEA-w 共享的小波块重建阶段。底部：小波频率增强交叉注意力分解阶段。

---

##### 基于小波变换的频率增强注意力机制（FEA-w）  
FEA-w 包含与 FEB-w 类似的分解阶段和重建阶段。这里我们保持重建阶段不变。唯一的区别在于分解阶段。相同的分解矩阵用于分别分解 $\boldsymbol{q}, \boldsymbol{k}, \boldsymbol{v}$ 信号，并且 $\boldsymbol{q}, \boldsymbol{k}, \boldsymbol{v}$ 共享相同的模块组来处理它们。如上所述，带有小波分解块的频率增强块（FEB-w）包含三个用于信号处理的 FEB-f 块。我们可以将 FEB-f 视为自注意力机制的替代。我们采用一种直接的方法来构建带有小波分解的频率增强交叉注意力，将每个 FEB-f 替换为 FEA-f 模块。此外，还添加了另一个 FEA-f 模块来处理剩余的粗信号 $q(L), k(L), v(L)$。

### 3.4. 基于专家混合的季节-趋势分解

由于在实际数据中常见的复杂周期性模式与趋势分量耦合，使用固定窗口的平均池化提取趋势可能会很困难。为了解决这个问题，我们设计了一个专家混合分解块（MOEDecomp）。它包含一组不同大小的平均滤波器，用于从输入信号中提取多个趋势分量，以及一组数据依赖的权重，用于将它们组合为最终趋势。形式上，我们有：

$$
\begin{equation*}
\mathbf{X}_{\text {trend }}=\boldsymbol{\operatorname { S o f t m a x }}(L(x)) *(F(x)), \tag{10}
\end{equation*}
$$

其中 $F(\cdot)$ 是一组平均池化滤波器，$\operatorname{Softmax}(L(x))$ 是用于混合这些提取趋势的权重。

### 3.5. 复杂度分析

对于 FEDformer-f，由于在 FEB 和 FEA 块中固定了随机选择的模式数量，时间和内存的计算复杂度为 $O(L)$。我们默认设置模式数量 $M=64$。尽管通过 FFT 进行完整 DFT 变换的复杂度为 $O(L \log (L))$，但我们的模型只需要 $O(L)$ 的成本和内存复杂度，因为预选了傅里叶基以实现快速计算。对于 FEDformer-w，当我们将递归分解步骤设置为固定数量 $L$ 并使用与 FEDformer-f 相同的固定随机选择模式数量时，时间复杂度和内存使用量也为 $O(L)$。在实践中，我们默认选择 $L=3$ 和模式数量 $M=64$。表 1 总结了训练和测试推理步骤中的时间复杂度和内存使用量的比较。可以看出，提出的 FEDformer 在基于 Transformer 的预测模型中实现了最佳的整体复杂度。

## 4. 实验

为了评估提出的 FEDformer，我们在六个流行的真实世界数据集上进行了广泛的实验，包括能源、经济、交通、天气和疾病。由于经典模型如 ARIMA 和基本的 RNN/CNN 模型表现相对较差（如 (Zhou et al., 2021) 和 (Wu et al., 2021) 所示），我们主要将四种最先进的基于 Transformer 的模型作为比较基准，即 Autoformer (Wu et al., 2021)、Informer (Zhou et al., 2021)、LogTrans (Li et al., 2019) 和 Reformer (Kitaev et al., 2020)。需要注意的是，由于 Autoformer 在所有六个基准测试中表现最佳，因此将其作为主要基准模型进行比较。有关基线模型、数据集和实现的更多详细信息分别见附录 A.2、F.1 和 F.2。

### 4.1. 主要结果

为了更好地进行比较，我们遵循 (Wu et al., 2021) 中 Autoformer 的实验设置，其中输入长度固定为 96，训练和评估的预测长度分别固定为 $96, 192, 336$ 和 720。

---

**多变量结果**。对于多变量预测，FEDformer 在所有六个基准数据集上的所有预测范围内均取得了最佳性能，如表 2 所示。与 Autoformer 相比，提出的 FEDformer 实现了总体 $\mathbf{14.8\%}$ 的相对均方误差（MSE）降低。值得注意的是：

- 表 2. 六个数据集上的多变量长期序列预测结果，输入长度 $I=96$，预测长度 $O \in \{96,192,336,720\}$（对于 ILI 数据集，输入长度 $I=36$，预测长度 $O \in\{24,36,48,60\}$）。MSE 越低表示性能越好，最佳结果以粗体标出。

---

对于一些数据集，例如 Exchange 和 ILI，改进甚至更为显著（超过 $20\%$）。需要注意的是，Exchange 数据集的时间序列并未表现出明显的周期性，但 FEDformer 仍然能够取得优异的性能。总体而言，FEDformer 的改进在不同预测范围内保持一致，表明其在长期预测中的优势。更多关于 ETT 完整基准测试的详细结果见附录 F.3。

---

**单变量结果**。单变量时间序列预测的结果总结在表 3 中。与 Autoformer 相比，FEDformer 实现了总体 $\mathbf{22.6\%}$ 的相对 MSE 降低，并且在某些数据集（如交通和天气）上，改进幅度甚至超过 $30\%$。这再次验证了 FEDformer 在长期预测中的有效性。需要注意的是，由于傅里叶基和小波基的差异，FEDformer-f 和 FEDformer-w 在不同数据集上表现优异，使它们成为长期预测中的互补选择。更多关于 ETT 完整基准测试的详细结果见附录 F.3。

- 表 3. 六个数据集上的单变量长期序列预测结果，输入长度 $I=96$，预测长度 $O \in \{96,192,336,720\}$（对于 ILI 数据集，输入长度 $I=36$，预测长度 $O \in\{24,36,48,60\}$）。MSE 越低表示性能越好，最佳结果以粗体标出。

### 4.2. 消融实验

在本节中，我们进行了消融实验，旨在比较频率增强块及其替代方案的性能。使用自相关机制的 Autoformer 的当前最先进（SOTA）结果作为基线。测试了 FEDformer 的三种消融变体：  
1) **FEDformer V1**：仅使用 FEB 替换自注意力机制；  
2) **FEDformer V2**：仅使用 FEA 替换交叉注意力机制；  
3) **FEDformer V3**：使用 FEA 替换自注意力和交叉注意力机制。  

FEDformer-f 的消融版本与 SOTA 模型的比较结果如表 4 所示，如果消融版本相比 Autoformer 带来了改进，则使用粗体数字标注。由于篇幅限制，我们省略了 FEDformer-w 的类似结果。从表 4 可以看出，FEDformer V1 在 10/16 的情况下带来了改进，而 FEDformer V2 在 12/16 的情况下带来了改进。最佳性能出现在我们设计的 FEDformer 中，其中包含 FEB 和 FEA 块，在所有 $16/16$ 的情况下都改进了性能。这验证了设计的 FEB 和 FEA 在替换自注意力和交叉注意力机制方面的有效性。此外，在 ETT 和 Weather 数据集上的实验表明，采用的 MOEDecomp（专家混合分解）方案相比单一分解方案平均带来了 $2.96\%$ 的改进。更多细节见附录 F.5。

- 表 4. 消融实验：ETTm1 和 ETTm2 上的多变量长期序列预测结果，输入长度 $I=96$，预测长度 $O \in\{96,192,336,720\}$。FEDformer-f 的三种变体与基线模型进行比较。最佳结果以粗体标出。

---

### 4.3. 模式选择策略

离散傅里叶基的选择是有效表示信号并保持模型线性复杂度的关键。正如我们在第 2 节中讨论的那样，在预测任务中，随机傅里叶模式选择是更好的策略。更重要的是，随机策略不需要对输入的先验知识，并且在新任务中更容易泛化。在这里，我们通过实验比较了随机选择策略与固定选择策略，并将实验结果总结在图 6 中。可以观察到，采用的随机策略比仅保留低频模式的常见固定策略取得了更好的性能。同时，随机策略表现出一定的模式饱和效应，表明选择适当数量的随机模式而不是所有模式会带来更好的性能，这也与第 2 节中的理论分析一致。

- 图 6. 两种基模式选择方法（Fix\&Rand）的比较。Rand 策略表示随机选择一部分模式，Fix 策略表示选择最低频模式。在 ETT 完整基准（h1, m1, h2, m2）上，对多种基模式数量 $M \in\{2,4,8 \ldots 256\}$ 进行了比较。

### 4.4. 预测输出的分布分析

在本节中，我们定量评估了不同 Transformer 模型输入序列与预测输出之间的分布相似性。在表 5 中，我们应用 Kolmogrov-Smirnov 检验来检查不同模型在 ETTm1 和 ETTm2 数据集上的预测结果是否与输入序列一致。具体来说，我们测试固定 96 个时间步的输入序列是否与预测序列来自同一分布，零假设为两个序列来自同一分布。在两个数据集上，将常见的 P 值设为 0.01，除 Autoformer 外，其他现有 Transformer 基线模型的 P 值均远小于 0.01，这表明它们的预测输出更有可能来自与输入序列不同的分布。相比之下，Autoformer 和 FEDformer 的 P 值远高于其他模型，这主要归功于它们的季节-趋势分解机制。尽管两种模型在 ETTm2 上得到了接近的结果，但提出的 FEDformer 在 ETTm1 上的 P 值更大。并且它是唯一一个在两个数据集的所有情况下零假设无法被拒绝（P 值大于 0.01）的模型，这意味着 FEDformer 生成的输出序列与输入序列的分布更为相似，从而验证了我们在第 1 节中讨论的 FEDformer 设计动机。更多详细分析见附录 E。

- 表 5. 不同 Transformer 模型在 ETTm1 和 ETTm2 数据集上长期预测输出的 Kolmogrov-Smirnov 检验 P 值。值越大，表明零假设（输入序列和预测输出来自同一分布）被拒绝的可能性越小。最佳结果以粗体标出。

---

### 4.5. 与 Autoformer 基线的差异

由于我们使用了与 Autoformer 相同的分解编码器-解码器整体架构，因此我们认为强调差异至关重要。在 Autoformer 中，作者提出了一个巧妙的想法，即使用 top-k 子序列相关性（自相关）模块代替逐点注意力，并应用傅里叶方法来提高子序列级别相似性计算的效率。总体而言，Autoformer 可以被视为将序列分解为多个时域子序列以提取特征。相比之下，我们使用频率变换将序列分解为多个频域模式以提取特征。特别是，我们没有采用选择性方法进行子序列选择。相反，所有频率特征都是从整个序列中计算的，这种全局特性使我们的模型在长序列上表现更佳。

---

## 5. 结论

本文提出了一种用于长期序列预测的频率增强 Transformer 模型，该模型实现了最先进的性能，并具有线性计算复杂度和内存成本。我们提出了一种在频率域中进行低秩近似的注意力机制以及专家混合分解机制，以控制分布偏移。所提出的频率增强结构解耦了输入序列长度和注意力矩阵维度，从而实现了线性复杂度。此外，我们从理论和实验上证明了采用的随机模式选择策略在频率域中的有效性。最后，大量实验表明，与四种最先进的算法相比，所提出的模型在六个基准数据集上实现了最佳的预测性能。

## A. 相关工作

在本节中，我们将对时间序列预测的文献进行概述。相关工作包括传统时间序列模型（A.1）、深度学习模型（A.1）、基于 Transformer 的模型（A.2）以及神经网络中的傅里叶变换（A.3）。

## A.1. 传统时间序列模型

数据驱动的时间序列预测帮助研究人员理解系统的演变，而无需构建其背后的精确物理定律。经过数十年的发展，时间序列模型已经得到了很好的发展，并成为众多应用领域中各种项目的核心。第一代数据驱动方法可以追溯到 1970 年代。ARIMA（Box \& Jenkins, 1968; Box \& Pierce, 1970）遵循马尔可夫过程，并构建了一个自回归模型用于递归序列预测。然而，自回归过程不足以处理非线性和非平稳序列。随着新世纪深度神经网络的兴起，循环神经网络（RNN）被专门设计用于涉及序列数据的任务。在 RNN 家族中，LSTM（Hochreiter \& Schmidhuber, 1997）和 GRU（Chung et al., 2014）采用门控结构来控制信息流，以解决梯度消失或爆炸问题。DeepAR（Flunkert et al., 2017）使用序列架构，通过结合二项式似然进行概率预测。基于注意力的 RNN（Qin et al., 2017）使用时间注意力来捕捉长程依赖关系。然而，循环模型无法并行化，并且难以处理长依赖关系。时间卷积网络（Sen et al., 2019）是另一种在序列任务中高效的模型家族。然而，受限于卷积核的感受野，提取的特征仍然是局部的，长程依赖关系难以捕捉。

## A.2. 基于 Transformer 的时间序列预测

随着 Transformer 在自然语言处理（Vaswani et al., 2017; Devlin et al., 2019）和计算机视觉任务（Dosovitskiy et al., 2021; Rao et al., 2021）中的创新，基于 Transformer 的模型也在时间序列预测中被讨论、改进和应用（Zhou et al., 2021; Wu et al., 2021）。在序列到序列的时间序列预测任务中，编码器-解码器架构被广泛采用。自注意力和交叉注意力机制被用作 Transformer 的核心层。然而，当使用逐点连接的矩阵时，Transformer 会面临二次计算复杂度的挑战。

---

为了在不牺牲太多性能的情况下实现高效计算，最早的改进是通过预定义模式来指定注意力矩阵。例如：（Qiu et al., 2020）使用块状注意力，将复杂度降低到块大小的平方。Longformer（Beltagy et al., 2020）采用固定间隔的滑动窗口。LogTrans（Li et al., 2019）使用对数稀疏注意力，实现了 $N \log ^{2} N$ 的复杂度。Htransformer（Zhu \& Soricut, 2021）使用分层模式对注意力矩阵进行稀疏近似，复杂度为 $O(n)$。一些工作结合了上述提到的多种模式（如 BIGBIRD（Zaheer et al., 2020））。另一种策略是使用动态模式：Reformer（Kitaev et al., 2020）引入了局部敏感哈希，将复杂度降低到 $N \log N$。（Zhu \& Soricut, 2021）引入了分层模式。Sinkhorn（Tay et al., 2020）采用块排序方法，仅通过局部窗口实现准全局注意力。

---

同样，一些工作采用 top-k 截断来加速计算：Informer（Zhou et al., 2021）使用基于 KL 散度的方法在注意力矩阵中选择 top-k。这种稀疏矩阵的复杂度仅为 $N \log N$。Autoformer（Wu et al., 2021）引入自相关块代替经典注意力，以获取子序列级别的注意力，借助快速傅里叶变换和自相关矩阵中的 top-k 选择，实现了 $N \log N$ 的复杂度。

---

另一种新兴策略是采用注意力矩阵的低秩近似。Linformer（Wang et al., 2020）使用可训练的线性投影来压缩序列长度，实现了 $O(n)$ 的复杂度，并基于 JL 引理从理论上证明了近似误差的边界。Luna（Ma et al., 2021）开发了一种嵌套线性结构，复杂度为 $O(n)$。Nyströformer（Xiong et al., 2021）利用 Nyström 近似的思想在注意力机制中实现了 $O(n)$ 的复杂度。Performer（Choromanski et al., 2021）采用正交随机特征方法，有效地建模了可核化的注意力机制。

## A.3. Transformer 中的傅里叶变换

得益于快速傅里叶变换（FFT）算法，傅里叶变换的计算复杂度从 $N^{2}$ 压缩到 $N \log N$。傅里叶变换具有时域卷积等价于频域乘法的特性，因此 FFT 可用于加速卷积网络（Mathieu et al., 2014）。FFT 还可用于高效计算自相关函数，这在构建神经网络块（Wu et al., 2021）以及众多异常检测任务（Homayouni et al., 2020）中非常有用。（Li et al., 2020; Gupta et al., 2021）首次在求解偏微分方程（PDEs）中引入了傅里叶神经算子（FNO）。FNO 作为网络的内嵌块，用于在低频域中进行高效的表示学习。FNO 在计算机视觉任务（Rao et al., 2021）中也证明是高效的。它还作为构建小波神经算子（WNO）的基础，最近被引入用于求解偏微分方程（Gupta et al., 2021）。虽然 FNO 保留了低频的频谱模式，但随机傅里叶方法使用随机选择的模式。（Rahimi \& Recht, 2008）提出将输入数据映射到随机低维特征空间，以加速核机器的训练。（Rawat et al., 2019）提出了随机傅里叶 softmax（RF-softmax）方法，利用强大的随机傅里叶特征实现更高效和准确的近似 softmax 分布采样。

---

据我们所知，我们提出的方法是首个通过频域中的低秩近似变换实现快速注意力机制的时间序列预测工作。

## B. 注意力的低秩近似

在本节中，我们讨论注意力机制的低秩近似。首先，我们在 B.1 中介绍受限等距性质（RIP）矩阵，其近似误差边界可以从理论上给出。然后在 B.2 中，我们遵循先前的工作，展示如何利用 RIP 矩阵和注意力机制。

---

如果感兴趣的信号在固定基上是稀疏的或可压缩的，那么可以从较少的测量中恢复信号。（Wang et al., 2020; Xiong et al., 2021）提出，注意力矩阵是低秩的，因此如果将其投影到注意力矩阵稀疏的子空间中，则可以很好地近似注意力矩阵。为了高效计算注意力矩阵，如何正确选择投影基仍然是一个开放性问题。遵循 RIP 的基是一个潜在的候选。

## B.1. RIP 矩阵

RIP 矩阵的定义如下：

**定义 B.1. RIP 矩阵**。设 $m<n$ 为正整数，$\Phi$ 为具有实元素的 $m \times n$ 矩阵，$\delta>0$，且 $K<m$ 为整数。如果对于每个 $K$-稀疏向量 $x \in \mathbb{R}^{n}$，我们有 $(1-\delta)\|x\| \leq \|\Phi x\| \leq(1+\delta)\|x\|$，则称 $\Phi$ 是 $(K, \delta)-RIP$ 矩阵。

RIP 矩阵是满足受限等距性质的矩阵，由 D. Donoho、E. Candès 和 T. Tao 在压缩感知领域发现。由于其良好的性质，RIP 矩阵可能是低秩近似的良好选择。随机矩阵不满足 RIP 的概率可以忽略不计，并且许多类型的矩阵已被证明是 RIP 的，例如高斯基、伯努利基和傅里叶基。

---

**定理 2**。设 $m<n$ 为正整数，$\delta>0$，且 $K=O\left(\frac{m}{\log ^{4} n}\right)$。设 $\Phi$ 为由以下方法之一定义的随机矩阵：

（高斯基）令 $\Phi$ 的元素独立同分布，服从正态分布 $N\left(0, \frac{1}{m}\right)$。

（伯努利基）令 $\Phi$ 的元素独立同分布，服从伯努利分布，取值 $\pm \frac{1}{\sqrt{m}}$，每种概率为 50\%。

（随机选择的离散傅里叶基）令 $A \subset \{0, \ldots, n-1\}$ 为大小为 $m$ 的随机子集。令 $\Phi$ 为从离散傅里叶变换矩阵（即矩阵 $F$，其元素为 $F[l, j]=\exp ^{-2 \pi i l j / n} / \sqrt{n}$，其中 $l, j \in\{0, \ldots, n-1\}$）中选择由 $A$ 索引的行得到的矩阵。

则 $\Phi$ 以概率 $p \approx 1-e^{-n}$ 是 $(K, \sigma)-RIP$ 矩阵。

定理 2 指出，高斯基、伯努利基和傅里叶基遵循 RIP。在接下来的部分中，我们将以傅里叶基为例，展示如何在注意力机制中使用 RIP 基进行低秩近似。

## B.2. 使用傅里叶基/勒让德多项式的低秩近似

Linformer（Wang et al., 2020）证明，注意力机制可以通过低秩矩阵进行近似。Linformer 使用以高斯分布初始化的可训练核进行低秩近似，而我们提出的 FEDformer 使用傅里叶基/勒让德多项式。高斯基、傅里叶基和勒让德多项式都遵循 RIP，因此可以得出类似的结论。

从 Johnson-Lindenstrauss 引理（Johnson, 1984）出发，并使用（Arriaga \& Vempala, 2006）中的版本，Linformer 证明了注意力矩阵可以进行低秩近似。

---

设 $\Phi \in \mathbb{R}^{N \times M}$ 为随机选择的傅里叶基/勒让德多项式。$\Phi$ 是 RIP 矩阵。根据定理 2，对于任意 $x \in \mathbb{R}^{N}$，以概率 $p \approx 1-e^{-n}$，我们有：

$$
\begin{equation*}
(1-\delta)\|x\| \leq\|\Phi x\| \leq(1+\delta)\|x\| \tag{11}
\end{equation*}
$$

根据（Arriaga \& Vempala, 2006），对于任意 $x_{1}, x_{2} \in \mathbb{R}^{N}$，以概率 $p \approx 1-4 e^{-n}$，我们有：

$$
\begin{equation*}
(1-\delta)\left\|x_{1} x_{2}^{\top}\right\| \leq\left\|x_{1} \Phi^{\top} \Phi x_{2}^{\top}\right\| \leq(1+\delta)\left\|x_{1} x_{2}^{\top}\right\| \tag{12}
\end{equation*}
$$

基于上述不等式，我们现在讨论注意力机制中的情况。设注意力矩阵 $B=\operatorname{softmax}\left(\frac{Q K^{\top}}{\sqrt{d}}\right)=\exp (A) \cdot D_{A}^{-1}$，其中 $\left(D_{A}\right)_{i i}=$ $\sum_{n=1}^{N} \exp \left(A_{n i}\right)$。根据 Linformer，我们可以得出以下定理（详细证明请参考（Wang et al., 2020））：

---

**定理 3**。对于矩阵 $B$ 的任意行向量 $p \in \mathbb{R}^{N}$ 和矩阵 $V$ 的任意列向量 $v \in \mathbb{R}^{N}$，以概率 $p=1-o(1)$，我们有：

$$
\begin{equation*}
\left\|b \Phi^{\top} \Phi v^{\top}-b v^{\top}\right\| \leq \delta\left\|b v^{\top}\right\| \tag{13}
\end{equation*}
$$

定理 3 指出，在注意力矩阵 $(P)$ 和值 $(V)$ 的乘法之间使用傅里叶基/勒让德多项式 $\Phi$，可以将计算复杂度从 $O\left(N^{2} d\right)$ 降低到 $O(N M d)$，其中 $d$ 是矩阵的隐藏维度。同时，低秩近似的误差是有界的。然而，定理 3 仅讨论了没有激活函数的情况。

---

此外，利用柯西不等式以及指数函数在紧致区域内是 Lipschitz 连续的事实（证明请参考（Wang et al., 2020）），我们可以得出以下定理：

**定理 4**。对于矩阵 $A$（$A=\frac{Q K^{\top}}{\sqrt{d}}$）中的任意行向量 $A_{i} \in \mathbb{R}^{N}$，以概率 $p=1-o(1)$，我们有：

$$
\begin{equation*}
\left\|\exp \left(A_{i} \Phi^{\top}\right) \Phi v^{\top}-\exp \left(A_{i}\right) v^{\top}\right\| \leq \delta\left\|\exp \left(A_{i}\right) v^{\top}\right\| . \tag{14}
\end{equation*}
$$

定理 4 表明，在激活函数（softmax）存在的情况下，上述讨论的误差界仍然成立。

---

总之，我们可以利用 RIP 矩阵对注意力进行低秩近似。此外，在注意力机制中使用随机选择的傅里叶基进行低秩近似时，存在理论上的误差界。

## C. 傅里叶分量选择

设 $X_{1}(t), \ldots, X_{m}(t)$ 为 $m$ 个时间序列。通过对每个时间序列应用傅里叶变换，我们将每个 $X_{i}(t)$ 转换为向量 $a_{i}=\left(a_{i, 1}, \ldots, a_{i, d}\right)^{\top} \in \mathbb{R}^{d}$。将所有傅里叶变换向量放入矩阵中，我们得到 $A=\left(a_{1}, a_{2}, \ldots, a_{m}\right)^{\top} \in \mathbb{R}^{m \times d}$，其中每一行对应不同的时间序列，每一列对应不同的傅里叶分量。在这里，我们提出从 $d$ 个傅里叶分量中均匀随机选择 $s$ 个分量（$s<d$）。更具体地说，我们记 $i_{1}<i_{2}<\ldots<i_{s}$ 为随机选择的分量。我们构建矩阵 $S \in\{0,1\}^{s \times d}$，如果 $i=i_{k}$，则 $S_{i, k}=1$，否则 $S_{i, k}=0$。然后，我们对多元时间序列的表示变为 $A^{\prime}=A S^{\top} \in \mathbb{R}^{m \times s}$。以下定理表明，尽管傅里叶基是随机选择的，但在温和条件下，$A^{\prime}$ 可以保留 $A$ 中的大部分信息。

---

**定理 5**。假设矩阵 $A$ 的相干性度量 $\mu(A)$ 为 $\Omega(k / n)$。那么，以高概率，我们有

$$
\left|A-P_{A^{\prime}}(A)\right| \leq(1+\epsilon)\left|A-A_{k}\right|
$$

如果 $s=O\left(k^{2} / \epsilon^{2}\right)$。

**证明**。根据（Drineas et al., 2007）中定理 3 的分析，我们有

$$
\begin{aligned}
\left|A-P_{A^{\prime}}(A)\right| & \leq\left|A-A^{\prime}\left(A^{\prime}\right)^{\dagger} A_{k}\right| \\
& =\left|A-\left(A S^{\top}\right)\left(A S^{\top}\right)^{\dagger} A_{k}\right| \\
& =\left|A-\left(A S^{\top}\right)\left(A_{k} S^{\top}\right)^{\dagger} A_{k}\right| .
\end{aligned}
$$

使用（Drineas et al., 2007）中的定理 5，我们以至少 0.7 的概率得到

$$
\left|A-\left(A S^{\top}\right)\left(A_{k} S^{\top}\right)^{\dagger} A_{k}\right| \leq(1+\epsilon)\left|A-A_{k}\right|
$$

如果 $s=O\left(k^{2} / \epsilon^{2} \times \mu(A) n / k\right)$。由于 $\mu(A)=O(k / n)$，定理得证。

## D. 小波变换

在本节中，我们介绍了一些关于小波变换的技术背景，这些技术背景被用于我们提出的框架中。

## D.1. 连续小波变换

首先，让我们看看如何将函数 $f(t)$ 分解为一组称为小波的基函数 $\psi_{\mathrm{s}, \tau}(t)$。这被称为连续小波变换或 $CWT$。更正式地，它可以表示为：

$$
\gamma(s, \tau)=\int f(t) \Psi_{s, \tau}^{*}(t) d t
$$

其中，* 表示复共轭。该方程表明，变量 $\gamma(s, \tau)$、$s$ 和 $\tau$ 分别是小波变换后的新维度、尺度和平移。

---

小波是由一个单一的基本小波 $\Psi(t)$（即所谓的母小波）通过缩放和平移生成的，其形式为：

$$
\psi_{s, \tau}(t)=\frac{1}{\sqrt{s}} \psi\left(\frac{t-\tau}{s}\right)
$$

其中，$s$ 是缩放因子，$\tau$ 是平移因子，$\sqrt{s}$ 用于在不同尺度之间进行能量归一化。

## D.2. 离散小波变换

连续小波变换将一维信号映射为二维的时间-尺度联合表示，这种表示具有高度的冗余性。为了解决这个问题，人们引入了离散小波变换（DWT），其母小波为：

$$
\psi_{j, k}(t)=\frac{1}{\sqrt{s_{0}^{j}}} \psi\left(\frac{t-k \tau_{0} s_{0}^{j}}{s_{0}^{j}}\right)
$$

离散小波变换不能连续缩放和平移，但可以以离散的步骤进行缩放和平移。这里，$j$ 和 $k$ 是整数，$s_{0}>1$ 是固定的缩放步长。平移因子 $\tau_{0}$ 取决于缩放步长。离散化小波的效果是，时间-尺度空间现在以离散间隔采样。我们通常选择 $s_{0}=2$，以便频率轴的采样对应于二进采样。对于平移因子，我们通常选择 $\tau_{0}=1$，以便时间轴也进行二进采样。

---

当使用离散小波变换连续信号时，结果将是一系列小波系数，这被称为小波分解。

## D.3. 正交多项式

接下来我们需要关注的是正交多项式（OPs），它们将作为我们之前介绍的母小波函数。要成为母小波，需要满足许多条件，如容许条件、正则性条件和消失矩。简而言之，我们感兴趣的是在有限域上非零且在其他地方几乎为零的正交多项式。Legendre 多项式是我们工作中使用的一组流行的正交多项式。其他一些流行的正交多项式，如 Chebyshev，也可以在此使用，且无需太多修改。

## D.4. Legendre 多项式

Legendre 多项式是相对于均匀权重函数 $w_{L}(x)=1$ 定义的，其中 $-1 \leqslant x \leqslant 1$ 或 $w_{L}(x)=\mathbf{1}_{[-1,1]}(x)$，使得

$$
\int_{-1}^{1} P_{i}(x) P_{j}(x) d x= \begin{cases}\frac{2}{2 i+1} & i=j \\ 0 & i \neq j\end{cases}
$$

这里函数定义在 $[-1,1]$ 上，但可以通过不同的平移和缩放操作扩展到任何区间 $[a, b]$。

## D.5. 多小波

我们在本工作中使用的多小波结合了小波和之前介绍的正交多项式的优势。与将给定函数投影到单一小波函数上不同，多小波将其投影到有限次数的多项式子空间上。在本工作中，我们限制了对正交多项式家族的研究：Legendre 多项式。

---

首先，基的定义如下：相对于测度 $\mu$ 的一组正交基 $\phi_{0}, \ldots, \phi_{k-1}$ 满足 $\left\langle\phi_{i}, \phi_{j}\right\rangle_{\mu}=\delta_{i j}$。对于特定的测度（权重函数 $w(x)$），正交性条件可以写为 $\int \phi_{i}(x) \phi_{j}(x) w(x) d x=\delta_{i j}$。

---

根据（Gupta et al., 2021）中的推导，通过使用高斯求积和 Gram-Schmidt 正交化工具，使用 Legendre 多项式的多小波滤波器系数可以写为

$$
\begin{aligned}
H_{i j}^{(0)} & =\sqrt{2} \int_{0}^{1 / 2} \phi_{i}(x) \phi_{j}(2 x) w_{L}(2 x-1) d x \\
& =\frac{1}{\sqrt{2}} \int_{0}^{1} \phi_{i}(x / 2) \phi_{j}(x) d x \\
& =\frac{1}{\sqrt{2}} \sum_{i=1}^{k} \omega_{i} \phi_{i}\left(\frac{x_{i}}{2}\right) \phi_{j}\left(x_{i}\right)
\end{aligned}
$$

例如，如果 $k=3$，根据公式，滤波器系数可以推导如下：
$$
\begin{aligned}
& H^{0}=\left[\begin{array}{ccc}
\frac{1}{\sqrt{2}} & 0 & 0 \\
-\frac{\sqrt{3}}{2 \sqrt{2}} & \frac{1}{2 \sqrt{2}} & 0 \\
0 & -\frac{\sqrt{15}}{4 \sqrt{2}} & \frac{1}{4 \sqrt{2}}
\end{array}\right], H^{1}=\left[\begin{array}{ccc}
\frac{1}{\sqrt{2}} & 0 & 0 \\
\frac{\sqrt{3}}{2 \sqrt{2}} & \frac{1}{2 \sqrt{2}} & 0 \\
0 & \frac{\sqrt{15}}{4 \sqrt{2}} & \frac{1}{4 \sqrt{2}}
\end{array}\right], \\
& G^0=\left[\begin{array}{ccc}
\frac{1}{2 \sqrt{2}} & \frac{\sqrt{3}}{2 \sqrt{2}} & 0 \\
0 & \frac{1}{4 \sqrt{2}} & \frac{\sqrt{15}}{4 \sqrt{2}} \\
0 & 0 & \frac{1}{\sqrt{2}}
\end{array}\right],G^1=\left[\begin{array}{ccc}
-\frac{1}{2 \sqrt{2}} & \frac{\sqrt{3}}{2 \sqrt{2}} & 0 \\
0 & -\frac{1}{4 \sqrt{2}} & \frac{\sqrt{15}}{4 \sqrt{2}} \\
0 & 0 & -\frac{1}{\sqrt{2}}
\end{array}\right]
\end{aligned}
$$

## E. 输出分布分析

## E.1. 不良案例分析

使用普通 Transformer 作为基线模型，我们在 ETTm1 数据集中展示了两个不良的长期序列预测案例，如下面的图 7 所示。

- 图 7. 在真实世界 ETTm1 数据集中，普通 Transformer 的预测输出与真实值之间的分布差异。左：频率模式和趋势偏移。右：趋势偏移。

---

图 7 中的预测偏移与普通 Transformer 模型采用的逐点生成机制特别相关。与自回归积分滑动平均（ARIMA）等经典模型不同，这些模型具有预定义的输出分布数据偏差结构，而基于 Transformer 的模型独立预测每个点，仅基于整体均方误差（MSE）损失学习。这会导致在某些情况下，预测输出与真实值的分布不同，从而导致性能下降。

## E.2. Kolmogorov-Smirnov 检验

我们采用 Kolmogorov-Smirnov（KS）检验来检查两个数据样本是否来自同一分布。KS 检验是一种非参数检验，用于检验连续或离散的二维概率分布的相等性。本质上，该检验回答了“这两组样本来自同一（但未知）概率分布的概率是多少”的问题。它量化了两个样本的经验分布函数之间的距离。Kolmogorov-Smirnov 统计量为：

$$
D_{n, m}=\sup _{x}\left|F_{1, n}(x)-F_{2, m}(x)\right|
$$

其中，$F_{1, n}$ 和 $F_{2, m}$ 分别是第一和第二个样本的经验分布函数，sup 是上确界函数。对于大样本，如果满足以下条件，则在显著性水平 $\alpha$ 下拒绝零假设：

$$
D_{n, m}>\sqrt{-\frac{1}{2} \ln \left(\frac{\alpha}{2}\right)} \cdot \sqrt{\frac{n+m}{n \cdot m}}
$$

其中，$n$ 和 $m$ 分别是第一和第二个样本的大小。

## E.3. 分布实验与分析

尽管 KS 检验忽略了输入和输出序列的时间信息，但它可以作为一种工具来测量预测输出序列与输入序列的全局属性。零假设是两个样本来自同一分布。如果 KS 检验的 P 值较大，则零假设被拒绝的可能性较小，表明输出分布更接近真实分布。

---

我们在 ETTm1 和 ETTm2 数据集上对 96-720 预测任务的输出序列应用了 KS 检验，结果总结在表 6 中。在检验中，我们将固定的 96 时间步输入序列分布与不同长度的输出序列分布进行比较。使用 0.01 的 P 值作为统计量，除 Autoformer 外，各种现有的 Transformer 基线模型的 P 值远小于 0.01，表明它们更有可能来自不同的分布。Autoformer 和 FEDformer 的 P 值远高于其他模型，这主要归功于它们的季节趋势分解机制。尽管在 ETTm1 数据集上，两个模型的结果接近，但提出的 FEDformer 在 ETTm1 上的 P 值更大。并且它是唯一一个在两个数据集的所有情况下零假设无法被拒绝（P 值大于 0.01）的模型，这意味着 FEDformer 生成的输出序列与输入序列的分布更为相似，从而验证了我们在第 1 节中讨论的 FEDformer 设计动机。

---

需要注意的是，在 ETTm1 数据集中，真实输出序列的 P 值比 FEDformer 的预测输出更小，这表明模型通过控制实现了接近的输出分布，而不仅仅是更准确的预测。这一分析揭示了为什么季节-趋势分解架构能够在长期预测中提供更好的性能。该设计用于约束输出分布的趋势（均值）。受此启发，我们设计了频率增强块来约束输出分布的季节性（频率模式）。

## F. 补充实验

## F.1. 数据集详情

在本段中，实验数据集的详细信息总结如下：  
1) **ETT**（Zhou et al., 2021）数据集包含两个子数据集：ETT1 和 ETT2，分别从两个变电站的电力变压器收集。每个数据集有两个不同分辨率（15 分钟和 1 小时）的版本。ETT 数据集包含多个负载序列和一个油温序列。  
2) **Electricity** 数据集包含客户的电力消耗数据，每列对应一个客户。  
3) **Exchange**（Lai et al., 2018）数据集包含 8 个国家的汇率数据。  
4) **Traffic** 数据集包含加利福尼亚州高速公路系统的占用率数据。  
5) **Weather** 数据集包含德国一年内的 21 个气象指标。  
6) **Illness** 数据集包含美国流感样疾病患者的数据。  

表 7 总结了六个数据集的特征详情（序列长度：Len，维度：Dim，频率：Freq）。所有数据集按 7:1:2 的比例分为训练集、验证集和测试集。

## F.2. 实现细节

我们的模型使用 ADAM（Kingma \& Ba, 2017）优化器进行训练，学习率为 $1 e^{-4}$。批量大小设置为 32。如果验证集上的损失没有改善，则在三个 epoch 后使用早停计数器停止训练过程。均方误差（MSE）和平均绝对误差（MAE）用作评估指标。所有实验重复 5 次，最终结果使用指标的平均值。所有深度学习网络均使用 PyTorch（Paszke et al., 2019）实现，并在 NVIDIA V100 32GB GPU 上进行训练。

## F.3. ETT 完整基准测试

我们在表 8（多变量预测）和表 9（单变量预测）中展示了四个 ETT 数据集（Zhou et al., 2021）的完整基准测试结果。ETTh1 和 ETTh2 是每小时记录的，而 ETTm1 和 ETTm2 是每 15 分钟记录的。ETTh1 和 ETTm1 的时间序列遵循相同的模式，唯一的区别是采样率，ETTh2 和 ETTm2 也是如此。平均而言，我们的 FEDformer 在多变量预测中相比 Autoformer 的最先进结果实现了 $\mathbf{11.5\%}$ 的相对 MSE 降低，在单变量预测中实现了 $\mathbf{9.4\%}$ 的降低。

## F.4. 交叉注意力可视化

$\sigma\left(\tilde{\boldsymbol{Q}} \cdot \tilde{\boldsymbol{K}}^{\top}\right)$ 可以被视为我们提出的频率增强交叉注意力块的交叉注意力权重。可以使用几种不同的激活函数来激活注意力矩阵。本文测试了 tanh 和 softmax，它们在不同数据集上表现出不同的性能。我们默认使用 tanh。图 8 可视化了不同的注意力模式。这里展示了使用 tanh 和 softmax 分别训练 FEDformer-f 在 ETTm2 数据集上的两个交叉注意力图样本。可以看出，使用 softmax 作为激活函数的注意力似乎比使用 tanh 更稀疏。总体而言，我们可以看到频域中的注意力比时域中的普通注意力图稀疏得多，这表明我们提出的注意力可以更紧凑地表示信号。此外，这种紧凑表示支持我们的随机模式选择机制实现线性复杂度。

## F.5. 专家混合分解的改进

我们设计了一种专家混合分解机制，采用一组平均池化层来提取趋势，并使用一组数据依赖的权重来组合它们。默认的平均池化层包含核大小为 $7,12,14,24$ 和 48 的滤波器。作为比较，我们使用单一专家分解机制作为基线，该机制采用固定核大小为 24 的单一平均池化层。在表 10 中，展示了使用 FEDformer-f 模型在两个典型数据集上进行多变量预测的比较研究。结果表明，设计的专家混合分解机制比单一分解方案带来了更好的性能。

## F.6. 多次随机运行

表 11 列出了 FEDformer-f 和 Autoformer 在 5 次运行中的均值和标准差（STD）。我们观察到，尽管频率选择具有随机性，但 FEDformer-f 的性能方差较小。

## F.7. 对模式数量的敏感性：ETTx1 vs ETTx2

模式数量的选择取决于数据的复杂性。表现出更高复杂模式的时间序列需要更多的模式数量。为了验证这一观点，我们在表 12 中总结了 ETT 数据集的复杂性，通过排列熵和 SVD 熵进行测量。可以看出，ETTx1 的复杂性显著高于 ETTx2（对应更高的熵值），因此需要更多的模式数量。

## F.8. 傅里叶/小波模型表现更好的情况

我们模型部署的高级原则是：傅里叶模型通常更适合复杂性较低的时间序列，而小波模型通常更适合复杂的时间序列。具体来说，我们发现小波模型在多元时间序列上更有效，而傅里叶模型通常在单变量时间序列上表现更好。如表 13 所示，多元时间序列的复杂性度量高于单变量时间序列。
