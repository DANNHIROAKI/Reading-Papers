## DiskANN: Fast Accurate Billion-point Nearest Neighbor Search on a Single Node 



## 0. Abstract  

当前最先进的近似最近邻搜索(ANNS)算法生成的索引必须存储在主存(内存)中，以便快速高召回率搜索。这使得它们的成本高昂，并且限制了数据集的规模。我们提出了一种新的基于图的索引和搜索系统，称为DiskANN，它可以在仅使用64 GB内存和廉价固态硬盘(SSD)的单台工作站上对十亿点数据库进行索引、存储和搜索。与当前的认知相反，我们证明了DiskANN构建的基于SSD的索引能够满足大规模ANNS的三个需求：高召回率、低查询延迟和高密度(每个节点索引的点数)。在十亿点的SIFT1B bigann数据集上，DiskANN在16核机器上每秒处理超过5000个查询，平均延迟小于3毫秒，并实现了超过95%的1-recall@1(即95%的查询能够准确找到最近邻点)。相比之下，具有类似内存占用的最先进的十亿点ANNS算法如FAISS [18]和IVFOADC+G+P [8]在1-recall@1方面的表现大约停滞在50%左右。或者，在高召回率的情况下，DiskANN可以相比于最先进的基于图的方法如HNSW [21]和NSG [13]，在每个节点上索引和处理5-10倍更多的点。最后，作为我们整体DiskANN系统的一部分，我们引入了一种新的基于图的ANNS索引——Vamana，该索引即使对于内存中的索引来说，也比现有的图索引更为通用。



## 1. Intro

---

不幸的是，由于所谓的维度诅咒现象 [10]，往往无法在不进行线性扫描的情况下检索到精确的最近邻(参见例如[15, 23])。因此，人们通常求助于寻找近似最近邻(ANN)，其目标是检索到接近最优的 $k$ 个邻居。更正式地说，考虑一个查询 $q$，假设算法输出了一组候选的 $k$ 个近邻点集 $X$，并且 $G$ 是从基础数据集中与 $q$ 距离最近的 $k$ 个邻居的真实集。然后，我们定义集合 $X$ 的 $k$-recall@ $k$ 为 $\cfrac{|X \cap G|}{k}$。因此，ANN 算法的目标是在尽可能快地检索结果的同时最大化召回率，这就产生了召回率与延迟之间的权衡。

---

针对这个问题，存在许多算法，具有多样的索引构建方法，并在索引构建时间、召回率和查询时间等方面有不同的权衡。例如，k-d树在低维度时生成紧凑的索引并且搜索速度很快，但当维度 $d$ 超过20时，通常会变得非常慢。另一方面，基于局部敏感哈希(LSH)的方法 [2, 4] 在最坏情况下提供了接近最优的索引大小和搜索时间的权衡保证，但它们无法利用点的分布，并且在实际数据集上被更近期的基于图的方法超越。基于数据相关的LSH方案的最新研究(例如[3])尚未在大规模数据集上证明其有效性。截至本文撰写时，基于搜索时间与召回率权衡的最佳算法通常是基于图的算法，例如HNSW [21] 和 NSG [13]，其中索引算法在基础点集上构建一个可导航的图，搜索过程则是一种从选定的(或随机的)点开始的最佳优先遍历，并沿着图的边逐步接近查询点，直到收敛于局部最小值。Li等人的最新工作 [20] 提供了一个优秀的ANN算法综述和比较。

---

许多应用需要在欧几里得度量下对数十亿个点进行快速且精确的搜索。如今，索引大规模数据集的高层方法基本上有两种。

---

第一种方法基于倒排索引 + 数据压缩，包括FAISS [18] 和 IVFOADC+G+P [8] 等方法。这些方法将数据集划分为 $M$ 个分区，并仅将查询与几个离查询最近的分区(例如 $m<<M$ 个分区)中的点进行比较。此外，由于全精度向量无法全部放入主存中，数据点通过使用产品量化(Product Quantization)[17] 等量化方案进行压缩。虽然这些方案的内存占用较小——在128维的十亿点索引中，内存占用不到64 GB，并且可以使用GPU或其他硬件加速器在不到5毫秒内检索结果，但它们的1-recall@1值相对较低(约为0.5)，因为数据压缩是有损的。这些方法在更弱的1-recall@100(即真实最近邻存在于输出的100个候选列表中的概率)定义下报告了更高的召回率值。然而，在许多应用中，这种度量标准可能并不理想。

---

第二种方法是将数据集划分为不相交的分片，并为每个分片构建一个内存中的索引。然而，由于这些索引存储了索引和未压缩的数据点，它们的内存占用比第一种方法大。例如，对于128维度的1亿个浮点向量，NSG索引的内存占用大约为75 GB (NSG 指数的平均度数可根据数据集的内在结构而变化，在此我们假设指数度数为 50，这对于内在结构较少的数据集来说是合理的)。因此，对十亿个点的索引服务需要几台机器来托管这些索引。据报道[13]，这种方案被用于淘宝(阿里巴巴的电商平台)，他们将包含20亿个128维点的数据集划分为32个分片，并为每个分片在不同的机器上托管索引。查询会被路由到所有分片，并将所有分片的结果进行聚合。使用这种方法，他们报告的100-recall@100值为0.98，延迟约为5毫秒。需要注意的是，将这种方法扩展到包含数千亿个点的网页规模数据集将需要数千台机器。

---

这两类算法的可扩展性受限于它们构建的索引必须从主存中提供服务的事实。如果将这些索引移到磁盘上，即使是SSD，也会导致搜索延迟的灾难性增加以及吞吐量的相应下降。关于搜索必须依赖主存的当前共识在FAISS的博客文章中有所反映 [11]：“Faiss仅支持从RAM中进行搜索，因为磁盘数据库的速度要慢几个数量级。是的，即使是SSD也一样。”

---

确实，基于SSD的索引的搜索吞吐量受限于每个查询所需的随机磁盘访问次数，而延迟则受限于往返磁盘的次数(每次往返可能包括多次读取)。一块廉价的零售级SSD需要几百微秒来处理一次随机读取，并且每秒可以服务大约 $\sim 30万$ 次随机读取。另一方面，具有多阶段管道的搜索应用程序(例如网页搜索)要求最近邻搜索的平均延迟在几毫秒以内。因此，设计高性能的基于SSD的索引的主要挑战在于减少 (a) 随机SSD访问的次数到几十次，以及 (b) 向磁盘发出往返请求的次数到十次以内，最好是五次。将传统的内存中ANNS算法生成的索引直接映射到SSD上，会导致每个查询需要几百次磁盘读取，从而导致不可接受的延迟。

### 1.1. Our technical contribution  

我们提出了DiskANN，这是一种基于SSD的近似最近邻搜索(ANNS)系统，基于我们全新的图索引算法Vamana，打破了当前的认知，证明即使是普通的SSD也可以有效支持大规模ANNS。我们工作的几个有趣之处是：

- DiskANN可以在一台具有64GB内存的工作站上，对一个包含数百维度的十亿点数据集进行索引和服务，提供$95 \%$以上的1-recall@1，延迟低于5毫秒。
- 一种名为Vamana的新算法可以生成比NSG和HNSW更小直径的图索引，使得DiskANN能够最大限度地减少顺序磁盘读取的次数。
- Vamana生成的图索引也可以用于内存中，其搜索性能与最先进的内存中算法(如HNSW和NSG)相当甚至更好。
- 对于一个大数据集的重叠分区，较小的Vamana索引可以轻松合并为一个索引，该索引的搜索性能几乎与为整个数据集构建的单次索引相同。这使得无法完全放入内存的数据集也能进行索引。
- 我们展示了Vamana可以与现成的向量压缩方案(如产品量化)相结合，以构建DiskANN系统。图索引以及数据集的全精度向量存储在磁盘上，而压缩向量则缓存在内存中。

### 1.2. Notation  

在本文的其余部分，我们令 $P$ 表示数据集，其中 $|P|=n$。我们考虑一个带有顶点对应于 $P$ 中点的有向图，以及点之间的边。稍微滥用一下符号，我们将此类图表示为 $G=(P, E)$，其中 $P$ 也用来表示顶点集。给定有向图中的一个点 $p \in P$，我们令 $N_{\text {out }}(p)$ 表示与 $p$ 相关的出边集合。最后，我们用 $\mathrm{x}_p$ 表示与点 $p$ 对应的向量数据，令 $d(p, q)=\left\|\mathrm{x}_p-\mathrm{x}_q\right\|$ 表示两个点 $p$ 和 $q$ 之间的度量距离。本文中所有的实验均使用欧几里得度量。

### 1.3. Paper Outline  

第2节介绍了我们的新图索引构建算法Vamana，第3节解释了DiskANN的整体系统设计。第4节对Vamana与HNSW和NSG的内存索引进行了实证比较，并展示了DiskANN在普通机器上处理十亿点数据集的搜索特性。



## 2. The Vamana Graph Construction Algorithm  

我们首先简要介绍了基于图的 ANNS 算法，然后介绍了 Vamana 算法的细节。

### 2.1. Relative Neighborhood Graphs and the GreedySearch algorithm  

大多数基于图的近似最近邻搜索(ANNS)算法按如下方式工作：在索引构建阶段，它们基于数据集 $P$ 的几何特性构建一个图 $G=(P, E)$。在搜索时，对于查询向量 $\mathrm{x}_q$，搜索会在 $G$ 上使用自然的贪心或最佳优先遍历，例如算法1所示。搜索从某个指定点 $s \in P$ 开始，逐步接近 $\mathrm{x}_q$。

---

已有大量研究致力于理解如何构建稀疏图，使得贪心搜索 $\left(s, \mathrm{x}_q, k, L\right)$ 能够快速收敛到任意查询的(近似)最近邻。一个充分条件是所谓的稀疏邻域图(SNG)，至少当查询点接近数据集中的点时，这个条件是成立的。SNG的概念由[5]引入(这一概念本身受到了1960年代首次定义的相关性质——相对邻域图(RNG)性质[16]的启发)。在SNG中，确定每个点 $p$ 的出邻居的方式如下：初始化一个集合 $S=P \backslash\{p\}$。只要 $S \neq \emptyset$，就从 $p$ 添加一条指向 $p^*$ 的有向边，其中 $p^*$ 是 $S$ 中距离 $p$ 最近的点，然后从 $S$ 中移除所有满足 $d\left(p, p^{\prime}\right)>d\left(p^*, p^{\prime}\right)$ 的点 $p^{\prime}$。由此可以很容易地看出，对于所有基础点 $p \in P$，从任意 $s \in P$ 开始的贪心搜索 $\left(s, x_p, 1,1\right)$ 将会收敛到 $p$。

---

虽然这种构建在原理上是理想的，但对于即使是中等规模的数据集，构建这样的图是不可行的，因为其运行时间为 $\widetilde{O}\left(n^2\right)$。基于这一直觉，已经有一系列研究设计了更实用的算法来生成SNG的良好近似 [21, 13]。然而，由于这些算法本质上都试图近似SNG性质，因此在控制算法生成图的直径和密度方面几乎没有灵活性。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921164120144.png" alt="image-20240921164120144" style="zoom:35%;" />  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921164202671.png" alt="image-20240921164202671" style="zoom: 33%;" /> 

### 2.2. The Robust Pruning Procedure  

如前所述，满足SNG性质的图都是适合用于贪心搜索(GreedySearch)过程的候选者。然而，这些图的直径可能会非常大。例如，如果这些点在一维的实线上呈线性排列，那么 $O(n)$ 直径的线性图(每个点连接到其两个相邻的点，其中一个位于末端)是满足SNG性质的图。搜索存储在磁盘上的此类图时，在算法1的搜索路径上访问的顶点的邻居将涉及许多顺序磁盘读取。

---

为了解决这个问题，我们希望确保沿搜索路径的每个节点到查询点的距离按乘法因子 $\alpha>1$ 递减，而不仅仅像SNG性质那样逐步减少。考虑一个有向图，其中每个点 $p$ 的出邻居由算法2中的RobustPrune $(p, \mathcal{V}, \alpha, R)$ 过程决定。请注意，如果每个 $p \in P$ 的出邻居由RobustPrune $(p, P \backslash\{p\}, \alpha, n-1)$ 决定，那么从任意 $s$ 开始的贪心搜索 $(s, p, 1,1)$ 将会在对数级别的步骤内收敛到 $p \in P$，如果 $\alpha>1$。然而，这将导致索引构建的运行时间为 $\widetilde{O}\left(n^2\right)$。因此，基于[21, 13]的思想，Vamana在调用RobustPrune $(p, \mathcal{V}, \alpha, R)$ 时，为 $\mathcal{V}$ 精心选择了远少于 $n-1$ 的节点，以提高索引构建时间。

### 2.3. The Vamana Indexing Algorithm  

Vamana以迭代方式构建有向图 $G$。该图初始时，每个顶点有 $R$ 个随机选择的出邻居。请注意，虽然当 $R>\log n$ 时，图的连接性很好，但随机连接并不能确保贪心搜索算法能收敛到好的结果。接下来，我们令 $s$ 表示数据集 $P$ 的中心点(medoid)，它将作为搜索算法的起始节点。然后，算法随机遍历 $p \in P$ 中的所有点，并在每一步更新图，使其更适合贪心搜索 $\left(s, \mathrm{x}_p, 1, L\right)$ 收敛到 $p$。实际上，在与点 $p$ 对应的迭代中，Vamana首先在当前的图 $G$ 上运行贪心搜索 $\left(s, x_p, 1, L\right)$，并将 $\mathcal{V}_p$ 设置为贪心搜索 $\left(s, \mathrm{x}_p, 1, L\right)$ 访问的所有点的集合。接着，算法通过运行RobustPrune $\left(p, \mathcal{V}_p, \alpha, R\right)$ 来更新图 $G$，以确定 $p$ 的新出邻居。然后，Vamana通过为所有 $p^{\prime} \in N_{\text {out }}(p)$ 添加反向边 $\left(p^{\prime}, p\right)$ 来更新图 $G$。这样可以确保在搜索路径上访问的顶点与 $p$ 之间有连接，从而保证更新后的图更适合贪心搜索 $\left(s, \mathrm{x}_p, 1, L\right)$ 收敛到 $p$。

---

然而，添加形式为 $\left(p^{\prime}, p\right)$ 的反向边可能导致 $p^{\prime}$ 的度数超限，因此每当任何顶点 $p^{\prime}$ 的出度超过度数阈值 $R$ 时，图会通过运行RobustPrune $\left(p^{\prime}, N_{\text {out }}\left(p^{\prime}\right), \alpha, R\right)$ 来进行修改，其中 $N_{\text {out }}\left(p^{\prime}\right)$ 是 $p^{\prime}$ 的现有出邻居集合。随着算法的进行，图将持续优化，使得贪心搜索变得更快更高效。我们的整体算法对数据集进行两次遍历，第一次遍历时 $\alpha=1$，第二次遍历时使用用户定义的 $\alpha \geq 1$。我们观察到，第二次遍历会生成更好的图，而两次遍历都使用用户定义的 $\alpha$ 会使索引算法变慢，因为第一次遍历生成的图具有更高的平均度数，耗时更长。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921164817682.png" alt="image-20240921164817682" style="zoom: 36%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921165012796.png" alt="image-20240921165012796" style="zoom: 46%;" /> 

图1：Vamana索引算法(如算法3所述)在包含200个点的二维数据库上生成的图的演变过程。请注意，算法首先进行 $\alpha=1$ 的第一次遍历，随后在第二次遍历中引入了长距离边。

### 2.4. Comparison of Vamana with HNSW [21] and NSG [13]  

从总体上看，Vamana与HNSW和NSG这两种非常流行的ANNS算法相当相似。这三种算法都对数据集 $P$ 进行迭代，并使用贪心搜索 $\left(s, \times_p, 1, L\right)$ 和 RobustPrune $(p, \mathcal{V}, \alpha, R)$ 的结果来确定 $p$ 的邻居。然而，这些算法之间也存在一些重要的区别。最关键的是，HNSW和NSG都没有可调参数 $\alpha$，并且隐式地使用 $\alpha=1$。这也是Vamana能够在图的度数和直径之间实现更好权衡的主要原因。其次，HNSW将用于剪枝过程的候选集 $\mathcal{V}$ 设置为贪心搜索 $(s, p, 1, L)$ 输出的 $\mathcal{L}$ 个候选点的最终结果集，而Vamana和NSG则将 $\mathcal{V}$ 设置为贪心搜索 $(s, p, 1, L)$ 访问的所有顶点集。直观上，这一特性帮助Vamana和NSG添加长距离边，而HNSW由于仅向附近点添加局部边，还需额外一步为嵌套的数据集样本序列构建图的层次结构。下一个区别与初始图有关：NSG将起始图设置为数据集的近似 $K$ 最近邻图，这一步既耗时又占用内存，而HNSW和Vamana的初始化则更简单，前者从空图开始，Vamana从随机图开始。我们观察到，从随机图开始会比从空图开始生成的图质量更高。最后，Vamana会对数据集进行两次遍历，而HNSW和NSG只进行一次遍历，这基于我们的观察，即第二次遍历可以提高图的质量。



## 3. DiskANN: Constructing SSD-Resident Indices  

现在，我们分两部分介绍 DiskANN 的总体设计。在第一部分中，我们将解释索引构建算法；在第二部分中，我们将解释搜索算法。

### 3.1. The DiskANN Index Design  

这个高层次的思路很简单：在数据集 $P$ 上运行Vamana，并将生成的图存储在SSD上。在搜索时，当算法1需要点 $p$ 的出邻居时，我们只需从SSD中获取这些信息即可。然而，值得注意的是，仅存储十亿个点的100维向量数据就会远远超过工作站的内存！这就引出了两个问题：如何在十亿个点上构建一个图？在算法的搜索过程中，如果我们无法存储向量数据，又如何在查询点和候选列表中的点之间进行距离比较？

---

解决第一个问题的一种方法是使用聚类技术(如k-means)将数据划分为多个较小的分片，为每个分片构建一个单独的索引，并在搜索时仅将查询路由到几个分片。然而，这种方法会导致搜索延迟增加和吞吐量下降，因为查询需要被路由到多个分片。

---

事后看来，我们的想法很简单：与其在搜索时将查询路由到多个分片，不如将每个基础点发送到多个附近的中心，以获得重叠的簇？实际上，我们首先使用 $k$-means 将十亿个点的数据集划分为 $k$ 个簇(例如，$k=40$)，然后将每个基础点分配给离它最近的 $\ell$ 个中心(通常 $\ell=2$ 就足够了)。接着，我们为分配到每个簇的点构建Vamana索引(这些簇现在只有大约 $N \cfrac{\ell}{k}$ 个点，因此可以在内存中索引)，最后通过简单的边集并集将所有不同的图合并为一个单一的图。实验证明，不同簇的重叠性为贪心搜索算法提供了足够的连通性，即使查询的最近邻实际上分布在多个分片之间，算法也能成功。我们还想指出，之前的一些研究[9, 22]也通过合并多个较小的、重叠的索引来为大数据集构建索引。然而，它们构建重叠簇的思路有所不同，且有必要对这些不同技术进行更详细的比较。

---

我们解决第二个问题的自然想法是将每个数据库点 $p \in P$ 的压缩向量 $\widetilde{x_p}$ 存储在主存中，同时将图存储在SSD上。我们使用了一种广泛应用的压缩方案，称为产品量化(Product Quantization)$[17]$ (虽然更复杂的压缩方法，如 [14、19、18] 等，可以提供质量更好的近似值，但我们发现简单的乘积量化就足以满足我们的目的)，它将数据点和查询点编码为短码(例如，每个数据点32字节)，可以在算法1中查询时有效地获得近似距离 $d\left(\widetilde{x_p}, \mathrm{x}_q\right)$。我们想指出，Vamana在构建图索引时使用的是全精度坐标，因此能够高效地引导搜索到图的正确区域，尽管在搜索时我们仅使用压缩数据。

### 3.2. DiskANN Index Layout  

我们将所有数据点的压缩向量存储在内存中，并将图和全精度向量存储在SSD上。在磁盘上，对于每个点 $i$，我们存储其全精度向量 $\times_i$，随后存储其最多 $R$ 个邻居的标识。如果某个节点的度小于 $R$，我们用零进行填充，这样可以简单地计算磁盘上与任意点 $i$ 对应的数据的偏移量，而不需要在内存中存储偏移量。我们将在下一节解释为什么需要存储全精度坐标。

### 3.3. DiskANN Beam Search  

一种自然的方式是针对给定查询 $x_q$，运行算法1，根据需要从SSD中获取邻域信息 $N_{\text {out }}\left(p^*\right)$。通过压缩向量进行距离计算，以指导读取最合适的顶点(及其邻域)信息。虽然这种方法是合理的，但它需要多次往返SSD(每次耗时几百微秒)，从而导致较高的延迟。为了减少往返SSD的次数(顺序获取邻域信息)，同时不显著增加计算量(距离计算)，我们一次性获取 $\mathcal{L} \backslash \mathcal{V}$ 中距离最近的少量点(例如4或8个)的邻域信息，并将 $\mathcal{L}$ 更新为前 $L$ 个候选点及本步骤中检索到的所有邻居。请注意，从SSD中获取少量随机扇区与获取一个扇区所需的时间几乎相同。我们将这种修改后的搜索算法称为BeamSearch。如果 $W=1$，此搜索类似于普通的贪心搜索。需要注意的是，如果 $W$ 过大，例如16或更多，可能会浪费计算资源和SSD带宽。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921172112046.png" alt="image-20240921172112046" style="zoom: 25%;" />  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921172141370.png" alt="image-20240921172141370" style="zoom: 25%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921172222647.png" alt="image-20240921172222647" style="zoom:25%;" /> 

 图2：(a) 在SIFT bigann数据集上，1-recall@1与延迟的关系。R128和R128/Merged系列分别代表一次性和合并的Vamana索引构建方式。(b) 在DEEP1B数据集上，1-recall@1与延迟的关系。(c) 在ANN_SIFT1M数据集中，达到98% 5-recall@5时的平均跳数与最大图度数的关系。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921172517031.png" alt="image-20240921172517031" style="zoom: 25%;" /> <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921172543278.png" alt="image-20240921172543278" style="zoom:25%;" /> 

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240921172609945.png" alt="image-20240921172609945" style="zoom:27%;" /> 

图 3：HNSW、NSG 和 Vamana 的延迟(微秒)与召回率对比图。

---

尽管基于NAND闪存的SSD每秒可以处理超过50万次随机读取，但要实现最大读取吞吐量需要使所有I/O请求队列达到饱和。然而，在高峰吞吐量下运行并且队列积压时，磁盘读取延迟会超过一毫秒。因此，有必要在较低的负载系数下运行SSD以获得较低的搜索延迟。我们发现，在较小的光束宽度(例如 $W=2,4,8$)下运行可以在延迟和吞吐量之间取得良好的平衡。在这种情况下，SSD的负载系数在30%-40%之间，每个运行我们搜索算法的线程在查询处理时间中有40%-50%的时间用于I/O操作。

### 3.4. DiskANN Caching Frequently Visited Vertices  

为了进一步减少每次查询的磁盘访问次数，我们将与一部分顶点相关的数据缓存到DRAM中，缓存策略可以基于已知的查询分布，或者简单地缓存距离起始点 $s$ 为 $C=3$ 或4跳的所有顶点。由于索引图中距离为 $C$ 的节点数量随着 $C$ 呈指数增长，较大的 $C$ 值会导致内存占用过大。

### 3.5. DiskANN Implicit Re-Ranking Using Full-Precision Vectors  

由于产品量化(Product Quantization)是一种有损压缩方法，使用基于PQ的近似距离计算出来的查询最近的 $k$ 个候选点和实际距离之间会存在差异。为了弥补这一差距，我们使用存储在磁盘上每个点邻域旁边的全精度坐标。事实上，当我们在搜索过程中检索某个点的邻域时，我们也检索该点的全精度坐标，而不会增加额外的磁盘读取开销。这是因为，将 $4KB$ 对齐的磁盘地址读取到内存中与读取 $512B$ 的开销是一样的，并且一个顶点的邻域(对于度为128的图，其长度为 $4 * 128$ 字节)和全精度坐标可以存储在同一个磁盘扇区中。因此，当BeamSearch加载搜索边界的邻域时，它还可以缓存搜索过程中访问的所有节点的全精度坐标，而无需额外读取SSD。这使我们能够基于全精度向量返回前 $k$ 个候选点。独立于我们的工作，文章[24]中也使用了从SSD中获取并重新排序全精度坐标的想法，但该算法一次性获取所有向量进行重新排序，这会导致数百次随机磁盘访问，从而对吞吐量和延迟产生不利影响。我们将在第4.3节中提供更详细的解释。就我们的情况而言，全精度坐标实际上是搭载在扩展邻域的成本上。



## 4. Evaluation  

我们现在将Vamana与其他相关的近似最近邻搜索算法进行比较。首先，对于内存中的搜索，我们将我们的算法与NSG [13] 和 HNSW [21] 进行比较，它们在大多数公共基准数据集上提供了最佳的延迟与召回率的平衡。接下来，对于十亿点的大规模数据集，我们将DiskANN与基于压缩的技术如FAISS [18] 和 IVF-OADC+G+P [8] 进行比较。

---

我们在以下两台机器上进行所有实验：

- z840：一台裸机中档工作站，配备双Xeon E5-2620v4处理器(16核)、64GB DDR4内存，以及两个三星960 EVO 1TB SSD，配置为RAID-0。
- M64-32ms：一台虚拟机，配备双Xeon E7-8890v3处理器(32个虚拟CPU)和1792GB DDR3内存，用于为十亿点数据集构建一次性内存索引。

### 4.1. Comparison of HNSW, NSG and Vamana for In-Memory Search Performance  

我们在三个常用的公共基准上将Vamana与HNSW和NSG进行了比较：SIFT1M(128维)、GIST1M(960维)，这两个都是图像描述符的百万点数据集 [1]，以及DEEP1M(96维)，它是DEEP1B的随机百万点样本，DEEP1B是一个由机器学习生成的十亿向量集 [6]。对于这三种算法，我们进行了参数扫描，并为最佳的召回率与延迟权衡选择了接近最优的参数。所有HNSW索引都使用了 $M=128, ef_C=512$，而Vamana索引使用了 $R=70, L=75, \alpha=1.2$。对于SIFT1M和GIST1M上的NSG，我们使用了他们在仓库中列出的参数 (https://github.com/ZJULearning/nsg)，因为它们的性能非常优异，且在DEEP1M上使用了 $R=60, L=70, C=500$。此外，由于本文的主要关注点在于基于SSD的搜索，我们并没有实现自己的内存搜索算法来测试Vamana，而是直接在NSG仓库的优化搜索算法实现上使用Vamana生成的索引。

---

从图3中，我们可以看到一个明显的趋势——在所有情况下，NSG和Vamana的表现都优于HNSW，而在960维的GIST1M数据集上，Vamana优于NSG和HNSW。此外，Vamana的索引构建时间在所有三次实验中都比HNSW和NSG要快。例如，在z840上对DEEP1M进行索引时，Vamana、HNSW和NSG的总索引构建时间分别为129秒、219秒和480秒 (由于 NSG 需要一个起始 k 近邻图，因此我们也将 EFANNA [12] 所花费的时间包括在内)。通过这些实验，我们得出结论，Vamana在来自不同来源的百维和千维数据集上，能够与当前最好的ANNS方法相匹配，甚至有时超越它们。

### 4.2. Comparison of HNSW, NSG and Vamana for Number of Hops  

Vamana比其他基于图的算法更适合用于基于SSD的服务，因为在大数据集上，与HNSW和NSG相比，它的搜索收敛所需跳数少了2-3倍。这里的跳数指的是搜索关键路径上磁盘读取的轮次数。在BeamSearch中，这对应于搜索边界通过并行进行 $W$ 次磁盘读取来扩展的次数。跳数很重要，因为它直接影响搜索的延迟。对于HNSW，我们假设除了基础层外，其他所有层的节点都被缓存到DRAM中，只计算基础层图上的跳数。对于NSG和Vamana索引，我们假设围绕导航节点的前3个BFS层可以缓存到DRAM中。我们通过改变图的最大度数来比较实现98% 5-recall@5目标所需的跳数，结果如图2(c)所示。我们注意到HNSW和NSG都表现出跳数停滞的趋势，而Vamana在最大度数和 $\alpha$ 增加时跳数减少。使用较大的 $R$ 和 $\alpha$ 值可以添加更多长距离边，因而减少跳数。因此，我们得出结论，Vamana在 $\alpha>1$ 时比其他基于图的方法更好地利用了SSD提供的大容量。

### 4.3. Comparison on Billion-Scale Datasets: One-Shot Vamana vs Merged Vamana  

在接下来的实验中，我们将重点评估包含 $10^9$ 个点的ANN_SIFT1B [1] bigann数据集，它由128个uint8类型的SIFT图像描述符组成。为了展示第3节中描述的mergedVamana方案的有效性，我们使用Vamana构建了两个索引。第一个是在整个十亿点数据集上构建的单一索引，使用参数 $L=50, R=128, \alpha=1.2$。该过程在 $\mathrm{M}64-32\mathrm{~ms}$ 上耗时大约2天，峰值内存使用量约为1100GB，生成的索引平均度为113.9。第二个是合并索引，其构建步骤如下：(1) 使用k-means聚类将数据集划分为 $k=40$ 个分片，(2) 将数据集中的每个点发送到最接近的 $\ell=2$ 个分片，(3) 为每个分片构建索引，使用参数 $L=50, R=64, \alpha=1.2$，(4) 合并所有图的边集。最终得到一个348 GB的索引，平均度为92.1。索引在z840上构建，耗时约5天，整个过程中的内存使用量保持在64 GB以下。分片和合并过程快速，可以在同一台机器上通过磁盘完成。

---

我们在图2(a)中比较了两种配置下，在bigann数据集上针对1万次查询的1-recall@1与延迟的关系。通过这一实验，我们得出以下结论：(a) 单一索引的表现优于合并索引，因为合并索引需要遍历更多的链接才能到达相同的邻域，从而增加了搜索延迟。这可能是因为合并索引中每个节点的入边和出边仅限于大约 $\cfrac{\ell}{k}=5\%$ 的所有点。(b) 尽管如此，合并索引仍然是针对数十亿规模k-ANN索引和单节点服务的非常优秀的选择，和单一索引相比，在目标召回率下其延迟仅增加不超过20%，并且轻松超越了现有的最先进方法。另一方面，单一索引实现了新的最先进1-recall@1，达到98.68%，延迟低于5毫秒。合并索引对DEEP1B数据集来说也是一个不错的选择。图2(b)展示了在z840机器上使用 $k=40$ 分片和 $\ell=2$ 构建的DEEP1B数据集的合并DiskANN索引的召回率与延迟曲线。

### 4.4. Comparison on Billion-Scale Datasets: DiskANN vs IVF-based Methods  

我们的最终比较对象是FAISS[18]和IVFOADC $+\mathrm{G}+\mathrm{P}$ [7]，这是两种最近在单节点上构建十亿点索引的最新方法。两种方法都使用了倒排索引和基于产品量化的压缩方案来开发具有低内存占用的索引，这些索引能够以高吞吐量和良好的1-recall@100响应查询。由于[7]展示了IVFOADC$+\mathrm{G}+\mathrm{P}$相对于FAISS具有更好的召回率，因此我们仅将DiskANN与IVFOADC $+\mathrm{G}+\mathrm{P}$进行比较。此外，FAISS在十亿规模索引上的应用需要GPU，而某些平台可能无法提供GPU资源。

---

IVFOADC $+\mathrm{G}+\mathrm{P}$使用HNSW作为路由层，以获得一小组集群，这些集群随后通过一种新颖的分组和剪枝策略进一步优化。使用其开源代码，我们在SIFT1B基础数据集上构建了带有16字节和32字节OPQ码本的索引。图2(a)中的IVFOADC+G+P-16和IVFOADC+G+P-32曲线分别表示这两种配置。虽然IVFOADC+G+P-16的1-recall@1停滞在$37.04\%$，但较大的IVFOADC+G+P-32索引达到了$62.74\%$的1-recall@1。与IVFOADC+G+P-32相同的内存占用下，DiskANN在3.5毫秒内1-recall@1超过$95\%$，并在最终达到$100\%$的完美召回率。因此，DiskANN在与基于压缩方法相同内存占用的情况下，可以在相同延迟下实现显著更高的召回率。由于坐标的有损压缩导致精度下降，从而使得距离计算略微不准确，压缩方法提供了较低的召回率。

---

Zoom [24]是一种与IVFOADC $+\mathrm{G}+\mathrm{P}$类似的基于压缩的方法，它通过压缩向量识别出近似的$K^{\prime}>K$个候选项，并通过从磁盘中获取全精度的坐标来重新排序，最终输出$K$个候选项。然而，Zoom存在两个缺点：(a)它通过同时进行随机磁盘读取来获取所有$K^{\prime}$(即使$K=1$时，$K^{\prime}$通常接近100)全精度向量，这会影响延迟和吞吐量；(b)它需要使用成百上千个质心来进行昂贵的$k$-means聚类，以构建基于HNSW的路由层。例如，文献[24]中描述的聚类步骤在1000万个基础数据集上使用了20万个质心，这可能无法轻松扩展到十亿点数据集。



## 5. Conclusion  

我们提出并评估了一种用于近似最近邻搜索(ANNS)的新图索引算法Vamana，其索引在高召回率场景下与当前最先进的内存搜索方法相当。此外，我们展示了如何使用仅64GB主存，在十亿点数据集上构建一个高质量的SSD常驻索引DiskANN。我们详细介绍并阐明了使我们能够使用廉价的零售级SSD，以几毫秒延迟提供这些索引的算法改进。通过结合图算法的高召回率、低延迟特性与压缩方法的内存效率和可扩展性，我们确立了十亿点数据集索引和服务的新业界标杆。