# 0. ABSTRACT  

高维度度量空间中的相似性搜索在许多应用中得到了广泛应用，包括基于内容的图像检索、生物信息学、数据挖掘和推荐系统等。通过使用索引，可以加速搜索。然而，构建高维索引可能非常昂贵，并且如果对数据的查询次数不多，可能无法产生足够的收益。在这种情况下，适应性地构建索引是有利的，同时响应查询工作负载。现有的多维适应性索引工作将空间划分为正交体（即超矩形单元）。然而，在高维空间中，这种方法的效果非常差。在本文中，我们提出了AV-tree：一种替代的适应性高维索引方法，它利用先前计算的距离，并使用查询中心作为参照点。我们的实验研究表明，AV-tree在前几百个甚至几千个查询中的累积成本远低于预先构建的索引。经过成千上万个查询后，AV-tree的每查询性能趋于稳定，甚至超过了最先进的MVP-tree。可以说，我们的方法在期望查询次数不多的环境中具有优势，特别是那些需要尽快开始回答查询的应用场景，例如数据更新频繁且过时的数据很快变得无效的情况。

# 1. INTRODUCTION  

设 $O$ 是一个高维度度量空间中的物体集合。给定一个查询物体 $q$、一个距离界限 $\epsilon$ 和一个距离度量 $d()$，范围相似性查询旨在寻找满足 $d(q, o) \leq \epsilon$ 的物体 $o \in O$。类似地，给定一个正整数 $k$，$k$-最近邻（$k \mathrm{NN}$）相似性查询旨在找到 $O$ 中距离 $q$ 最近的 $k$ 个物体 $o$，使得它们的 $d(q, o)$ 小于集合 $O$ 中所有其他物体的距离。范围查询和 $k \mathrm{NN}$ 相似性查询在基于相似度的搜索和数据挖掘任务中得到广泛应用（例如聚类和最近邻分类），并应用于多个领域，包括计算机视觉 [44]、信息检索 [10]、空间网络中的 kNN 搜索 [1] 和推荐系统 [34]。

---

**动机。** 我们考虑在度量空间中数据是短暂的，并且在数据变得过时之前预计会有相对较少的查询的应用场景。例如，卫星图像展示了天气现象或其他短暂的信息，这些图像是定期接收的，并自动转换为适合相似度计算的特征向量。数据科学家对图像集合进行相似度搜索，以检测现象。当下一批图像到达时，这些图像变得过时，因此对同一批数据执行的查询数量预计不会很大。在这种环境下，为每一批数据在查询处理之前建立索引是非常昂贵的，因此可能不值得这样做。相反，可以通过线性扫描直接在原始特征向量上评估每个查询。延迟数据结构化 [31] 提出了利用每个查询所做的工作来逐步构建索引的概念。这个概念后来以数据库破解 [18, 24, 26, 33, 45, 46] 和渐进合并 [16] 的形式重新提出，进而发展出混合版本 [21, 22, 27]。一些研究通过超矩形划分将这一思想应用于多维数据 [23, 41, 42, 52]，然而，这种方法（i）无法扩展到高维空间，且（ii）无法应用于通用度量空间。此外，这些方法不适用于 $k$NN 查询，这是高维空间中最流行的查询类型。在本文中，我们开发了一种适用于通用高维度度量空间的自适应索引，它避免了超矩形划分，并能够响应范围查询和 $k$NN 查询。

---

**方法论**。考虑到现代内存的大小，并且我们面向的应用场景中数据是短期存在的，因此数据量不大，我们假设数据存储在内存中，像大多数自适应索引的前期工作一样[18, 24, 41]。例如，$D$维向量空间中的一个对象集合$O$可以作为一个数据数组存储，数组由特征向量序列组成，形式为$\left\langle o_{i d}, o_1, o_2, o_3, \ldots, o_D\right\rangle$，其中$o_{i d}$是对象$o \in O$的标识符，$o_i$是对象在第$i$维度（特征）上的值。设$(q_1, \epsilon_1)$为第一个（范围）查询。在对数据数组进行线性扫描以推导查询结果的同时，我们进行对象交换，将数组划分为两部分：一部分包含所有查询结果，另一部分包含剩余对象。与此同时，我们初始化一个自适应视点树（AV-tree），以$(q_1, \epsilon_1)$作为根节点。随着新查询的到来，我们使用AV树将其与过去的查询进行比较，并仅搜索可能包含查询结果的数组部分。在三角不等式的引导下，我们避免访问数组中不相关的部分，同时引入新的裂缝和树节点对应新查询。为了防止树变得过大，我们对裂开的部分设定阈值$\theta$；如果某个部分的元素少于$\theta$，则固定该部分，不再进一步裂开。此外，我们发现，基于一个部分中所有数据点的中位距离进行裂解，而不是基于当前查询的界限$\epsilon_i$，能够得到一个更好的索引结构。对于固定的部分，我们缓存先前计算的距离，并利用排序顺序来实现比较的提前终止。

---

图1展示了我们提出的AV-tree在实验中的一个真实数据集（MNIST）上的累计成本，该数据集包含70K个50维的点。我们迭代地执行1000个范围查询，其查询中心$q$从数据中采样，并将我们的AV-tree与以下方法进行比较： (i) 线性扫描方法，该方法穷举扫描所有对象并计算它们与每个查询的距离；(ii) 构建（在第一个查询之前）并使用MVP-tree [6]的成本，MVP-tree是高维点的最先进索引[7]；以及(iii) AKD-tree [41]，最先进的多维自适应索引。值得注意的是，AV-tree展现了理想自适应索引的期望行为：(i) 即使在执行几个查询后，它也比线性扫描要快得多；(ii) 它的累计成本在执行几千次查询后趋于MVP-tree的成本，并且之后不会变得更差；(iii) 它始终比AKD-tree更快。

---

除了比以前的多维自适应索引更高效外，我们的AV-tree还是第一个支持$k$-NN查询的多维自适应索引。我们强调，基于距离的范围查询和$k$-NN查询是高维度度量空间中最通用和最常见的操作，具有广泛的应用[7]。AV-tree不仅适用于使用例如$L_p$-范数距离（如欧几里得距离）的向量空间，也适用于一般的度量空间；例如，用于索引一组字符串，以支持基于编辑距离的相似性搜索。我们的实验评估展示了AV-tree对不同度量空间和距离的鲁棒性。

------

我们的贡献可以总结如下：

- 我们首次研究了构建基于距离的多维自适应索引的问题。
- 我们定义了AV-tree，这是一种高效适应查询工作负载的索引，形成了一个统一的解决方案，支持距离范围查询和$k$-NN查询。
- 我们对AV-tree进行了若干改进。
- 我们进行了广泛的实验研究，显示AV-tree的表现符合理想自适应索引的标准。

# 2. RELATED WORK  

我们的工作与度量空间索引和数据库中的自适应索引相关。在本节中，我们回顾了相关工作，重点介绍了最先进的研究成果。

## 2.1. Indexing metric spaces  

索引高维空间是一个难题，主要有两个原因。首先，由于维度灾难[5]，如果数据点均匀分布，则两点之间的距离过近或过远的概率非常低，这使得相似性搜索大多没有意义。然而，在许多实际应用中，数据通常会形成簇，因此这种困境并不适用。第二，采用将数据空间划分为正交体的索引（例如，R树[17]、KD树[4]等）表现不佳，因为它们必须仅使用有限的维度，因此正交体最终会跨越大部分维度，不能很好地分离对象。此外，这类索引仅适用于向量空间，且主要适用于超矩形范围查询，而不是基于距离的搜索。

------

参考文献[7]（另见[8]）是关于度量空间中精确相似性搜索的现有索引的最近综合调查。根据这项研究，基于枢轴的索引是最有效的。这些方法选择少数枢轴点（也称为视点、标志物、代表点），基于这些枢轴划分数据空间，并使用枢轴点来修剪搜索空间，遵循三角不等式。基于枢轴的索引即使在距离不是使用点坐标计算时，也能适用，适用于任意度量空间（例如，图中的最短路径[2]）。除了非常高效外，基于枢轴的索引还为相似性查询提供精确且可解释的结果，这在公共安全[7]、生物信息学[12]和计算机取证[35]等应用领域中至关重要。因此，在原始度量空间中执行精确搜索可能比使用机器学习技术（例如嵌入方法[39]）将数据转换为向量空间并在转换后的空间中进行搜索或基于LSH的近似索引[14, 15, 37, 48]更为可取；后者主要适用于对象不易分离的空间，因此精确相似性搜索可能不具有意义或不重要。

------

我们详细介绍了三种代表性的主内存基于枢轴的度量索引，作为与我们方法竞争的对比方法。

### 2.1.1. SimplePivot.   

SimplePivot使用最远优先遍历（Farthest First Traversal，FFT）[20]来选择视点。FFT算法从随机选择一个点作为第一个枢轴开始。在随后的每次迭代中，它选择一个对象$u$作为枢轴，使得$\min _{p \in P} d(u, p)$最大，其中$P$是之前选择的枢轴集合。之后，我们计算每个数据点与所有视点的距离。在查询评估时，我们使用预计算的距离来避免不必要的距离计算。具体来说，考虑一个查询点$q$和半径$\epsilon$。在查询评估开始时，我们计算并缓存所有$p_i$与$q$之间的距离$d(p_i, q)$。对于每个数据对象$o \in S$，如果存在一个枢轴$p_i$，使得$\left|d(p_i, o)-d(p_i, q)\right|>\epsilon$，则$o$肯定不是查询结果，因此我们不需要计算$\operatorname{dist}(q, o)$。由于$d(p_i, o)$已经预先计算，针对每个对象$o$的剪枝测试需要$O(m)$时间，其中$m$是枢轴的数量。

### 2.1.2. Spatial Approximation Tree.  

空间近似树（SAT）[40]是一种层次数据结构，其中节点的子节点是其在整个数据集的德劳内图中的邻居。为了找到查询对象$q$的最近邻（NN），我们从根节点$n$开始；如果$n$比它的子节点离$q$更近，则搜索停止，并报告$n$为最近邻。否则，我们导航到离$q$最近的子节点，并递归地进行搜索。为了评估范围查询$(q, \epsilon)$，SAT利用三角不等式来剪枝那些保证距离$q$超过$\epsilon$的节点（及其对应的子树）。

### 2.1.3 VP-tree and MVP-tree.

视点树（VP-tree）[50]基于每个节点的视点将数据层次化地划分。从根节点开始，根节点索引所有数据，在每个节点$v$处，从该节点索引的对象中随机选择一个视点对象（枢轴）$p_v$。然后，节点$v$下的数据被分成两个子集，具体如下：设$\mu$为节点$v$下点到枢轴$p_v$的平均距离。与$p_v$的距离小于或等于$\mu$的对象被放入左子树；其余对象则进入右子树。为了评估范围查询，我们递归地遍历VP树。对于查询$(q, \epsilon)$，在每个节点$v$处，若枢轴$p_v$的与其索引点的中位距离为$\mu$，我们进行以下检查：

- 如果$\operatorname{dist}(q, p_v) \leq \epsilon$，则$p_v$是查询结果；
- 如果$\left|\operatorname{dist}(q, p_v)-\mu\right| \leq \epsilon$，则搜索左子树；
- 如果$\operatorname{dist}(q, p_v) + \epsilon > \mu$，则搜索右子树。

---

对于单个查询，我们可能会根据上述规则沿着VP-tree的多个路径进行搜索。MVP-tree [6]将VP-tree推广为一个$m$叉树。它不是将对象划分为两个部分，而是按与视点的距离对对象进行排序，并将其划分为$m$个大小相等的组。在搜索过程中，它使用每个$m$个组的平均距离来剪枝那些不能包含查询结果的子树。根据[7]中的广泛实验研究，MVP-tree在与多种主内存度量空间索引（包括基于枢轴的方法[13, 29]和SAT [40]）的比较中表现最佳。优化为二级存储的著名度量空间索引包括M-tree [9]和PM-tree [47]。

------

我们提出的AV-tree与VP-tree变种在以下几个方面有所不同：（i）它通过查询评估逐步构建树，而不是提前构建；（ii）它从查询中自适应地选择枢轴，而不是从数据集中选择；（iii）它将数据保存在一个单一的数组中，并通过交换数据来将其划分为子数组。

## 2.2. Adaptive indexing  

自适应索引通过适应已评估的查询工作负载按需构建静态数据集的数据结构[31]。每个查询根据其结果划分数据空间，触发搜索树的构建，从而引导后续查询的评估。这个思想已被应用于数据库索引[24]，通过逐步分裂一个最初未排序的数组，将其划分为符合全序的段，并构建二叉搜索树来剪枝不包含查询结果的子数组。图2展示了基于查询$10 < x \leq 20$对一个未排序数组的分裂过程，以及相应二叉搜索树逐步构建的一个步骤。首先，根据$10 < x$对数组进行分裂。索引$i$和$j$分别向前和向后扫描数组。对于每个发现的乱序值（即，$i$处的值大于10，$j$处的值小于等于10），如果$i < j$，则交换这两个值，过程继续；否则，分裂停止。在根据$10 < x$分裂后，我们更新二叉搜索树：所有小于10的值位于数组位置0到1之间，所有大于或等于10的值位于数组位置2到7之间。第二次分裂基于$x \leq 20$，应用于根节点的右子树，并将相应的子数组分裂为两个部分：位置2到3包含小于或等于20的键，位置4到7包含大于20的键。每次分裂后，搜索树（例如AVL树）都会重新平衡。

---

实际上，分裂过程通过查询逐步触发并执行快速排序的增量操作。这个过程已被扩展以有效处理更新[25]。作为快速排序的替代方法，可以增量执行归并排序操作[16]，或者采用合并与分裂相结合的混合方法[27]，或首先将域划分为不相交的区间，然后对这些区间进行分裂[46]。

------

像典型的数据库索引一样，分裂操作应用于单个属性[26]。在多维空间的自适应索引研究中，已探索了针对空间数据和低维数据的处理。QUASII [42]为维度$D$构建了一个$D$级树。它首先将每个查询投影到$x$维度上并进行分裂，然后在$x$维度的分裂结果上进行$y$维度的分裂，以此类推。AKD-tree [23]通过超平面逐步划分数据，并通过kd树[4]对生成的子集进行索引。后者在点数据上优于QUASII[30, 41]；对于具有空间扩展的对象，两者都不及AIR-tree [51]，这是一种在本次会议中提出的自适应索引，它通过每个索引级别的超矩形管理所有对象的扩展。然而，正如我们所展示的，在高维度中，基于距离的划分优于超矩形划分。

# 3. DEFINITIONS AND PRELIMINARIES  

我们研究度量空间中的相似性查询。度量空间定义为一对${M, d}$。$M$是一个域，其中的对象被实例化。例如，如果$M$是一个$D$维向量空间，那么其中的每个对象$o$的形式为$\left\langle id, o_1, o_2, \ldots, o_D\right\rangle$，其中$id$是标识符，$o_i \in [0,1]$是对象$o$在第$i$维的值；$d$是应用于$M$中对象之间的度量距离函数；在向量空间中，$d$通常是欧几里得距离，即$d(q, o) = \sqrt{\sum_{i=1}^D\left(q_i - o_i\right)^2}$。一般来说，一个度量距离函数$d$具有以下四个性质：

- 恒等性：一个对象到自身的距离为$0$；$d(x, x) = 0$。
- 非负性：两个不同对象之间的距离为正数；如果$x \neq y$，则$d(x, y) > 0$。
- 对称性：从$x$到$y$的距离与从$y$到$x$的距离相同；$d(x, y) = d(y, x)$。
- 三角不等式：对于任意三个对象$x, y, z$，$d(x, z) \leq d(x, y) + d(y, z)$。

---

我们考虑度量空间$M$中的一组对象$O$。最常见的基于相似性的查询是范围查询和$k$-最近邻查询。

定义 1. 范围查询。给定一个对象$q$和一个距离界限$\epsilon$，范围查询返回所有距离$q$小于等于$\epsilon$的对象$o \in O$，即$d(q, o) \leq \epsilon$。

定义 2. $k$-最近邻查询（$k \mathbf{N N}$）。给定一个对象$q$和一个正整数$k$，且$k \leq |O|$，$k$-最近邻查询返回一个子集$R \subseteq O$，使得$|R|=k$，并且对所有$o \in R$和$o^{\prime} \in O \backslash R$，有$d(q, o) \leq d(q, o^{\prime})$；换句话说，$k$-最近邻查询找到一个包含$k$个对象的集合$R$，这些对象到$q$的距离不大于集合外其他对象的距离。

---

回答一个查询等同于找到结果集中对象$o \in O$的标识符。我们假设对象按行存储在内存中，即第一个对象的整个元组排在第二个对象的元组之前，以此类推；这种表示方式有助于高效的距离计算。

------

我们的目标是利用先前评估过的查询来加速后续查询的评估。为此，使用三角不等式，我们可以快速确定之前查询$\left(q_i, \epsilon_i\right)$的范围与当前查询$\left(q_j, \epsilon_j\right)$的范围是否重叠，以及如何重叠。具体来说，有四种情况，如图3所示：

(a) $\left(q_i, \epsilon_i\right)$与$\left(q_j, \epsilon_j\right)$不重叠，即$d\left(q_i, q_j\right)>\epsilon_i + \epsilon_j$。

(b) $\left(q_i, \epsilon_i\right)$与$\left(q_j, \epsilon_j\right)$重叠，但它们没有包含关系。

(c) $\left(q_i, \epsilon_i\right)$包含$\left(q_j, \epsilon_j\right)$，即$\epsilon_i \geq d\left(q_i, q_j\right) + \epsilon_j$。

(d) $\left(q_j, \epsilon_j\right)$包含$\left(q_i, \epsilon_i\right)$，即$\epsilon_j \geq d\left(q_i, q_j\right) + \epsilon_i$。

# 4. THE AV-TREE  

在本节中，我们介绍了为高维度度量空间提出的自适应视点树（AV-tree）。我们首先展示了在评估范围查询的过程中，AV-tree是如何逐步构建的，接着介绍了对应的$k$NN查询算法。值得注意的是，范围查询可以与$k$NN查询交替进行，且不会影响数据结构及其效果。在第4.3节中，我们对索引的基础版本进行了若干增强，并在第4.4节中进行了成本分析。

## 4.1. Range Query  

我们的算法基于一维数据库裂解的框架，但有一些显著的不同之处：首先，索引数据没有总顺序来指导过程。其次，与现有的多维裂解方法[41, 42, 52]不同，我们不是使用超矩形划分空间，而是基于查询与数据之间的距离进行空间划分。

------

我们通过扫描整个数据数组$O$来评估第一个查询$\left(q_1, \epsilon_1\right)$，在计算结果的同时，执行裂解操作：将满足$d\left(q_1, o\right) \leq \epsilon$的数据点$o \in O$放在前面，而将满足$d\left(q_1, o\right) > \epsilon$的数据点放在后面。同时，我们定义自适应视点树（AV-tree）的根，作为一个二叉搜索树，用来帮助识别后续查询的相关数据并避免冗余计算。对于每个后续查询，我们使用AV-tree来指导搜索，并通过引入新的裂解来扩展它。

---

AV-tree中的每个节点$v$包含两个元素：$v$的范围$[v.lo, v.hi]$，即$v$索引的数组索引范围；以及查询$(v.q, v.\epsilon)$，如果$v$不是叶节点，查询用于指导$v$中的搜索。树的根节点有$lo=0$和$hi=|O|-1$，其中$|O|$是数据对象的数量。如果$v$是叶节点，则$v.q$为空。否则，$v$有两个指针$v.left$和$v.right$，分别指向其左子节点和右子节点。对于$v.left$范围内的每个对象$o$，满足$d(q, o) \leq v.\epsilon$，而对于$v.right$范围内的每个对象$o$，满足$d(q, o) > v.\epsilon$。

------

算法1详细描述了搜索与裂解过程，分为两个步骤。主要的递归过程`SEARCH-AND-CRACK`接受输入：包含数据点的数组$O$、查询点$q$及其对应的距离界限$\epsilon$，以及应用于$A V$-tree节点$v$的查询。对于一个新的查询，我们初始化查询结果为$R=\emptyset$，并调用该过程，将$v$设置为树的根节点。如果$v.q$为空，则$v$是一个叶节点，因此我们通过过程`CRACK-IN-TWO`（稍后描述）进行裂解，产生两个新的子节点作为$v$的孩子节点。如果节点$v$不是叶节点，则我们检查节点查询范围$(v.q, v.\epsilon)$与新查询范围$(q, \epsilon)$之间的关系，如第3节所述。如果两个范围不重叠，则$v.left$范围下的所有数据都不属于查询结果，因此我们调用`SEARCH-AND-CRACK`处理右子树$v.right$，因为它的范围可能包含查询结果。另一方面，如果新查询范围与$(v.q, v.\epsilon)$有重叠，我们区分两种情况。如果$(v.q, v.\epsilon)$完全包含在$(q, \epsilon)$内，则我们将$v.left$范围内的所有数据作为查询结果添加到$R$，并对$v$的右子树调用`SEARCH-AND-CRACK`。否则，如果$(q, \epsilon)$完全包含在$(v.q, v.\epsilon)$内，我们只对$v$的左子树调用`SEARCH-AND-CRACK`。最后，如果$(v.q, v.\epsilon)$与$(q, \epsilon)$之间没有包含关系，如图3(b)所示，我们也调用`SEARCH-AND-CRACK`处理右子树。

---

过程`CRACK-IN-TWO`基于Hoare的快速排序分区方法[19]；它从节点$v$的数组$O$的范围$[lo, hi]$扫描，并交换数据项，将$[lo, hi]$划分为两部分：$[lo, j]$，包括那些满足$d(q, o) \leq \epsilon$的数据点，以及$[j+1, hi]$，包括剩余的数据点。我们将前一部分$[lo, j]$添加到查询结果$R$中，并生成两个新节点作为调用节点$v$的子节点，分别对应两个新的范围。注意，在某些情况下，两个范围中的一个可能为空，例如当$v$的范围（i）没有查询结果，或者（ii）只有查询结果。在前一种情况下，$v_L.lo = lo$，$v_L.hi = lo-1$；在后一种情况下，$v_R.lo = hi+1$，$v_R.hi = hi$。过程`SEARCH-AND-CRACK`不会对具有空范围的子节点进行递归调用，因为这些节点无法提供查询结果。我们称无法继续裂解的叶节点为空叶节点，因为它们的范围为空。

------

示例：图4展示了一个详细的例子。数据数组$O$包括八个二维点$p_1$到$p_8$，最初树只有一个节点$v_1$，其范围为$[0,7]$。在第一次查询$\left(q_1, \epsilon_1\right)$时，`CRACK-IN-TWO`对根节点$v_1$（第3行）进行操作，生成两个新节点$v_2$和$v_3$，分别作为其左子节点和右子节点，如图4a所示。查询结果$R$是$v_2$的范围（即$p_7, p_2$和$p_5$）。对于第二次查询$\left(q_2, \epsilon_2\right)$，`SEARCH-AND-CRACK`对$(q, \epsilon) = \left(q_2, \epsilon_2\right)$进行处理。图4b显示，查询范围$q_2$与根节点查询范围$\left(q_1, \epsilon_1\right)$重叠，因此进入第13行和第15行。第13行的调用对$v_1$的左子节点（即节点$v_2$）进行`CRACK-IN-TWO`操作。但这次裂解没有产生结果，因为$v_2$的范围内所有点都不在$q_2$的范围内；因此，新生成的节点$v_4$作为$v_2$的左子节点，其范围为空$[0,-1]$（图中阴影部分）。生成的右子节点$v_5$与其父节点$v_2$的范围相同。第15行的递归调用对$v_1$的右子节点（即$v_3$）进行`CRACK-IN-TWO`操作。现在，我们得到了查询结果$[3,4]$（即点$p_4$和$p_6$），并生成两个新节点$v_6$和$v_7$，它们的范围非空，作为$v_3$的新子节点。图4c显示了下一个查询$\left(q_3, \epsilon_3\right)$的效果。查询范围$\left(q_3, \epsilon_3\right)$位于根节点$v_1$的范围之外，因此我们只访问其右子节点$v_3$（第7行）。我们发现$\left(q_3, \epsilon_3\right)$完全包含在$\left(q_2, \epsilon_2\right)$内，因此只访问$v_3$的左子节点$v_6$（第13行），在$v_6$中没有找到$q_3$的结果，产生了空子节点$v_8$，并生成与$v_6$相同的子节点$v_9$。

## 4.2. Nearest-Neighbor Query  

像范围查询一样，$k$NN查询也会裂解数据数组，并逐步构建和使用AV树作为二叉搜索树来指导它们查找相关数据，但现在我们以最佳优先顺序访问AV树的节点，这种顺序适用于$k$NN搜索。此外，由于查询通常没有明确的距离范围，响应$k$NN查询的自适应索引提出了一个独特的挑战。我们定义了这样的一个范围，即查询对象与其当前第$k$个最近邻之间的距离。算法2详细描述了这一过程。一个查询$(q, k)$由查询点$q$和所需的最近邻个数$k$组成。我们使用两个优先队列：一个最小堆`searchPQ`，按照到查询点$q$的最小可能距离来组织未访问的节点，以最佳优先顺序引导搜索，初始化时包含根节点；一个最大堆`resultPQ`，保存当前的$k$NN查询结果数据对象。在每次while循环迭代（第5-20行）中，我们从`searchPQ`中弹出最顶部的元素$v$。

------

如果$v$是叶节点（第8行），我们计算$v$中每个数据对象到查询点$q$的距离，并相应地更新`resultPQ`，保持追踪距离$q$最近的$k$个数据对象；`resultPQ`是一个按数据对象到查询点的距离排序的最大堆，因此顶部元素是距离$q$最远的，即当前的第$k$个最近邻。在遍历叶节点中的数据对象时（第9行），如果某个对象比当前第$k$个最近邻更接近$q$（第10行），那么我们会删除当前顶部元素（第11行），并将该对象添加到`resultPQ`（第12行）。我们还会用距离`resultPQ`中顶部元素的距离作为边界（将在第4.3节中增强），来裂解该叶节点（第13行）。

---

如果$v$是一个内部树节点（第14行），那么我们将它的子节点推入`searchPQ`，并设置它们的优先级键为$v$下的某个对象到$q$的最小可能距离，利用三角不等式。由于左子树包含距离$v$的视点小于$v.\epsilon$的点，$q$从中某个对象的最小可能距离是0与$v$的视点到查询点$q$的距离减去$v.\epsilon$的最大值（第15行）；如果后者是负数，则说明$q$位于$v$的球体内。右子节点则相反，包含的是位于以$v$的视点为中心、半径为$v.\epsilon$的球体外的对象，因此$q$从这些对象的最小可能距离是0与$v$的视点到$q$的距离减去$v.\epsilon$的最大值（第18行）；如果后者是负数，则说明$q$位于$v$的球体外。在这两种情况下，如果最小距离不小于`resultPQ`顶部的当前第$k$小距离，则我们不需要进一步查找该子节点；否则（第16行和第19行），我们将该节点添加到`searchPQ`（第17行和第20行）。当`searchPQ`为空时，搜索终止。

## 4.3. Enhancements  

我们引入了三个增强措施，这些措施显著提高了AV树的性能：首先，我们通过叶子中对象到查询点的中位距离来进行分裂。其次，我们避免构造空叶子以及对包含少量对象的叶子进行分裂。第三，我们缓存并排序那些无法进一步分裂的叶子中对象的最后计算距离，以避免对那些显然不是查询结果的对象重新进行距离计算。

### 4.3.1. Cracking based on mediocre distances.

在基于分裂和基于支点的索引方法中，一个常见的做法是根据数据的内在中位值而不是外在阈值来进行分区。例如，VP-tree [50] 就是根据数据点到支点的中位距离来进行分区。同样，在一维分裂中，使用中位值或中等值进行分裂，而不是使用查询阈值，可以带来效率上的优势 [51]。受到这些前人的启发，我们在叶子分裂时使用基于样本的中等支点。当我们对对应于叶子节点$v$的子数组进行分裂时，我们计算$v$中几个样本点到查询点$q$的距离，并使用这些距离的中位数作为$v . \epsilon$进行分裂。我们通过实验验证了，即使仅使用3个样本，基于中等值的分裂策略也优于默认的基于查询范围$\epsilon$或当前第$k$近邻的分裂策略。此外，基于中等值的分裂有助于生成平衡的树结构，因为每次分裂都会产生两个几乎相等大小的分区。

### 4.3.2 Avoiding empty leaves and applying a cracking threshold.

正如我们在第4.1节中所看到的，我们的默认算法可能会向AV树中添加空叶子节点。这样的空叶子增加了空间和搜索开销。在我们的实现中，我们不创建具有空范围的叶子节点，即我们不进行会导致空$v_L$或$v_R$的叶子$v$的分裂，而让$v$保持为叶子节点。此外，默认算法会无条件地对叶子$v$进行分裂；然而，实际中，包含少量对象的叶子节点不值得进行分裂，因为它们增加了树的高度，却没有带来显著的剪枝优势。像一维分裂[24]中一样，我们不会对包含少于分裂阈值$\theta$个对象的叶子进行分裂，这样有效地限制了AV树的高度。因此，我们期望平均叶子大小收敛到$\theta / 2$，树的高度为$1+\log_2 \frac{|O|}{\theta}$。我们称这些不再分裂的叶子为固定叶子。当查询到达一个固定叶子时，我们通过线性扫描从中获取查询结果。

### 4.3.3 Distance Caching. 

由于距离计算消耗了大部分查询评估的成本，减少距离计算是非常重要的。在进行分裂时，我们可以缓存每个对象与当前查询的距离。因此，我们可以获得叶子$v$中每个对象$o$与叶子父节点$p(v)$的距离。下次在处理另一个查询$q$时，我们可以利用三角不等式，结合$p(v)$与$q$的距离$d(p(v), q)$和缓存的距离$d(p(v), o)$来判断$o$是否可以作为查询结果，并相应地修剪掉不符合条件的对象或将符合条件的对象添加到结果集$R$中，从而避免重新计算距离。

------

为了最小化其开销，我们仅在固定叶子中应用距离缓存；我们缓存叶子$v$中每个对象$o$与叶子的父节点$p(v)$之间的距离。我们将对象按与$p(v)$的距离排序，并按照这个顺序进行比较，只需进行少量的比较，并在早期终止，修剪掉那些已经确定为查询结果或不为查询结果的对象的距离计算。

------

算法3展示了我们如何使用缓存的距离来处理固定叶子，并修改了算法1中的搜索和分裂过程。我们展示了范围查询的不同部分；对于$k$NN查询，我们使用结果堆顶元素的距离作为比较的阈值，代替$\epsilon$，并在访问符合条件的数据对象时更新$k$NN结果集。接下来，我们详细讨论如何在四种情况下修剪距离计算，如图5所示：两种情况是对于指向其父节点查询范围$(p(v), p(v). \epsilon)$内数据的左子节点固定叶子$v$，另外两种情况是对于指向查询范围外数据的右子节点固定叶子。

---

在情况L1中，$d(q, p(v)) > \epsilon$。然后，根据三角不等式，满足$d(p(v), o) < d(q, p(v)) - \epsilon$或$d(p(v), o) > d(q, p(v)) + \epsilon$的对象$o$无法是查询结果。我们通过在已排序的缓存距离上进行二分查找，找到第一个满足$d(p(v), o) \geq d(q, p(v)) - \epsilon$的对象$o$的位置，以及最后一个满足$d(p(v), o) \leq d(q, p(v)) + \epsilon$的对象$o$的位置，扫描其间的对象，并将与查询点$q$的距离不超过$\epsilon$的对象包含在结果中（第10-14行）。在图5a中，假设$p(v)$是固定叶子$v$的父节点，且对象$p_1$到$p_5$按与$p(v)$的距离排序。只有$p_3$和$p_4$可能是查询结果；我们仅计算这两个对象的距离，得出$p_4$作为查询结果。

------

在情况L2中，$d(q, p(v)) \leq \epsilon$。此时，满足$d(p(v), o) \leq \epsilon - d(q, p(v))$的对象$o$必定是查询结果。因此，我们通过二分查找找到第一个满足$d(p(v), o) > \epsilon - d(q, p(v))$的对象$o$的位置（例如图5b中的$p_3$），将此之前的所有对象（例如图5b中的$p_1$和$p_2$）添加到查询结果中，并仅对其后的对象进行距离计算（第17-24行）。

------

在情况R1中，查询$q$和$p(v)$的范围是不相交的，即$d(q, p(v)) > p(v) . \epsilon + \epsilon$。此时，位于$p(v)$查询范围外的对象$o$，若满足$d(p(v), o) < d(q, p(v)) - \epsilon$或$d(p(v), o) > d(q, p(v)) + \epsilon$（例如图5c中的$p_1$和$p_3$），则不能是查询结果；因此，和情况L1一样，我们通过二分查找找到候选查询结果的范围，仅对这些对象计算距离。情况$\mathbf{R} 2$适用于$d(q, p(v)) \leq p(v) . \epsilon + \epsilon$，其处理方式与情况R1相似，不同之处在于，此时没有任何对象$o$满足$d(p(v), o) < d(q, p(v)) - \epsilon$，因此只需进行一次二分查找即可。

---

此外，当可能时，我们避免使用二分查找来计算扫描对象并计算距离的范围。例如，在情况L1中，如果$p(v) . \epsilon \leq d(q, p(v))$，则在$p(v)$的查询范围内没有任何对象满足$d(p(v), o) > d(q, p(v)) + \epsilon$，因此`high`是$v$中最后一个对象的位置。类似地，在情况L2中，如果$d(q, p(v)) + p(v) . \epsilon \leq \epsilon$，则$p(v)$查询范围内的所有对象都是查询结果，因此我们不需要进行任何比较。我们还利用了一个事实，即固定叶子中的对象是已排序的，$v$中的第一个和最后一个对象分别为$d(p(v), o)$提供了下界和上界。因此，在情况R1和R2中，如果最后一个缓存的距离$d_{Last}$满足$d_{Last} \leq d(q, p(v)) + \epsilon$，则我们将`high`设置为$v$中最后一个对象的位置，避免使用二分查找。

## 4.4. Cost Analysis  

在这里，我们分析了具有所有增强功能的AV-tree索引的成本。假设AV-tree的叶子节点在大小不超过$\theta$时不会被裂开，我们预计树在经历大量查询后会达到最大大小，此时不再执行裂开操作。在这种状态下，每个叶子节点平均有$\theta / 2$个对象，因此预计叶子节点的数量为$\frac{2 n}{\theta}$，其中$n$是集合$O$中的对象数。由于AV-tree是一棵二叉树，预计节点的数量为$\frac{4 n}{\theta}-1$，因此索引的空间复杂度为$O(n / \theta)$。查询处理的最坏情况成本是$O(n)$，即访问所有叶子节点和数据对象，并计算它们与查询点$q$的距离。这也是在未裂开的数组上进行首次查询时的成本。然而，在大量查询后，我们预计每次查询的成本会降低，并趋近于使用完全构建的VP-tree的成本。最后，距离缓存的空间需求为$O(n)$，即每个对象一个标量，而存储$D$维数据数组的空间需求为$O(D n)$。

# 5. EXPERIMENTAL EVALUATION  

我们将AV-tree与以下竞争者进行了评估：

- **线性扫描**：对所有$o \in O$计算距离$d(q, o)$，并且不执行任何数据数组的重组或索引。
- **SimplePivot**：如第2节所述。在实验调整后，我们选择将枢轴数$m$设置为5，这个值能获得最佳性能；该参数选择与[7]中的实验设置一致。
- **MVP-tree** [6]：如第2节所述。我们使用了与[7]中相同的实现${ }^3$。MVP-tree有两个参数，桶大小（相当于AV-tree中的阈值$\theta$）和分支因子（即每个节点的子节点数量）。我们通过实验发现，MVP-tree在桶大小为64，分支因子为5时表现最佳。
- **AKD-tree** [41]：是多维点的最先进的自适应索引；我们使用了作者提供的实现${ }^4$。为了防止树的过度增长，我们在实验评估了不同的值后，选择128作为大小阈值。原始实现处理矩形查询，即$L_{\max}$距离度量。为了将其适应$L_2$距离度量，我们首先使用给定的$q$和$\epsilon$执行$L_{\max}$查询，找到周围的切线框，然后通过基于$L_2$的线性扫描过滤假阳性。
- **SAT**：是空间逼近树[40]（参见第2.1.2节）的实现，具有无构造参数。

## 5.1. Experimental Settings  

**环境。** 我们在g++ 9.4.0中编译了所有代码，使用了标志-O3、-mavx和-march=native，并在一台配备32GB内存的Ubuntu 20.04.3 LTS机器上运行实验，该机器搭载Intel Core i9-10900K CPU @3.70GHz。

------

**数据集。** 我们使用了两个公开可用的真实数据集和合成生成的高维向量。表1总结了数据的统计信息，默认参数值和距离度量以粗体显示。更多细节如下：

- **MNIST** 是一个包含70K手写数字的数据库[11]。每个数字存储为28 x 28像素的灰度图像。MNIST已被用于许多相似性搜索研究（例如，[3, 28, 36]）。我们使用UMAP [38]降维方法，创建不同的向量表示，数据维度$D$为5-100。
- **Words** 是一个包含65万个人名、首字母缩略词和复合词的数据库，数据来自Moby项目，长度从2到33个字符不等。在Words数据集上，查询目标是通过编辑距离找到与给定查询字符串相似的单词。
- **Synthetic** 是生成的合成数据集，由10个不重叠的、大小相等的簇组成，每个簇包含100维的数据点，使用Python的sklearn [43]库中的make_blobs函数生成，标准差为0.5，生成的簇为各向同性高斯球形。

---

**查询。** 我们进行了范围查询和$k$最近邻查询（$k$ NN）的工作负载。与之前的工作[24, 41]一致，我们的查询工作负载由从目标数据集中随机抽取的1000个查询点组成。对于范围查询，我们调整了$\epsilon$，确保查询返回所需数量的结果。查询选择性范围从20到1000（默认值为100）。对于$k$最近邻查询，$k$的取值范围为${5, 20, 50, 100}$（默认值为$k=20$）。

------

**成本度量。** 我们通过以下两项指标评估所有方法：（i）每次查询的成本和（ii）随着查询工作负载的进展的累计成本；我们对5次运行的结果进行了平均。由于SimplePivot、MVP-tree和SAT是预先构建的，我们将它们的构建成本加入到第一次查询之前的累计成本中。线性扫描、AV-tree和AKD-tree没有预处理成本。

## 5.2. Enhancements and parameter tuning  

### 5.2.1 Effect of AV-tree enhancements. 

首先，我们在50维MNIST数据集上使用1000个查询评估不同版本的AV-tree，包括作为参考的线性扫描性能。我们将AV-tree的基本版本（使用标准裂开，没有任何增强功能，标记为“standard”）与以下几种变体进行比较：（i）使用中等裂开的变体（第4.3.1节，标记为“mediocre”）；（ii）使用中等裂开和阈值$\theta=128$的变体（第4.3.2节，标记为“mediocre128”）；（iii）应用所有增强功能（包括缓存）的变体（第4.3.3节，标记为“mediocre-128 caching”）。

------

图6a和6b分别展示了所有AV-tree变体在MNIST上的每次查询成本和累计成本，而图6c和6d展示了它们在Synthetic和Words上的累计成本。值得注意的是，中等裂开和阈值化都提升了性能，其中阈值化对MNIST的影响较小。中等裂开创建了一个平衡的树，因为每次裂开都会将一个叶子分裂成两个大致相等大小的部分，而阈值化则避免了构建过于高大的树，这种树会对性能产生不利影响，因为其遍历的收益并不足以弥补所花费的时间。另一方面，缓存对于距离计算开销较大的情况非常有效，例如在Words数据集上，我们使用的是编辑距离。

------

表2展示了在50维MNIST数据集上进行1000次范围查询后的累计成本、距离计算次数和AV-tree节点数。经过充分优化的AV-tree在各方面超越了所有其他版本。尽管在这些数据上，没有阈值的Mediocre表现与Mediocre-128相似，但它通过构建比Standard更大的AV-tree，导致了显著的空间开销。对于所有其他数据集也适用这一点；由于篇幅原因，我们省略了相应的表格。

------

图7展示了使用$L_1$距离代替$L_2$（欧几里得距离）时，AV-tree变体在MNIST和Synthetic上的累计成本。请注意，使用$L_1$时性能差异不显著。因此，从此以后，我们在AV-tree中采用所有增强功能，并在MNIST和Synthetic上使用$L_2$作为距离度量。

### 5.2.2 Parameter setting. 

我们在MNIST50和Words数据集上调整了AV-tree、AKD-tree和MVP-tree的阈值参数。图8绘制了1K查询工作负载的总成本与不同阈值之间的关系。正如图中所示，AV-tree、AKD-tree和MVP-tree的最优阈值分别为128、128和64。

## 5.3. Comparative study  

### 5.3.1 MNIST. 

接下来，我们在MNIST数据集上使用范围查询和$k$最近邻查询（$k$NN）对完全增强的AV-tree与第5节中列出的竞争者进行比较。

------

**维度。** 图10展示了随着查询工作负载（选择性$s=100$）的进行，MNIST数据集在不同维度下的每次查询成本和累计成本。AV-tree表现出了自适应索引的理想行为：它的每次查询成本逐渐下降，并最终达到MVP-tree的水平。其累计成本超过了所有竞争者，最终与MVP-tree相匹配。在低维度下，这一过程较慢；在高维度下，两个曲线大约在100次查询后交汇。AKD-tree仅在非常低的维度（$D=5$）下与AV-tree竞争力较强，此时基于超平面的分区效果良好。直到达到大小阈值，AKD-tree每次裂开都会创建$2D$个新层级，导致树的高度过大，遍历代价昂贵，因此在高维度下表现不佳。SimplePivot在$D$较小的时候，表现不如MVP-tree和SAT，尤其是在低维度情况下。这些结果与[7]中的发现一致。MVP-tree在中等维度数据上比SAT的每次查询成本更低，但在高维空间中，两者的成本相似。不过，SAT相较于MVP-tree，其启动（即构建）成本非常高。

---

图11重复了使用$k$最近邻查询（$k$NN）的实验，将$k$设置为20。我们将AKD-tree排除在比较之外，因为它不支持$k$NN查询。我们的发现与范围查询的结果一致，因为在两种情况下，数据的重组（即裂开）是类似的，从而得到了良好的数据结构，同时AV-tree中使用的两个优先队列有效地避免了冗余搜索。

------

**选择性。** 图12展示了在不同选择性$s$的工作负载下，各方法的对比；它们的相对性能基本不受选择性影响，唯一明显的例外是AKD-tree，它对较大的$s$敏感，因为$L_2$过滤开销较大；随着$s$的增大，需要扫描的项数也增加。图13显示，在$k$NN查询中，成本几乎不受$k$的影响。AV-tree对选择性具有鲁棒性，因为裂开操作不受查询结果数量的影响，并且得益于它使用的数据结构来管理$k$NN查询结果。

---

**成本拆解。** 图9展示了在默认的MNIST50和Synthetic数据集上，AV-tree和AKD-tree在默认范围工作负载下的总运行时间拆解。总时间包括以下几部分成本：（i）在索引中搜索相关分区的成本（索引搜索）；（ii）索引重组，即创建新节点和交换（适应）；以及（iii）扫描未进一步裂开的固定叶子中的数据对象的成本（扫描）——在AKD-tree中，扫描包括$L_2$过滤的时间。AKD-tree的所有成本都高于AV-tree：（i）索引搜索成本，由于基于超平面的分区效果差和更大的索引大小，（ii）适应成本，因为生成的（基于超平面）分区比AV-tree更多，（iii）扫描成本，由于精细化的球形范围查询。

### 5.3.2 Words. 

接下来，我们比较了所有方法在Words数据集上的范围查询和$k$NN查询工作负载的表现。由于AKD-tree不支持非向量数据和非$L_p$距离度量，我们将其排除在比较之外。首先，我们通过选择1000个固定长度的随机单词（长度为4到10）创建范围查询工作负载，设置$\epsilon=2$，并测量所有方法的累计成本。如图14所示，AV-tree优于所有竞争者，在较短的查询单词长度下表现更好。随着单词长度的增加，维度的诅咒开始显现，所有基于索引的方法的成本接近线性扫描；然而，即便如此，AV-tree仍然超过了SimplePivot，并与MVP-tree相当。

------

图15展示了在相同的长度为6的查询工作负载下，所有方法的表现，同时调节$\epsilon$的值，即变化选择性。当查询更具选择性（较低的$\epsilon$）时，基于索引的方法优于线性扫描，AV-tree获得了优势。然而，当查询的选择性较低（$\epsilon=4$）时，索引的效果较差，AV-tree的优势有所减小。最后，图16展示了AV-tree在$k$NN查询中的表现与$k$值的关系。结果与范围查询的选择性变化类似。总体而言，AV-tree在Words数据集上表现出了理想的行为，其累计成本始终低于所有其他方法，在前几百次查询中差异尤为显著。

### 5.3.3 Synthetic data. 

我们现在比较了所有方法在不同规模的合成数据集上的表现，这些数据集的生成方式如第5.1节所述。图17展示了在范围查询工作负载下的累计成本。值得注意的是，AV-tree的优越性能对数据规模不敏感；在前几个查询中，其成本接近线性扫描的成本，并在几百次查询后与MVP-tree相匹配，而线性扫描依然过于缓慢。正如图18所示，AV-tree在$k$NN查询工作负载下对数据规模的鲁棒性同样表现得很好。

## 5.4. Index Size  

最后，我们比较了AV-tree、AKD-tree和MVP-tree在默认工作负载下（1000个选择性为100的范围查询）后的最终索引大小。如表3所示，AV-tree是轻量级的，其大小由阈值控制，并且每个对象最多缓存一个距离。与表1中相应数据集的大小相比，AV-tree占用的空间很小；这是我们方法的另一个优势。如果我们在AV-tree中避免使用距离缓存（第二列），则索引的大小比MVP-tree还要小，代价是搜索性能略有小幅开销。

# 6. CONCLUSIONS  

我们介绍了自适应视点（vantage）树（AV-tree），这是我们所知的首个专为高维度度量空间量身定制的自适应索引。与先前提出的单列自适应索引 [18, 24] 和少量属性自适应索引 [41, 42] 相似，AV-tree能够优雅地适应查询工作负载，逐步构建出一个完整的高质量索引。然而，与以往的自适应索引方法不同，AV-tree将查询中心周围的空间划分为由超球体定义的单位，使用中等距离界限，这些界限自然适应数据分布，而不是划分为正交多胞体（即超矩形单元）。我们在两种具有不同性质的真实数据集上进行的实验研究，采用了多种距离度量，结果表明，AV-tree与以下几种方法相比，能够实现较低的累计查询成本：（i）迭代应用线性扫描；（ii）使用预构建的MVP-tree，度量空间的最先进索引；（iii）采用AKD-tree，适用于多维数据的最先进自适应索引。未来，我们计划研究多路AV-tree（MAV-tree）的性能，与当前的二分空间划分不同，MAV-tree将基于多个距离界限将每个查询周围的空间划分为多个层级。我们还计划研究测量距离的其他方式 [49] 和自适应多维摘要 [32]。

























