# $\textbf{1. BEIR}$检索数据集

> ```txt
> Following [43], we use the development set for our experiments on MS MARCO, and use the test set on the other datasets.
> ```
>
> ## $\textbf{1.1. MS-MARCO}$
>
> > ### $\textbf{1.1.1. }$数据集的结构
> >
> > > :one:使用的是[$\text{MS-MARCO}$](https://huggingface.co/datasets/microsoft/ms_marco/tree/main/v2.1)的$\text{Development}$集，也就是$\text{Validation}$集
> > >
> > > :two:$\text{MS-MARCO-v1.1}$的结构
> > >
> > > 1. 第一列`answers`：人工生成的，问题的高质量参考答案
> > >
> > > 2. 第二列`passage`：一个$\text{Json}$对象
> > >
> > >    ```json
> > >    {
> > >      "is_selected": [ 0, 0, 0, 1, 0, 0, 0, 0, 0 ],
> > >      "passage_text": [ "text1", "text2", "text3", ... ],
> > >      "url": [ "http://example.com/1", "http://example.com/2", ... ]
> > >    }
> > >    ```
> > >
> > >    - `is_selected`：一个二进制数组，指示`passage_text`中哪些段落是相关的，分布如下
> > >
> > >      ```txt
> > >      0 个1的占比: 3.39%
> > >      1 个1的占比: 86.97%
> > >      2 个1的占比: 8.71%
> > >      3 个1的占比: 0.78%
> > >      4 个1的占比: 0.14%
> > >      5 个1的占比: 0.01%
> > >      ```
> > >
> > >    - `passage_text`：包含候选段落文本的列表
> > >
> > >    - `url`：相关段落的来源网址列表
> > >
> > > 3. 第三列`query`：用户输入的查询
> > >
> > > 4. 第四列`query_id`：查询的唯一标识符
> > >
> > > 5. 第五列`query_type`：查询类型(数数值/人物/描述/实体)
> > >
> > > 6. 第六行`wellFormedAnswers`：全部都是空的
> > >
> > >    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20250108230336012.png" alt="image-20250108230336012" width=400 />  
> > >
> > > :three:$\text{MS-MARCO-v2.1}$的结构：比起第一版本
> > >
> > > 1. 第六行`wellFormedAnswers`不全为空了，用于存储更高质量的答案
> > > 2. 数据集规模更大了
> >
> > ### $\textbf{1.1.2. }$数据集的使用流程
> >
> > > :one:数据准备
> > >
> > > 1. 查询：`query`
> > > 2. 候选集：`passage`中的所有`passage_text`
> > > 3. 标签：`passage`中`is_selected=1`的`passage_text`
> > > 4. 先不用管`answers`因为这又不是问答任务
> > >
> > > :two:对模型的测试：针对Recall@1-10
> > >
> > > 1. 对单个`query`，计算与其所有10个`passage`的相似度
> > > 2. Recall@1：选取得分Top-1的文档，如果该文档被标记为了1则认为找到了
> > > 3. Recall@5：选取得分Top-5的文档，如果5个文档里有文档标记为了1则认为找到了
> > >
> > > :three:对模型的测试：针对Recall@10以上
> > >
> > > 1. 对$\text{MS-MARCO}$进行取样，比如取k个query，那么就一共有10k个段落
> > > 2. 对于每个查询，暴力计算每个query和所有10k个段落的距离
> > > 3. 如果在排名前k个段落中，有与该query相关(即`is_selected=1`)的段落，则认为找到了

# $\textbf{2. ColBERTv2}$模型用于嵌入

# $\textbf{3. }$对$\textbf{Muvera}$的原始复现

> :one:假设读取了以下内容
>
> 1. 一个查询的嵌入集，固定了是32个嵌入，即`q1,q2,...,q32`
> 2. 一个段落的嵌入集，固定了是128个嵌入，即`p1,p2,...,p128`
>
> :two:对所有的点进行$\text{SimHash}$
>
> 1. 先选取$k_{\text{sim}}=5$
> 2. 也就是将所有的`q1,q2,...,q32`加上`p1,p2,...,p128`，装进$B\text{=}2^{k_{\mathrm{sim}}}\text{=}32$个桶
>
> :three:每个桶的子向量生成：分三种情况
>
> 1. 一个桶内只一个p：则该桶中，查询子向量Q=所有桶内q的加和，段落子向量P=p
> 2. 一个桶内有多个p：则该桶中，查询子向量Q=所有桶内q的加和，段落子向量P=所有p质心
>    - 所谓质心就是所有p的加和，除以p的数量
> 3. 一个桶内有零个p：则该桶中，查询子向量Q=所有桶内q的加和，段落子向量P=离桶最近的p
>
> :four:合并成定长向量：
>
> 1. 将所有桶的查询子向量Q从左到右依次拼接，变成大Q
> 2. 将所有桶的段落子向量P从左到右依次拼接，变成大P
>
> :five:计算：大Q，大P的内积，得到相似度
>
> :six:程序的整体流程
>
> - 遍历所有的`query`，假设当前为`query_i`，包含`q1,q2,...,q32`共32个点
>
>   - 遍历所有的`passage`，假设当前为`passage_j`，包含`p1,p2,...,p128`共128个点
>
>     - 对所有的点进行`Simhash(query_i, passage_j)`，得到B个桶及桶里的内容
>     - 遍历所有的B个桶
>       - 如果该桶内只有一个p，进行`case_1(B,p,q)`生成相应子向量
>       - 如果该桶内只有多个p，进行`case_n(B,p,q)`生成相应子向量
>       - 如果该桶内只有零个p，进行`case_n(B,q)`生成相应子向量
>     - 对所有桶中的子向量执行拼接`concatenate()`，形成大Q和大P
>     - 计算`sim(大P,大Q)`，得到最终`query_i`和`passage_j`的相似度
>
>   - 得到`query_i`对所有`passage`的相似度，执行`rank(query_i,passage_sim)`，记录Top-100
>
>     ```C++
>     按这种json方式存储
>     query_i ->
>     top-1-passage
>     {
>        sim=相似度，由我们刚刚的计算得到
>        query_id=文章所属的查询的id，在原始的passage.tsv中有记录
>        is_selected=文章是否和query_id所对应的query有关(=1)，也在passage.tsv中有记录
>     }
>     top-2-passage
>     top-3-passage
>     .......
>     先记录top-100
>     ```
>
> - 进行评估`evaluate()`，先计算Recall@100，计算一个查询的`top-100文章中有多少个is_selected=1`，对所有 查询求平均



## 复现第一版

:zero:主程序：

- 运行`data_prepare()`完成所有的数据准备
- 遍历`join.csv`的每行
  - 找到该行`query_id`和`passage_id`各自的多向量嵌入(在`join_emb.csv`中)
  - 运行`fix_dimension()`函数得到二者的单向量表示
  - 计算两个单向量的内积，作为二者的得分，并将结果写入该行的`sim`列
- 运行`recall_one()`得到Recall@1得分

:one:数据准备：封装入`data_prepare()`函数

- 读取`join.csv`(结构如下)，`is_selected=1`代表`query/passage`二者相关

  ```csv
  query_id,passage_id,is_selected
  ```

- 为`join.csv`开辟第四列，命名为`sim`，并将该列值全部初始化为0

- 读取`join_emb.csv`(结构如下)，其中`p_embedding/q_embedding`为`(N,1024)`形状的多向量嵌入(N为可变的嵌入数量+1024为每个嵌入的固定维读)

  ```csv
  query_id,passage_id,q_embedding,p_embedding
  ```

:two:转为单向量：封装入`fix_dimension()`函数

- 输入形状为`(M,1024)`的`p_embedding`，和形状为`(N,1024)`的`q_embedding` 
- 调用`sim_hash()`将一共`M+N`个嵌入映射到`B=2^k_sim`个桶中
- 调用`sub_vector()`得到每个桶的子向量
- 将每个桶的子向量从左到右依次合并，得到最终的固定维度单向量

:three:哈希的实现：封装入`sim_hash()`

- 从高斯分布中随机采样`k_sim(此处=6)`个向量，对所有的向量执行归一化
- 对所有`M+N`个嵌入应用划分函数`part_func()`，生成每个嵌入的二进制串
- 将二进制串相同的嵌入放到一起，也就是所谓的桶

:four:划分函数：封装入`part_func()`，参考`simhash`中对于划分函数的定义

- 对于从高斯分布中抽取$k_{\text{sim}} \text{≥} 1$个向量$g_{1}, \ldots, g_{k_{\text{sim}}} \text{∈} \mathbb{R}^{d}$ 
- $\varphi(x)\text{=}\left(\mathbf{1}\left(\left\langle g_{1}, x\right\rangle\text{>}0\right), \ldots, \mathbf{1}\left(\left\langle g_{k_{\text{sim}}}, x\right\rangle\text{>}0\right)\right)$
- $\mathbf{1}\left(\left\langle g_{i}, x\right\rangle\text{>}0\right)$表示$\langle g_{i}, x\rangle\text{>}0$成立，即$x$投影在超平面的正侧时，将该位设为$1$ 

:five:子向量生成：封装入`sub_vector()`

- 遍历每个桶
  - 如果该桶内只有一个嵌入来自于`p_embedding`
    - 子向量`p_sub_vec`等于`p_embedding`
    - 子向量`q_sub_vec`等于所有来自`p_embedding`的嵌入的总和
  - 如果该桶内有多个嵌入来自于`p_embedding`
    - 子向量`p_sub_vec`等于所有来自`p_embedding`的嵌入的总和，除以嵌入的数量，即质心
    - 子向量`q_sub_vec`等于所有来自`p_embedding`的嵌入的总和
  - 如果该桶内没有嵌入来自于`p_embedding`
    - 选择离该桶最近(海明距离)的一个来自`p_embedding`的嵌入，作为子向量`p_sub_vec`
    - 子向量`q_sub_vec`等于所有来自`p_embedding`的嵌入的总和

:six:计算Recall@1：封装入`recall_one()`

- 读取`join.csv`，现在的结构应该变成了

  ```txt
  query_id,passage_id,is_selected,sim
  ```

- 按照`query_id`将所有的行分组(数据集是经过精心设计的，每组固定是十行)

- 将组内所有的元素按照`sim`的大小进行排序

- 如果组内Top-1的行中，`is_selected=1`记改组Recall=1，如果`is_selected=0`记改组Recall=0

- 将Recall=1的组数，除以总的组数，得到平均的Recall





