#### Abstract

Neural embedding models have become a fundamental component of modern information retrieval (IR) pipelines. These models produce a single embedding $x \in \mathbb{R}^{d}$ per data-point, allowing for fast retrieval via highly optimized maximum inner product search (MIPS) algorithms. Recently, beginning with the landmark ColBERT paper, multi-vector models, which produce a set of embedding per data point, have achieved markedly superior performance for IR tasks. Unfortunately, using these models for IR is computationally expensive due to the increased complexity of multi-vector retrieval and scoring.

---

In this paper, we introduce Muvera (Multi-Vector Retrieval Algorithm), a retrieval mechanism which reduces multi-vector similarity search to single-vector similarity search. This enables the usage of off-the-shelf MIPS solvers for multivector retrieval. MuVERA asymmetrically generates Fixed Dimensional Encodings (FDEs) of queries and documents, which are vectors whose inner product approximates multi-vector similarity. We prove that FDEs give high-quality $\varepsilon$ approximations, thus providing the first single-vector proxy for multi-vector similarity with theoretical guarantees. Empirically, we find that FDEs achieve the same recall as prior state-of-the-art heuristics while retrieving $2-5 \times$ fewer candidates. Compared to prior state of the art implementations, MUVERA achieves consistently good end-to-end recall and latency across a diverse set of the BEIR retrieval datasets, achieving an average of $10 \%$ improved recall with $90 \%$ lower latency.


## 1 Introduction

Over the past decade, the use of neural embeddings for representing data has become a central tool for information retrieval (IR) [56], among many other tasks such as clustering and classification [39]. Recently, multi-vector (MV) representations, introduced by the late-interaction framework in ColBERT [29], have been shown to deliver significantly improved performance on popular IR benchmarks. ColBERT and its variants [17, 21, 32, 35, 42, 44, 49, 54] produce multiple embeddings per query or document by generating one embedding per token. The query-document similarity is then scored via the Chamfer Similarity (ยง1.1), also known as the MaxSim operation, between the two sets of vectors. These multi-vector representations have many advantages over single-vector (SV) representations, such as better interpretability [15, 50] and generalization [16, 36, 51, 55].

---

Despite these advantages, multi-vector retrieval is inherently more expensive than single-vector retrieval. Firstly, producing one embedding per token increases the number of embeddings in a dataset by orders of magnitude. Moreover, due to the non-linear Chamfer similarity scoring, there is a lack of optimized systems for multi-vector retrieval. Specifically, single-vector retrieval is generally accomplished via Maximum Inner Product Search (MIPS) algorithms, which have been highly-optimized over the past few decades [18]. However, SV MIPS alone cannot be used for MV retrieval. This is because the MV similarity is the sum of the SV similarities of each embedding in a query to the nearest embedding in a document. Thus, a document containing a token with high similarity to a single query token may not be very similar to the query overall. Thus, in an effort to close the gap between SV and MV retrieval, there has been considerable work in recent years to design custom MV retrieval algorithms with improved efficiency [12, 21, 42, 43].

---

The most prominent approach to MV retrieval is to employ a multi-stage pipeline beginning with single-vector MIPS. The basic version of this approach is as follows: in the initial stage, the most similar document tokens are found for each of the query tokens using SV MIPS. Then the corresponding documents containing these tokens are gathered together and rescored with the original Chamfer similarity. We refer to this method as the single-vector heuristic. ColBERTv2 [44] and its optimized retrieval engine PLAID [43] are based on this approach, with the addition of several intermediate stages of pruning. In particular, PLAID employs a complex four-stage retrieval and pruning process to gradually reduce the number of final candidates to be scored (Figure 1). Unfortunately, as described above, employing SV MIPS on individual query embeddings can fail to find the true MV nearest neighbors. Additionally, this process is expensive, since it requires querying a significantly larger MIPS index for every query embedding (larger because there are multiple embeddings per document). Finally, these multi-stage pipelines are complex and highly sensitive to parameter setting, as recently demonstrated in a reproducibility study [37], making them difficult to tune. To address these challenges and bridge the gap between single and multi-vector retrieval, in this paper we seek to design faster and simplified MV retrieval algorithms.

---

**Contributions.** We propose MUVERA: a multi-vector retrieval mechanism based on a light-weight and provably correct reduction to single-vector MIPS. MUVERA employs a fast, data-oblivious transformation from a set of vectors to a single vector, allowing for retrieval via highly-optimized MIPS solvers before a single stage of re-ranking. Specifically, MuVERA transforms query and document MV sets $Q, P \subset \mathbb{R}^{d}$ into single fixed-dimensional vectors $\vec{q}, \vec{p}$, called Fixed Dimensional Encodings (FDEs), such that the the dot product $\vec{q} \cdot \vec{p}$ approximates the multi-vector similarity between $Q, P(\S 2)$. Empirically, we show that retrieving with respect to the FDE dot product significantly outperforms the single vector heuristic at recovering the Chamfer nearest neighbors (ยง3.1). For instance, on MS MARCO, our FDEs Recall@ $N$ surpasses the Recall@2-5N achieved by the SV heuristic while scanning a similar total number of floats in the search.

---

We prove in (ยง2.1) that our FDEs have strong approximation guarantees; specifically, the FDE dot product gives an $\varepsilon$-approximation to the true MV similarity. This gives the first algorithm with provable guarantees for Chamfer similarity search with strictly faster than brute-force runtime (Theorem 2.2). Thus, Muvera provides the first principled method for MV retrieval via a SV proxy.

---

We compare the end-to-end retrieval performance of MUVERA to PLAID on several of the BEIR IR datasets, including the well-studied MS MARCO dataset. We find Muvera to be a robust and efficient retrieval mechanism; across the datasets we evaluated, MUVERA obtains an average of $10 \%$ higher recall, while requiring $90 \%$ lower latency on average compared with PLAID. Additionally, MUVERA crucially incorporates a vector compression technique called product quantization that enables us to compress the FDEs by $32 \times$ (i.e., storing 10240 dimensional FDEs using 1280 bytes) while incurring negligible quality loss, resulting in a significantly smaller memory footprint.

### 1.1 Chamfer Similarity and the Multi-Vector Retrieval Problem

Given two sets of vectors $Q, P \subset \mathbb{R}^{d}$, the Chamfer Similarity is given by

$$
\operatorname{ChAmFER}(Q, P)=\sum_{q \in Q} \max _{p \in P}\langle q, p\rangle
$$

where $\langle\cdot, \cdot\rangle$ is the standard vector inner product. Chamfer similarity is the default method of MV similarity used in the late-interaction architecture of ColBERT, which includes systems like ColBERTv2 [44], Baleen [28], Hindsight [41], DrDecr [34], and XTR [32], among many others. These models encode queries and documents as sets $Q, P \subset \mathbb{R}^{d}$ (respectively), where the query-document similarity is given by Chamfer $(Q, P)$. We note that Chamfer Similarity (and its distance variant) itself has a long history of study in the computer vision (e.g., [4, 6, 14, 27, 45]) and graphics [33] communities, and had been previously used in the ML literature to compare sets of embeddings [3, 5, 30, 48]. In these works, Chamfer is also referred to as MaxSim or the relaxed earth mover distance; we choose the terminology Chamfer due to its historical precedence [6].

---

In this paper, we study the problem of Nearest Neighbor Search (NNS) with respect to the Chamfer Similarity. Specifically, we are given a dataset $D=\left\{P_{1}, \ldots, P_{n}\right\}$ where each $P_{i} \subset \mathbb{R}^{d}$ is a set of vectors. Given a query subset $Q \subset \mathbb{R}^{d}$, the goal is to quickly recover the nearest neighbor $P^{*} \in D$, namely:
$$
P^{*}=\arg \max _{P_{i} \in D} \operatorname{CHAMFER}\left(Q, P_{i}\right)
$$

For the retrieval system to be scalable, this must be achieved in time significantly faster than bruteforce scoring each of the $n$ similarities Chamfer $\left(Q, P_{i}\right)$.

### 1.2 Our Approach: Reducing Multi-Vector Search to Single-Vector MIPS

MUVERA is a streamlined procedure that directly reduces the Chamfer Similarity Search to MIPS. For a pre-specified target dimension $d_{\text {FDE }}$, MUVERA produces randomized mappings $\mathbf{F}_{\mathrm{q}}: 2^{\mathbb{R}^{d}} \rightarrow \mathbb{R}^{d_{\mathrm{FDE}}}$ (for queries) and $\mathbf{F}_{\text {doc }}: 2^{\mathbb{R}^{d}} \rightarrow \mathbb{R}^{d_{\mathrm{FDE}}}$ (for documents) such that, for all query and document multivector representations $Q, P \subset \mathbb{R}^{d}$, we have:

$$
\left\langle\mathbf{F}_{\mathrm{q}}(Q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle \approx \operatorname{CHAMFER}(Q, P)
$$

We refer to the vectors $\mathbf{F}_{\mathrm{q}}(Q), \mathbf{F}_{\mathrm{doc}}(P)$ as Fixed Dimensional Encodings (FDEs). Muvera first applies $\mathbf{F}_{\text {doc }}$ to each document representation $P \in D$, and indexes the set $\left\{\mathbf{F}_{\text {doc }}(P)\right\}_{P \in D}$ into a MIPS solver. Given a query $Q \subset \mathbb{R}^{d}$, MUVERA quickly computes $\mathbf{F}_{\mathrm{q}}(Q)$ and feeds it to the MIPS solver to recover top- $k$ most similar document FDE's $\mathbf{F}_{\text {doc }}(P)$. Finally, we re-rank these candidates by the original Chamfer similarity. See Figure 1 for an overview. We remark that one important advantage of the FDEs is that the functions $\mathbf{F}_{q}, \mathbf{F}_{\text {doc }}$ are data-oblivious, making them robust to distribution shifts, and easily usable in streaming settings.

### 1.3 Related Work on Multi-Vector Retrieval

The early multi-vector retrieval systems, such as ColBERT [29], all implement optimizations of the previously described SV heuristic, where the initial set of candidates is found by querying a MIPS index for every query token $q \in Q$. In ColBERTv2 [44], the document token embeddings are first clustered via k -means, and the first round of scoring using cluster centroids instead of the original token. This technique was further optimized in PLAID [43] by employing a four-stage pipeline to progressively prune candidates before a final reranking (Figure 1).

---

An alternative approach with proposed in DESSERT [12], whose authors also pointed out the limitations of the SV heuristic, and proposed an algorithm based on Locality Sensitive Hashing (LSH) [20]. They prove that their algorithm recovers $\varepsilon$-approximate nearest neighbors in time $\tilde{O}(n|Q| T)$, where $T$ is roughly the maximum number of document tokens $p \in P_{i}$ that are similar to any query token $q \in Q$, which can be as large as $\max _{i}\left|P_{i}\right|$. Thus, in the worst case, their algorithm runs no faster than brute-force. Conversely, our algorithm recovers $\varepsilon$-approximate nearest neighbors and always runs in time $\tilde{O}(n|Q|)$. Experimentally, DESSERT is $2-5 \times$ faster than PLAID, but attains worse recall (e.g. 2-2.5\% R@1000 on MS MARCO). Conversely, we match and sometimes strongly exceed PLAID's recall with up to $5.7 \times$ lower latency. Additionally, DESSERT still employs an initial filtering stage based on $k$-means clustering of individual query token embeddings (in the manner of ColBERTv2), thus they do not truly avoid the aforementioned limitations of the SV heuristic.