## A Missing Proofs from Section 2.1

In this section, we provide the missing proofs in Section 2.1. For convenience, we also reproduce theorem statements as they appear in the main text before the proofs. We begin by analyzing the runtime to compute query and document FDEs, as well as the sparsity of the queries.

Lemma A.1. For any FDE parameters $k_{\text {sim }}, d_{\text {proj }}, R_{\text {reps }} \geqslant$ and sets $Q, P \subset \mathbb{R}^{d}$, we can compute $\mathbf{F}_{q}(Q)$ in time $T_{q}:=O\left(R_{\text {reps }}|Q| d\left(d_{\text {proj }}+k_{\text {sim }}\right)\right)$, and $\mathbf{F}_{q}(P)$ in time $O\left(T_{q}+R_{\text {reps }}|P| 2^{k_{\text {sim }}} k_{\text {sim }}\right)$. Moreover, $\mathbf{F}_{q}(Q)$ has at most $O\left(|Q| d_{\text {proj }} R_{\text {reps }}\right)$ non-zero entries.

Proof. We first consider the queries. To generate the queries, we must first project each of the $|Q|$ queries via the inner random linear productions $\psi_{i}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d_{\mathrm{proj}}}$, which requires $O\left(|Q| d d_{\mathrm{proj}} R_{\mathrm{reps}}\right)$ time to perform the matrix-query products for all repetitions. Next, we must compute $\varphi_{i}(q)$ for each $q \in Q$ and repetition $i \in\left[R_{\text {reps }}\right]$, Each such value can be compute in $d \cdot k_{\text {sim }}$ time to multiply the $q \in \mathbb{R}^{d}$ by the $k_{\text {sim }}$ Gaussian vectors. Thus the total running time for this step i $O\left(R_{\text {reps }}|Q| d k_{\text {sim }}\right)$. Finally, summing the relevant values into the FDE once $\varphi_{i}(q), \boldsymbol{\psi}_{i}(q)$ are computed can be done in $O\left(|Q| d_{\text {proj }}\right)$ time. For sparsity, note that only the coordinate blocks in the FDE corresponding to clusters $k$ in a repetition $i$ with at least one $q \in|Q|$ with $\varphi_{i}(q)=k$ are non-zero, and there can be at most $O\left(R_{\text {reps }}|Q|\right)$ of these blocks, each of which has $O\left(d_{\text {proj }}\right)$ coordinates.

The document runtime is similar, except with the additional complexity required to carry out the fill_empty_clusters option. For each repetition, the runtime required to find the closest $p \in P$ to a give cluster $k$ is $O\left(|P| \cdot k_{\text {sim }}\right)$, since we need to run over all $|p|$ values of $\varphi(p)$ and check how many bits disagree with $k$. Thus, the total runtime is $O\left(R_{\text {reps }}|P| B k_{\text {sim }}\right)=O\left(R_{\text {reps }}|P| 2^{k_{\text {sim }}} k_{\text {sim }}\right)$.

In what follows, we will need the following standard fact that random projections approximately preserve dot products. The proof is relatively standard, and can be found in [2], or see results on approximate matrix product [52] for more general bounds.

Fact A. 2 ([2]). Fix $\varepsilon, \delta>0$. For any $d \geqslant 1$ and $x, y \in \mathbb{R}^{d}$, let $S \in \mathbb{R}^{t \times d}$ by a matrix of independent entries distributed uniformly over $\{1,-1\}$, where $t=O\left(1 / \varepsilon^{2} \cdot \log \delta^{-1}\right)$. Then we have $\mathbb{E}[\langle S x, S y\rangle]=\langle x, y\rangle$, and moreover with probability at least $1-\delta$ we have

$$
|\langle S x, S y\rangle-\langle x, y\rangle| \leqslant \varepsilon\|x\|_{2}\|y\|_{2}
$$

To anaylze the approximations of our FDEs, we begin by proving an upper bound on the value of the FDE dot product. In fact, we prove a stronger result: we show that our FDEs have the desirable property of being one-sided estimators - namely, they never overestimate the true Chamfer similarity. This is summarized in the following Lemma.

Lemma A. 3 (One-Sided Error Estimator). Fix any sets $Q, P \subset \mathbb{R}^{d}$ of unit vectors with $|Q|+|P|=m$. Then if $d=d_{\text {proj }}$, we always have

$$
\frac{1}{|Q|}\left\langle\mathbf{F}_{q}(Q), \mathbf{F}_{d o c}(P)\right\rangle \leqslant \operatorname{NChAMFER}(Q, P)
$$

Furthermore, for any $\delta>0$, if we set $d_{\text {proj }}=O\left(\frac{1}{\varepsilon^{2}} \log (m / \delta)\right)$, then we have $\frac{1}{|Q|}\left\langle\mathbf{F}_{q}(Q), \mathbf{F}_{d o c}(P)\right\rangle \leqslant \operatorname{NChAMFER}(Q, P)+\varepsilon$ in expectation and with probability at least $1-\delta$.

Proof. First claim simply follows from the fact that the average of a subset of a set of numbers can't be bigger than the maximum number in that set. More formally, we have:

$$
\begin{align*}
\frac{1}{|Q|}\left\langle\mathbf{F}_{\mathbf{q}}(Q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle & =\frac{1}{|Q|} \sum_{k=1}^{B} \sum_{\substack{q \in Q \\
\varphi(q)=k}} \frac{1}{\left|P \cap \boldsymbol{\varphi}^{-1}(k)\right|} \sum_{\substack{p \in P \\
\varphi(p)=k}}\langle q, p\rangle \\
& \leqslant \frac{1}{|Q|} \sum_{k=1}^{B} \sum_{\substack{q \in Q \\
\varphi(q)=k}} \frac{1}{\left|P \cap \boldsymbol{\varphi}^{-1}(k)\right|} \sum_{\substack{p \in P \\
\varphi(p)=k}} \max _{p^{\prime} \in P}\left\langle q, p^{\prime}\right\rangle  \tag{4}\\
& =\frac{1}{|Q|} \sum_{k=1}^{B} \sum_{\substack{q \in Q \\
\varphi(q)=k}} \max _{p^{\prime} \in p}\langle q, p\rangle=\operatorname{NCHAMFER}(Q, P)
\end{align*}
$$

Which completes the first part of the lemma. For the second part, to analyze the case of $d_{\text {proj }}<d$, when inner random projections are used, by applying Fact A.2, firstly we have $\mathbb{E}[\langle\boldsymbol{\psi}(p), \boldsymbol{\psi}(q)]=$ $\langle q, p\rangle$ for any $q \in Q, p \in P$, and secondly, after a union bound we over $|P| \cdot|Q| \leqslant m^{2}$ pairs, we have $\langle q, p\rangle=\langle\boldsymbol{\psi}(p), \boldsymbol{\psi}(q)\rangle \pm \varepsilon$ simultaneously for all $q \in Q, p \in P$, with probability $1-\delta$, for any constant $C>1$. The second part of the Lemma then follows similarly as above.

We are now ready to give the proof of our main FDE approximation theorem.
Theorem 2.1 (FDE Approximation). Fix any $\varepsilon, \delta>0$, and sets $Q, P \subset \mathbb{R}^{d}$ of unit vectors, and let $m=|Q|+|P|$. Then setting $k_{\text {sim }}=O\left(\frac{\log \left(m \delta^{-1}\right)}{\varepsilon}\right), d_{\text {proj }}=O\left(\frac{1}{\varepsilon^{2}} \log \left(\frac{m}{\varepsilon \delta}\right)\right), R_{\text {reps }}=1$, so that $d_{F D E}=(m / \delta)^{O(1 / \varepsilon)}$, we have

$$
\operatorname{NChAmFER}(Q, P)-\varepsilon \leqslant \frac{1}{|Q|}\left\langle\mathbf{F}_{q}(Q), \mathbf{F}_{d o c}(P)\right\rangle \leqslant \operatorname{NChamFER}(Q, P)+\varepsilon
$$

in expectation, and with probability at least $1-\delta$.
Proof of Theorem 2.1. The upper bound follows from Lemma A.3, so it will suffice to prove the lower bound. We first prove the result in the case when there are no random projections $\psi$, and remove this assumption at the end of the proof. Note that, by construction, $\mathbf{F}_{\mathrm{q}}$ is a linear mapping so that $\mathbf{F}_{\mathrm{q}}(Q)=\sum_{q \in Q} \mathbf{F}(q)$, thus

$$
\left\langle\mathbf{F}_{\mathbf{q}}(Q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle=\sum_{q \in Q}\left\langle\mathbf{F}_{\mathbf{q}}(q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle
$$

So it will suffice to prove that

$$
\begin{equation*}
\operatorname{Pr}\left[\left\langle\mathbf{F}_{\mathrm{q}}(q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle \geqslant \max _{p \in P}\langle q, p\rangle-\varepsilon\right] \geqslant 1-\varepsilon \delta /|Q| \tag{5}
\end{equation*}
$$

for all $q \in Q$, since then, by a union bound 5 will hold for all over all $q \in Q$ with probability at least $1-\varepsilon \delta$, in which case we will have

$$
\begin{align*}
\frac{1}{|Q|}\left\langle\mathbf{F}_{\mathrm{q}}(Q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle & \geqslant \frac{1}{|Q|} \sum_{q \in Q}\left(\max _{p \in P}\langle q, p\rangle-\varepsilon\right)  \tag{6}\\
& =\operatorname{NChAMFER}(Q, P)-\varepsilon
\end{align*}
$$

which will complete the theorem.
In what follows, for any $x, y \in \mathbb{R}^{d}$ let $\theta(x, y) \in[0, \pi]$ be the angle between $x, y$. Now fix any $q \in Q$, and let $p^{*}=\arg \max _{p \in P}\langle q, p\rangle$, and let $\theta^{*}=\theta\left(q, p^{*}\right)$. By construction, there always exists some set of points $S \subset P$ such that

$$
\left\langle\mathbf{F}_{\mathrm{q}}(q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle=\left\langle q, \frac{1}{|S|} \sum_{p \in S} p\right\rangle
$$

Moreover, the RHS of the above equation is always bounded by 1 in magnitude, since it is an average of dot products of normalized vectors $q, p \in \mathcal{S}^{d-1}$. In particular, there are two cases. In case (A) $S$ is the set of points $p$ with $\varphi(p)=\varphi(q)$, and in case (B) $S$ is the single point $\arg \min _{p \in P}\|\varphi(p)-\varphi(q)\|_{0}$, where $\|x-y\|_{0}$ denotes the hamming distance between any two bitstrings $x, y \in\{0,1\}^{k_{\mathrm{sim}}}$, and we are interpreting $\boldsymbol{\varphi}(p), \boldsymbol{\varphi}(q) \in\{0,1\}^{k_{\mathrm{sim}}}$ as such bit-strings. Also let $g_{1}, \ldots, g_{k_{\text {sim }}} \in \mathbb{R}^{d}$ be the random Gaussian vectors that were drawn to define the partition function $\varphi$. To analyze $S$, we first prove the following:

Claim A.4. For any $q \in Q$ and $p \in P$, we have

$$
\operatorname{Pr}\left[\left|\|\varphi(p)-\varphi(q)\|_{0}-k_{\mathrm{sim}} \cdot \frac{\theta(q, p)}{\pi}\right|>\sqrt{\varepsilon} k_{\mathrm{sim}}\right] \leqslant\left(\frac{\varepsilon \delta}{m^{2}}\right)
$$

Proof. Fix any such $p$, and for $i \in\left[k_{\text {sim }}\right]$ let $Z_{i}$ be an indicator random variable that indicates the event that $\mathbf{1}\left(\left\langle g_{i}, p\right\rangle>0\right) \neq \mathbf{1}\left(\left\langle g_{i}, q\right\rangle>0\right)$. First then note that $\|\varphi(p)-\varphi(q)\|_{0}=$ $\sum_{i=1}^{k_{\text {sim }}} Z_{i}$. Now by rotational invariance of Gaussians, for a Gaussian vector $g \in \mathbb{R}^{d}$ we have $\operatorname{Pr}[\mathbf{1}(\langle g, x\rangle>0) \neq \mathbf{1}(\langle g, y\rangle>0)]=\frac{\theta(x, y)}{\pi}$ for any two vectors $x, y \in \mathbb{R}^{d}$. It follows that $Z_{i}$ is a Bernoulli random variable with $\mathbb{E}\left[Z_{i}\right]=\frac{\theta(x, y)}{\pi}$. By a simple application of Hoeffding's inequality, we have

$$
\begin{align*}
\operatorname{Pr}\left[\left|\|\boldsymbol{\varphi}(p)-\boldsymbol{\varphi}(q)\|_{0}-k_{\mathrm{sim}} \cdot \frac{\theta(q, p)}{\pi}\right|>\sqrt{\varepsilon} k_{\mathrm{sim}}\right] & =\operatorname{Pr}\left[\left|\sum_{i=1}^{k_{\mathrm{sim}}} Z_{i}-\mathbb{E}\left[\sum_{i=1}^{k_{\mathrm{sim}}} Z_{i}\right]\right|>\sqrt{\varepsilon} k_{\mathrm{sim}}\right] \\
& \leqslant \exp \left(-2 \varepsilon k_{\mathrm{sim}}\right) \\
& \leqslant\left(\frac{\varepsilon \delta}{m^{2}}\right) \tag{7}
\end{align*}
$$

where we took $k_{\text {sim }} \geqslant 1 / 2 \cdot \log \left(\frac{m^{2}}{\varepsilon \delta}\right) / \varepsilon$, which completes the proof.

We now condition on the event in Claim A. 4 occurring for all $p \in P$, which holds with probability at least $1-|P| \cdot\left(\frac{\varepsilon \delta}{m^{2}}\right)>1-\left(\frac{\varepsilon \delta}{m}\right)$ by a union bound. Call this event $\mathcal{E}$, and condition on it in what follows.

Now first suppose that we are in case (B), and the set $S$ of points which map to the cluster $\varphi(q)$ is given by $S=\left\{p^{\prime}\right\}$ where $p^{\prime}=\arg \min _{p \in P}\|\boldsymbol{\varphi}(p)-\boldsymbol{\varphi}(q)\|_{0}$. Firstly, if $p^{\prime}=p^{*}$, then we are done as $\left\langle\mathbf{F}_{\mathrm{q}}(q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle=\left\langle q, p^{*}\right\rangle$, and 5 follows. Otherwise, by Claim A. 4 we must have had $\left|\theta\left(q, p^{\prime}\right)-\theta\left(q, p^{*}\right)\right| \leqslant \pi \cdot \sqrt{\varepsilon}$. Using that the Taylor expansion of cosine is $\cos (x)=1-x^{2} / 2+O\left(x^{4}\right)$, we have

$$
\left|\cos \left(\theta\left(q, p^{\prime}\right)\right)-\cos \left(\theta\left(q, p^{*}\right)\right)\right| \leqslant O(\varepsilon)
$$

Thus

$$
\begin{align*}
\left\langle\mathbf{F}_{\mathrm{q}}(q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle & =\left\langle q, p^{\prime}\right\rangle \\
& =\cos \left(\theta\left(q, p^{\prime}\right)\right) \\
& \geqslant \cos \left(\theta\left(q, p^{*}\right)\right)-O(\varepsilon)  \tag{8}\\
& =\max _{p \in P}\langle q, p\rangle-O(\varepsilon)
\end{align*}
$$

which proves the desired statement 5 after a constant factor rescaling of $\varepsilon$.
Next, suppose we are in case (A) where $S=\left\{p \in P^{\prime} \mid \boldsymbol{\varphi}(p)=\varphi(q)\right\}$ is non-empty. In this case, $S$ consists of the set of points $p$ with $\|\varphi(p)-\varphi(q)\|_{0}=0$. From this, it follows again by Claim A. 4
that $\theta(q, p) \leqslant \sqrt{\varepsilon} \pi$ for any $p \in S$. Thus, by the same reasoning as above, we have

$$
\begin{align*}
\left\langle\mathbf{F}_{\mathrm{q}}(q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle & =\frac{1}{|S|} \sum_{p \in S} \cos \left(\theta\left(q, p^{\prime}\right)\right) \\
& \geqslant \frac{1}{|S|} \sum_{p \in S}(1-O(\varepsilon))  \tag{9}\\
& \geqslant \frac{1}{|S|} \sum_{p \in S}\left(\left\langle q, p^{*}\right\rangle-O(\varepsilon)\right) \\
& =\max _{p \in P}\langle q, p\rangle-O(\varepsilon)
\end{align*}
$$

which again proves the desired statement 5 in case (A), thereby completing the full proof in the case where there are no random projections.

To analyze the expectation, note that using the fact that $\left|\left\langle\mathbf{F}_{\mathbf{q}}(q), \mathbf{F}_{\text {doc }}(P)\right\rangle\right| \leqslant 1$ deterministically, the small $O(\varepsilon \delta)$ probability of failure (i.e. the event that $\mathcal{E}$ does not hold) above can introduce at most a $O(\varepsilon \delta) \leqslant \varepsilon$ additive error into the expectation, which is acceptable after a constant factor rescaling of $\varepsilon$.

Finally, to incorporate projections, by standard consequences of the Johnson Lindenstrauss Lemma (Fact A.2) setting $d_{\text {proj }}=O\left(\frac{1}{\varepsilon^{2}} \log \frac{m}{\varepsilon}\right)$ and projecting via a random Gaussian or $\pm 1$ matrix from $\boldsymbol{\psi}$ : $\mathbb{R}^{d} \rightarrow \mathbb{R}^{d_{\text {proj }}}$, for any set $S \subset P$ we have that $\mathbb{E}\left[\left\langle\boldsymbol{\psi}(q), \boldsymbol{\psi}\left(\frac{1}{|S|} \sum_{p \in S} p\right)\right\rangle\right]=\left\langle q, \frac{1}{|S|} \sum_{p \in S} p\right\rangle$, and moreover that $\left\langle q, \frac{1}{|S|} \sum_{p \in S} p\right\rangle=\left\langle\boldsymbol{\psi}(q), \boldsymbol{\psi}\left(\frac{1}{|S|} \sum_{p \in S} p\right)\right\rangle\|q\|_{2}\left\|\frac{1}{|S|} \sum_{p \in S} p\right\|_{2} \pm \varepsilon$ for all $q \in Q, p \in$ $P$ with probability at least $1-\varepsilon \delta$. Note that $\|q\|_{2}=1$, and by triangle inequality $\left\|\frac{1}{|S|} \sum_{p \in S} p\right\|_{2} \leqslant$ $\frac{1}{|S|} \sum_{p \in S}\|p\|_{2}=1$. Thus, letting $\mathbf{F}_{\mathrm{q}}(Q), \mathbf{F}_{\mathrm{doc}}(P)$ be the FDE values without the inner projection $\psi$ and $\mathbf{F}_{\mathrm{q}}^{\psi}(Q), \mathbf{F}_{\mathrm{doc}}^{\psi}(P)$ be the FDE values with the inner projection $\psi$, conditioned on the above it follows that

$$
\begin{align*}
\frac{1}{|Q|}\left\langle\mathbf{F}_{\mathrm{q}}^{\psi}(Q), \mathbf{F}_{\mathrm{doc}}^{\psi}(P)\right\rangle & =\frac{1}{|Q|} \sum_{q \in Q}\left\langle\mathbf{F}_{\mathrm{q}}^{\psi}(q), \mathbf{F}_{\mathrm{doc}}^{\psi}(P)\right\rangle \\
& =\frac{1}{|Q|} \sum_{q \in Q}\left(\left\langle\mathbf{F}_{\mathrm{q}}(q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle \pm \varepsilon\right)  \tag{10}\\
& =\frac{1}{|Q|}\left\langle\mathbf{F}_{\mathrm{q}}(Q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle \pm \varepsilon
\end{align*}
$$

Finally, to analyze the expectation, note that since

$$
\left|\frac{1}{|Q|}\left\langle\mathbf{F}_{\mathrm{q}}(Q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle\right| \leqslant \frac{1}{|Q|} \sum_{q \in Q}\left|\left\langle\mathbf{F}_{\mathrm{q}}(q), \mathbf{F}_{\mathrm{doc}}(P)\right\rangle\right| \leqslant 1
$$

as before conditioning on this small probability event changes the expectation of 5 by at most a $\varepsilon$ additive factor, which completes the proof of the Theorem after a constant factor rescaling of $\varepsilon$.

Equipped with Theorem 2.1, as well as the sparsity bounds from Lemma A.1, we are now prepared to prove our main theorem on approximate nearest neighbor search under the Chamfer Similarity.
Theorem 2.2. Fix any $\varepsilon>0$, query $Q$, and dataset $P=\left\{P_{1}, \ldots, P_{n}\right\}$, where $Q \subset \mathbb{R}^{d}$ and each $P_{i} \subset \mathbb{R}^{d}$ is a set of unit vectors. Let $m=|Q|+\max _{i \in[n]}\left|P_{i}\right|$. Then setting $k_{\text {sim }}=O\left(\frac{\log m}{\varepsilon}\right)$, $d_{\text {proj }}=O\left(\frac{1}{\varepsilon^{2}} \log (m / \varepsilon)\right)$ and $R_{\text {reps }}=O\left(\frac{1}{\varepsilon^{2}} \log n\right)$ so that $d_{F D E}=m^{O(1 / \varepsilon)} \cdot \log n$. Then setting $i^{*}=\arg \max _{i \in[n]}\left\langle\mathbf{F}_{q}(Q), \mathbf{F}_{d o c}\left(P_{i}\right)\right\rangle$, with high probability (i.e. $1-1 / \operatorname{poly}(n)$ ) we have:

$$
\operatorname{NChAmFER}\left(Q, P_{i^{*}}\right) \geqslant \max _{i \in[n]} \operatorname{NChAMFER}\left(Q, P_{i}\right)-\varepsilon
$$

Given the query $Q$, the document $P^{*}$ can be recovered in time $O\left(|Q| \max \{d, n\} \frac{1}{\varepsilon^{4}} \log \left(\frac{m}{\varepsilon}\right) \log n\right)$.

Proof of Theorem 2.2. First note, for a single repetition, for any subset $P_{j} \in D$, by Theorem 2.1 we have

$$
\mathbb{E}\left[\left\langle\mathbf{F}_{\mathrm{q}}(Q), \mathbf{F}_{\mathrm{doc}}\left(P_{j}\right)\right\rangle\right]=\operatorname{NChamFer}(Q, P) \pm \varepsilon
$$

Moreover, as demonsrated in the proof of Theorem 2.1, setting $\delta=1 / 10$, we have

$$
\left|\frac{1}{|Q|}\left\langle\mathbf{F}_{\mathrm{q}}(Q), \mathbf{F}_{\mathrm{doc}}\left(P_{j}\right)\right\rangle\right| \leqslant \frac{1}{|Q|} \sum_{q \in Q}\left|\left\langle\mathbf{F}_{\mathrm{q}}(q), \mathbf{F}_{\mathrm{doc}}\left(P_{j}\right)\right\rangle\right| \leqslant 1
$$

It follows that for each repetition $i \in\left[R_{\text {reps }}\right]$, letting $\mathbf{F}_{\mathbf{q}}(Q)^{i}, \mathbf{F}_{\text {doc }}\left(P_{j}\right)^{i}$ be the coordinates in the final FDE vectors coordeesponding to that repetition, the random variable $X_{i}=\frac{1}{|Q|}\left\langle\mathbf{F}_{\mathbf{q}}^{i}(Q), \mathbf{F}_{\mathrm{doc}}^{i}\left(P_{j}\right)\right\rangle$ is bounded in $[-1,1]$ and has expectation $\operatorname{NChamFer}\left(Q, P_{j}\right) \pm \varepsilon$. By Chernoff bounds, averaging over $R_{\text {reps }}=O\left(\frac{1}{\varepsilon^{2}} \log (n)\right)$ repetitions, we have

$$
\begin{equation*}
\left|\sum_{i=1}^{R_{\text {reps }}} \frac{1}{R_{\text {reps }}|Q|}\left\langle\mathbf{F}_{\mathrm{q}}^{i}(Q), \mathbf{F}_{\mathrm{doc}}^{i}\left(P_{j}\right)\right\rangle-\operatorname{NChAMFER}\left(Q, P_{j}\right)\right| \leqslant 2 \varepsilon \tag{11}
\end{equation*}
$$

with probability $1-1 / n^{C}$ for any arbitrarily large constant $C>1$. Note also that $\sum_{i=1}^{R_{\text {reps }}} \frac{1}{R_{\text {reps }}|Q|}\left\langle\mathbf{F}_{\mathrm{q}}^{i}(Q), \mathbf{F}_{\text {doc }}^{i}\left(P_{j}\right)\right\rangle=\frac{1}{R_{\text {reps }}|Q|}\left\langle\mathbf{F}_{\mathrm{q}}(Q), \mathbf{F}_{\text {doc }}\left(P_{j}\right)\right\rangle$, where $\mathbf{F}_{\mathrm{q}}(Q), \mathbf{F}_{\text {doc }}\left(P_{j}\right)$ are the final FDEs. We can then condition on (11) holding for all documents $j \in[n]$, which holds with probability with probability $1-1 / n^{C-1}$ by a union bound. Conditioned on this, we have

$$
\begin{align*}
\operatorname{NChAMFER}\left(Q, P_{i^{*}}\right) & \geqslant \frac{1}{R_{\mathrm{reps}}|Q|}\left\langle\mathbf{F}_{\mathrm{q}}(Q), \mathbf{F}_{\mathrm{doc}}\left(P_{i^{*}}\right)\right\rangle-2 \varepsilon \\
& =\max _{j \in[n]} \frac{1}{R_{\mathrm{reps}}|Q|}\left\langle\mathbf{F}_{\mathrm{q}}(Q), \mathbf{F}_{\mathrm{doc}}\left(P_{j}\right)\right\rangle-2 \varepsilon  \tag{12}\\
& \geqslant \max _{j \in[n]} \operatorname{NChAMFER}\left(Q, P_{j}\right)-6 \varepsilon
\end{align*}
$$

which completes the proof of the approximation after a constant factor scaling of $\varepsilon$. The runtime bound follows from the runtime required to compute $\mathbf{F}_{\mathrm{q}}(Q)$, which is $O\left(|Q| R_{\text {reps }} d\left(d_{\text {proj }}+k_{\text {sim }}\right)\right)=$ $O\left(|Q| \frac{\log n}{\varepsilon^{2}} d\left(\frac{1}{\varepsilon^{2}} \log (m / \varepsilon)+\frac{1}{\varepsilon} \log m\right)\right.$, plus the runtime required to brute force search for the nearest dot product. Specifically, note that each of the $n$ FDE dot products can be computed in time proportional to the sparsity of $\mathbf{F}_{\mathrm{q}}(Q)$, which is at most $O\left(|Q| d_{\text {proj }} R_{\text {reps }}\right)=O\left(|Q| \frac{1}{\varepsilon^{4}} \log (m / \varepsilon) \log n\right)$. Adding these two bounds together yields the desired runtime.

## B Additional Dataset Information

In Table 8 we provide further dataset-specific information on the BEIR retrieval datasets used in this paper. Specifically, we state the sizes of the query and corpuses used, as well as the average number of embeddings produced by the ColBERTv2 model per document. Specifically, we consider the six BEIR retrieval datasets MS MARCO [40], NQ [31], HotpotQA [53], ArguAna [47], SciDocs [11], and Quora [46], Note that the MV corpus (after generating MV embeddings on all documents in a corpus) will have a total of \#Corpus $\times$ (Avg \# Embeddings per Doc) token embeddings. For even further details, see the BEIR paper [46].

|                                     | MS MARCO | HotpotQA |   NQ   | Quora  | SciDocs | ArguAna |
| :---------------------------------: | :------: | :------: | :----: | :----: | :-----: | :-----: |
|              \#Queries              |  6,980   |  7,405   | 3,452  | 10,000 |  1,000  |  1,406  |
|              \#Corpus               |  8.84 M  |  5.23 M  | 2.68 M | 523 K  | 25.6 K  |  8.6 K  |
| Avg <br> \# Embeddings <br> per Doc |   78.8   |  68.65   | 100.3  | 18.28  | 165.05  | 154.72  |

Figure 8: Dataset Specific Statistics for the BEIR datasets considered in this paper.

## C Additional Experiments and Plots

In this Section, we provide additional plots to support the experimental results from Section 3. We providing plots for all six of the datasets and additional ranges of the $x$-axis for our experiments in Section (§3.1), as well as additional experimental results, such as an evaluation of variance, and of the quality of final projections in the FDEs.

FDE vs. SV Heuristic Experiments. In Figures 9 and 10, we show further datasets and an expanded recall range for the comparison of the SV Heuristic to retrieval via FDEs. We find that our $4 \mathrm{k}+$ dimensional FDE methods outperform even the deduplciated SV heuristic (whose cost is somewhat unrealistic, since the SV heuristic must over-retrieve to handle duplicates) on most datasets, especially in lower recall regimes. In Table 1, we compare how many candidates must be retrieved by the SV heuristic, both with and without the deduplciation step, as well as by our FDE methods, in order to exceed a given recall threshold.

| Recall <br> Threshold | SV non-dedup | SV dedup | 20k FDE | 10k FDE | 4k FDE | 2k FDE |
| :-------------------: | :----------: | :------: | :-----: | :-----: | :----: | :----: |
|        $80 \%$        |     1200     |   300    |   60    |   60    |   80   |  200   |
|        $85 \%$        |     2100     |   400    |   90    |   100   |  200   |  300   |
|        $90 \%$        |     4500     |   800    |   200   |   200   |  300   |  800   |
|        $95 \%$        |   $>10000$   |   2100   |   700   |   800   |  1200  |  5600  |

Table 1: FDE retrieval vs SV Heuristic: number of candidates that must be retrieved by each method to exceed a given recall on MS MARCO. The first two columns are for the SV non-deduplicated and deduplicated heuristics, respectively, and the remaining four columns are for the FDE retrieved candidates with FDE dimensions $\{20480,10240,4096,2048\}$, respectively. Recall@ $N$ values were computed in increments of 10 between 10-100, and in increments of 100 between 100-10000, and were not computed above $N>10000$.

Retrieval quality with respect to exact Chamfer. In Figure 11, we display the full plots for FDE Recall with respects to recovering the 1-nearest neighbor under Chamfer Similarity for all six BEIR datasets that we consider, including the two omitted from the main text (namely, SciDocs and ArguAna).

## C. 1 Variance of FDEs.

Since the FDE generation is a randomized process, one natural concern is whether there is large variance in the recall quality across different random seeds. Fortunately, we show that this is not the case, and the variance of the recall of FDE is essentially negligible, and can be easily accounted for via minor extra retrieval. To evaluate this, we chose four sets of FDE parameters ( $R_{\text {reps }}, k_{\text {sim }}, d_{\text {proj }}$ ) which were Pareto optimal for their respective dimensionalities, generated 10 independent copies of the query and document FDEs for the entire MS MARCO dataset, and computed the average recall@100 and 1000 and standard deviation of these recalls. The results are shown in Table 2, where for all of the experiments the standard deviation was between $0.08-0.3 \%$ of a recall point, compared to the $80-95 \%$ range of recall values. Note that Recall@1000 had roughly twice as small standard deviation as Recall@100.

| FDE params $\left(R_{\text {reps }}, k_{\text {sim }}, d_{\text {proj }}\right)$ | $(20,5,32)$ | $(20,5,16)$ | $(20,4,16)$ | $(20,4,8)$ |
| :----------------------------------------------------------: | :---------: | :---------: | :---------: | :--------: |
|                        FDE Dimension                         |    20480    |    10240    |    5120     |    2560    |
|                         Recall@ 100                          |    83.68    |    82.82    |    80.46    |   77.75    |
|                      Standard Deviation                      |    0.19     |    0.27     |    0.29     |    0.17    |
|                         Recall@ 1000                         |    95.37    |    94.88    |    93.67    |   91.85    |
|                      Standard Deviation                      |    0.08     |    0.11     |    0.16     |    0.12    |

Table 2: Variance of FDE Recall Quality on MS MARCO.
![](2025_01_02_dd29457f8dd190f63af6g-21.jpg)

Figure 9: FDE retrieval vs SV Heuristic, Recall@100-5000
![](2025_01_02_dd29457f8dd190f63af6g-21.jpg)

Figure 10: FDE retrieval vs SV Heuristic, Recall@5-500

|  Experiment  | w/o projection | w/ projection | w/o projection | w/ projection |
| :----------: | :------------: | :-----------: | :------------: | :-----------: |
|  Dimension   |      2460      |     2460      |      5120      |     5120      |
| Recall@ 100  |     77.71      |     78.82     |     80.37      |     83.35     |
| Recall@1000  |     91.91      |     91.62     |     93.55      |     94.83     |
| Recall@10000 |     97.52      |     96.64     |     98.07      |     98.33     |

Table 3: Recall Quality of Final Projection based FDEs with $d_{\text {FDE }} \in\{2460,5120\}$
![](2025_01_02_dd29457f8dd190f63af6g-22.jpg)

Figure 11: Comparison of FDE recall with respect to the most similar point under Chamfer.

|  Experiment  | w/o projection | w/ projection | w/o projection | w/ projection |
| :----------: | :------------: | :-----------: | :------------: | :-----------: |
|  Dimension   |     10240      |     10240     |     20480      |     20480     |
| Recall@ 100  |     82.31      |     85.15     |     83.36      |     86.00     |
| Recall@ 1000 |     94.91      |     95.68     |     95.58      |     95.95     |
| Recall@10000 |     98.76      |     98.93     |     98.95      |     99.17     |

Table 4: Recall Quality of Final Projection based FDEs with $d_{\text {FDE }} \in\{10240,20480\}$

## C. 2 Comparison to Final Projections.

We now show the effect of employing final projections to reduce the target dimensionality of the FDE's. For all experiments, the final projection $\psi^{\prime}$ is implemented in the same way as inner projections are: namely, via multiplication by a random $\pm 1$ matrix. We choose four target dimensions, $d_{\text {FDE }} \in\{2460,5120,10240,20480\}$, and choose the Pareto optimal parameters $\left(R_{\text {reps }}, k_{\text {sim }}, d_{\text {proj }}\right)$ from the grid search without final projections in Section 3.1, which are $(20,4,8),(20,5,8),(20,5,16),(20,5,32)$. We then build a large dimensional FDE with the parameters $\left(R_{\text {reps }}, k_{\text {sim }}, d_{\text {proj }}\right)=(40,6,128)$. Here, since $d=d_{\text {proj }}$, we do not use any inner productions when constructing the FDE. We then use a single random final projection to reduce the dimensionality of this FDE from $R_{\text {reps }} \cdot 2^{k_{\text {sim }}} \cdot d_{\text {proj }}=327680$ down to each of the above target dimensions $d_{\text {FDE }}$. The results are show in Tables 3 and 4. Notice that incorporating final projections can have a non-trivial impact on recall, especially for Recall@100, where it can increase by around $3 \%$. In particular, FDEs with the final projections are often better than FDEs with twice the dimensionality without final projections. The one exception is the 2460 -dimensional FDE, where the Recall@100 only improved by $1.1 \%$, and the Recall@1000 was actually lower bound $0.3 \%$.

## C. 3 Ball Carving

We now provide further details on the ball carving technique described in Section 3.2 that is used in our online experiments. Specifically, to improve rescoring latency, we reduce the number of query embeddings by a pre-clustering stage. Specifically, we group the queries $Q$ into clusters $C_{1}, \ldots, C_{k}$, set $c_{i}=\sum_{q \in C_{i}} q$ and $Q_{C}=\left\{c_{1}, \ldots, c_{k}\right\}$. Then, after retrieving a set of candidate documents with the FDEs, instead of rescoring via CHAMFER $(Q, P)$ for each candidate $P$, we rescore via CHAMFER $\left(Q_{C}, P\right)$, which runs in time $O\left(\left|Q_{C}\right| \cdot|P|\right)$, offering speed-ups when the number of clusters is small. Instead of fixing $k$, we perform a greedy ball-carving procedure to allow $k$ to adapt to $Q$. Specifically, given a threshold $\tau$, we select an arbitrary point $q \in Q$, cluster it with all other points $q^{\prime} \in Q$ with $\left\langle q, q^{\prime}\right\rangle \geqslant \tau$, remove the clustered points and repeat until all points are clustered.
![](2025_01_02_dd29457f8dd190f63af6g-23.jpg)

Figure 12: Plots showing the trade-off between the threshold used for ball carving and the end-to-end recall.
![](2025_01_02_dd29457f8dd190f63af6g-23.jpg)

Figure 13: Per-Core Re-ranking QPS versus Ball Carving Threshold, on MS MARCO dataset.

In Figure 12, we show the the trade-off between end-to-end Recall@ $k$ of MUVERA and the ball carving threshold used. Notice that for both $k=100$ and $k=1000$, the Recall curves flatten dramatically after a threshold of $\tau=0.6$, and for all datasets they are essentially flat after $\tau \geqslant 0.7$. Thus, for such thresholds we incur essentially no quality loss by the ball carving. For this reason, we choose the value of $\tau=0.7$ in our end-to-end experiments.

On the other hand, we show that ball-carving at this threshold of 0.7 gives non-trivial efficiency gains. Specifically, in Figure 13, we plot the per-core queries-per-second of re-ranking (i.e. computing CHAMFER $\left(Q_{C}, P\right)$ ) against varying ball carving thresholds for the MS MARCO dataset. For sequential re-ranking, ball carving at a $\tau=0.7$ threshold provides a $25 \%$ QPS improvement, and when re-ranking is being done in parallel (over all cores simultaneously) it yields a $20 \%$ QPS improvement. Moreover, with a threshold of $\tau=0.7$, there were an average of 5.9 clusters created per query on MS Marco. This reduces the number of embeddings per query by $5.4 \times$, down from the initial fixed setting of $|Q|=32$. This suggests that pre-clustering the queries before re-ranking gives non-trivial runtime improvements with negligible quality loss. This also suggests that a fixed setting of $|Q|=32$ query embeddings per model is likely excessive for MV similarity quality, and that fewer queries could achieve a similar performance.

## C. 4 Product Quantization

PQ Details We implemented our product quantizers using a simple "textbook" $k$-means based quantizer. Recall that $\mathrm{AH}-C-G$ means that each consecutive group of $G$ dimensions is represented by $C$ centers. We train the quantizer by: (1) taking for each group of dimensions the coordinates of a sample of at most 100,000 vectors from the dataset, and (2) running $k$-means on this sample using $k=C=256$ centers until convergence. Given a vector $x \in \mathbb{R}^{d}$, we can split $x$ into $d / G$ blocks of coordinates $x_{(1)}, \ldots, x_{(d / G)} \in \mathbb{R}^{G}$ each of size $G$. The block $x_{(i)}$ can be compressed by representing $x_{(i)}$ by the index of the centroid from the $i$-th group that is nearest to $x_{(i)}$. Since there are 256 centroids per group, each block $x_{(i)}$ can then be represented by a single byte.

Figure 14: Plots showing the QPS vs. Recall@100 for MUVERA on the BEIR datasets we evaluate in this paper. The different curves are obtained by using different PQ methods on 10240-dimensional FDEs.

Figure 15: Plots showing the QPS vs. Recall@ 1000 for MUVERA on the BEIR datasets we evaluate in this paper. The different curves are obtained by using different PQ methods on 10240-dimensional FDEs.

Results In Figures 14 and 15 we show the full set of results for our QPS experiments from Section 3.2 on all of the BEIR datasets that we evaluated in this paper. We include results for both Recall@100 (Figure 14) and Recall@1000 (Figure 15).

We find that PQ-256-8 is consistently the best performing PQ codec across all of the datasets that we tested. Not using PQ at all results in significantly worse results (worse by at least $5 \times$ compared to using PQ) at the same beam width for the beam; however, the recall loss due to using PQ-256-8 is minimal, and usually only a fraction of a percent. Since our retrieval engine works by over-retrieving with respect to the FDEs and then reranking using Chamfer similarity, the loss due to approximating the FDEs using PQ can be handled by simply over-retrieving slightly more candidates.

We also observe that the difference between different PQ codecs is much more pronounced in the lower-recall regime when searching for the top 1000 candidates for a query. For example, most of the plots in Figure 15 show significant stratification in the QPS achieved in lower recall regimes, with PQ-256-16 (the most compressed and memory-efficient format) usually outperforming all others; however, for achieving higher recall, $\mathrm{PQ}-256-16$ actually does much worse than slightly less compressed formats like PQ-256-8 and PQ-256-4.

