

## 2 Fixed Dimensional Encodings

We now describe our process for generating FDEs. Our transformation is reminiscent of the technique of probabilistic tree embeddings [1, 7, 10, 13], which can be used to transform a set of vectors into a single vector. For instance, they have been used to embed the Earth Mover's Distance into the $\ell_{1}$ metric [1, 10, 22, 24], and to embed the weight of a Euclidean MST of a set of vectors into the Hamming metric [9, 22, 23]. However, since we are working with inner products, which are not metrics, instead of $\ell_{p}$ distances, an alternative approach for our transformation will be needed.

---

The intuition behind our transformation is as follows. Hypothetically, for two MV representations $Q, P \subset \mathbb{R}^{d}$, if we knew the optimal mapping $\pi: Q \rightarrow P$ in which to match them, then we could create vectors $\vec{q}, \vec{p}$ by concatenating all the vectors in $Q$ and their corresponding images in $P$ together, so that $\langle\vec{q}, \vec{p}\rangle=\sum_{q \in Q}\langle q, \pi(q)\rangle=\operatorname{CHAMFER}(Q, P)$. However, since we do not know $\pi$ in advance, and since different query-document pairs have different optimal mappings, this simple concatenation clearly will not work. Instead, our goal is to find a randomized ordering over all the points in $\mathbb{R}^{d}$ so that, after clustering close points together, the dot product of any query-document pair $Q, P \subset \mathbb{R}^{d}$ concatenated into a single vector under this ordering will approximate the Chamfer similarity.

---

The first step is to partition the latent space $\mathbb{R}^{d}$ into $B$ clusters so that vectors that are closer are more are more likely to land in the same cluster. Let $\varphi: \mathbb{R}^{d} \rightarrow[B]$ be such a partition; $\varphi$ can be implemented via Locality Sensitive Hashing (LSH) [20], $k$-means, or other methods; we discuss choices for $\varphi$ later in this section. After partitioning via $\varphi$, the hope is that for each $q \in Q$, the closest $p \in P$ lands in the same cluster (i.e. $\varphi(q)=\varphi(p)$ ). Hypothetically, if this were to occur, then:
$$
\begin{equation*}
\operatorname{CHAMFER}(Q, P)=\sum_{k=1}^{B} \sum_{\substack{q \in Q \\ \varphi(q)=k}} \max _{\substack{p \in P \\ \varphi(p)=k}}\langle q, p\rangle \tag{1}
\end{equation*}
$$

If $p$ is the only point in $P$ that collides with $q$, then (1) can be realized as a dot product between two vectors $\vec{q}, \vec{p}$ by creating one block of $d$ coordinates in $\vec{q}, \vec{p}$ for each cluster $k \in[B]$ (call these blocks $\vec{q}_{(k)}, \vec{p}_{(k)} \in \mathbb{R}^{d}$ ), and setting $\vec{q}_{(k)}, \vec{p}_{(k)}$ to be the sum of all $q \in Q$ (resp. $p \in P$ ) that land in the $k$-th cluster under $\varphi$. However, if multiple $p^{\prime} \in P$ collide with $q$, then $\langle\vec{q}, \vec{p}\rangle$ will differ from (1), since every $p^{\prime}$ with $\boldsymbol{\varphi}\left(p^{\prime}\right)=\boldsymbol{\varphi}(q)$ will contribute at least $\left\langle q, p^{\prime}\right\rangle$ to $\langle\vec{q}, \vec{p}\rangle$. To resolve this, we set $\vec{p}_{(k)}$ to be the centroid of the $p \in P$ 's with $\varphi(p)=\varphi(q)$. Formally, for $k=1, \ldots, B$, we define

$$
\begin{equation*}
\vec{q}_{(k)}=\sum_{\substack{q \in Q \\ \varphi(q)=k}} q, \quad \vec{p}_{(k)}=\frac{1}{\left|P \cap \boldsymbol{\varphi}^{-1}(k)\right|} \sum_{\substack{p \in P \\ \varphi(p)=k}} p \tag{2}
\end{equation*}
$$

Setting $\vec{q}=\left(\vec{q}_{(1)}, \ldots, \vec{q}_{(B)}\right)$ and $\vec{p}=\left(\vec{p}_{(1)}, \ldots, \vec{p}_{(B)}\right)$, then we have

$$
\begin{equation*}
\langle\vec{q}, \vec{p}\rangle=\sum_{k=1}^{B} \sum_{\substack{q \in Q \\ \varphi(q)=k}} \frac{1}{\left|P \cap \varphi^{-1}(k)\right|} \sum_{\substack{p \in P \\ \varphi(p)=k}}\langle q, p\rangle \tag{3}
\end{equation*}
$$

---

Note that the resulting dimension of the vectors $\vec{q}, \vec{p}$ is $d B$. To reduce the dependency on $d$, we can apply a random linear projection $\psi: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d_{\text {proj }}}$ to each block $\vec{q}_{(k)}, \vec{p}_{(k)}$, where $d_{\text {proj }}<d$. Specifically, we define $\boldsymbol{\psi}(x)=\left(1 / \sqrt{d_{\text {proj }}}\right) S x$, where $S \in \mathbb{R}^{d_{\text {proj }} \times d}$ is a random matrix with uniformly distributed $\pm 1$ entries. We can then define $\vec{q}_{(k), \boldsymbol{\psi}}=\boldsymbol{\psi}\left(\vec{q}_{(k)}\right)$ and $\vec{p}_{(k), \boldsymbol{\psi}}=\boldsymbol{\psi}\left(\vec{p}_{(k)}\right)$, and define the FDE's with inner projection as $\vec{q}_{\boldsymbol{\psi}}=\left(\vec{q}_{(1), \boldsymbol{\psi}}, \ldots, \vec{q}_{(B), \boldsymbol{\psi}}\right)$ and $\vec{p}_{\boldsymbol{\psi}}=\left(\vec{p}_{(1), \boldsymbol{\psi}}, \ldots, \vec{p}_{(B), \boldsymbol{\psi}}\right)$. When $d=d_{\text {proj }}$, we simply define $\psi$ to be the identity mapping, in which case $\vec{q}_{\psi}, \vec{p}_{\psi}$ are identical to $\vec{q}, \vec{p}$. To increase accuracy of (3) in approximating (1), we repeat the above process $R_{\text {reps }} \geqslant 1$ times independently, using different randomized partitions $\varphi_{1}, \ldots, \boldsymbol{\varphi}_{R_{\text {reps }}}$ and projections $\boldsymbol{\psi}_{1}, \ldots, \boldsymbol{\psi}_{R_{\text {reps }}}$. We denote the vectors resulting from $i$-th repetition by $\vec{q}_{i, \psi}, \vec{p}_{i, \psi}$. Finally, we concatenate these $R_{\text {reps }}$ vectors together, so that our final FDEs are defined as $\mathbf{F}_{\mathrm{q}}(Q)=\left(\vec{q}_{1, \psi}, \ldots, \vec{q}_{R_{\text {reps }}, \psi}\right)$ and $\mathbf{F}_{\mathrm{doc}}(P)=\left(\vec{p}_{1, \psi}, \ldots, \vec{p}_{R_{\text {ress }}, \psi}\right)$. Observe that a complete FDE mapping is specified by the three parameters $\left(B, d_{\mathrm{proj}}, R_{\mathrm{reps}}\right)$, resulting in a final dimension of $d_{\text {FDE }}=B \cdot d_{\mathrm{proj}} \cdot R_{\text {reps }}$.

---

**Choice of Space Partition.** When choosing the partition function $\varphi$, the desired property is that points are more likely to collide (i.e. $\varphi(x)=\varphi(y)$ ) the closer they are to each other. Such functions with this property exist, and are known as locality-sensitive hash functions (LSH) (see [20]). When the vectors are normalized, as they are for those produced by ColBERT-style models, SimHash [8] is the standard choice of LSH. Specifically, for any $k_{\text {sim }} \geqslant 1$, we sample random Gaussian vectors $g_{1}, \ldots, g_{k_{\text {sim }}} \in \mathbb{R}^{d}$, and set $\varphi(x)=\left(\mathbf{1}\left(\left\langle g_{1}, x\right\rangle>0\right), \ldots, \mathbf{1}\left(\left\langle g_{k_{\text {sim }}}, x\right\rangle>0\right)\right)$, where $\mathbf{1}(\cdot) \in\{0,1\}$ is the indicator function. Converting the bit-string to decimal, $\varphi(x)$ gives a mapping from $\mathbb{R}^{d}$ to $[B]$ where $B=2^{k_{\mathrm{sim}}}$. In other words, SimHash partitions $\mathbb{R}^{d}$ by drawing $k_{\mathrm{sim}}$ random half-spaces, and each of the the $2^{k_{\mathrm{sim}}}$ clusters is formed by the $k_{\mathrm{sim}}$-wise intersection of each of these halfspaces or their complement. Another natural approach is to choose $k_{\text {CENTER }} \geqslant 1$ centers from the collection of all token embeddings $\cup_{i=1}^{n} P_{i}$, either randomly or via $k$-means, and set $\varphi(x) \in\left[k_{\text {CENTER }}\right]$ to be the index of the center nearest to $x$. We compare this method to SimHash in (ยง3.1).

---

**Filling Empty Clusters.** A key source of error in the FDE's approximation is when the nearest vector $p \in P$ to a given query embedding $q \in Q$ maps to a different cluster, namely $\varphi(p) \neq \varphi(q)=k$. This can be made less likely by decreasing $B$, at the cost of making it more likely for other $p^{\prime} \in P$ to also map to the same cluster, moving the centroid $\vec{p}_{(k)}$ farther from $p$. If we increase $B$ too much, it is possible that no $p \in P$ collides with $\varphi(q)$. To avoid this trade-off, we directly ensure that if no $p \in P$ maps to a cluster $k$, then instead of setting $\vec{p}_{(k)}=0$ we set $\vec{p}_{(k)}$ to the point $p$ that is closest to cluster $k$. As a result, increasing $B$ will result in a more accurate estimator, as this results in smaller clusters. Formally, for any cluster $k$ with $P \cap \varphi^{-1}(k)=\emptyset$, if fill_empty_clusters is enabled, we set $\vec{p}_{(k)}=p$ where $p \in P$ is the point for which $\varphi(p)$ has the fewest number of disagreeing bits with $k$ (both thought of as binary strings), with ties broken arbitrarily. We do not enable this for query FDEs, as doing so would result in a given $q \in Q$ contributing to the dot product multiple times.

---

**Final Projections.** A natural approach to reducing the dimensionality is to apply a final projection $\psi^{\prime}: \mathbb{R}^{d_{\text {FDE }}} \rightarrow \mathbb{R}^{d_{\text {final }}}$ (also implemented via multiplication by a random $\pm 1$ matrix) to the FDE's, reducing the final dimensionality to any $d_{\text {final }}<d_{\text {FDE }}$. Experimentally, we find that final projections can provides small but non-trivial 1-2\% recall boosts for a fixed dimension (see ยงC.2).

### 2.1 Theoretical Guarantees for FDEs

We now state our theoretical guarantees for our FDE construction. For clarity, we state our results in terms of normalized Chamfer similarity $\operatorname{NChamFer}(Q, P)=\frac{1}{|Q|} \operatorname{ChamFER}(Q, P)$. This ensures NChamfer $(Q, P) \in[-1,1]$ whenever the vectors in $Q, P$ are normalized. Note that this factor of $1 /|Q|$ does not affect the relative scoring of documents for a fixed query. In what follows, we assume that all token embeddings are normalized (i.e. $\|q\|_{2}=\|p\|_{2}=1$ for all $q \in Q, p \in P$ ). Note that ColBERT-style late interaction MV models indeed produce normalized token embeddings. We will always use the fill_empty _clusters method for document FDEs, but never for queries.

---

Our main result is that FDEs give $\varepsilon$-additive approximations of the Chamfer similarity. The proof uses the properties of LSH (SimHash) to show that for each query point $q \in Q$, the point $q$ gets mapped to a cluster $\varphi(q)$ that only contains points $p \in P$ that are close to $q$ (within $\varepsilon$ of the closest point to $q$ ); the fact that at least one point collides with $q$ uses the fill_empty_partitions method.

Theorem 2.1 (FDE Approximation). Fix any $\varepsilon, \delta>0$, and sets $Q, P \subset \mathbb{R}^{d}$ of unit vectors, and let $m=|Q|+|P|$. Then setting $k_{\text {sim }}=O\left(\frac{\log \left(m \delta^{-1}\right)}{\varepsilon}\right), d_{\text {proj }}=O\left(\frac{1}{\varepsilon^{2}} \log \left(\frac{m}{\varepsilon \delta}\right)\right), R_{\text {reps }}=1$, so that $d_{F D E}=(m / \delta)^{O(1 / \varepsilon)}$, then in expectation and with probability at least $1-\delta$ we have

$$
\operatorname{NChAmFER}(Q, P)-\varepsilon \leqslant \frac{1}{|Q|}\left\langle\mathbf{F}_{q}(Q), \mathbf{F}_{d o c}(P)\right\rangle \leqslant \operatorname{NChAmFER}(Q, P)+\varepsilon
$$

---

Finally, we show that our FDE's give an $\varepsilon$-approximate solution to Chamfer similarity search, using FDE dimension that depends only logarithmically on the size of the dataset $n$. Using the fact that our query FDEs are sparse (Lemma A.1), one can run exact MIPS over the FDEs in time $\tilde{O}(|Q| \cdot n)$, improving on the brute-force runtime of $O\left(|Q| \max _{i}\left|P_{i}\right| n\right)$ for Chamfer similarity search.

Theorem 2.2. Fix any $\varepsilon>0$, query $Q$, and dataset $P=\left\{P_{1}, \ldots, P_{n}\right\}$, where $Q \subset \mathbb{R}^{d}$ and each $P_{i} \subset \mathbb{R}^{d}$ is a set of unit vectors. Let $m=|Q|+\max _{i \in[n]}\left|P_{i}\right|$. Let $k_{\text {sim }}=O\left(\frac{\log m}{\varepsilon}\right)$, $d_{\text {proj }}=O\left(\frac{1}{\varepsilon^{2}} \log (m / \varepsilon)\right)$ and $R_{\text {reps }}=O\left(\frac{1}{\varepsilon^{2}} \log n\right)$ so that $d_{F D E}=m^{O(1 / \varepsilon)} \cdot \log n$. Then if $i^{*}=\arg \max _{i \in[n]}\left\langle\mathbf{F}_{q}(Q), \mathbf{F}_{\text {doc }}\left(P_{i}\right)\right\rangle$, with high probability (i.e. $\left.1-1 / \operatorname{poly}(n)\right)$ we have:

$$
\operatorname{NChAmFER}\left(Q, P_{i^{*}}\right) \geqslant \max _{i \in[n]} \operatorname{NChAmFER}\left(Q, P_{i}\right)-\varepsilon
$$

Given the query $Q$, the document $P^{*}$ can be recovered in time $O\left(|Q| \max \{d, n\} \frac{1}{\varepsilon^{4}} \log \left(\frac{m}{\varepsilon}\right) \log n\right)$.