

## 3 Evaluation

In this section, we evaluate our FDEs as a method for MV retrieval. First, we evaluate the FDEs themselves (offline) as a proxy for Chamfer similarity (§3.1). In (§3.2), we discuss the implementation of Muvera, as well as several optimizations made in the search. Then we evaluate the latency of MUVERA compared to PLAID, and study the effects of the aforementioned optimizations.

---

**Datasets.** Our evaluation includes results from six of the well-studied BEIR [46] information retrieval datasets: MS MARCO [40] (CC BY-SA 4.0), HotpotQA (CC BY-SA 4.0) [53], NQ (Apache-2.0) [31], Quora (Apache-2.0) [46], SciDocs (CC BY 4.0) [11], and ArguAna (Apache-2.0) [47]. These datasets were selected for varying corpus size ( $8 \mathrm{~K}-8.8 \mathrm{M}$ ) and average number of document tokens (18-165); see (§B) for further dataset statistics. Following [43], we use the development set for our experiments on MS MARCO, and use the test set on the other datasets.

---

**MV Model, MV Embedding Sizes, and FDE Dimensionality.** We compute our FDEs on the MV embeddings produced by the ColBERTv2 model [44] (MIT License), which have a dimension of $d=128$ and a fixed number $|Q|=32$ of embeddings per query. The number of document embeddings is variable, ranging from an average of 18.3 on Quora to 165 on Scidocs. This results in 2,300-21,000 floats per document on average (e.g. 10,087 for MS MARCO). Thus, when constructing our FDEs we consider a comparable range of dimensions $d_{\text {FDE }}$ between $1,000-20,000$. Furthermore, using product quantization, we show in (§3.2) that the FDEs can be significantly compressed by $32 \times$ with minimal quality loss, further increasing the practicality of FDEs.

### 3.1 Offline Evaluation of FDE Quality

We evaluate the quality of our FDEs as a proxy for the Chamfer similarity, without any re-ranking and using exact (offline) search. We first demonstrate that FDE recall quality improves dependably as the dimension $d_{\text {FDE }}$ increases, making our method relatively easy to tune. We then show that FDEs are a more effective method of retrieval than the SV heuristic. Specifically, the FDE method achieves Recall@ $N$ exceeding the Recall@2-4N of the SV heuristic, while in principle scanning a similar number of floats in the search. This suggests that the success of the SV heuristic is largely due to the significant effort put towards optimizing it (as supported by [37]), and similar effort for FDEs may result in even bigger efficiency gains. Additional plots can be found in (§C). All recall curves use a single FDE instantiation, since in (§C.1) we show the variance of FDE recall is negligible.

---

**FDE Quality vs. Dimensionality.** We study how the retrieval quality of FDE's improves as a function of the dimension $d_{\text {FDE }}$. We perform a grid search over FDE parameters $R_{\text {reps }} \in\{1,5,10,15,20\}, k_{\text {sim }} \in\{2,3,4,5,6\}, d_{\text {proj }} \in\{8,16,32,64\}$, and compute recall on MS MARCO (Figure 3). We find that Pareto optimal parameters are generally achieved by larger $R_{\text {reps }}$, with $k_{\text {sim }}, d_{\text {proj }}$ playing a lesser role in improving quality. Specifically, $\left(R_{\text {reps }}, k_{\text {sim }}, d_{\text {proj }}\right) \in\{(20,3,8),(20,4,8)(20,5,8),(20,5,16)\}$ were all Pareto optimal for their respective dimensions (namely $R_{\text {reps }} \cdot 2^{k_{\text {sim }}} \cdot d_{\text {proj }}$ ). While there are small variations depending on the parameter choice, the FDE quality is tightly linked to dimensionality; increase in dimensionality will generally result in quality gains. We also evaluate using $k$-means as a method of partitioning instead of SimHash. Specifically, we cluster the document embeddings with $k$-means and set $\varphi(x)$ to be the index of the nearest centroid to $x$. We perform a grid search over the same parameters (but with $k \in\{4,8,16,32,64\}$ to match $B=2^{k_{\text {sim }}}$ ). We find that $k$-means partitioning offers no quality gains on the Pareto Frontier over SimHash, and is often worse. Moreover, FDE construction with $k$-means is no longer data oblivious. Thus, SimHash is chosen as the preferred method for partitioning for the remainder of our experiments.

---

In Figure 4, we evaluate the FDE retrieval quality with respect to the Chamfer similarity (instead of labelled ground truth data). We compute 1Recall@ $N$, which is the fraction of queries for which the Chamfer 1-nearest neighbor is among the top- $N$ most similar in FDE dot product. We choose FDE parameters which are Pareto optimal for the dimension from the above grid search. We find that FDE's with fewer dimensions that the original MV representations achieve significantly good recall across multiple BEIR retrieval datasets. For instance, on MS MARCO (where $d \cdot m_{\text {avg }} \approx 10 \mathrm{~K}$ ) we achieve $95 \%$ recall while retrieving only 75 candidates using $d_{\mathrm{FDE}}=5120$.

---

**Single Vector Heuristic vs. FDE retrieval.** We compare the quality of FDEs as a proxy for retrieval against the previously described SV heuristic, which is the method underpinning PLAID. Recall that in this method, for each of the $i=1, \ldots, 32$ query vectors $q_{i}$ we compute the $k$ nearest neighbors $p_{1, i}, \ldots, p_{k, i}$ from the set $\cup_{i} P_{i}$ of all documents token embeddings. To compute Recall $@ N$, we create an ordered list $\ell_{1,1}, \ldots, \ell_{1,32}, \ell_{2,1}, \ldots$, where $\ell_{i, j}$ is the document ID containing $p_{i, j}$, consisting of the 1-nearest neighbors of the queries, then the 2-nearest neighbors, and so on. When re-ranking, one firsts removes duplicate document IDs from this list. Since duplicates cannot be detected while performing the initial 32 SV MIPS queries, the SV heuristic needs to over-retrieve to reach a desired number of unique candidates. Thus, we note that the true recall curve of implementations of the SV heuristic (e.g. PLAID) is somewhere between the case of no deduplication and full deduplication; we compare to both in Figure 5.

---

To compare the cost of the SV heuristic to running MIPS over the FDEs, we consider the total number of floats scanned by both using a brute force search. The FDE method must scan $n \cdot d_{\text {FDE }}$ floats to compute the $k$-nearest neighbors. For the SV heuristic, one runs 32 brute force scans over $n \cdot m_{\text {avg }}$ vectors in 128 dimensions, where $m_{\text {avg }}$ is the average number embeddings per document (see $\S$ В for values of $m_{a v g}$ ). For MS MARCO, where $m_{a v g}=78.8$, the SV heuristic searches through $32 \cdot 128 \cdot 78.8 \cdot n$ floats. This allows for an FDE dimension of $d_{\text {FDE }}=322,764$ to have comparable cost! We can extend this comparison to fast approximate search - suppose that approximate MIPS over $n$ vectors can be accomplished in sublinear $n^{\varepsilon}$ time, for some $\varepsilon \in(0,1)$. Then even in the unrealistic case of $\varepsilon=0$, we can still afford an FDE dimension of $d_{\text {FDE }}=32 \cdot 128=4096$.

---

The results can be found in Figure 5. We build FDEs once for each dimension, using $R_{\text {reps }}=$ $40, k_{\text {sim }}=6, d_{\text {proj }}=d=128$, and then applying a final projection to reduce to the target dimension (see C. 2 for experiments on the impact of final projections). On MS MARCO, even the 4096dimensional FDEs match the recall of the (deduplicated) SV heuristic while retrieving 1.75-3.75× fewer candidates (our Recall@ $N$ matches the Recall@1.75-3.75 $N$ of the SV heuristic), and 10.5-15× fewer than to the non-deduplicated SV heuristic. For our 10240-dimension FDEs, these numbers are $2.6-5 \times$ and 20-22.5 $\times$ fewer, respectively. For instance, we achieve $80 \%$ recall with 60 candidates when $d_{\mathrm{FDE}}=10240$ and 80 candidates when $d_{\mathrm{FDE}}=4096$, but the SV heuristic requires 300 and 1200 candidates (for dedup and non-dedup respectively). See Table 1 for further comparisons.

---

**Variance.** Note that although the FDE generation is a randomized process, we show in (§C.1) that the variance of the FDE Recall is essentially negligible; for instance, the standard deviation Recall@1000 is at most $0.08-0.16 \%$ for FDEs with $2-10 k$ dimensions.

### 3.2 Online Implementation and End-to-End Evaluation

We implemented Muvera, an FDE generation and end-to-end retrieval engine in C++. We discussed FDE generation and various optimizations and their tradeoffs in (§3.1). Next, we discuss how we perform retrieval over the FDEs, and additional optimizations.

**Single-Vector MIPS Retrieval using DiskANN** Our single-vector retrieval engine uses a scalable implementation [38] of DiskANN [25] (MIT License), a state-of-the-art graph-based ANNS algorithm. We build DiskANN indices by using the uncompressed document FDEs with a maximum degree of 200 and a build beam-width of 600 . Our retrieval works by querying the DiskANN index using beam search with beam-width $W$, and subsequently reranking the retrieved candidates with Chamfer similarity. The only tuning knob in our system is $W$; increasing $W$ increases the number of candidates retrieved by MUVERA, which improves the recall.

**Ball Carving.** To improve re-ranking speed, we reduce the number of query embeddings by clustering them via a ball carving method and replacing the embeddings in each cluster with their sum. This speeds up reranking without decreasing recall; we provide further details in (§C.3).

---

**Product Quantization (PQ).** To further improve the memory usage of MUVERA, we use a textbook vector compression technique called product quantization (PQ) with asymmetric querying [19, 26] on the FDEs. We refer to product quantization with $C$ centers per group of $G$ dimensions as PQ-C-G. For example, PQ-256-8, which we find to provide the best tradeoff between quality and compression in our experiments, compresses every consecutive set of 8 dimensions to one of 256 centers. Thus PQ-256-8 provides $32 \times$ compression over storing each dimension using a single float, since each block of 8 floats is represented by a single byte. See (§C.4) for further experiments and details on PQ.

**Experimental Setup** We run our online experiments on an Intel Sapphire Rapids machine on Google Cloud (c3-standard-176). The machine supports up to 176 hyper-threads. We run latency experiments using a single thread, and run our QPS experiments on all 176 threads.

---

**QPS vs. Recall** A useful metric for retrieval is the number of queries per second ( $Q P S$ ) a system can serve at a given recall; evaluating the QPS of a system tries to fully utilize the system resources (e.g., the bandwidth of multiple memory channels and caches), and deployments where machines serve many queries simultaneously. Figure 6 shows the QPS vs. Recall@ 100 for MUVERA on a subset of the BEIR datasets, using different PQ schemes over the FDEs. We show results for additional datasets, as well as Recall@1000, in the Appendix. Using PQ-256-8 not only reduces the space usage of the FDEs by $32 \times$, but also improves the QPS at the same query beamwidth by up to $20 \times$, while incurring a minimal loss in end-to-end recall. Our method has a relatively small dependence on the dataset size, which is consistent with prior studies on graph-based ANNS data structures, since the number of distance comparisons made during beam search grows roughly logarithmically with increasing dataset size [25, 38]. We tried to include QPS numbers for PLAID [43], but unfortunately their implementation does not support running multiple queries in parallel, and is optimized for measuring latency.

---

**Latency and Recall Results vs. PLAID** [43] We evaluated Muvera and PLAID [43] on the 6 datasets from the BEIR benchmark described earlier in (§3); Figure 7 shows that MUVERA achieves essentially equivalent Recall@ $k$ as PLAID (within $0.4 \%$ ) on MS MARCO, while obtaining up to $1.56 \times$ higher recall on other datasets (e.g. HotpotQA). We ran PLAID using the recommended settings for their system, which reproduced their recall results for MS MARCO. Compared with PLAID, on average over all 6 datasets and $k \in\{100,1000\}$, MUVERA achieves $10 \%$ higher Recall@ $k$ (up to $56 \%$ higher), and $90 \%$ lower latency (up to $5.7 \times$ lower).

Importantly, MUVERA has consistently high recall and low latency across all of the datasets that we measure, and our method does not require costly parameter tuning to achieve this-all of our results
use the same 10240-dimensional FDEs that are compressed using PQ with PQ-256-8; the only tuning in our system was to pick the first query beam-width over the $k$ that we rerank to that obtained recall matching that of PLAID. As Figure 7 shows, in cases like NQ and HotpotQA, MUVERA obtains much higher recall while obtaining lower latency. Given these results, we believe a distinguishing feature of MUVERA compared to prior multi-vector retrieval systems is that it achieves consistently high recall and low latency across a wide variety of datasets with minimal tuning effort.

## 4 Conclusion

In this paper, we presented MUVERA: a principled and practical MV retrieval algorithm which reduces MV similarity to SV similarity by constructing Fixed Dimensional Encoding (FDEs) of a MV representation. We prove that FDE dot products give high-quality approximations to Chamfer similarity (§2.1). Experimentally, we show that FDEs are a much more effective proxy for MV similarity, since they require retrieving $2-4 \times$ fewer candidates to achieve the same recall as the SV Heuristic (§3.1). We complement these results with an end-to-end evaluation of MUVERA, showing that it achieves an average of $10 \%$ improved recall with $90 \%$ lower latency compared with PLAID. Moreover, despite the extensive optimizations made by PLAID to the SV Heuristic, we still achieve significantly better latency on 5 out of 6 BEIR datasets we consider (§3). Given their retrieval efficiency compared to the SV heuristic, we believe that there are still significant gains to be obtained by optimizing the FDE method, and leave further exploration of this to future work.

---

Broader Impacts and Limitations: While retrieval is an important component of LLMs, which themselves have broader societal impacts, these impacts are unlikely to result from our retrieval algorithm. Our contribution simply improves the efficiency of retrieval, without enabling any fundamentally new capabilities. As for limitations, while we outperformed PLAID, sometimes significantly, on 5 out of the 6 datasets we studied, we did not outperform PLAID on MS MARCO, possibly due to their system having been carefully tuned for MS MARCO given its prevalence. Additionally, we did not study the effect that the average number of embeddings $m_{\text {avg }}$ per document has on retrieval quality of FDEs; this is an interesting direction for future work.

