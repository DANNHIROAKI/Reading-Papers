#### iTransformer：倒置的Transformers在时间序列预测中表现优异

## 摘要

近年来，线性预测模型的兴起引发了对Transformer架构修改的持续热情的质疑。这些预测模型利用Transformer来建模时间序列中时间标记之间的全局依赖关系，每个时间标记由同一时间戳的多个变量组成。然而，Transformer在预测具有较大回看窗口的时间序列时面临挑战，主要由于性能下降和计算爆炸。此外，每个时间标记的嵌入融合了多个变量，这些变量可能代表潜在延迟事件和不同的物理测量值，这可能导致无法学习到以变量为中心的表示，并产生无意义的注意力图。在本研究中，我们反思了Transformer组件的职责，并在不修改基本组件的情况下重新设计了Transformer架构。我们提出了iTransformer，它简单地将注意力和前馈网络应用于倒置的维度。具体来说，将各个时间点嵌入为变量标记，这些标记被注意力机制用于捕捉多变量相关性；同时，前馈网络应用于每个变量标记以学习非线性表示。iTransformer模型在具有挑战性的现实世界数据集上达到了最先进的性能，进一步提升了Transformer家族的预测性能、跨变量的泛化能力以及对任意回看窗口的更好利用，使其成为时间序列预测基础模型的一个优秀替代品。代码可在以下仓库获取：https://github.com/thuml/iTransformer。

# 1 引言

Transformer（Vaswani 等，2017）在自然语言处理（Brown 等，2020）和计算机视觉（Dosovitskiy 等，2021）领域取得了巨大成功，并逐渐成为遵循扩展定律（Kaplan 等，2020）的基础模型。受其广泛领域成功的启发，Transformer 以其强大的描述序列中成对依赖关系和多层次表示提取能力，开始在时间序列预测中崭露头角（Wu 等，2021，Nie 等，2023）。

---

然而，研究人员最近开始质疑基于 Transformer 的预测模型的有效性。这些模型通常将同一时间戳的多个变量嵌入到不可区分的通道中，并在这些时间标记上应用注意力机制以捕捉时间依赖关系。考虑到时间点之间更多的是数值关系而非语义关系，研究人员发现，可以追溯到统计预测模型（Box & Jenkins，1968）的简单线性层，在性能和效率上都超过了复杂的 Transformer（Zeng 等，2023，Das 等，2023）。与此同时，确保变量的独立性并利用互信息的重要性在最近的研究中愈发凸显，这些研究通过显式建模多变量相关性来实现准确预测（Zhang & Yan，2023，Ekambaram 等，2023），但这一目标在不颠覆传统 Transformer 架构的情况下很难实现。

---

考虑到基于 Transformer 的预测模型的争议，我们反思了为什么 Transformer 在时间序列预测中的表现甚至不如线性模型，而在许多其他领域却表现优异。我们注意到，现有的基于 Transformer 的预测模型结构可能并不适合多变量时间序列预测。如图 2 顶部所示，值得注意的是，同一时间步的点通常代表由不一致测量记录的完全不同的物理意义，它们被嵌入到一个标记中，导致多变量相关性被抹去。而由单个时间步形成的标记由于过于局部的感受野和由同时时间点代表的时间未对齐事件，可能难以揭示有益信息。此外，虽然序列变化可能受到序列顺序的极大影响，但时间维度上却不适当地采用了排列不变的注意力机制（Zeng 等，2023）。因此，Transformer 在捕捉关键序列表示和描绘多变量相关性方面被削弱，限制了其在多样化时间序列数据上的能力和泛化能力。

- 图 2：传统 Transformer（顶部）与提出的 iTransformer（底部）的对比。Transformer 将包含每个时间步多变量表示的时间标记嵌入。iTransformer 将每个序列独立嵌入为变量标记，使得注意力模块能够描绘多变量相关性，而前馈网络则编码序列表示。

---

关于将时间戳的多变量点嵌入为（时间）标记的潜在风险，我们采用了一种倒置的视角来看待时间序列，并将每个变量的整个时间序列独立嵌入为一个（变量）标记，这是 Patching（Nie 等，2023）的极端情况，旨在扩大局部感受野。通过倒置，嵌入的标记聚合了序列的全局表示，使其更加以变量为中心，并更好地利用蓬勃发展的注意力机制进行多变量关联。同时，前馈网络可以足够熟练地学习从任意回看序列编码的不同变量的可泛化表示，并解码以预测未来序列。

---

基于上述动机，我们认为并不是 Transformer 在时间序列预测中无效，而是它被不当使用。在本文中，我们重新审视了 Transformer 的结构，并提倡将 iTransformer 作为时间序列预测的基础骨干。在技术上，我们将每个时间序列嵌入为变量标记，采用注意力机制捕捉多变量相关性，并利用前馈网络学习序列表示。在实验中，所提出的 iTransformer 在图 1 所示的现实世界预测基准上实现了最先进的性能，并出人意料地解决了基于 Transformer 的预测模型的痛点。我们的贡献体现在三个方面：

- 我们反思了 Transformer 的架构，并指出原生 Transformer 组件在多变量时间序列上的能力尚未被充分挖掘。

- 我们提出了 iTransformer，它将独立的时间序列视为标记，通过自注意力捕捉多变量相关性，并利用层归一化和前馈网络模块学习更好的序列全局表示，用于时间序列预测。

- 在实验中，iTransformer 在现实世界基准上实现了全面的最先进性能。我们广泛分析了倒置模块和架构选择，为未来基于 Transformer 的预测模型的改进指明了一个有前景的方向。


# 2 相关工作

随着自然语言处理和计算机视觉领域的逐步突破，精心设计的 Transformer 变体被提出以应对无处不在的时间序列预测应用。超越同时期的 TCNs（Bai 等，2018；Liu 等，2022a）和基于 RNN 的预测模型（Zhao 等，2017，Rangapuram 等，2018；Salinas 等，2020），Transformer 展示了强大的序列建模能力和有前途的模型可扩展性，引领了为时间序列预测而进行的热情修改趋势。

---

通过对基于 Transformer 的预测模型进行系统性回顾，我们得出结论：现有的修改可以根据是否修改组件和架构分为四类。如图 3 所示，第一类（Wu 等，2021；Li 等，2021；Zhou 等，2022）是最常见的做法，主要关注组件适配，特别是用于时间依赖建模的注意力模块以及对长序列的复杂性优化。然而，随着线性预测模型的迅速涌现（Oreshkin 等，2019，Zeng 等，2023，Das 等，2023，Liu 等，2023），其令人印象深刻的性能和效率不断挑战这一方向。不久之后，第二类尝试充分利用 Transformer。它更加关注时间序列的固有处理，例如平稳化（Liu 等，2022b）、通道独立性和 Patching（Nie 等，2023），这些方法带来了持续的性能提升。此外，面对多变量独立性和相互交互的日益重要性，第三类在组件和架构两方面对 Transformer 进行了改造。代表性工作（Zhang & Yan，2023）通过改进的注意力机制和架构显式捕捉跨时间和跨变量的依赖关系。

---

与以往的工作不同，iTransformer 没有修改 Transformer 的任何原生组件。相反，我们在倒置的维度上采用这些组件，并改变了架构，据我们所知，这是唯一属于第四类的方法。我们相信这些组件的能力已经经过了广泛的考验，事实是 Transformer 的架构被不当采用。

# 3 ITRANSFORMER

在多变量时间序列预测中，给定历史观测值 $\mathbf{X}=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{T}\right\} \in \mathbb{R}^{T \times N}$，其中包含 $T$ 个时间步和 $N$ 个变量，我们预测未来的 $S$ 个时间步 $\mathbf{Y}=\left\{\mathbf{x}_{T+1}, \ldots, \mathbf{x}_{T+S}\right\} \in$ $\mathbb{R}^{S \times N}$。为了方便，我们将 $\mathbf{X}_{t,:}$ 表示为第 $t$ 步同时记录的时间点，将 $\mathbf{X}_{:, n}$ 表示为索引为 $n$ 的每个变量的整个时间序列。值得注意的是，$\mathbf{X}_{t, \text { : }}$ 可能不包含在现实场景中本质上反映同一事件的时间点，因为数据集中变量之间存在系统性的时间滞后。此外，$\mathbf{X}_{t, \text { : }}$ 的元素在物理测量和统计分布上可能彼此不同，而一个变量 $\mathbf{X}_{:, n}$ 通常共享这些特征。

## 3.1 结构概述

我们提出的 iTransformer 如图 4 所示，采用了 Transformer（Vaswani 等，2017）的仅编码器架构，包括嵌入、投影和 Transformer 模块。

- 图 4：iTransformer 的整体结构，与 Transformer 的编码器共享相同的模块排列。(a) 不同变量的原始序列被独立嵌入为标记。(b) 自注意力应用于嵌入的变量标记，增强了可解释性，揭示了多变量相关性。(c) 每个标记的序列表示由共享的前馈网络提取。(d) 采用层归一化来减少变量之间的差异。

---

将整个序列嵌入为标记。大多数基于 Transformer 的预测模型通常将同一时间的多个变量视为（时间）标记，并遵循预测任务的生成式公式。然而，我们发现这种方法在数值模态上对学习注意力图的指导性较低，而 Patching（Dosovitskiy 等，2021；Nie 等，2023）的广泛应用支持了这一点，它扩大了感受野。同时，线性预测模型的成功也挑战了采用复杂的编码器-解码器 Transformer 生成标记的必要性。相反，我们提出的仅编码器的 iTransformer 专注于表示学习和多变量序列的自适应关联。每个由底层复杂过程驱动的时间序列首先被标记化以描述变量的特性，通过自注意力进行相互交互，并通过前馈网络单独处理以生成序列表示。值得注意的是，生成预测序列的任务本质上被交给线性层，这在之前的工作中已被证明是胜任的（Das 等，2023），我们将在下一节中提供详细分析。

---

基于上述考虑，在 iTransformer 中，基于回看序列 $\mathbf{X}_{:, n}$ 预测每个特定变量 $\hat{\mathbf{Y}}_{:, n}$ 的未来序列的过程简单表述如下：

$$
\begin{align*}
\mathbf{h}_{n}^{0} & =\operatorname{Embedding}\left(\mathbf{X}_{:, n}\right), \\
\mathbf{H}^{l+1} & =\operatorname{TrmBlock}\left(\mathbf{H}^{l}\right), l=0, \ldots, L-1,  \tag{1}\\
\hat{\mathbf{Y}}_{:, n} & =\operatorname{Projection}\left(\mathbf{h}_{n}^{L}\right),
\end{align*}
$$

其中 $\mathbf{H}=\left\{\mathbf{h}_{1}, \ldots, \mathbf{h}_{N}\right\} \in \mathbb{R}^{N \times D}$ 包含 $N$ 个维度为 $D$ 的嵌入标记，上标表示层索引。嵌入：$\mathbb{R}^{T} \mapsto \mathbb{R}^{D}$ 和投影：$\mathbb{R}^{D} \mapsto \mathbb{R}^{S}$ 均由多层感知机（MLP）实现。获得的变量标记通过自注意力相互交互，并在每个 TrmBlock 中由共享的前馈网络独立处理。具体来说，由于序列的顺序隐含地存储在前馈网络的神经元排列中，因此这里不再需要传统 Transformer 中的位置嵌入。

---

iTransformers。该架构本质上对 Transformer 变体没有更多的特定要求，除了注意力机制适用于多变量相关性。因此，一系列高效的注意力机制（Li 等，2021，Wu 等，2022，Dao 等，2022）可以作为插件，在变量数量增加时降低复杂性。此外，由于注意力的输入灵活性，标记数量可以从训练到推理变化，并且模型可以在任意数量的变量上进行训练。倒置的 Transformer，称为 iTransformers，在第 4.2 节的实验中进行了广泛评估，并在时间序列预测中展示了优势。

## 3.2 倒置的 Transformer 组件

我们组织了一组由层归一化、前馈网络和自注意力模块组成的 $L$ 个块。但它们在倒置维度上的职责被仔细重新考虑。

**层归一化**。层归一化（Ba 等，2016）最初被提出用于提高深度网络的收敛性和训练稳定性。在典型的基于 Transformer 的预测模型中，该模块对同一时间戳的多变量表示进行归一化，逐渐融合各个变量。一旦收集的时间点不代表同一事件，该操作还会引入非因果或延迟过程之间的交互噪声。在我们的倒置版本中，归一化应用于单个变量的序列表示，如公式 2 所示，该方法已被研究并证明在解决非平稳问题中有效（Kim 等，2021；Liu 等，2022b）。此外，由于所有序列作为（变量）标记被归一化为高斯分布，不一致测量引起的差异可以被减少。相比之下，在之前的架构中，不同时间步的标记会被归一化，导致时间序列过度平滑。

$$
\begin{equation*}
\operatorname{LayerNorm}(\mathbf{H})=\left\{\left.\frac{\mathbf{h}_{n}-\operatorname{Mean}\left(\mathbf{h}_{n}\right)}{\sqrt{\operatorname{Var}\left(\mathbf{h}_{n}\right)}} \right\rvert\, n=1, \ldots, N\right\} \tag{2}
\end{equation*}
$$

---

**前馈网络**。Transformer 采用前馈网络（FFN）作为编码标记表示的基本构建块，并且它被相同地应用于每个标记。如前所述，在传统 Transformer 中，形成标记的同一时间戳的多个变量可能错位且过于局部化，无法揭示足够的预测信息。在倒置版本中，FFN 被用于每个变量标记的序列表示。根据通用逼近定理（Hornik，1991），它们可以提取复杂的表示来描述时间序列。通过倒置块的堆叠，它们致力于编码观察到的时间序列，并使用密集的非线性连接解码表示以预测未来序列，这与最近完全基于 MLP 的工作一样有效（Tolstikhin 等，2021，Das 等，2023）。

---

更有趣的是，对独立时间序列进行相同的线性操作，作为最近线性预测模型（Zeng 等，2023）和通道独立性（Nie 等，2023）的结合，对我们理解序列表示具有指导意义。最近对线性预测模型的重新审视（Li 等，2023）强调，MLP 提取的时间特征应该在不同的时间序列之间共享。我们提出了一个合理的解释，即 MLP 的神经元被教导描绘任何时间序列的内在属性，例如振幅、周期性甚至频谱（神经元作为滤波器），作为一个比应用于时间点的自注意力更具优势的预测表示学习器。在实验中，我们在第 4.3 节中验证了分工有助于享受线性层的好处，例如提供更大的回看序列时提升性能，以及对未见变量的泛化能力。

---

**自注意力**。虽然注意力机制通常被用于促进先前预测模型中的时间依赖建模，但倒置模型将每个变量的整个序列视为一个独立过程。具体来说，通过全面提取每个时间序列的表示 $\mathbf{H}=\left\{\mathbf{h}_{0}, \ldots, \mathbf{h}_{N}\right\} \in \mathbb{R}^{N \times D}$，自注意力模块采用线性投影来获得查询、键和值 $\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{N \times d_{k}}$，其中 $d_{k}$ 是投影维度。

---

以 $\mathbf{q}_{i}, \mathbf{k}_{j} \in \mathbb{R}^{d_{k}}$ 表示一个（变量）标记的特定查询和键，我们注意到预 Softmax 得分的每个条目被公式化为 $\mathbf{A}_{i, j}=\left(\mathbf{Q} \mathbf{K}^{\top} / \sqrt{d_{k}}\right)_{i, j} \propto \mathbf{q}_{i}^{\top} \mathbf{k}_{j}$。由于每个标记先前在其特征维度上被归一化，这些条目可以在某种程度上揭示变量之间的相关性，整个得分图 $\mathbf{A} \in \mathbb{R}^{N \times N}$ 展示了成对变量标记之间的多变量相关性。因此，高度相关的变量在下一个表示交互中会被赋予更大的权重，使用值 $\mathbf{V}$。基于这种直觉，所提出的机制被认为对多变量序列预测更自然和可解释。我们在第 4.3 节和附录 E.1 中进一步提供了得分图的可视化分析。

# 4 实验

我们在各种时间序列预测应用中全面评估了提出的 iTransformer，验证了所提出框架的通用性，并进一步深入研究了将 Transformer 组件应用于时间序列倒置维度的有效性。

**数据集**。我们在实验中广泛包含了 7 个真实世界的数据集，包括 Autoformer（Wu 等，2021）使用的 ECL、ETT（4 个子集）、Exchange、Traffic、Weather，LSTNet（Lai 等，2018）提出的 Solar-Energy 数据集，以及 SCINet（Liu 等，2022a）评估的 PEMS（4 个子集）。我们还在附录 F.4 中提供了对 Market（6 个子集）的实验。它记录了支付宝在线交易应用程序的分钟级服务器负载，包含数百个变量，我们始终优于其他基线。详细的数据集描述见附录 A.1。

## 4.1 预测结果

在本节中，我们进行了广泛的实验，以评估我们提出的模型与先进深度预测模型的预测性能。

**基线模型**。我们精心选择了 10 个广受认可的预测模型作为基准，包括：（1）基于 Transformer 的方法：Autoformer（Wu 等，2021）、FEDformer（Zhou 等，2022）、Stationary（Liu 等，2022b）、Crossformer（Zhang & Yan，2023）、PatchTST（Nie 等，2023）；（2）基于线性模型的方法：DLinear（Zeng 等，2023）、TiDE（Das 等，2023）、RLinear（Li 等，2023）；（3）基于 TCN 的方法：SCINet（Liu 等，2022a）、TimesNet（Wu 等，2023）。

---

**主要结果**。综合预测结果列于表 1 中，最佳结果用红色标出，次佳结果用下划线标出。较低的 MSE/MAE 表示更准确的预测结果。与其他预测模型相比，iTransformer 尤其擅长预测高维时间序列。此外，作为之前的最先进模型，PatchTST 在许多 PEMS 数据集的情况下表现不佳，这可能源于该数据集极其波动的序列，而 PatchTST 的 patching 机制可能会在处理快速波动时失去对特定局部性的关注。相比之下，所提出的模型通过聚合整个序列变化来生成序列表示，能够更好地应对这种情况。值得注意的是，作为显式捕捉多变量相关性的代表，Crossformer 的性能仍然逊色于 iTransformer，这表明来自不同多变量的时间未对齐 patch 的交互会为预测带来不必要的噪声。因此，原生 Transformer 组件在时间建模和多变量相关性建模方面是胜任的，而所提出的倒置架构能够有效应对现实世界的时间序列预测场景。

- 表 1：多变量预测结果，预测长度 $S \in\{12,24,36,48\}$ 用于 PEMS，$S \in\{96,192,336,720\}$ 用于其他数据集，回看长度固定为 $T=96$。结果是对所有预测长度的平均值。Avg 表示进一步对子集进行平均。完整结果列于附录 F.4。

## 4.2 ITRANSFORMER 的通用性

在本节中，我们通过将我们的框架应用于 Transformer 及其变体来评估 iTransformer，这些变体通常解决了自注意力机制的二次复杂度问题，包括 Reformer（Kitaev 等，2020）、Informer（Li 等，2021）、Flowformer（Wu 等，2022）和 FlashAttention（Dao 等，2022）。令人惊喜的发现表明，简单的倒置视角可以增强基于 Transformer 的预测模型，提升性能、效率、对未见变量的泛化能力以及对历史观测的更好利用。

---

**性能提升**。我们评估了 Transformer 及相应的 iTransformer，并在表 2 中报告了性能提升。值得注意的是，该框架一致地改进了各种 Transformer。总体而言，它在 Transformer 上实现了平均 $\mathbf{38.9\%}$ 的提升，在 Reformer 上为 $\mathbf{36.1\%}$，在 Informer 上为 $\mathbf{28.5\%}$，在 Flowformer 上为 $\mathbf{16.8\%}$，在 Flashformer 上为 $\mathbf{32.2\%}$，揭示了之前 Transformer 架构在时间序列预测中的不当使用。此外，由于在我们的倒置结构中，注意力机制被应用于变量维度，引入具有线性复杂度的高效注意力机制本质上解决了由于变量数量众多带来的计算问题，这在现实应用中普遍存在，但对通道独立性（Nie 等，2023）可能是资源消耗的。因此，iTransformer 的理念可以广泛应用于基于 Transformer 的预测模型，以利用蓬勃发展的高效注意力机制。

- 表 2：通过我们的倒置框架获得的性能提升。Flashformer 表示配备了硬件加速 FlashAttention（Dao 等，2022）的 Transformer。我们报告了平均性能和相对 MSE 减少（提升）。完整结果见附录 F.2。

---

**变量泛化**。通过倒置传统 Transformer，值得注意的是，模型被赋予了在未见变量上的泛化能力。首先，得益于输入标记数量的灵活性，变量通道的数量不再受到限制，因此可以在训练和推理之间变化。此外，前馈网络在 iTransformer 中被相同地应用于独立的变量标记。如前所述，作为滤波器的神经元学习了任何时间序列的内在模式，这些模式倾向于在不同变量之间共享和转移。

---

为了验证这一假设，我们将倒置与另一种泛化策略进行比较：通道独立性，训练一个共享骨干来预测所有变量。我们将每个数据集的变量划分为五个文件夹，仅使用一个文件夹的 $20\%$ 变量训练模型，并在不进行微调的情况下直接预测所有变量。我们在图 5 中比较了性能，每个条形图表示所有文件夹的平均结果，以避免划分的随机性。CI-Transformer 在推理期间需要逐个预测每个变量，耗时较长，而 iTransformer 直接预测所有变量，并且通常表现出较小的增加，表明 FFN 能够胜任学习可转移的时间序列表示。这为在 iTransformer 上构建基础模型留下了潜在方向，其中具有不同变量数量的多样化多变量时间序列可以一起训练。

- 图 5：在未见变量上的泛化性能。我们将每个数据集的变量划分为五个文件夹，使用 $20\%$ 变量训练模型，并使用部分训练的模型预测所有变量。iTransformer 可以高效地训练，并以良好的泛化能力进行预测。

---

**增加回看长度**。之前的研究已经观察到，在 Transformer 上，预测性能并不一定会随着回看长度的增加而提高（Nie 等，2023；Zeng 等，2023），这可能归因于对增长输入的注意力分散。然而，在理论上，统计方法（Box & Jenkins，1968）支持利用更多历史信息，线性预测模型通常表现出期望的性能提升。由于注意力和前馈网络的工作维度被倒置，我们在图 6 中评估了 Transformer 和 iTransformer 在增加回看长度时的性能。结果出人意料地验证了在时间维度上利用 MLP 的合理性，使得 Transformer 可以从扩展的回看窗口中受益，从而实现更精确的预测。

- 图 6：回看长度 $T \in\{48,96,192,336,720\}$ 和固定预测长度 $S=96$ 的预测性能。虽然基于 Transformer 的预测模型的性能并不一定会因增加回看长度而提高，但倒置框架使传统 Transformer 及其变体在扩展的回看窗口上表现出更好的性能。

### 4.3 模型分析

**消融实验**。为了验证 Transformer 组件的合理性，我们提供了详细的消融实验，包括替换组件（Replace）和移除组件（w/o）实验。结果列于表 3 中。在变量维度上利用注意力并在时间维度上利用前馈网络的 iTransformer 通常表现最佳。值得注意的是，传统 Transformer（第三行）在这些设计中表现最差，揭示了传统架构的潜在风险，我们在附录 E.3 中对此进行了详细描述。

- 表 3：iTransformer 的消融实验。我们在各自维度上替换不同组件以学习多变量相关性（变量）和序列表示（时间），此外还进行了组件移除实验。这里列出了所有预测长度的平均结果。

---

**序列表示分析**。为了进一步验证前馈网络更适合提取序列表示的说法，我们基于中心核对齐（CKA）相似性（Kornblith 等，2019）进行了表示分析。更高的 CKA 表示更相似的表示。对于 Transformer 变体和 iTransformer，我们计算了第一个和最后一个块的输出特征之间的 CKA。值得注意的是，之前的研究表明，时间序列预测作为一种低级别的生成任务，更喜欢更高的 CKA 相似性（Wu 等，2023，Dong 等，2023）以获得更好的性能。如图 7 所示，展示了一条清晰的分界线，表明 iTransformer 通过倒置维度学习了更合适的序列表示，从而实现了更准确的预测。结果也表明，倒置 Transformer 值得作为预测骨干的根本性革新。

---

**多变量相关性分析**。通过将多变量相关性的职责分配给注意力机制，学习到的映射图具有增强的可解释性。我们在图 7 中展示了 Solar-Energy 数据集的案例可视化，该数据集在回看和未来窗口中具有不同的相关性。可以观察到，在浅层注意力层中，学习到的映射图与原始输入序列的相关性有许多相似之处。随着进入更深层次，学习到的映射图逐渐类似于未来序列的相关性，这验证了倒置操作为相关性提供了可解释的注意力，并且编码过去和解码未来的过程本质上是在前馈过程中通过序列表示进行的。

- 图 7：序列表示和多变量相关性分析。左：Transformer 和 iTransformer 的 MSE 和 CKA 相似性比较。更高的 CKA 相似性表示更适合准确预测的表示。右：原始时间序列的多变量相关性和通过倒置自注意力学习到的得分图的案例可视化。

---

**高效训练策略**。由于自注意力的二次复杂度，在训练大量变量时可能会不堪重负，这在实际场景中非常常见。除了高效注意力机制外，我们提出了一种新的高维多变量序列训练策略，利用之前展示的变量生成能力。具体来说，我们在每批中随机选择部分变量，并仅使用选定的变量训练模型。由于我们的倒置操作使得变量通道的数量具有灵活性，模型可以预测所有变量。如图 8 所示，我们提出的策略在部分变量训练上的性能仍然与全变量训练相当，同时内存占用可以显著减少。

- 图 8：高效训练策略分析。虽然在不同采样比例的部分变量训练中性能（左）保持稳定，但内存占用（右）可以大幅减少。我们在附录 D 中提供了全面的模型效率分析。

## 5 结论与未来工作

考虑到多变量时间序列的特性，我们提出了 iTransformer，它在不修改任何原生模块的情况下倒置了 Transformer 的结构。iTransformer 将独立序列视为变量标记，通过注意力捕捉多变量相关性，并利用层归一化和前馈网络学习序列表示。实验表明，iTransformer 实现了最先进的性能，并通过有前景的分析展示了显著的框架通用性。未来，我们将探索大规模预训练和更多时间序列分析任务。

## A 实现细节

## A.1 数据集描述

我们在 7 个真实世界的数据集上进行了实验，以评估所提出的 iTransformer 的性能，包括：（1）ETT（Li 等，2021）包含从 2016 年 7 月到 2018 年 7 月的电力变压器的 7 个因素。有四个子集，其中 ETTh1 和 ETTh2 每小时记录一次，ETTm1 和 ETTm2 每 15 分钟记录一次。（2）Exchange（Wu 等，2021）收集了 1990 年至 2016 年 8 个国家的每日汇率面板数据。（3）Weather（Wu 等，2021）包括 2020 年马克斯普朗克生物地球化学研究所气象站每 10 分钟收集的 21 个气象因素。（4）ECL（Wu 等，2021）记录了 321 个客户每小时的电力消耗数据。（5）Traffic（Wu 等，2021）收集了 2015 年 1 月至 2016 年 12 月旧金山湾区高速公路 862 个传感器测量的每小时道路占用率。（6）Solar-Energy（Lai 等，2018）记录了 2006 年 137 个光伏电站的太阳能发电量，每 10 分钟采样一次。（7）PEMS 包含加州公共交通网络数据，每 5 分钟收集一次。我们使用了 SCINet（Liu 等，2022a）中采用的四个公共子集（PEMS03、PEMS04、PEMS07、PEMS08）。

---

除了广泛用作预测基准的公共数据集外，我们还收集了一组现实应用中的 Market 数据集，这些数据集记录了 2023 年 1 月 30 日至 2023 年 4 月 9 日期间支付宝在线交易的分钟级服务器负载，变量数量从 285 到 759 不等。它包括 6 个子数据集，根据不同的交易领域划分。

---

我们遵循 TimesNet（Wu 等，2023）中使用的相同数据处理和训练-验证-测试集划分协议，其中训练、验证和测试数据集严格按照时间顺序划分，以确保没有数据泄露问题。至于预测设置，我们在 ETT、Weather、ECL、Solar-Energy、PEMS 和 Traffic 中将回看序列的长度固定为 96，预测长度在 $\{96,192,336,720\}$ 之间变化。对于 PEMS 数据集，预测长度在 $\{12,24,36,48\}$ 之间变化，与之前在该数据集上表现最好的 SCINet 相同。对于 Market 数据集，回看包含过去一天的观测数据，共 144 个时间点，预测长度在 $\{12,24,72,144\}$ 之间变化。数据集的详细信息见表 4。

- 表 4：详细的数据集描述。Dim 表示每个数据集的变量数量。Dataset Size 表示（训练、验证、测试）划分中的总时间点数。Prediction Length 表示要预测的未来时间点，每个数据集包括四种预测设置。Frequency 表示时间点的采样间隔。

---

所有实验均在 PyTorch（Paszke 等，2019）中实现，并在单个 NVIDIA P100 16GB GPU 上进行。我们使用 ADAM（Kingma & Ba，2015）优化器，初始学习率为 $\left\{10^{-3}, 5 \times 10^{-4}, 10^{-4}\right\}$，并使用 L2 损失进行模型优化。批量大小统一设置为 32，训练轮数固定为 10。我们将提出的模型中的倒置 Transformer 块数量设置为 $L \in\{2,3,4\}$。序列表示的维度 $D$ 设置为 $\{256,512\}$。我们复现的所有基线模型均基于 TimesNet（Wu 等，2023）仓库的基准实现，该基准公平地建立在每个模型原始论文或官方代码提供的配置基础上。我们在算法 1 中提供了 iTransformer 的伪代码。我们还在表 5 中报告了 iTransformer 在不同随机种子下五次运行的标准差，表明 iTransformer 的性能是稳定的。

## B 消融实验

为了详细说明 Transformer 组件的合理性，我们进行了详细的消融实验，包括替换组件（Replace）和移除组件（w/o）。由于论文篇幅限制，平均结果列于表 3 中，我们在此提供详细结果和分析。

---

如表 6 所示，在各种架构设计中，iTransformer 通常表现出更优的性能，它通过自注意力学习多变量相关性，并通过 FFN 编码序列表示。然而，传统 Transformer 的安排可能导致性能退化，表明 Transformer 组件在时间序列模态上的误用。基于第二（两个注意力）和第三（传统 Transformer）设计相对较差的结果，原因之一可能在于对滞后时间序列的时间标记的注意力模块，我们在第 E.3 节中结合数据集支持对此进行了详细阐述。

- 表 6：iTransformer 消融实验的完整结果。我们在各自维度上应用不同组件以学习多变量相关性（变量）和序列表示（时间），此外还移除了 Transformer 的特定组件。

---

值得注意的是，在变量数量较少的数据集（如 Weather，21 个变量）上，在两个维度上应用 FFN 也能取得不错的性能。然而，随着具有挑战性的多变量预测任务中变量数量的增加，捕捉多变量相关性的重要性愈发凸显。我们注意到，传统 Transformer 很难考虑变量的异质性。在嵌入过程中，变量被投影到不可区分的通道中，这忽略了不一致的物理测量，因此无法保持变量的独立性，更不用说捕捉和利用多变量相关性了。因此，通过结合先进的注意力模块进行变量相关性建模，第一（iTransformer）和第五（在变量上应用注意力）的设计在具有挑战性的多变量数据集上表现更为有效。

---

简而言之，时间依赖性和多变量相关性对于多变量时间序列预测都很重要。所提出的 iTransformer 采用自注意力模块来解耦变量标记之间的相关性，证明其比前馈网络更强大且更具可解释性，从而进一步提升了在具有挑战性的多变量数据集上的性能，并增强了模型能力。

## C 超参数敏感性

我们评估了 iTransformer 在以下因素上的超参数敏感性：学习率 $l r$、Transformer 块数量 $L$ 和变量标记的隐藏维度 $D$。结果如图 9 所示。我们发现，学习率作为最常见的影响因素，在变量数量较多时（如 ECL、Traffic）应谨慎选择。在 iTransformer 中，块数量和隐藏维度并不一定越大越好。

- 图 9：关于学习率、Transformer 块数量和变量标记隐藏维度的超参数敏感性。结果记录于回看窗口长度 $T=96$ 和预测窗口长度 $S=96$。

## D 模型效率

我们全面比较了以下模型的预测性能、训练速度和内存占用：iTransformer、采用我们高效训练策略的 iTransformer 以及采用高效流注意力模块的 iTransformer（Wu 等，2022）；线性模型：DLinear（Zeng 等，2023）和 TiDE（Das 等，2023）；Transformer：Transformer（Vaswani 等，2017）、PatchTST（Nie 等，2023）和 Crossformer（Zhang & Yan，2023）。结果记录了官方模型配置和相同的批量大小。在图 10 中，我们比较了在两个代表性数据集（Weather 有 21 个变量，Traffic 有 862 个变量）上，回看时间步长为 96 时的效率。

---

简而言之，在变量数量相对较少的数据集（如 Weather）上，iTransformer 的效率超过了其他 Transformer。在变量数量较多的数据集（如 Traffic）上，内存占用与传统 Transformer 基本相同，但 iTransformer 的训练速度更快。基于注意力模块的 $\mathcal{O}\left(N^{2}\right)$ 复杂度，其中 $N$ 是标记数量，Transformer 在这种情况下效率超过 iTransformer，因为时间标记的 $N=96$，而变量标记的 $N=862$。与此同时，iTransformer 在变量数量较多时表现更好，因为可以显式利用多变量相关性。通过采用线性复杂度注意力（Wu 等，2022）或如图 8 中提出的高效训练策略（在 $20\%$ 变量上训练并预测所有变量），iTransformer 可以获得与线性模型相当的速度和内存占用。此外，这两种策略可以同时采用。

## E 案例展示

## E.1 多变量相关性的可视化

通过在变量标记上使用注意力机制，生成的学习映射图更具可解释性。为了直观理解多变量相关性，我们在图 11 中提供了 Solar-Energy 数据集的三个随机选择的时间序列案例可视化。我们通过以下公式提供原始序列中每个变量的皮尔逊相关系数：

- 图 11：回看序列和未来序列的多变量相关性，以及不同层倒置自注意力学习到的得分图。所有案例均来自 Solar-Energy 数据集。

$$
\rho_{x y}=\frac{\sum_{i}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sqrt{\sum_{i}\left(x_{i}-\bar{x}\right)^{2}} \sqrt{\sum_{i}\left(y_{i}-\bar{y}\right)^{2}}},
$$

其中 $x_{i}, y_{i} \in \mathbb{R}$ 遍历要相关的成对变量的所有时间点。所有案例在回看和预测窗口中具有不同的多变量相关性，因为该数据集在白天和夜间表现出明显的季节性变化。在每个案例的第二行，我们提供了自注意力模块在第一层和最后一层学习到的预 Softmax 映射图。正如我们在浅层注意力层（左）中观察到的，我们发现学习到的映射图与原始回看序列的相关性相似。随着我们深入更深层次（右），学习到的映射图逐渐变得更类似于要预测的未来序列的相关性。这表明倒置操作允许在相关性建模中实现可解释的注意力，并且对过去的编码和对未来的解码是通过层堆叠中的序列表示进行的。

---

我们在图 12 中展示了另一个有趣的观察结果，以表明 iTransformer 的注意力模块具有增强的可解释性。我们提供了从 Market 数据集中随机选择的多变量时间序列。在该数据集中，每个变量代表某种服务接口的监控值，这些服务可以进一步细分为具体的应用类别。我们将这些变量划分为相应的应用程序（如顶部栏 App 中列出的），使得相邻变量属于同一应用程序，并通过顶部栏揭示应用程序索引。

- 图 12：Market 数据集中变量的可视化及其学习到的多变量相关性。每个变量代表一个应用程序的监控接口值，这些应用程序可以进一步细分为具体类别。颜色条与图 11 共享。

---

我们可视化了变量的时间序列，并绘制了学习到的多变量相关性，同时标记了变量之间的特定相关性。一方面，我们在多变量相关性映射图中观察到清晰的分区，表明变量的分组。另一方面，标记的相关性值可以反映原始序列的相关性，其中来自同一应用程序的变量对的相似性比来自不同组的变量对更接近。因此，高度相关的变量将被用于下一次交互，从而有利于多变量预测。

## E.2 预测结果的可视化

为了清晰地比较不同模型，我们在图 13 至图 16 中列出了四个代表性数据集的补充预测案例，这些案例由以下模型提供：iTransformer、PatchTST（Nie 等，2023）、DLinear（Zeng 等，2023）、Crossformer（Zhang & Yan，2023）、Autoformer（Wu 等，2021）、Transformer（Vaswani 等，2017）。在众多模型中，iTransformer 预测的未来序列变化最为精确，表现出卓越的性能。

## E.3 嵌入时间戳多变量点的风险

如前所述，之前 Transformer 的嵌入方法融合了代表潜在延迟事件和不同物理测量的多个变量，这可能导致无法学习以变量为中心的表示，并生成无意义的注意力图。我们提供了 Traffic（Liu 等，2022a）的可视化案例，该数据集收集自洛杉矶城市不同区域道路上的传感器。如图 17 所示，我们可以观察到数据集中多变量时间序列之间存在很强的相关性，但它们也表现出明显的相位偏移，这是由于每条序列描述的道路占用率存在系统性时间滞后。由于传感器安装在高速公路的不同区域，一个事件（如交通堵塞）可能会以不同的延迟影响道路占用率。

- 图 17：Traffic 数据集部分变量的可视化。我们可以观察到一些序列表现出很强的同步性（如传感器 2 和传感器 4），同时序列之间也存在明显的延迟和提前（如传感器 1 和传感器 2，传感器 859 和传感器 861）。

---

此外，我们观察到表 6 中第二和第三种设计（在时间标记上应用注意力）在 Traffic 数据集上的性能显著下降。我们认为，通过注意力捕捉时间依赖性并不是一个大问题，但这基于一个事实，即每个时间戳的时间点本质上反映了同一事件以形成一个语义表示。由于时间点之间存在固有的延迟，性能可能会因为无意义的注意力图而大幅下降，除非模型具有扩大的感受野来学习衰减或因果过程。

---

其他风险可能源于不同的变量测量，例如在 Weather 数据集（Wu 等，2021）中将不同的气象指标（温度和降雨量）组织在一起，以及在 ILI（Wu 等，2023）中同一观测值的数量和比例。鉴于这些潜在风险，iTransformer 提出了一种新范式，将整个序列嵌入为变量标记，这可以更广泛地适应现实场景，例如延迟事件、不一致的测量、不规则（不均匀间隔）时间序列、监控器的系统性延迟以及生成和记录不同时间序列的时间间隔。

## F 完整结果

## F.1 完整性能提升结果

我们在表 7 中比较了 Transformer 和 iTransformer 在所有数据集上的性能。一致且显著的性能提升表明，在倒置维度上应用注意力和前馈网络极大地增强了 Transformer 在多变量时间序列预测中的能力，为构建广泛时间序列数据的基础模型提供了指导方向。

- 表 7：传统 Transformer 与提出的 iTransformer 的完整性能对比。结果是所有四种预测长度的平均值。

## F.2 完整框架通用性结果

我们将提出的倒置框架应用于 Transformer 及其变体：Transformer（Vaswani 等，2017）、Reformer（Kitaev 等，2020）、Informer（Li 等，2021）、Flowformer（Wu 等，2022）、Flashformer（Dao 等，2022）。由于篇幅限制，表 2 中展示了平均结果。我们在表 8 中提供了补充的预测结果。结果表明，我们的 iTransformers 框架可以一致地提升这些 Transformer 变体，并利用蓬勃发展的高效注意力机制。

- 表 8：应用我们倒置框架的 Transformer 完整结果。Flashformer 表示配备了硬件加速 FlashAttention（Dao 等，2022）的 Transformer。

## F.3 变量泛化的完整结果

我们将每个数据集的变量划分为五个文件夹，仅使用一个文件夹的 $20\%$ 变量训练模型，并在不进行微调的情况下直接预测所有变量。我们采用两种策略使 Transformer 在未见变量上泛化：（1）CI-Transformers（Nie 等，2023）：通道独立性将时间序列的每个变量视为独立通道，并使用共享骨干进行训练。在推理过程中，模型逐个预测变量，但这一过程可能耗时较长。（2）iTransformers：由于注意力机制的灵活性，输入标记的数量可以动态变化，变量作为标记的数量不再受到限制，因此可以在训练和推理之间变化，甚至允许模型在任意变量上进行训练。

---

如表 18 所示，iTransformers 可以自然地使用 $20\%$ 变量进行训练，并完成对所有变量的预测，具备学习可转移表示的能力。

- 图 18：在未见变量上的完整泛化性能，比较 iTransformers 与 CI-Transformers。我们将每个数据集的变量划分为五个文件夹，使用 $20\%$ 变量训练模型，并使用训练好的模型预测所有变量。我们绘制了所有五个文件夹的平均结果。

## F.4 完整预测结果

由于正文篇幅限制，完整的多变量预测结果在以下部分提供。我们在具有挑战性的预测任务上广泛评估了竞争模型。表 9 包含 PEMS（Liu 等，2022a）四个公共子集的预测结果。表 10 包含九个广受认可的预测基准的所有预测长度的详细结果。表 11 记录了支付宝服务器负载预测的 Market 数据集结果。所提出的模型在现实世界预测应用中实现了全面的最先进性能。

- 表 9：PEMS 预测任务的完整结果。我们按照 SCINet（2022a）的设置，在不同预测长度下比较了多个竞争模型。所有基线的输入长度设置为 96。Avg 表示所有四个预测长度的平均结果。

- 表 10：长期预测任务的完整结果。我们按照 TimesNet（2023）的设置，在不同预测长度下比较了多个竞争模型。所有基线的输入序列长度设置为 96。Avg 表示所有四个预测长度的平均结果。

## G 讨论与进一步改进

## G.1 关于无架构方法的讨论

通道独立性（CI）（Nie 等，2023）将时间序列的变量视为独立的，并采用共享骨干，作为一种无架构方法，在预测中因其性能提升而越来越受欢迎。最近的研究（Han 等，2023；Li 等，2023）发现，虽然通道依赖性（CD）在理想情况下受益于更高的容量，但由于当前大多数预测基准的数据量不足，CI 可以显著提升性能。我们认为，保持变量独立性是必要的，尤其是在存在附录 E.3 中提到的嵌入风险时，CD 的理想模型容量会因过于局部的感受野而受到限制。然而，CI 的本质是将多变量时间序列视为单变量，可能导致训练和推理耗时较长，并成为可扩展性的障碍。此外，多变量相关性无法被显式利用。与这些工作垂直，iTransformer 重新设计了架构，利用原生 Transformer 模块来解决这些问题。

---

RevIN（Kim 等，2021）和平稳化（Liu 等，2022b）作为无架构技术，已被广泛应用于处理分布偏移（非平稳性）。这些工作致力于更好地揭示时间依赖性。iTransformer 通过层归一化实现了这一点，但仍需进一步改进以应对分布偏移。

## G.2 关于线性预测模型的讨论

线性预测模型在建模时间依赖性方面具有天然优势。密集权重（Zeng 等，2023；Li 等，2023）可以揭示同一变量时间点之间与测量无关的关系。更先进的线性预测模型专注于结构化逐点建模（Oreshkin 等，2019；Liu 等，2022，2023）。相比之下，iTransformer 特别擅长预测高维时间序列（具有复杂相关性的众多变量，这在现实预测应用中常见且实际）。对于变量相关性建模，嵌入保持变量独立性，注意力模块可用于挖掘相关性。在单变量场景下，iTransformer 实际上成为一个可堆叠的线性预测模型（注意力退化），这为进一步增强以更好地利用时间依赖性留下了空间。

## G.3 关于 Transformer 的讨论

我们强调，iTransformer 实际上提出了一种新的视角来思考多变量时间序列模态，特别是如何考虑变量和标记化。我们在图 19 中列出了几个代表性模型。Transformer 将时间序列视为自然语言，但时间对齐的嵌入可能会在多维序列中带来风险。这个问题可以通过扩大感受野来缓解。尽管人们认为 Patching（Zhang & Yan，2023；Nie 等，2023）可以更细粒度，但它也带来了更高的计算复杂性和时间未对齐 patch 之间的潜在交互噪声。如果当前的嵌入（通过 MLP 实现）能够通过更多的归纳偏差（如 TCN）增强，它可能能够处理更复杂的案例，并享受 Transformer 在标记数量可变性上的灵活性。

---

我们相信 Transformer 的能力和可扩展性已经通过了广泛领域的考验，但基于倒置架构精心设计组件仍有改进空间，例如用于多变量相关性的高效注意力、分布偏移下的结构化时间依赖性建模、细粒度的变量标记化以及精心设计的嵌入机制。

图 19：代表性 Transformer 的多变量时间序列模态的标记化。
