# ITRANSFORMER: INVERTED TRANSFORMERS ARE EFFECTIVE FOR TIME SERIES FORECASTING

# ITRANSFORMER：倒置变压器（Inverted Transformers）对时间序列预测有效

Yong Liu," Tengge Hu," Haoran Zhang," Haixu Wu,Shiyu Wang ${}^{§}$ ,Lintao Ma ${}^{§}$ ,Mingsheng Long ${}^{ \boxtimes  }$ School of Software, BNRist, Tsinghua University, Beijing 100084, China SAnt Group, Hangzhou, China

刘永（Yong Liu）、胡腾格（Tengge Hu）、张浩然（Haoran Zhang）、吴海旭（Haixu Wu）、王诗雨（Shiyu Wang） ${}^{§}$、马林涛（Lintao Ma） ${}^{§}$、龙明盛（Mingsheng Long） ${}^{ \boxtimes  }$ 清华大学软件学院，北京智源人工智能研究院，中国北京 100084 蚂蚁集团，中国杭州

\{liuyong21,htg21,z-hr20,whx20\}@mails.tsinghua.edu.cn

\{liuyong21,htg21,z-hr20,whx20\}@mails.tsinghua.edu.cn

\{weiming.wsy,lintao.mlt\}@antgroup.com, mingsheng@tsinghua.edu.cn

\{weiming.wsy,lintao.mlt\}@antgroup.com, mingsheng@tsinghua.edu.cn

## Abstract

## 摘要

The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting. Code is available at this repository: https://github.com/thuml/iTransformer.

近期线性预测模型的兴起，对基于Transformer的预测器进行架构修改的持续热情提出了质疑。这些预测器利用Transformer对时间序列的时间标记（每个标记由同一时间戳的多个变量构成）之间的全局依赖关系进行建模。然而，由于性能下降和计算量爆炸，Transformer在预测具有更大回溯窗口的序列时面临挑战。此外，每个时间标记的嵌入融合了代表潜在延迟事件和不同物理测量值的多个变量，这可能无法学习以变量为中心的表示，并导致注意力图失去意义。在这项工作中，我们重新审视了Transformer组件的职责，并在不修改基本组件的情况下重新调整了Transformer架构。我们提出了iTransformer，它仅在倒置维度上应用注意力机制和前馈网络。具体而言，将单个序列的时间点嵌入到变量标记中，注意力机制利用这些标记来捕捉多变量之间的相关性；同时，对每个变量标记应用前馈网络以学习非线性表示。iTransformer模型在具有挑战性的真实世界数据集上达到了最先进水平，这进一步提升了Transformer家族的性能、跨不同变量的泛化能力，并更好地利用了任意回溯窗口，使其成为时间序列预测的基础骨干网络的理想选择。代码可在以下仓库获取：https://github.com/thuml/iTransformer。

## 1 INTRODUCTION

## 1 引言

Transformer (Vaswani et al. 2017) has achieved tremendous success in natural language processing (Brown et al., 2020) and computer vision (Dosovitskiy et al. 2021), growing into the foundation model that follows the scaling law (Kaplan et al. 2020). Inspired by the immense success in extensive fields, Transformer with strong capabilities of depicting pairwise dependencies and extracting multi-level representations in sequences is emerging in time series forecasting (Wu et al. 2021; Nie et al. 2023).

Transformer（瓦希尼等人，2017年）在自然语言处理（布朗等人，2020年）和计算机视觉（多索维茨基等人，2021年）领域取得了巨大成功，发展成为遵循缩放定律的基础模型（卡普兰等人，2020年）。受其在广泛领域取得巨大成功的启发，具有强大的描述成对依赖关系和提取序列多级表示能力的Transformer开始应用于时间序列预测（吴等人，2021年；聂等人，2023年）。

<!-- Media -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_0.jpg?x=1090&y=1529&w=401&h=404&r=0"/>

Figure 1: Performance of iTrans-former. Average results (MSE) are reported following TimesNet (2023).

图1：iTrans - former的性能。按照TimesNet（2023年）的方法报告平均结果（均方误差）。

<!-- Media -->

However, researchers have recently begun to question the validity of Transformer-based forecasters, which typically embed multiple variates of the same timestamp into indistinguishable channels and apply attention on these temporal tokens to capture temporal dependencies. Considering the numerical but less semantic relationship among time points, researchers find that simple linear layers, which can be traced back to statistical forecasters (Box & Jenkins, 1968), have exceeded complicated Transformers on both performance and efficiency (Zeng et al. 2023; Das et al. 2023). Meanwhile, ensuring the independence of variate and utilizing mutual

然而，研究人员最近开始质疑基于Transformer的预测器的有效性，这类预测器通常将同一时间戳的多个变量嵌入到难以区分的通道中，并对这些时间标记应用注意力机制以捕捉时间依赖关系。考虑到时间点之间的数值关系而非语义关系，研究人员发现，可追溯到统计预测器（Box & Jenkins，1968）的简单线性层，在性能和效率上都超过了复杂的Transformer（Zeng等人，2023；Das等人，2023）。同时，确保变量的独立性并利用相互

---

<!-- Footnote -->

*Equal Contribution

*同等贡献

<!-- Footnote -->

---

<!-- Media -->

<!-- figureText: Transformer Temporal Tokens Representations Attention over FFN on Series Variate Tokens Invert View -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_1.jpg?x=326&y=231&w=1141&h=466&r=0"/>

Figure 2: Comparison between the vanilla Transformer (top) and the proposed iTransformer (bottom). Transformer embeds the temporal token, which contains the multivariate representation of each time step. iTransformer embeds each series independently to the variate token, such that the attention module depicts the multivariate correlations and the feed-forward network encodes series representations.

图2：原始Transformer（上）与所提出的iTransformer（下）的比较。Transformer嵌入时间标记，其中包含每个时间步的多变量表示。iTransformer将每个序列独立地嵌入到变量标记中，这样注意力模块可以描绘多变量相关性，前馈网络可以对序列表示进行编码。

<!-- Media -->

information is ever more highlighted by recent research that explicitly models multivariate correlations to achieve accurate forecasting (Zhang & Yan, 2023; Ekambaram et al. 2023), but this goal can be hardly achieved without subverting the vanilla Transformer architecture.

近期的研究更加凸显了明确建模多变量相关性以实现准确预测的重要性（Zhang & Yan，2023；Ekambaram等人，2023），但如果不颠覆原始的Transformer架构，这一目标很难实现。

Considering the disputes of Transformer-based forecasters, we reflect on why Transformers perform even worse than linear models in time series forecasting while acting predominantly in many other fields. We notice that the existing structure of Transformer-based forecasters may be not suitable for multivariate time series forecasting. As shown on the top of Figure 2, it is notable that the points of the same time step that basically represent completely different physical meanings recorded by inconsistent measurements are embedded into one token with wiped-out multivariate correlations. And the token formed by a single time step can struggle to reveal beneficial information due to excessively local receptive field and time-unaligned events represented by simultaneous time points. Besides, while series variations can be greatly influenced by the sequence order, permutation-invariant attention mechanisms are improperly adopted on the temporal dimension (Zeng et al. 2023). Consequently, Transformer is weakened to capture essential series representations and portray multivariate correlations, limiting its capacity and generalization ability on diverse time series data.

考虑到基于Transformer的预测器存在的争议，我们反思了为何Transformer在时间序列预测中表现甚至不如线性模型，而在许多其他领域却占据主导地位。我们注意到，现有的基于Transformer的预测器结构可能并不适合多变量时间序列预测。如图2顶部所示，值得注意的是，同一时间步的点（这些点基本上代表着由不一致的测量所记录的完全不同的物理意义）被嵌入到一个标记中，多变量之间的相关性被消除。而且，由单个时间步形成的标记由于感受野过于局部以及同时刻点所代表的时间未对齐事件，可能难以揭示有用信息。此外，虽然序列变化会受到序列顺序的极大影响，但在时间维度上却不恰当地采用了排列不变的注意力机制（曾等人，2023）。因此，Transformer在捕捉关键序列表征和描绘多变量相关性方面的能力被削弱，限制了其在各种时间序列数据上的性能和泛化能力。

Concerning the potential risks of embedding multivariate points of a timestamp as a (temporal) token, we take an inverted view on time series and embed the whole time series of each variate independently into a (variate) token, the extreme case of Patching (Nie et al. 2023) that enlarges local receptive field. By inverting, the embedded token aggregates the global representations of series that can be more variate-centric and better leveraged by booming attention mechanisms for multivariate correlating. Meanwhile, the feed-forward network can be proficient enough to learn generalizable representations for distinct variates encoded from arbitrary lookback series and decoded to predict future series.

关于将时间戳的多变量点嵌入为（时间）标记的潜在风险，我们对时间序列采取反向视角，将每个变量的整个时间序列独立嵌入为一个（变量）标记，这是扩大局部感受野的补丁法（聂等人，2023年）的极端情况。通过反向操作，嵌入的标记聚合了序列的全局表示，这些表示可以更以变量为中心，并且能更好地被蓬勃发展的注意力机制用于多变量关联。同时，前馈网络足以熟练地学习从任意回溯序列编码并解码以预测未来序列的不同变量的可泛化表示。

Based on the above motivations, we believe it is not that Transformer is ineffective for time series forecasting, but rather it is improperly used. In this paper, we revisit the structure of Transformer and advocate iTransformer as a fundamental backbone for time series forecasting. Technically, we embed each time series as variate tokens, adopt the attention for multivariate correlations, and employ the feed-forward network for series representations. Experimentally, the proposed iTransformer achieves state-of-the-art performance on real-world forecasting benchmarks shown in Figure 1 and surprisingly tackles the pain points of Transformer-based forecasters. Our contributions lie in three aspects:

基于上述动机，我们认为并非Transformer在时间序列预测中无效，而是使用不当。在本文中，我们重新审视了Transformer的结构，并倡导将iTransformer作为时间序列预测的基础架构。从技术上讲，我们将每个时间序列嵌入为变量标记，采用注意力机制处理多变量相关性，并使用前馈网络进行序列表示。通过实验，所提出的iTransformer在图1所示的实际预测基准上达到了最先进的性能，并且令人惊讶地解决了基于Transformer的预测器的痛点。我们的贡献主要体现在三个方面：

- We reflect on the architecture of Transformer and refine that the competent capability of native Transformer components on multivariate time series is underexplored.

- 我们反思了Transformer的架构，并指出原生Transformer组件在多变量时间序列方面的能力尚未得到充分挖掘。

- We propose iTransformer that regards independent time series as tokens to capture multivariate correlations by self-attention and utilize layer normalization and feed-forward network modules to learn better series-global representations for time series forecasting.

- 我们提出了iTransformer，它将独立的时间序列视为标记，通过自注意力机制捕捉多变量相关性，并利用层归一化和前馈网络模块学习更好的序列全局表示，以用于时间序列预测。

- Experimentally, iTransformer achieves comprehensive state-of-the-art on real-world benchmarks. We extensively analyze the inverted modules and architecture choices, indicating a promising direction for the future improvement of Transformer-based forecasters.

- 通过实验，iTransformer在实际基准测试中全面达到了最先进水平。我们对倒置模块和架构选择进行了广泛分析，为基于Transformer的预测器的未来改进指明了一个有前景的方向。

## 2 RELATED WORK

## 2 相关工作

With the progressive breakthrough made in natural language processing and computer vision areas, elaboratively designed Transformer variants are proposed to tackle ubiquitous time series forecasting applications. Going beyond contemporaneous TCNs (Bai et al., 2018; Liu et al., 2022a) and RNN-based forecasters (Zhao et al., 2017; Rangapuram et al., 2018; Salinas et al., 2020), Transformer has exhibited powerful sequence modeling capability and promising model scalability, leading to the trend of passionate modifications adapted for time series forecasting.

随着自然语言处理和计算机视觉领域取得逐步突破，人们提出了精心设计的Transformer变体，以应对无处不在的时间序列预测应用。超越了同期的时间卷积网络（TCNs，白等人，2018年；刘等人，2022a）和基于循环神经网络（RNN）的预测器（赵等人，2017年；兰加普拉姆等人，2018年；萨利纳斯等人，2020年），Transformer展现出强大的序列建模能力和良好的模型可扩展性，引发了为时间序列预测进行积极改进的趋势。

Through a systematical review of Transformer-based forecasters, we conclude that existing modifications can be divided into four categories by whether to modify the component and architecture. As shown in Figure 3, the first category (Wu et al., 2021; Li et al., 2021; Zhou et al., 2022), which is the most common practice, mainly concerns the component adaptation, especially the attention module for the temporal dependency modeling and the complexity optimization on long sequences. Nevertheless, with the rapid emergence of linear forecasters (Oreshkin et al., 2019; Zeng et al., 2023; Das et al. 2023; Liu et al. 2023), the impressive performance and efficiency continuously challenge this direction. Soon afterward, the second category attempts to fully utilize Transformer. It pays more attention to the inherent processing of time series, such as Stationarization (Liu et al. 2022b), Channel Independence, and Patching (Nie et al. 2023), which bring about consistently improved performance. Moreover, faced with the increasing significance of the independence and mutual interactions of multiple variates, the third category refurbishes Transformer in both aspects of component and architecture. Representative (Zhang & Yan, 2023) explicitly captures the cross-time and cross-variate dependencies by the renovated attention mechanism and architecture.

通过对基于Transformer的预测模型进行系统综述，我们得出结论：根据是否对组件和架构进行修改，现有的改进方法可分为四类。如图3所示，第一类（吴等人，2021年；李等人，2021年；周等人，2022年）是最常见的做法，主要关注组件适配，尤其是用于时间依赖建模的注意力模块以及长序列的复杂度优化。然而，随着线性预测模型的迅速涌现（奥列什金等人，2019年；曾等人，2023年；达斯等人，2023年；刘等人，2023年），其出色的性能和效率不断对这一方向提出挑战。随后，第二类方法试图充分利用Transformer。它更关注时间序列的内在处理，如平稳化（刘等人，2022b）、通道独立性和分块处理（聂等人，2023），这些方法带来了持续提升的性能。此外，面对多个变量的独立性和相互作用日益重要的情况，第三类方法在组件和架构两方面对Transformer进行了改进。具有代表性的（张和严，2023年）通过改进的注意力机制和架构明确捕捉了跨时间和跨变量的依赖关系。

Unlike previous works, iTransformer modifies none of the native components of Transformer. Instead, we adopt the components on the inverted dimensions with the altered architecture, as the only one that belongs to the fourth category to our best knowledge. We believe the capabilities of the components have stood the test extensively, the truth is that the architecture of Transformer is improperly adopted.

与以往的工作不同，iTransformer没有对Transformer的任何原生组件进行修改。相反，我们采用了具有改变后架构的倒置维度上的组件，据我们所知，这是唯一属于第四类的方法。我们认为这些组件的能力已经经过了广泛的考验，事实是Transformer的架构采用方式并不恰当。

<!-- Media -->

<!-- figureText: Attention (I) Autoformer, Informer, . (III) Crossformer,... Component (IV) iTransformer (Ours) Feed-forward Add & Norm Modified Attn Series Processing -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_2.jpg?x=308&y=1143&w=1181&h=318&r=0"/>

Figure 3: Transformer-based forecasters categorized by component and architecture modifications.

图3：基于Transformer的预测器按组件和架构修改进行分类。

<!-- Media -->

## 3 ITRANSFORMER

## 3 iTransformer

In multivariate time series forecasting,given historical observations $\mathbf{X} = \left\{  {{\mathbf{x}}_{1},\ldots ,{\mathbf{x}}_{T}}\right\}   \in  {\mathbb{R}}^{T \times  N}$ with $T$ time steps and $N$ variates,we predict the future $S$ time steps $\mathbf{Y} = \left\{  {{\mathbf{x}}_{T + 1},\ldots ,{\mathbf{x}}_{T + S}}\right\}   \in$ ${\mathbb{R}}^{S \times  N}$ . For convenience,we denote ${\mathbf{X}}_{t, : }$ as the simultaneously recorded time points at the step $t$ ,and ${\mathbf{X}}_{ : ,n}$ as the whole time series of each variate indexed by $n$ . It is notable that ${\mathbf{X}}_{t, : }$ may not contain time points that essentially reflect the same event in real-world scenarios because of the systematical time lags among variates in the dataset. Besides,the elements of ${\mathbf{X}}_{t, : }$ can be distinct from each other in physical measurements and statistical distributions,for which a variate ${\mathbf{X}}_{ : ,n}$ generally shares.

在多变量时间序列预测中，给定具有 $T$ 个时间步长和 $N$ 个变量的历史观测值 $\mathbf{X} = \left\{  {{\mathbf{x}}_{1},\ldots ,{\mathbf{x}}_{T}}\right\}   \in  {\mathbb{R}}^{T \times  N}$，我们预测未来 $S$ 个时间步长 $\mathbf{Y} = \left\{  {{\mathbf{x}}_{T + 1},\ldots ,{\mathbf{x}}_{T + S}}\right\}   \in$ ${\mathbb{R}}^{S \times  N}$。为方便起见，我们将 ${\mathbf{X}}_{t, : }$ 表示为第 $t$ 步同时记录的时间点，将 ${\mathbf{X}}_{ : ,n}$ 表示为以 $n$ 为索引的每个变量的整个时间序列。值得注意的是，由于数据集中变量之间存在系统性的时间滞后，在现实场景中，${\mathbf{X}}_{t, : }$ 可能不包含本质上反映同一事件的时间点。此外，${\mathbf{X}}_{t, : }$ 的元素在物理测量和统计分布上可能彼此不同，而变量 ${\mathbf{X}}_{ : ,n}$ 通常具有这些特征。

### 3.1 STRUCTURE OVERVIEW

### 3.1 结构概述

Our proposed iTransformer illustrated in Figure 4 adopts the encoder-only architecture of Transformer (Vaswani et al. 2017), including the embedding, projection, and Transformer blocks.

我们提出的iTransformer（如图4所示）采用了Transformer（瓦斯瓦尼等人，2017年）的仅编码器架构，包括嵌入层、投影层和Transformer模块。

Embedding the whole series as the token Most Transformer-based forecasters typically regard multiple variates of the same time as the (temporal) token and follow the generative formulation of forecasting tasks. However, we find the approach on the numerical modality can be less instructive for learning attention maps, which is supported by increasing applications of Patching (Dosovitskiy et al. 2021; Nie et al. 2023) that broadens the respective field. Meanwhile, the triumph of linear forecasters also challenges the necessity of adopting a heavy encoder-decoder Transformer for generating tokens. Instead, our proposed encoder-only iTransformer focuses on representation learning and adaptive correlating of multivariate series. Each time series driven by the underlying complicated process is firstly tokenized to describe the properties of the variate, applied by self-attention for mutual interactions, and individually processed by feed-forward networks for series representations. Notably, the task to generate the predicted series is essentially delivered to linear layers, which has been proven competent by previous work (Das et al. 2023) and we provide a detailed analysis in the next section.

将整个序列嵌入为标记 大多数基于Transformer的预测器通常将同一时间的多个变量视为（时间）标记，并遵循预测任务的生成式公式。然而，我们发现数值模态的方法在学习注意力图方面的指导意义可能较小，这一点得到了越来越多的分块（Patching）应用的支持（多索维茨基等人（Dosovitskiy et al.）2021年；聂等人（Nie et al.）2023年），这些应用拓宽了相关领域。同时，线性预测器的成功也对采用复杂的编码器 - 解码器Transformer来生成标记的必要性提出了挑战。相反，我们提出的仅编码器的iTransformer专注于多元序列的表示学习和自适应关联。由底层复杂过程驱动的每个时间序列首先被标记化以描述变量的属性，通过自注意力机制进行相互交互，并由前馈网络单独处理以获得序列表示。值得注意的是，生成预测序列的任务本质上交给了线性层，先前的工作（达斯等人（Das et al.）2023年）已证明其具备胜任能力，我们将在下一节提供详细分析。

<!-- Media -->

<!-- figureText: Output $\uparrow$ Features $\widehat{x} = \frac{x - \mu }{\sigma }$ Projection LayerNorm Temporal LayerNorm Feed-forward LayerNorm Multivariate Attention Raw Embedding Embedding -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_3.jpg?x=306&y=227&w=1183&h=500&r=0"/>

Figure 4: Overall structure of iTransformer, which shares the same modular arrangement with the encoder of Transformer. (a) Raw series of different variates are independently embedded as tokens. (b) Self-attention is applied to embedded variate tokens with enhanced interpretability revealing multivariate correlations. (c) Series representations of each token are extracted by the shared feed-forward network. (d) Layer normalization is adopted to reduce the discrepancies among variates.

图4：iTransformer的整体结构，它与Transformer的编码器具有相同的模块排列方式。(a) 不同变量的原始序列被独立嵌入为标记。(b) 自注意力机制应用于嵌入后的变量标记，增强了解释性，揭示了多变量之间的相关性。(c) 每个标记的序列表示通过共享的前馈网络提取。(d) 采用层归一化来减少变量之间的差异。

<!-- Media -->

Based on the above considerations, in iTransformer, the process of predicting future series of each specific variate ${\widehat{\mathbf{Y}}}_{ : ,n}$ based on the lookback series ${\mathbf{X}}_{ : ,n}$ is simply formulated as follows:

基于上述考虑，在iTransformer中，基于历史序列${\mathbf{X}}_{ : ,n}$预测每个特定变量${\widehat{\mathbf{Y}}}_{ : ,n}$的未来序列的过程可简单表述如下：

$$
{\mathbf{h}}_{n}^{0} = \operatorname{Embedding}\left( {\mathbf{X}}_{ : ,n}\right) ,
$$

$$
{\mathbf{H}}^{l + 1} = \operatorname{TrmBlock}\left( {\mathbf{H}}^{l}\right) ,l = 0,\ldots ,L - 1, \tag{1}
$$

$$
{\widehat{\mathbf{Y}}}_{ : ,n} = \operatorname{Projection}\left( {\mathbf{h}}_{n}^{L}\right) ,
$$

where $\mathbf{H} = \left\{  {{\mathbf{h}}_{1},\ldots ,{\mathbf{h}}_{N}}\right\}   \in  {\mathbb{R}}^{N \times  D}$ contains $N$ embedded tokens of dimension $D$ and the superscript denotes the layer index. Embedding : ${\mathbb{R}}^{T} \mapsto  {\mathbb{R}}^{D}$ and Projection : ${\mathbb{R}}^{D} \mapsto  {\mathbb{R}}^{S}$ are both implemented by multi-layer perceptron (MLP). The obtained variate tokens interact with each other by self-attention and are independently processed by the shared feed-forward network in each TrmBlock. Specifically, as the order of sequence is implicitly stored in the neuron permutation of the feed-forward network, the position embedding in the vanilla Transformer is no longer needed here.

其中 $\mathbf{H} = \left\{  {{\mathbf{h}}_{1},\ldots ,{\mathbf{h}}_{N}}\right\}   \in  {\mathbb{R}}^{N \times  D}$ 包含维度为 $D$ 的 $N$ 嵌入标记，上标表示层索引。嵌入层（Embedding）：${\mathbb{R}}^{T} \mapsto  {\mathbb{R}}^{D}$ 和投影层（Projection）：${\mathbb{R}}^{D} \mapsto  {\mathbb{R}}^{S}$ 均由多层感知机（MLP）实现。所得到的变量标记通过自注意力机制相互交互，并在每个TrmBlock中由共享的前馈网络独立处理。具体而言，由于序列顺序隐式存储在前馈网络的神经元排列中，因此这里不再需要原始Transformer中的位置嵌入。

iTransformers The architecture essentially presupposes no more specific requirements on Transformer variants, other than the attention is applicable for multivariate correlation. Thus, a bundle of efficient attention mechanisms (Li et al., 2021, Wu et al., 2022, Dao et al., 2022) can be the plugins, reducing the complexity when the variate number grows large. Besides, with the input flexibility of attention, the token number can vary from training to inference, and the model is allowed to be trained on arbitrary numbers of variates. The inverted Transformers, named iTransformers, are extensively evaluated in experiments of Section 4.2 and demonstrate advantages on time series forecasting.

iTransformers（逆变压器模型） 该架构本质上对Transformer变体没有更多特定要求，除了注意力机制适用于多变量相关性。因此，一系列高效的注意力机制（李等人，2021年；吴等人，2022年；道等人，2022年）可以作为插件，在变量数量大幅增加时降低复杂度。此外，由于注意力机制的输入具有灵活性，从训练到推理阶段，标记数量可以有所变化，并且允许模型在任意数量的变量上进行训练。名为iTransformers的逆变压器模型在4.2节的实验中得到了广泛评估，并在时间序列预测方面展现出优势。

### 3.2 INVERTED TRANSFORMER COMPONENTS

### 3.2 逆变压器组件

We organize a stack of $L$ blocks composed of the layer normalization,feed-forward network,and self-attention modules. But their duties on the inverted dimension are carefully reconsidered.

我们组织了一组由层归一化、前馈网络和自注意力模块组成的$L$块。但我们仔细重新考虑了它们在逆维度上的作用。

Layer normalization Layer normalization (Ba et al. 2016) is originally proposed to increase the convergence and training stability of deep networks. In typical Transformer-based forecasters, the module normalizes the multivariate representation of the same timestamp, gradually fusing the variates with each other. Once the collected time points do not represent the same event, the operation will also introduce interaction noises between noncausal or delayed processes. In our inverted version, the normalization is applied to the series representation of individual variate as Equation 2, which has been studied and proved effective in tackling non-stationary problems (Kim et al., 2021; Liu et al. 2022b). Besides, since all series as (variate) tokens are normalized to a Gaussian distribution, the discrepancies caused by inconsistent measurements can be diminished. By contrast, in previous architecture, different tokens of time steps will be normalized, leading to oversmooth time series.

层归一化 层归一化（Ba等人，2016年）最初是为了提高深度网络的收敛性和训练稳定性而提出的。在典型的基于Transformer的预测模型中，该模块对同一时间戳的多变量表示进行归一化，逐步使各个变量相互融合。一旦收集到的时间点并不代表同一事件，该操作还会在非因果或延迟过程之间引入交互噪声。在我们的反向版本中，如公式2所示，归一化应用于单个变量的序列表示，这一方法已被研究并证明在处理非平稳问题方面是有效的（Kim等人，2021年；Liu等人，2022b）。此外，由于所有作为（变量）标记的序列都被归一化为高斯分布，因此可以减少因测量不一致而导致的差异。相比之下，在之前的架构中，时间步的不同标记会被归一化，从而导致时间序列过度平滑。

$$
\operatorname{LayerNorm}\left( \mathbf{H}\right)  = \left\{  {\left. \frac{{\mathbf{h}}_{n} - \operatorname{Mean}\left( {\mathbf{h}}_{n}\right) }{\sqrt{\operatorname{Var}\left( {\mathbf{h}}_{n}\right) }}\right| \;n = 1,\ldots ,N}\right\}   \tag{2}
$$

Feed-forward network Transformer adopts the feed-forward network (FFN) as the basic building block for encoding token representation and it is identically applied to each token. As aforementioned, in the vanilla Transformer, multiple variates of the same timestamp that form the token can be malpositioned and too localized to reveal enough information for predictions. In the inverted version, FFN is leveraged on the series representation of each variate token. By the universal approximation theorem (Hornik, 1991), they can extract complicated representations to describe a time series. With the stacking of inverted blocks, they are devoted to encoding the observed time series and decoding the representations for future series using dense non-linear connections, which work effectively as the recent works completely built on MLPs (Tolstikhin et al., 2021; Das et al., 2023).

前馈网络Transformer采用前馈网络（FFN）作为对标记表示进行编码的基本构建块，并且对每个标记都进行相同的应用。如前所述，在原始的Transformer中，构成标记的同一时间戳的多个变量可能会出现错位，并且过于局部化，无法为预测提供足够的信息。在倒置版本中，FFN被应用于每个变量标记的序列表示。根据通用逼近定理（霍尼克（Hornik），1991年），它们可以提取复杂的表示来描述时间序列。通过堆叠倒置块，它们致力于对观测到的时间序列进行编码，并使用密集的非线性连接对未来序列的表示进行解码，这与最近完全基于多层感知机（MLP）的研究工作（托尔斯季欣（Tolstikhin）等人，2021年；达斯（Das）等人，2023年）一样有效。

More interestingly, the identical linear operation on independent time series, which serves as the combination of the recent linear forecasters (Zeng et al. 2023) and Channel Independence (Nie et al. 2023), can be instructive for us to understand the series representations. Recent revisiting on linear forecasters (Li et al. 2023) highlights that temporal features extracted by MLPs are supposed to be shared within distinct time series. We propose a rational explanation that the neurons of MLP are taught to portray the intrinsic properties of any time series, such as the amplitude, periodicity, and even frequency spectrums (neuron as a filter), serving as a more advantageous predictive representation learner than the self-attention applied on time points. Experimentally, we validate that the division of labor helps enjoy the benefits of linear layers in Section 4.3, such as the promoted performance if providing enlarged lookback series, and the generalization ability on unseen variates.

更有趣的是，对独立时间序列进行的相同线性运算，它是近期线性预测器（曾等人，2023年）和通道独立性（聂等人，2023年）的结合，这有助于我们理解序列表示。近期对线性预测器的重新研究（李等人，2023年）强调，多层感知器（MLP）提取的时间特征应该在不同的时间序列中共享。我们提出了一个合理的解释，即多层感知器的神经元被训练来描绘任何时间序列的内在属性，如振幅、周期性，甚至频谱（神经元作为滤波器），这使其成为比应用于时间点的自注意力机制更具优势的预测表示学习器。在实验方面，我们在4.3节验证了分工有助于发挥线性层的优势，例如在提供更长的回溯序列时性能得到提升，以及对未见变量的泛化能力。

Self-attention While the attention mechanism is generally adopted for facilitating the temporal dependencies modeling in previous forecasters, the inverted model regards the whole series of one variate as an independent process. Concretely, with comprehensively extracted representations of each time series $\mathbf{H} = \left\{  {{\mathbf{h}}_{0},\ldots ,{\mathbf{h}}_{N}}\right\}   \in  {\mathbb{R}}^{N \times  D}$ ,the self-attention module adopts linear projections to get queries,keys,and values $\mathbf{Q},\mathbf{K},\mathbf{V} \in  {\mathbb{R}}^{N \times  {d}_{k}}$ ,where ${d}_{k}$ is the projected dimension.

自注意力机制 虽然在以往的预测模型中，注意力机制通常用于促进时间依赖关系的建模，但逆向模型将单个变量的整个序列视为一个独立的过程。具体而言，在全面提取每个时间序列 $\mathbf{H} = \left\{  {{\mathbf{h}}_{0},\ldots ,{\mathbf{h}}_{N}}\right\}   \in  {\mathbb{R}}^{N \times  D}$ 的表征后，自注意力模块采用线性投影来获取查询向量（queries）、键向量（keys）和值向量（values） $\mathbf{Q},\mathbf{K},\mathbf{V} \in  {\mathbb{R}}^{N \times  {d}_{k}}$，其中 ${d}_{k}$ 是投影维度。

With denotation of ${\mathbf{q}}_{i},{\mathbf{k}}_{j} \in  {\mathbb{R}}^{{d}_{k}}$ as the specific query and key of one (variate) token,we notice that each entry of the pre-Softmax scores is formulated as ${\mathbf{A}}_{i,j} = {\left( {\mathbf{{QK}}}^{\top }/\sqrt{{d}_{k}}\right) }_{i,j} \propto  {\mathbf{q}}_{i}^{\top }{\mathbf{k}}_{j}$ . Since each token is previously normalized on its feature dimension, the entries can somewhat reveal the variate-wise correlation,and the whole score map $\mathbf{A} \in  {\mathbb{R}}^{N \times  N}$ exhibits the multivariate correlations between paired variate tokens. Consequently, highly correlated variate will be more weighted for the next representation interaction with values $\mathbf{V}$ . Based on this intuition,the proposed mechanism is believed to be more natural and interpretable for multivariate series forecasting. We further provide the visualization analysis of the score map in Section 4.3 and Appendix E.1

将 ${\mathbf{q}}_{i},{\mathbf{k}}_{j} \in  {\mathbb{R}}^{{d}_{k}}$ 表示为一个（变量）标记的特定查询和键，我们注意到 Softmax 前得分的每个元素被公式化为 ${\mathbf{A}}_{i,j} = {\left( {\mathbf{{QK}}}^{\top }/\sqrt{{d}_{k}}\right) }_{i,j} \propto  {\mathbf{q}}_{i}^{\top }{\mathbf{k}}_{j}$。由于每个标记之前已在其特征维度上进行了归一化，这些元素在一定程度上可以揭示变量间的相关性，并且整个得分图 $\mathbf{A} \in  {\mathbb{R}}^{N \times  N}$ 展示了成对变量标记之间的多变量相关性。因此，对于下一次与值 $\mathbf{V}$ 的表示交互，高度相关的变量将被赋予更高的权重。基于这种直觉，所提出的机制被认为对于多变量序列预测更自然且更具可解释性。我们将在第 4.3 节和附录 E.1 中进一步提供得分图的可视化分析

## 4 EXPERIMENTS

## 4 实验

We thoroughly evaluate the proposed iTransformer on various time series forecasting applications, validate the generality of the proposed framework and further dive into the effectiveness of applying the Transformer components on the inverted dimensions of time series.

我们在各种时间序列预测应用中对所提出的iTransformer进行了全面评估，验证了所提出框架的通用性，并进一步深入研究了将Transformer组件应用于时间序列倒置维度的有效性。

Datasets We extensively include 7 real-world datasets in our experiments, including ECL, ETT (4 subsets), Exchange, Traffic, Weather used by Autoformer (Wu et al. 2021), Solar-Energy datasets proposed in LSTNet (Lai et al. 2018), and PEMS (4 subsets) evaluated in SCINet (Liu et al. 2022a). We also provide the experiments on Market ( 6 subsets) in Appendix F.4. It records the minute-sampled server load of Alipay online transaction application with hundreds of variates, where we consistently outperform other baselines. Detailed dataset descriptions are provided in Appendix A.1.

数据集 我们在实验中广泛纳入了7个真实世界的数据集，包括电力消耗数据集（ECL）、电力变压器温度数据集（ETT，4个子集）、汇率数据集（Exchange）、交通数据集（Traffic）、气象数据集（Weather，由Autoformer使用（Wu等人，2021年））、太阳能数据集（Solar - Energy，由LSTNet提出（Lai等人，2018年）），以及加州交通传感器数据集（PEMS，4个子集，由SCINet评估（Liu等人，2022a））。我们还在附录F.4中提供了市场数据集（Market，6个子集）的实验结果。该数据集记录了支付宝在线交易应用以分钟为采样间隔的服务器负载，包含数百个变量，在该数据集上我们的模型始终优于其他基线模型。详细的数据集描述见附录A.1。

### 4.1 FORECASTING RESULTS

### 4.1 预测结果

In this section, we conduct extensive experiments to evaluate the forecasting performance of our proposed model together with advanced deep forecasters.

在本节中，我们进行了广泛的实验，以评估我们提出的模型与先进的深度预测模型的预测性能。

Baselines We carefully choose 10 well-acknowledged forecasting models as our benchmark, including (1) Transformer-based methods: Autoformer (Wu et al. 2021), FEDformer (Zhou et al., 2022), Stationary (Liu et al., 2022b), Crossformer (Zhang & Yan, 2023), PatchTST (Nie et al., 2023); (2) Linear-based methods: DLinear (Zeng et al., 2023), TiDE (Das et al., 2023), RLinear (Li et al., 2023); and (3) TCN-based methods: SCINet (Liu et al. 2022a), TimesNet (Wu et al. 2023).

基线 我们精心挑选了10个广为人知的预测模型作为基准，其中包括：（1）基于Transformer的方法：Autoformer（吴等人，2021年）、FEDformer（周等人，2022年）、Stationary（刘等人，2022b）、Crossformer（张和严，2023年）、PatchTST（聂等人，2023年）；（2）基于线性的方法：DLinear（曾等人，2023年）、TiDE（达斯等人，2023年）、RLinear（李等人，2023年）；（3）基于TCN的方法：SCINet（刘等人，2022a）、TimesNet（吴等人，2023年）。

Main results Comprehensive forecasting results are listed in Table 1 with the best in red and the second underlined. The lower MSE/MAE indicates the more accurate prediction result. Compared with other forecasters, iTransformer is particularly good at forecasting high-dimensional time series. Besides, PatchTST as the previous state-of-the-art, fails in many cases of PEMS, which can stem from the extremely fluctuating series of the dataset, and the patching mechanism of PatchTST may lose focus on specific locality to handle rapid fluctuation. By contrast, the proposed model aggregating the whole series variations for series representations can better cope with this situation. Notably, as the representative that explicitly captures multivariate correlations, the performance of Crossformer is still subpar to iTransformer, indicating the interaction of time-unaligned patches from different multivariate will bring about unnecessary noise for forecasting. Therefore, the native Transformer components are competent for temporal modeling and multivariate correlating, and the proposed inverted architecture can effectively tackle real-world time series forecasting scenarios.

主要结果 综合预测结果列于表1，最佳结果用红色标注，次佳结果加下划线。均方误差（MSE）/平均绝对误差（MAE）越低，表示预测结果越准确。与其他预测器相比，iTransformer特别擅长预测高维时间序列。此外，作为先前的最优模型，PatchTST在PEMS（加州交通传感器数据集）的许多情况下表现不佳，这可能是由于该数据集的序列波动极大，并且PatchTST的分块机制可能会忽略特定局部信息，从而难以处理快速波动。相比之下，所提出的模型通过聚合整个序列的变化来进行序列表示，能够更好地应对这种情况。值得注意的是，作为明确捕捉多变量相关性的代表模型，Crossformer的性能仍逊于iTransformer，这表明来自不同多变量的时间未对齐分块之间的交互会给预测带来不必要的噪声。因此，原生Transformer组件能够胜任时间建模和多变量关联任务，并且所提出的倒置架构能够有效处理现实世界中的时间序列预测场景。

Table 1: Multivariate forecasting results with prediction lengths $S \in  \{ {12},{24},{36},{48}\}$ for PEMS and $S \in  \{ {96},{192},{336},{720}\}$ for others and fixed lookback length $T = {96}$ . Results are averaged from all prediction lengths. Avg means further averaged by subsets. Full results are listed in Appendix F.4.

表1：多变量预测结果，其中加州交通性能测量系统（PEMS）的预测长度为$S \in  \{ {12},{24},{36},{48}\}$，其他的预测长度为$S \in  \{ {96},{192},{336},{720}\}$，固定回溯长度为$T = {96}$。结果是所有预测长度的平均值。Avg表示按子集进一步求平均值。完整结果列于附录F.4。

<!-- Media -->

<table><tr><td>Models</td><td colspan="2">iTransformer (Ours)</td><td colspan="2">RLinear 2023)</td><td colspan="2">PatchTST 2023)</td><td colspan="2">Crossformer 2023)</td><td colspan="2">TiDE 2023)</td><td colspan="2">TimesNet 2023)</td><td colspan="2">DLinear 2023)</td><td colspan="2">SCINet 2022a)</td><td colspan="2">FEDformer 2022)</td><td colspan="2">Stationary 2022b)</td><td colspan="2">Autoformer (2021)</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td/><td>MSE MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>ECL</td><td>0.178</td><td>0.270</td><td>0.219 0.2</td><td>0.298</td><td>0.205</td><td>$\underline{0.290}$</td><td>0.244</td><td>0.334</td><td>0.251 0.344</td><td/><td>0.192 0.295</td><td/><td>0.212</td><td>0.300</td><td>0.268</td><td>0.365</td><td/><td>0.214 0.327</td><td/><td>0.193 0.296</td><td>0.227</td><td>0.338</td></tr><tr><td>ETT (Avg)</td><td>0.383</td><td>0.399</td><td>0.380</td><td>0.392</td><td>$\underline{0.381}$</td><td>0.397</td><td>0.685</td><td>0.578</td><td>0.482 0.470</td><td/><td>0.391</td><td>0.404</td><td>0.442</td><td>0.444</td><td>0.689</td><td>0.597</td><td>0.408</td><td>0.428</td><td/><td>0.471 0.464</td><td>0.465</td><td>0.459</td></tr><tr><td>Exchange</td><td>$\underline{0.360}$</td><td>0.403</td><td>0.378</td><td>0.417</td><td>0.367</td><td>$\underline{0.404}$</td><td>0.940</td><td>0.707</td><td>0.370 0.413</td><td/><td>0.416</td><td>0.443</td><td>0.354</td><td>0.414</td><td>0.750</td><td>0.626</td><td>0.519</td><td>0.429</td><td>0.461</td><td>0.454</td><td>0.613</td><td>0.539</td></tr><tr><td>Traffic</td><td>0.428</td><td>0.282</td><td>0.626</td><td>0.378</td><td>$\underline{0.481}$</td><td>0.304</td><td>0.550</td><td>$\underline{0.304}$</td><td>0.760 0.473</td><td/><td>0.620</td><td>0.336</td><td>0.625</td><td>0.383</td><td>0.804</td><td>0.509</td><td>0.610</td><td>0.376</td><td>0.624</td><td>0.340</td><td>0.628</td><td>0.379</td></tr><tr><td>Weather</td><td>0.258</td><td>0.278</td><td>0.272</td><td>0.291</td><td>0.259</td><td>0.281</td><td>0.259</td><td>0.315</td><td>0.271</td><td>0.320</td><td>0.259</td><td>0.287</td><td>0.265</td><td>0.317</td><td>0.292</td><td>0.363</td><td>0.309</td><td>0.360</td><td>0.288</td><td>0.314</td><td>0.338</td><td>0.382</td></tr><tr><td>Solar-Energy</td><td>0.233</td><td>0.262</td><td>0.369</td><td>0.356</td><td>0.270</td><td>0.307</td><td>0.641</td><td>0.639</td><td>0.347</td><td>0.417</td><td>0.301</td><td>0.319</td><td>0.330</td><td>0.401</td><td>0.282</td><td>0.375</td><td>0.291</td><td>0.381</td><td>0.261</td><td>0.381</td><td>0.885</td><td>0.711</td></tr><tr><td>EMS (Avg)</td><td>0.119</td><td>0.218</td><td>0.514</td><td>0.482</td><td>0.217</td><td>0.305</td><td>0.220</td><td>0.304</td><td>0.375</td><td>0.440</td><td>0.148</td><td>0.246</td><td>0.320</td><td>0.394</td><td>0.121</td><td>0.222</td><td>0.224</td><td>0.327</td><td>0.151</td><td>0.249</td><td>0.614</td><td>0.575</td></tr></table>

<table><tbody><tr><td>模型</td><td colspan="2">iTransformer（我们的方法）</td><td colspan="2">RLinear（2023年）</td><td colspan="2">PatchTST（2023年）</td><td colspan="2">Crossformer（2023年）</td><td colspan="2">TiDE（2023年）</td><td colspan="2">时间网络（TimesNet 2023）</td><td colspan="2">深度线性模型（DLinear 2023）</td><td colspan="2">时空卷积网络（SCINet 2022a）</td><td colspan="2">联邦变压器（FEDformer 2022）</td><td colspan="2">平稳模型（Stationary 2022b）</td><td colspan="2">自动变压器（Autoformer 2021）</td></tr><tr><td>指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td>事件对比损失（ECL）</td><td>0.178</td><td>0.270</td><td>0.219 0.2</td><td>0.298</td><td>0.205</td><td>$\underline{0.290}$</td><td>0.244</td><td>0.334</td><td>0.251 0.344</td><td></td><td>0.192 0.295</td><td></td><td>0.212</td><td>0.300</td><td>0.268</td><td>0.365</td><td></td><td>0.214 0.327</td><td></td><td>0.193 0.296</td><td>0.227</td><td>0.338</td></tr><tr><td>事件时间预测（平均）（ETT (Avg)）</td><td>0.383</td><td>0.399</td><td>0.380</td><td>0.392</td><td>$\underline{0.381}$</td><td>0.397</td><td>0.685</td><td>0.578</td><td>0.482 0.470</td><td></td><td>0.391</td><td>0.404</td><td>0.442</td><td>0.444</td><td>0.689</td><td>0.597</td><td>0.408</td><td>0.428</td><td></td><td>0.471 0.464</td><td>0.465</td><td>0.459</td></tr><tr><td>交换；交易</td><td>$\underline{0.360}$</td><td>0.403</td><td>0.378</td><td>0.417</td><td>0.367</td><td>$\underline{0.404}$</td><td>0.940</td><td>0.707</td><td>0.370 0.413</td><td></td><td>0.416</td><td>0.443</td><td>0.354</td><td>0.414</td><td>0.750</td><td>0.626</td><td>0.519</td><td>0.429</td><td>0.461</td><td>0.454</td><td>0.613</td><td>0.539</td></tr><tr><td>交通；流量</td><td>0.428</td><td>0.282</td><td>0.626</td><td>0.378</td><td>$\underline{0.481}$</td><td>0.304</td><td>0.550</td><td>$\underline{0.304}$</td><td>0.760 0.473</td><td></td><td>0.620</td><td>0.336</td><td>0.625</td><td>0.383</td><td>0.804</td><td>0.509</td><td>0.610</td><td>0.376</td><td>0.624</td><td>0.340</td><td>0.628</td><td>0.379</td></tr><tr><td>天气</td><td>0.258</td><td>0.278</td><td>0.272</td><td>0.291</td><td>0.259</td><td>0.281</td><td>0.259</td><td>0.315</td><td>0.271</td><td>0.320</td><td>0.259</td><td>0.287</td><td>0.265</td><td>0.317</td><td>0.292</td><td>0.363</td><td>0.309</td><td>0.360</td><td>0.288</td><td>0.314</td><td>0.338</td><td>0.382</td></tr><tr><td>太阳能（Solar-Energy）</td><td>0.233</td><td>0.262</td><td>0.369</td><td>0.356</td><td>0.270</td><td>0.307</td><td>0.641</td><td>0.639</td><td>0.347</td><td>0.417</td><td>0.301</td><td>0.319</td><td>0.330</td><td>0.401</td><td>0.282</td><td>0.375</td><td>0.291</td><td>0.381</td><td>0.261</td><td>0.381</td><td>0.885</td><td>0.711</td></tr><tr><td>能源管理系统平均值（EMS (Avg)）</td><td>0.119</td><td>0.218</td><td>0.514</td><td>0.482</td><td>0.217</td><td>0.305</td><td>0.220</td><td>0.304</td><td>0.375</td><td>0.440</td><td>0.148</td><td>0.246</td><td>0.320</td><td>0.394</td><td>0.121</td><td>0.222</td><td>0.224</td><td>0.327</td><td>0.151</td><td>0.249</td><td>0.614</td><td>0.575</td></tr></tbody></table>

<!-- Media -->

### 4.2 ITRANSFORMERS GENERALITY

### 4.2 ITRANSFORMERS的通用性

In this section,we evaluate $i$ Transformers by applying our framework to Transformer and its variants, which generally address the quadratic complexity of the self-attention mechanism, including Reformer (Kitaev et al., 2020), Informer (Li et al., 2021), Flowformer (Wu et al., 2022) and FlashAt-tention (Dao et al. 2022). Surprising and promising discoveries are exhibited, indicating the simple inverted perspective can enhance Transformer-based forecasters with promoted performance with efficiency, generalization on unseen variates, and better utilization of historical observations.

在本节中，我们通过将我们的框架应用于Transformer及其变体来评估$i$ Transformer，这些变体通常解决了自注意力机制的二次复杂度问题，包括Reformer（基塔耶夫等人，2020年）、Informer（李等人，2021年）、Flowformer（吴等人，2022年）和FlashAttention（道等人，2022年）。我们展示了令人惊讶且有前景的发现，表明简单的反向视角可以增强基于Transformer的预测器，使其在效率、对未见变量的泛化能力以及对历史观测值的更好利用方面表现更优。

Performance promotion We evaluate Transformers and the corresponding iTransformers with the reported performance promotions in Table 2. It is notable that the framework consistently improves various Transformers. Overall, it achieves averaged 38.9% promotion on Transformer, 36.1% on Reformer, 28.5% on Informer, 16.8% on Flowformer and 32.2% on Flashformer, revealing the previous improper usage of the Transformer architecture on time series forecasting. Moreover, since the attention mechanism is adopted on the variate dimension in our inverted structure, the introduction of efficient attentions with linear complexity essentially addresses the computational problem due to numerous variates, which is prevalent in real-world applications but can be resource-consuming for Channel Independence (Nie et al. 2023). Therefore, the idea of iTransformer can be widely practiced on Transformer-based forecasters to take advantage of booming efficient attention mechanisms.

性能提升 我们根据表2中报告的性能提升情况，对Transformer模型及其对应的iTransformer模型进行评估。值得注意的是，该框架持续改进了各种Transformer模型。总体而言，它在Transformer模型上平均实现了38.9%的提升，在Reformer模型上实现了36.1%的提升，在Informer模型上实现了28.5%的提升，在Flowformer模型上实现了16.8%的提升，在Flashformer模型上实现了32.2%的提升，这揭示了之前在时间序列预测中对Transformer架构的不当使用。此外，由于我们的倒置结构在变量维度上采用了注意力机制，引入具有线性复杂度的高效注意力机制从根本上解决了因大量变量导致的计算问题，这一问题在实际应用中普遍存在，但对于通道独立性（Nie等人，2023）而言可能会消耗大量资源。因此，iTransformer的理念可以广泛应用于基于Transformer的预测模型，以充分利用蓬勃发展的高效注意力机制。

<!-- Media -->

Table 2: Performance promotion obtained by our inverted framework. Flashformer means Transformer equipped with hardware-accelerated FlashAttention (Dao et al. 2022). We report the average performance and the relative MSE reduction (Promotion). Full results can be found in Appendix F.2.

表2：我们的倒置框架所实现的性能提升。Flashformer指配备了硬件加速的FlashAttention（Dao等人，2022）的Transformer模型。我们报告了平均性能和相对均方误差降低率（提升率）。完整结果见附录F.2。

<table><tr><td colspan="2">Models</td><td colspan="2">Transformer (2017)</td><td colspan="2">Reformer (2020)</td><td colspan="2">Informer (2021)</td><td colspan="2">Flowformer (2022)</td><td colspan="2">Flashformer (2022)</td></tr><tr><td colspan="2">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="3">ECL</td><td>Original</td><td>0.277</td><td>0.372</td><td>0.338</td><td>0.422</td><td>0.311</td><td>0.397</td><td>0.267</td><td>0.359</td><td>0.285</td><td>0.377</td></tr><tr><td>+Inverted</td><td>0.178</td><td>0.270</td><td>0.208</td><td>0.301</td><td>0.216</td><td>0.311</td><td>0.210</td><td>0.293</td><td>0.206</td><td>0.291</td></tr><tr><td>Promotion</td><td>35.6%</td><td>27.4%</td><td>38.4%</td><td>28.7%</td><td>30.5%</td><td>21.6%</td><td>21.3%</td><td>18.6%</td><td>27.8%</td><td>22.9%</td></tr><tr><td rowspan="3">Traffic</td><td>Original</td><td>0.665</td><td>0.363</td><td>0.741</td><td>0.422</td><td>0.764</td><td>0.416</td><td>0.750</td><td>0.421</td><td>0.658</td><td>0.356</td></tr><tr><td>+Inverted</td><td>0.428</td><td>0.282</td><td>0.647</td><td>0.370</td><td>0.662</td><td>0.380</td><td>0.524</td><td>0.355</td><td>0.492</td><td>0.333</td></tr><tr><td>Promotion</td><td>35.6%</td><td>22.3%</td><td>12.7%</td><td>12.3%</td><td>13.3%</td><td>8.6%</td><td>30.1%</td><td>15.6%</td><td>25.2%</td><td>6.4%</td></tr><tr><td rowspan="3">Weather</td><td>Original</td><td>0.657</td><td>0.572</td><td>0.803</td><td>0.656</td><td>0.634</td><td>0.548</td><td>0.286</td><td>0.308</td><td>0.659</td><td>0.574</td></tr><tr><td>+Inverted</td><td>0.258</td><td>0.279</td><td>0.248</td><td>0.292</td><td>0.271</td><td>0.330</td><td>0.266</td><td>0.285</td><td>0.262</td><td>0.282</td></tr><tr><td>Promotion</td><td>60.2%</td><td>50.8%</td><td>69.2%</td><td>55.5%</td><td>57.3%</td><td>39.8%</td><td>7.2%</td><td>7.7%</td><td>60.2%</td><td>50.8%</td></tr></table>

<table><tbody><tr><td colspan="2">模型</td><td colspan="2">Transformer模型（2017年）</td><td colspan="2">Reformer模型（2020年）</td><td colspan="2">Informer模型（2021年）</td><td colspan="2">Flowformer模型（2022年）</td><td colspan="2">Flashformer模型（2022年）</td></tr><tr><td colspan="2">指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td rowspan="3">欧几里得聚类损失（ECL）</td><td>原始的</td><td>0.277</td><td>0.372</td><td>0.338</td><td>0.422</td><td>0.311</td><td>0.397</td><td>0.267</td><td>0.359</td><td>0.285</td><td>0.377</td></tr><tr><td>+反转的</td><td>0.178</td><td>0.270</td><td>0.208</td><td>0.301</td><td>0.216</td><td>0.311</td><td>0.210</td><td>0.293</td><td>0.206</td><td>0.291</td></tr><tr><td>推广</td><td>35.6%</td><td>27.4%</td><td>38.4%</td><td>28.7%</td><td>30.5%</td><td>21.6%</td><td>21.3%</td><td>18.6%</td><td>27.8%</td><td>22.9%</td></tr><tr><td rowspan="3">流量</td><td>原始的</td><td>0.665</td><td>0.363</td><td>0.741</td><td>0.422</td><td>0.764</td><td>0.416</td><td>0.750</td><td>0.421</td><td>0.658</td><td>0.356</td></tr><tr><td>+反转的</td><td>0.428</td><td>0.282</td><td>0.647</td><td>0.370</td><td>0.662</td><td>0.380</td><td>0.524</td><td>0.355</td><td>0.492</td><td>0.333</td></tr><tr><td>推广</td><td>35.6%</td><td>22.3%</td><td>12.7%</td><td>12.3%</td><td>13.3%</td><td>8.6%</td><td>30.1%</td><td>15.6%</td><td>25.2%</td><td>6.4%</td></tr><tr><td rowspan="3">天气</td><td>原始的</td><td>0.657</td><td>0.572</td><td>0.803</td><td>0.656</td><td>0.634</td><td>0.548</td><td>0.286</td><td>0.308</td><td>0.659</td><td>0.574</td></tr><tr><td>+反转的</td><td>0.258</td><td>0.279</td><td>0.248</td><td>0.292</td><td>0.271</td><td>0.330</td><td>0.266</td><td>0.285</td><td>0.262</td><td>0.282</td></tr><tr><td>推广</td><td>60.2%</td><td>50.8%</td><td>69.2%</td><td>55.5%</td><td>57.3%</td><td>39.8%</td><td>7.2%</td><td>7.7%</td><td>60.2%</td><td>50.8%</td></tr></tbody></table>

<!-- Media -->

Variate generalization By inverting vanilla Transformers, it is notable that the models are empowered with the generalization capability on unseen variates. Firstly, benefiting from the flexibility of the number of input tokens, the amount of variate channels is no longer restricted and thus feasible to vary from training and inference. Besides, feed-forward networks are identically applied on independent variate tokens in iTransformer. As aforementioned, the neurons as filters learn the intrinsic patterns of any time series, which are inclined to be shared and transferable among distinct variates.

变量泛化 通过反转普通的Transformer模型，可以注意到这些模型被赋予了对未见变量的泛化能力。首先，得益于输入令牌数量的灵活性，变量通道的数量不再受限，因此在训练和推理过程中可以有所变化。此外，在iTransformer中，前馈网络同样应用于独立的变量令牌。如前所述，作为过滤器的神经元学习任何时间序列的内在模式，这些模式倾向于在不同变量之间共享和迁移。

To verify the hypothesis, we compare inverting with another generalizing strategy: Channel Independence, training a shared backbone to forecast all variates. We partition the variates of each dataset into five folders,train models with only ${20}\%$ of variates of one folder,and directly forecast all variates without fine-tuning. We compare the performance in Figure 5 and each bar presents the averaged results of all folders to avoid the randomness of partition. CI-Transformers take a long time to predict each variate one by one during inference while iTransformers directly predict all variates and generally present smaller increases, indicating FFN is competent to learn transferable time series representations. It leaves a potential direction to build a foundation model upon iTransformer, where diverse multivariate time series with different numbers of variates can be feasibly trained together.

为验证该假设，我们将反演（inverting）与另一种泛化策略进行比较：通道独立性（Channel Independence），即训练一个共享主干来预测所有变量。我们将每个数据集的变量划分为五个子集，仅使用一个子集中${20}\%$的变量来训练模型，并在不进行微调的情况下直接预测所有变量。我们在图5中比较了性能，每个条形图展示了所有子集的平均结果，以避免划分的随机性。在推理过程中，CI-Transformer需要花费很长时间逐个预测每个变量，而iTransformer可以直接预测所有变量，并且通常增长幅度较小，这表明前馈网络（FFN）能够学习可迁移的时间序列表示。这为在iTransformer的基础上构建基础模型留下了一个潜在的方向，在这个方向上，具有不同数量变量的各种多变量时间序列可以切实可行地一起训练。

<!-- Media -->

<!-- figureText: Solar-Energy -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_6.jpg?x=305&y=1526&w=1183&h=323&r=0"/>

Figure 5: Performance of generalization on unseen variates. We partition the variates of each dataset into five folders,train models with ${20}\%$ variates,and use the partially trained model to forecast all varieties. iTransformers can be trained efficiently and forecast with good generalizability.

图5：对未见变量的泛化性能。我们将每个数据集的变量划分为五个子集，使用${20}\%$个变量训练模型，并使用部分训练好的模型来预测所有变量。iTransformer可以高效地进行训练，并具有良好的泛化预测能力。

<!-- Media -->

Increasing lookback length Previous works have witnessed the phenomenon that the forecasting performance does not necessarily improve with the increase of lookback length on Transformers (Nie et al. 2023; Zeng et al. 2023), which can be attributed to the distracted attention on the growing input. However, the desired performance improvement is generally held on linear forecasts, theoretically supported by statistical methods (Box & Jenkins, 1968) with enlarged historical information to be utilized. As the working dimensions of attention and feed-forward network are inverted, we evaluate the performance of Transformers and iTransformer in Figure 6 with increased lookback length. The results surprisingly verify the rationality of leveraging MLPs on the temporal dimension such that Transformers can benefit from the extended lookback window for more precise predictions.

增加回溯长度 以往的研究已经观察到这样一种现象，即在Transformer模型上，预测性能并不一定会随着回溯长度的增加而提高（聂等人，2023年；曾等人，2023年），这可能是由于对不断增长的输入的注意力分散所致。然而，在线性预测中，通常认为增加回溯长度会改善性能，这在理论上得到了统计方法（博克斯和詹金斯，1968年）的支持，因为可以利用更多的历史信息。由于注意力机制和前馈网络的工作维度被倒置，我们在图6中评估了随着回溯长度增加时Transformer和iTransformer的性能。结果令人惊讶地证实了在时间维度上利用多层感知机（MLP）的合理性，这样Transformer可以从扩展的回溯窗口中受益，从而进行更精确的预测。

<!-- Media -->

<!-- figureText: ECL 0.5 0.4 0.35 0.30 0.20 0.15 -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_7.jpg?x=327&y=397&w=1136&h=482&r=0"/>

Figure 6: Forecasting performance with the lookback length $T \in  \{ {48},{96},{192},{336},{720}\}$ and fixed prediction length $S = {96}$ . While the performance of Transformer-based forecasters does not necessarily benefit from the increased lookback length, the inverted framework empowers the vanilla Transformer and its variants with improved performance on the enlarged lookback window.

图6：回溯长度为$T \in  \{ {48},{96},{192},{336},{720}\}$且预测长度固定为$S = {96}$时的预测性能。虽然基于Transformer的预测器不一定能从增加的回溯长度中受益，但倒置框架使原始Transformer及其变体在扩大的回溯窗口上性能得到提升。

<!-- Media -->

### 4.3 MODEL ANALYSIS

### 4.3 模型分析

Ablation study To verify the rational business of Transformer components, we provide detailed ablations covering both replacing components (Replace) and removing components (w/o) experiments. The results are listed in Table 3 iTransformer that utilizes attention on the variate dimension and feed-forward on the temporal dimension generally achieves the best performance. Notably, the performance of vanilla Transformer (the third row) performs the worst among these designs, revealing the potential risks of the conventional architecture, which we describe in detail in Appendix E.3

消融研究 为了验证Transformer组件的合理性，我们提供了详细的消融实验，涵盖替换组件（替换）和移除组件（无）实验。结果列于表3。iTransformer在变量维度上使用注意力机制，在时间维度上使用前馈网络，通常能取得最佳性能。值得注意的是，原始Transformer（第三行）在这些设计中表现最差，这揭示了传统架构的潜在风险，我们将在附录E.3中详细描述。

<!-- Media -->

Table 3: Ablations on iTransformer. We replace different components on the respective dimension to learn multivariate correlations (Variate) and series representations (Temporal), in addition to component removal. The average results of all predicted lengths are listed here.

表3：iTransformer的消融实验。除了移除组件外，我们在相应维度上替换不同组件，以学习多变量相关性（变量）和序列表示（时间）。这里列出了所有预测长度的平均结果。

<table><tr><td rowspan="2">Design</td><td rowspan="2">Variate</td><td rowspan="2">Temporal</td><td colspan="2">ECL</td><td colspan="2">Traffic</td><td colspan="2">Weather</td><td colspan="2">Solar-Energy</td></tr><tr><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>iTransformer</td><td>Attention</td><td>$\mathbf{{FFN}}$</td><td>0.178</td><td>0.270</td><td>0.428</td><td>0.282</td><td>0.258</td><td>0.278</td><td>0.233</td><td>0.262</td></tr><tr><td rowspan="3">Replace</td><td>Attention</td><td>Attention</td><td>0.193</td><td>0.293</td><td>0.913</td><td>0.500</td><td>0.255</td><td>0.280</td><td>0.261</td><td>0.291</td></tr><tr><td>FFN</td><td>Attention</td><td>0.202</td><td>0.300</td><td>0.863</td><td>0.499</td><td>0.258</td><td>0.283</td><td>0.285</td><td>0.317</td></tr><tr><td>FFN</td><td>FFN</td><td>0.182</td><td>0.287</td><td>0.599</td><td>0.348</td><td>0.248</td><td>0.274</td><td>0.269</td><td>0.287</td></tr><tr><td rowspan="2">w/o</td><td rowspan="2">Attention W/O</td><td>W/O</td><td>0.189</td><td>0.278</td><td>0.456</td><td>0.306</td><td>0.261</td><td>0.281</td><td>0.258</td><td>0.289</td></tr><tr><td>FFN</td><td>0.193</td><td>0.276</td><td>0.461</td><td>0.294</td><td>0.265</td><td>0.283</td><td>0.261</td><td>0.283</td></tr></table>

<table><tbody><tr><td rowspan="2">设计</td><td rowspan="2">变化；使多样化</td><td rowspan="2">时间的；暂时的</td><td colspan="2">事件条件逻辑（Event Condition Logic）</td><td colspan="2">交通；流量</td><td colspan="2">天气</td><td colspan="2">太阳能（Solar-Energy）</td></tr><tr><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td>iTransformer（iTransformer）</td><td>注意力机制（Attention）</td><td>$\mathbf{{FFN}}$</td><td>0.178</td><td>0.270</td><td>0.428</td><td>0.282</td><td>0.258</td><td>0.278</td><td>0.233</td><td>0.262</td></tr><tr><td rowspan="3">替换（Replace）</td><td>注意力机制（Attention）</td><td>注意力机制（Attention）</td><td>0.193</td><td>0.293</td><td>0.913</td><td>0.500</td><td>0.255</td><td>0.280</td><td>0.261</td><td>0.291</td></tr><tr><td>前馈网络（FFN）</td><td>注意力机制（Attention）</td><td>0.202</td><td>0.300</td><td>0.863</td><td>0.499</td><td>0.258</td><td>0.283</td><td>0.285</td><td>0.317</td></tr><tr><td>前馈网络（FFN）</td><td>前馈网络（FFN）</td><td>0.182</td><td>0.287</td><td>0.599</td><td>0.348</td><td>0.248</td><td>0.274</td><td>0.269</td><td>0.287</td></tr><tr><td rowspan="2">无（w/o）</td><td rowspan="2">无注意力机制（Attention W/O）</td><td>无（W/O）</td><td>0.189</td><td>0.278</td><td>0.456</td><td>0.306</td><td>0.261</td><td>0.281</td><td>0.258</td><td>0.289</td></tr><tr><td>前馈网络（FFN）</td><td>0.193</td><td>0.276</td><td>0.461</td><td>0.294</td><td>0.265</td><td>0.283</td><td>0.261</td><td>0.283</td></tr></tbody></table>

<!-- Media -->

Analysis of series representations To further validate the claim that feed-forward networks are more favored to extract the series representations. We conduct representation analysis based on the centered kernel alignment (CKA) similarity (Kornblith et al. 2019). A higher CKA indicates more similar representations. For Transformer variants and iTransformers, we calculate the CKA between the output features of the first and the last block. Notably, previous works have demonstrated that time series forecasting, as a low-level generative task, prefers the higher CKA similarity (Wu et al. 2023; Dong et al. 2023) for the better performance. As shown in Figure 7, a clear division line is exhibited, implying that iTransformers have learned more appropriate series representations by inverting the dimension and thus achieve more accurate predictions. The results also advocate inverting Transformer deserves a fundamental renovation of the forecasting backbone.

序列表示分析 为了进一步验证前馈网络更有利于提取序列表示这一观点，我们基于中心核对齐（CKA）相似度（科尔布利思等人，2019年）进行了表示分析。CKA值越高，表示表示越相似。对于Transformer变体和iTransformer，我们计算了第一个块和最后一个块的输出特征之间的CKA。值得注意的是，先前的研究表明，作为一项低级生成任务，时间序列预测更倾向于较高的CKA相似度（吴等人，2023年；董等人，2023年），以获得更好的性能。如图7所示，存在一条明显的分界线，这意味着iTransformer通过反转维度学习到了更合适的序列表示，从而实现了更准确的预测。这些结果也表明，反转Transformer值得对预测主干进行根本性的革新。

Analysis of multivariate correlations By assigning the duty of multivariate correlation to the attention mechanism, the learned map enjoys enhanced interpretability. We present the case visualization on series from Solar-Energy in Figure 7, which has distinct correlations in the lookback and future windows. It can be observed that in the shallow attention layer, the learned map shares lots of similarities to the correlations of raw input series. As it dives into deeper layers, the learned map become gradually alike to the correlations of future series, which validates the inverted operation empowers interpretable attention for correlating, and the processes of encoding the past and decoding for the future are essentially conducted in series representations during feed-forwarding.

多元相关性分析 通过将多元相关性的任务分配给注意力机制，学习得到的映射具有更强的可解释性。我们在图7中展示了太阳能（Solar - Energy）序列的案例可视化，该序列在回溯窗口和未来窗口中具有明显的相关性。可以观察到，在浅层注意力层中，学习得到的映射与原始输入序列的相关性有很多相似之处。随着深入到更深的层，学习得到的映射逐渐与未来序列的相关性相似，这验证了逆运算为相关性赋予了可解释的注意力，并且在前馈过程中，对过去进行编码和对未来进行解码的过程本质上是在序列表示中进行的。

<!-- Media -->

<!-- figureText: - 0.75 - 0.50 0.00 -0.25 -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_8.jpg?x=330&y=524&w=1137&h=417&r=0"/>

Figure 7: Analysis of series representations and multivariate correlations. Left: MSE and CKA similarity of representations comparison between Transformers and iTransformers. A higher CKA similarity indicates more favored representations for accurate predictions. Right: A case visualization of multivariate correlations of raw time series and the learned score maps by inverted self-attention.

图7：序列表示和多元相关性分析。左：Transformer和iTransformer之间表示的均方误差（MSE）和中心核对齐（CKA）相似度比较。较高的CKA相似度表明更有利于进行准确预测的表示。右：原始时间序列的多元相关性以及通过逆自注意力学习得到的得分图的案例可视化。

<!-- Media -->

Efficient training strategy Due to the quadratic complexity of self-attention, it can be overwhelming for training on numerous variates, which is very common in real-world scenarios. In addition to efficient attention mechanisms, we propose a novel training strategy for high-dimensional multivariate series by taking advantage of previously demonstrated variate generation capability. Concretely, we randomly choose part of the variates in each batch and only train the model with selected variates. Since the number of variate channels is flexible because of our inverting, the model can predict all the variates for predictions. As shown in Figure 8, the performance of our proposed strategy is still comparable with full-variate training, while the memory footprint can be reduced significantly.

高效训练策略 由于自注意力机制的复杂度为二次方，在大量变量上进行训练可能会带来巨大负担，而这在现实场景中非常常见。除了高效注意力机制外，我们利用先前已证明的变量生成能力，为高维多元序列提出了一种新颖的训练策略。具体而言，我们在每一批次中随机选择部分变量，仅使用所选变量对模型进行训练。由于我们的反演操作使得变量通道数量具有灵活性，模型可以预测所有变量。如图8所示，我们提出的策略的性能仍可与全变量训练相媲美，同时内存占用可显著降低。

<!-- Media -->

<!-- figureText: 0.4 -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_8.jpg?x=324&y=1378&w=1136&h=371&r=0"/>

Figure 8: Analysis of the efficient training strategy. While the performance (left) remains stable on partially trained variates of each batch with different sampled ratios, the memory footprint (right) can be cut off greatly. We provide the comprehensive model efficiency analysis in Appendix D.

图8：高效训练策略分析。虽然在每一批次中以不同采样比例对部分变量进行训练时，性能（左图）保持稳定，但内存占用（右图）可大幅减少。我们在附录D中提供了全面的模型效率分析。

<!-- Media -->

## 5 CONCLUSION AND FUTURE WORK

## 5 结论与未来工作

Considering the characteristics of multivariate time series, we propose iTransformer that inverts the structure of Transformer without modifying any native modules. iTransformer regards independent series as variate tokens to capture multivariate correlations by attention and utilize layer normalization and feed-forward networks to learn series representations. Experimentally, iTransformer achieves state-of-the-art performance and exhibits remarkable framework generality supported by promising analysis. In the future, we will explore large-scale pre-training and more time series analysis tasks.

考虑到多变量时间序列的特点，我们提出了iTransformer，它在不修改任何原生模块的情况下反转了Transformer的结构。iTransformer将独立序列视为变量标记，通过注意力机制捕捉多变量相关性，并利用层归一化和前馈网络来学习序列表示。实验表明，iTransformer取得了最先进的性能，并在有前景的分析支持下展现出显著的框架通用性。未来，我们将探索大规模预训练和更多的时间序列分析任务。

## 6 ETHICS STATEMENT

## 6 伦理声明

Our work only focuses on the time series forecasting problem, so there is no potential ethical risk.

我们的工作仅专注于时间序列预测问题，因此不存在潜在的伦理风险。

## 7 REPRODUCIBILITY STATEMENT

## 7 可重复性声明

In the main text, we have strictly formalized the model architecture with equations. All the implementation details are included in the Appendix, including dataset descriptions, metrics, model, and experiment configurations. The code will be made public once the paper is accepted.

在正文部分，我们已用公式严格规范了模型架构。所有实现细节都包含在附录中，包括数据集描述、指标、模型和实验配置。论文一旦被接受，代码将公开。

## ACKNOWLEDGMENTS

## 致谢

This work was supported by the National Key Research and Development Plan (2021YFB1715200), the National Natural Science Foundation of China (U2342217 and 62022050), the BNRist Innovation Fund (BNR2024RC01010), Ant Group through CCF-Ant Research Fund, and the National Engineering Research Center for Big Data Software.

本工作得到了国家重点研发计划（2021YFB1715200）、国家自然科学基金（U2342217和62022050）、北京智源人工智能研究院创新基金（BNR2024RC01010）、蚂蚁集团通过中国计算机学会 - 蚂蚁科研基金的资助，以及国家大数据软件工程技术研究中心的支持。

## REFERENCES

## 参考文献

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. https://arxiv.org/pdf/1607.06450.pdf, 2016.

吉米·雷·巴（Jimmy Lei Ba）、杰米·瑞安·基罗斯（Jamie Ryan Kiros）和杰弗里·E·辛顿（Geoffrey E. Hinton）。层归一化。https://arxiv.org/pdf/1607.06450.pdf，2016年。

Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2, 2018.

白少杰（Shaojie Bai）、J·齐科·科尔特（J Zico Kolter）和弗拉德连·科尔图恩（Vladlen Koltun）。序列建模中通用卷积和循环网络的实证评估。预印本arXiv:1803.01271，2，2018年。

George EP Box and Gwilym M Jenkins. Some recent advances in forecasting and control. Journal of the Royal Statistical Society. Series C (Applied Statistics), 17(2):91-109, 1968.

乔治·E·P·博克斯（George EP Box）和格威利姆·M·詹金斯（Gwilym M Jenkins）。预测与控制的一些最新进展。《皇家统计学会杂志》。C辑（应用统计学），17(2):91 - 109，1968年。

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020.

汤姆·布朗（Tom Brown）、本杰明·曼（Benjamin Mann）、尼克·赖德（Nick Ryder）、梅兰妮·苏比亚（Melanie Subbiah）、贾里德·D·卡普兰（Jared D Kaplan）、普拉富拉·达里瓦尔（Prafulla Dhariwal）、阿温德·尼尔坎坦（Arvind Neelakantan）、普拉纳夫·夏姆（Pranav Shyam）、吉里什·萨斯特里（Girish Sastry）、阿曼达·阿斯凯尔（Amanda Askell）等。语言模型是少样本学习者。神经信息处理系统大会（NeurIPS），2020年。

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. NeurIPS, 2022.

特里·道（Tri Dao）、丹·傅（Dan Fu）、斯特凡诺·埃尔蒙（Stefano Ermon）、阿特里·鲁德拉（Atri Rudra）和克里斯托弗·雷（Christopher Ré）。快速注意力机制（FlashAttention）：具备输入输出感知的快速且内存高效的精确注意力机制。神经信息处理系统大会（NeurIPS），2022年。

Abhimanyu Das, Weihao Kong, Andrew Leach, Rajat Sen, and Rose Yu. Long-term forecasting with tide: Time-series dense encoder. arXiv preprint arXiv:2304.08424, 2023.

阿比曼纽·达斯（Abhimanyu Das）、孔维豪（Weihao Kong）、安德鲁·利奇（Andrew Leach）、拉贾特·森（Rajat Sen）和于露丝（Rose Yu）。使用潮汐（TIDE）进行长期预测：时间序列密集编码器。预印本论文（arXiv preprint）arXiv:2304.08424，2023年。

Jiaxiang Dong, Haixu Wu, Haoran Zhang, Li Zhang, Jianmin Wang, and Mingsheng Long. Simmtm: A simple pre-training framework for masked time-series modeling. arXiv preprint arXiv:2302.00861, 2023.

董家祥（Jiaxiang Dong）、吴海旭（Haixu Wu）、张浩然（Haoran Zhang）、张莉（Li Zhang）、王建民（Jianmin Wang）和龙明盛（Mingsheng Long）。简单掩码时间序列建模预训练框架（SimMTM）：一种用于掩码时间序列建模的简单预训练框架。预印本论文（arXiv preprint）arXiv:2302.00861，2023年。

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.

阿列克谢·多索维茨基（Alexey Dosovitskiy）、卢卡斯·拜尔（Lucas Beyer）、亚历山大·科列斯尼科夫（Alexander Kolesnikov）、德克·魏森伯恩（Dirk Weissenborn）、翟晓华（Xiaohua Zhai）、托马斯·翁特希纳（Thomas Unterthiner）、莫斯塔法·德赫加尼（Mostafa Dehghani）、马蒂亚斯·明德勒（Matthias Minderer）、格奥尔格·海戈尔德（Georg Heigold）、西尔万·热利（Sylvain Gelly）等。一张图像胜似16×16个单词：大规模图像识别的Transformer模型。国际学习表征会议（ICLR），2021年。

Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting. KDD, 2023.

维杰·埃坎巴拉姆（Vijay Ekambaram）、阿林达姆·贾蒂（Arindam Jati）、南·阮（Nam Nguyen）、潘瓦迪·辛通（Phanwadee Sinthong）和贾扬特·卡拉格纳纳姆（Jayant Kalagnanam）。Tsmixer：用于多变量时间序列预测的轻量级多层感知器混合模型。知识发现与数据挖掘会议（KDD），2023年。

Lu Han, Han-Jia Ye, and De-Chuan Zhan. The capacity and robustness trade-off: Revisiting the channel independent strategy for multivariate time series forecasting. arXiv preprint arXiv:2304.05206, 2023.

鹿晗（Lu Han）、叶寒佳（Han-Jia Ye）和詹德传（De-Chuan Zhan）。容量与鲁棒性的权衡：重新审视多变量时间序列预测的通道独立策略。预印本arXiv:2304.05206，2023年。

Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2): 251-257, 1991.

库尔特·霍尼克（Kurt Hornik）。多层前馈网络的逼近能力。《神经网络》，4(2)：251 - 257，1991年。

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

贾里德·卡普兰（Jared Kaplan）、山姆·麦坎德利什（Sam McCandlish）、汤姆·亨尼根（Tom Henighan）、汤姆·B·布朗（Tom B Brown）、本杰明·切斯（Benjamin Chess）、雷翁·蔡尔德（Rewon Child）、斯科特·格雷（Scott Gray）、亚历克·拉德福德（Alec Radford）、杰弗里·吴（Jeffrey Wu）和达里奥·阿莫迪（Dario Amodei）。神经语言模型的缩放定律。预印本arXiv:2001.08361，2020年。

Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. ICLR, 2021.

金泰成（Taesung Kim）、金珍熙（Jinhee Kim）、太允元（Yunwon Tae）、朴千福（Cheonbok Park）、崔章镐（Jang - Ho Choi）和朱在杰（Jaegul Choo）。用于对抗分布偏移进行准确时间序列预测的可逆实例归一化。国际学习表征会议（ICLR），2021年。

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.

迪德里克·P·金马（Diederik P. Kingma）和吉米·巴（Jimmy Ba）。Adam：一种随机优化方法。国际学习表征会议（ICLR），2015年。

Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. ICLR, 2020.

尼基塔·基塔耶夫（Nikita Kitaev）、卢卡斯·凯泽（Łukasz Kaiser）和安塞尔姆·列夫斯卡亚（Anselm Levskaya）。改革者：高效的Transformer。国际学习表征会议（ICLR），2020年。

Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. Similarity of neural network representations revisited. ICML, 2019.

西蒙·科恩布利思（Simon Kornblith）、穆罕默德·诺鲁兹（Mohammad Norouzi）、洪拉克·李（Honglak Lee）和杰弗里·E·辛顿（Geoffrey E. Hinton）。重新审视神经网络表征的相似性。国际机器学习会议（ICML），2019年。

Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. SIGIR, 2018.

赖国坤（Guokun Lai）、张伟成（Wei-Cheng Chang）、杨一鸣（Yiming Yang）和刘瀚霄（Hanxiao Liu）。利用深度神经网络对长短期时间模式进行建模。《信息检索研究与发展会议》（SIGIR），2018年。

Jianxin Li, Xiong Hui, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. arXiv: 2012.07436, 2021.

李建新（Jianxin Li）、惠熊（Xiong Hui）和张万才（Wancai Zhang）。《告密者：超越高效变压器的长序列时间序列预测》。预印本arXiv: 2012.07436，2021年。

Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An investigation on linear mapping. arXiv preprint arXiv:2305.10721, 2023.

李哲（Zhe Li）、齐诗怡（Shiyi Qi）、李一铎（Yiduo Li）和徐增林（Zenglin Xu）。重新审视长期时间序列预测：对线性映射的研究。预印本arXiv:2305.10721，2023年。

Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet: time series modeling and forecasting with sample convolution and interaction. NeurIPS, 2022a.

刘浩（Minhao Liu）、曾爱玲（Ailing Zeng）、陈慕溪（Muxi Chen）、徐志坚（Zhijian Xu）、赖秋霞（Qiuxia Lai）、马玲娜（Lingna Ma）和徐强（Qiang Xu）。《Scinet：基于样本卷积和交互的时间序列建模与预测》。《神经信息处理系统大会》（NeurIPS），2022a。

Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Rethinking the stationarity in time series forecasting. NeurIPS, 2022b.

刘永（Yong Liu）、吴海旭（Haixu Wu）、王建民（Jianmin Wang）和龙明盛（Mingsheng Long）。非平稳变压器：重新思考时间序列预测中的平稳性。《神经信息处理系统大会》（NeurIPS），2022b。

Yong Liu, Chenyu Li, Jianmin Wang, and Mingsheng Long. Koopa: Learning non-stationary time series dynamics with koopman predictors. arXiv preprint arXiv:2305.18803, 2023.

刘勇（Yong Liu）、李晨宇（Chenyu Li）、王建民（Jianmin Wang）和龙明盛（Mingsheng Long）。《Koopa：使用库普曼预测器学习非平稳时间序列动态》。预印本arXiv:2305.18803，2023年。

Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. ICLR, 2023.

聂玉琪（Yuqi Nie）、阮南河（Nam H Nguyen）、潘瓦迪·辛通（Phanwadee Sinthong）和贾扬特·卡拉格纳纳姆（Jayant Kalagnanam）。《一个时间序列价值64个字：使用Transformer进行长期预测》。国际学习表征会议（ICLR），2023年。

Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. ICLR, 2019.

鲍里斯·N·奥列什金（Boris N Oreshkin）、德米特里·卡尔波夫（Dmitri Carpov）、尼古拉斯·查帕多斯（Nicolas Chapados）和约书亚·本吉奥（Yoshua Bengio）。《N - BEATS：用于可解释时间序列预测的神经基扩展分析》。国际学习表征会议（ICLR），2019年。

Adam Paszke, S. Gross, Francisco Massa, A. Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Z. Lin, N. Gimelshein, L. Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 2019.

亚当·帕斯克（Adam Paszke）、S. 格罗斯（S. Gross）、弗朗西斯科·马萨（Francisco Massa）、A. 勒雷尔（A. Lerer）、詹姆斯·布拉德伯里（James Bradbury）、格雷戈里·查南（Gregory Chanan）、特雷弗·基林（Trevor Killeen）、Z. 林（Z. Lin）、N. 吉梅尔申（N. Gimelshein）、L. 安蒂加（L. Antiga）、阿尔班·德斯梅森（Alban Desmaison）、安德里亚斯·科普夫（Andreas Köpf）、爱德华·杨（Edward Yang）、扎克·德维托（Zach DeVito）、马丁·赖森（Martin Raison）、阿利汗·特贾尼（Alykhan Tejani）、萨桑克·奇拉姆库尔蒂（Sasank Chilamkurthy）、贝努瓦·施泰纳（Benoit Steiner）、卢芳（Lu Fang）、白俊杰（Junjie Bai）和苏米特·钦塔拉（Soumith Chintala）。PyTorch：一种命令式风格的高性能深度学习库。神经信息处理系统大会（NeurIPS），2019年。

Syama Sundar Rangapuram, Matthias W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting. NeurIPS, 2018.

西亚马·桑达尔·兰加普拉姆（Syama Sundar Rangapuram）、马蒂亚斯·W·西格（Matthias W Seeger）、扬·加施豪斯（Jan Gasthaus）、洛伦佐·斯特拉（Lorenzo Stella）、王宇阳（Yuyang Wang）和蒂姆·亚努绍夫斯基（Tim Januschowski）。用于时间序列预测的深度状态空间模型。神经信息处理系统大会（NeurIPS），2018年。

David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3): 1181-1191, 2020.

大卫·萨利纳斯（David Salinas）、瓦伦丁·弗伦克特（Valentin Flunkert）、扬·加施豪斯（Jan Gasthaus）和蒂姆·亚努绍夫斯基（Tim Januschowski）。DeepAR：基于自回归循环网络的概率预测。《国际预测期刊》（International Journal of Forecasting），36(3)：1181 - 1191，2020年。

Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-terthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. NeurIPS, 2021.

伊利亚·O·托尔斯季欣（Ilya O Tolstikhin）、尼尔·霍尔兹比（Neil Houlsby）、亚历山大·科列斯尼科夫（Alexander Kolesnikov）、卢卡斯·拜尔（Lucas Beyer）、翟晓华（Xiaohua Zhai）、托马斯·翁特蒂纳（Thomas Unterthiner）、杰西卡·杨（Jessica Yung）、安德烈亚斯·施泰纳（Andreas Steiner）、丹尼尔·凯泽斯（Daniel Keysers）、雅各布·乌兹科雷特（Jakob Uszkoreit）等。Mlp - 混合器：一种用于视觉的全多层感知机架构。神经信息处理系统大会（NeurIPS），2021年。

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.

阿什什·瓦斯瓦尼（Ashish Vaswani）、诺姆·沙泽尔（Noam Shazeer）、尼基·帕尔马尔（Niki Parmar）、雅各布·乌兹科雷特（Jakob Uszkoreit）、利昂·琼斯（Llion Jones）、艾丹·N·戈麦斯（Aidan N Gomez）、卢卡斯·凯泽（Lukasz Kaiser）和伊利亚·波洛苏金（Illia Polosukhin）。注意力就是你所需要的一切。神经信息处理系统大会（NeurIPS），2017年。

Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with Auto-Correlation for long-term series forecasting. NeurIPS, 2021.

吴海旭（Haixu Wu）、徐杰辉（Jiehui Xu）、王建民（Jianmin Wang）和龙明盛（Mingsheng Long）。自动分解变换器（Autoformer）：用于长期序列预测的具有自相关的分解变换器。神经信息处理系统大会（NeurIPS），2021年。

Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Flowformer: Linearizing transformers with conservation flows. ICML, 2022.

吴海旭（Haixu Wu）、吴佳龙（Jialong Wu）、徐杰辉（Jiehui Xu）、王建民（Jianmin Wang）和龙明盛（Mingsheng Long）。流变换器（Flowformer）：用守恒流线性化变换器。国际机器学习会议（ICML），2022年。

Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. ICLR, 2023.

吴海旭（Haixu Wu）、胡腾格（Tengge Hu）、刘永（Yong Liu）、周航（Hang Zhou）、王建民（Jianmin Wang）和龙明盛（Mingsheng Long）。Timesnet：用于通用时间序列分析的时间二维变化建模。国际学习表征会议（ICLR），2023年。

Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? AAAI, 2023.

曾爱玲（Ailing Zeng）、陈慕溪（Muxi Chen）、张磊（Lei Zhang）和徐强（Qiang Xu）。Transformer对时间序列预测有效吗？美国人工智能协会会议（AAAI），2023年。

Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. ICLR, 2023.

张云皓（Yunhao Zhang）和闫俊驰（Junchi Yan）。Crossformer：利用跨维度依赖进行多变量时间序列预测的Transformer。国际学习表征会议（ICLR），2023年。

Zheng Zhao, Weihai Chen, Xingming Wu, Peter CY Chen, and Jingmeng Liu. Lstm network: a deep learning approach for short-term traffic forecast. IET Intelligent Transport Systems, 11(2):68-75, 2017.

赵政（Zheng Zhao）、陈威海（Weihai Chen）、吴兴明（Xingming Wu）、陈彼得（Peter CY Chen）和刘景梦（Jingmeng Liu）。长短期记忆网络（LSTM）：一种用于短期交通预测的深度学习方法。《工程技术学会智能交通系统》（IET Intelligent Transport Systems），11(2):68 - 75，2017年。

Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. ICML, 2022.

周天（Tian Zhou）、马子清（Ziqing Ma）、文青松（Qingsong Wen）、王雪（Xue Wang）、孙亮（Liang Sun）和金榕（Rong Jin）。FEDformer：用于长期序列预测的频率增强分解Transformer。国际机器学习会议（ICML），2022年。

## A IMPLEMENTATION DETAILS

## 实现细节

### A.1 DATASET DESCRIPTIONS

### A.1 数据集描述

We conduct experiments on 7 real-world datasets to evaluate the performance of the proposed iTransformer including (1) ETT (Li et al. 2021) contains 7 factors of electricity transformer from July 2016 to July 2018. There are four subsets where ETTh1 and ETTh2 are recorded every hour, and ETTm1 and ETTm2 are recorded every 15 minutes. (2) Exchange (Wu et al. 2021) collects the panel data of daily exchange rates from 8 countries from 1990 to 2016. (3) Weather (Wu et al. 2021) includes 21 meteorological factors collected every 10 minutes from the Weather Station of the Max Planck Biogeochemistry Institute in 2020. (4) ECL (Wu et al., 2021) records the hourly electricity consumption data of 321 clients. (5) Traffic (Wu et al. 2021) collects hourly road occupancy rates measured by 862 sensors of San Francisco Bay area freeways from January 2015 to December 2016. (6) Solar-Energy (Lai et al. 2018) records the solar power production of ${137}\mathrm{{PV}}$ plants in 2006, which are sampled every 10 minutes. (7) PEMS contains the public traffic network data in California collected by 5-minute windows. We use the same four public subsets (PEMS03, PEMS04, PEMS07, PEMS08) adopted in SCINet (Liu et al., 2022a).

我们在7个真实世界的数据集上进行实验，以评估所提出的iTransformer的性能，这些数据集包括：（1）ETT（Li等人，2021年），包含2016年7月至2018年7月电力变压器的7个因素。有四个子集，其中ETTh1和ETTh2按小时记录，ETTm1和ETTm2按15分钟记录。（2）Exchange（Wu等人，2021年）收集了1990年至2016年8个国家的每日汇率面板数据。（3）Weather（Wu等人，2021年）包含2020年马克斯·普朗克生物地球化学研究所（Max Planck Biogeochemistry Institute）气象站每10分钟收集的21个气象因素。（4）ECL（Wu等人，2021年）记录了321个客户的每小时用电量数据。（5）Traffic（Wu等人，2021年）收集了2015年1月至2016年12月旧金山湾区高速公路862个传感器测量的每小时道路占有率。（6）Solar - Energy（Lai等人，2018年）记录了2006年${137}\mathrm{{PV}}$个发电厂的太阳能发电量，每10分钟采样一次。（7）PEMS包含加利福尼亚州以5分钟为窗口收集的公共交通网络数据。我们使用了SCINet（Liu等人，2022a）中采用的相同的四个公共子集（PEMS03、PEMS04、PEMS07、PEMS08）。

Apart from the public datasets widely used as forecasting benchmarks, we also collect a set of Market datasets of a real-world application, which records the minute-sampled server load of Alipay online transactions between January 30th, 2023, and April 9th, 2023 with the number of variates varied from 285 to 759. It includes 6 sub-datasets, which are divided according to diverse transaction domains.

除了广泛用作预测基准的公共数据集外，我们还收集了一组来自实际应用的市场数据集，该数据集记录了2023年1月30日至2023年4月9日期间支付宝在线交易的分钟级服务器负载，变量数量从285个到759个不等。它包含6个子数据集，这些子数据集是根据不同的交易领域划分的。

We follow the same data processing and train-validation-test set split protocol used in TimesNet (Wu et al. 2023), where the train, validation, and test datasets are strictly divided according to chronological order to make sure there are no data leakage issues. As for the forecasting settings, we fix the length of the lookback series as 96 in ETT, Weather, ECL, Solar-Energy, PEMS, and Traffic, and the prediction length varies in $\{ {96},{192},{336},{720}\}$ . For the PEMS dataset,the prediction length varies in $\{ {12},{24},{36},{48}\}$ ,which is the same as SCINet,the previous state-of-the-art on this dataset. For the Market dataset, the lookback contains the past one day observations with 144 time points and the forecasting length varies in $\{ {12},{24},{72},{144}\}$ . The details of datasets are provided in Table 4

我们遵循与TimesNet（Wu等人，2023年）相同的数据处理和训练-验证-测试集划分协议，其中训练集、验证集和测试集严格按照时间顺序划分，以确保不存在数据泄露问题。至于预测设置，在ETT、Weather、ECL、Solar - Energy、PEMS和Traffic数据集中，我们将历史序列长度固定为96，预测长度在$\{ {96},{192},{336},{720}\}$范围内变化。对于PEMS数据集，预测长度在$\{ {12},{24},{36},{48}\}$范围内变化，这与SCINet（该数据集上先前的最优模型）相同。对于Market数据集，历史序列包含过去一天144个时间点的观测值，预测长度在$\{ {12},{24},{72},{144}\}$范围内变化。数据集的详细信息见表4

### A.2 IMPLEMENTATION DETAILS

### A.2 实现细节

<!-- Media -->

Algorithm 1 iTransformer - Overall Architecture.

算法1 iTransformer - 总体架构。

---

Require: Input lookback time series $\mathbf{X} \in  {\mathbb{R}}^{T \times  N}$ ; input Length $T$ ; predicted length $S$ ; variates

		number $N$ ; token dimension $D$ ; iTransformer block number $L$ .

		$\mathbf{X} = \mathbf{X}$ .transpose $\vartriangleright  \mathbf{X} \in  {\mathbb{R}}^{N \times  T}$

	2: $\vartriangleright$ Multi-layer Perceptron works on the last dimension to embed series into variate tokens.

		${\mathbf{H}}^{0} = \operatorname{MLP}\left( \mathbf{X}\right)$ $\vartriangleright  {\mathbf{H}}^{0} \in  {\mathbb{R}}^{N \times  D}$

		for $l$ in $\{ 1,\ldots ,L\}$ : $\vartriangleright$ Run through iTransformer blocks.

				$\vartriangleright$ Self-attention layer is applied on variate tokens.

			${\mathbf{H}}^{l - 1} = \operatorname{LayerNorm}\left( {{\mathbf{H}}^{l - 1} + \operatorname{Self} - \operatorname{Attn}\left( {\mathbf{H}}^{l - 1}\right) }\right)$

			$\vartriangleright$ Feed-forward network is utilized for series representations,broadcasting to each token.

			${\mathbf{H}}^{l} =$ LayerNorm $\left( {{\mathbf{H}}^{l - 1} + \text{Feed-Forward}\left( {\mathbf{H}}^{l - 1}\right) }\right) \; \vartriangleright  {\mathbf{H}}^{l} \in  {\mathbb{R}}^{N \times  D}$

				$\vartriangleright$ LayerNorm is adopted on series representations to reduce variates discrepancies.

		End for

		$\widehat{\mathbf{Y}} = \operatorname{MLP}\left( {\mathbf{H}}^{L}\right)$ $\vartriangleright$ Project tokens back to predicted series, $\widehat{\mathbf{Y}} \in  {\mathbb{R}}^{N \times  S}$

		$\widehat{\mathbf{Y}} = \widehat{\mathbf{Y}}$ .transpose $\vartriangleright  \widehat{\mathbf{Y}} \in  {\mathbb{R}}^{S \times  N}$

		Return $\widehat{\mathbf{Y}}$ $\vartriangleright$ Return the prediction result $\widehat{\mathbf{Y}}$

---

Table 4: Detailed dataset descriptions. Dim denotes the variate number of each dataset. Dataset Size denotes the total number of time points in (Train, Validation, Test) split respectively. Prediction Length denotes the future time points to be predicted and four prediction settings are included in each dataset. Frequency denotes the sampling interval of time points.

表4：详细的数据集描述。Dim表示每个数据集的变量数量。数据集大小分别表示（训练集、验证集、测试集）划分中的时间点总数。预测长度表示要预测的未来时间点，每个数据集包含四种预测设置。频率表示时间点的采样间隔。

<table><tr><td>Dataset</td><td>Dim</td><td>Prediction Length</td><td>Dataset Size</td><td>Frequency</td><td>Information</td></tr><tr><td>ETTh1, ETTh2</td><td>7</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(8545, 2881, 2881)</td><td>Hourly</td><td>Electricity</td></tr><tr><td>ETTm1, ETTm2</td><td>7</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(34465, 11521, 11521)</td><td>15min</td><td>Electricity</td></tr><tr><td>Exchange</td><td>8</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(5120, 665, 1422)</td><td>Daily</td><td>Economy</td></tr><tr><td>Weather</td><td>21</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(36792, 5271, 10540)</td><td>10min</td><td>Weather</td></tr><tr><td>ECL</td><td>321</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(18317, 2633, 5261)</td><td>Hourly</td><td>Electricity</td></tr><tr><td>Traffic</td><td>862</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(12185, 1757, 3509)</td><td>Hourly</td><td>Transportation</td></tr><tr><td>Solar-Energy</td><td>137</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(36601, 5161, 10417)</td><td>10min</td><td>Energy</td></tr><tr><td>PEMS03</td><td>358</td><td>$\{ {12},{24},{48},{96}\}$</td><td>(15617, 5135, 5135)</td><td>5min</td><td>Transportation</td></tr><tr><td>PEMS04</td><td>307</td><td>$\{ {12},{24},{48},{96}\}$</td><td>(10172, 3375, 3375)</td><td>5min</td><td>Transportation</td></tr><tr><td>PEMS07</td><td>883</td><td>$\{ {12},{24},{48},{96}\}$</td><td>(16911, 5622, 5622)</td><td>5min</td><td>Transportation</td></tr><tr><td>PEMS08</td><td>170</td><td>$\{ {12},{24},{48},{96}\}$</td><td>(10690, 3548, 3548)</td><td>5min</td><td>Transportation</td></tr><tr><td>Market-Merchant</td><td>285</td><td>$\{ {12},{24},{72},{144}\}$</td><td>(7045, 1429, 1429)</td><td>10min</td><td>Transaction</td></tr><tr><td>Market-Wealth</td><td>485</td><td>$\{ {12},{24},{72},{144}\}$</td><td>(7045, 1429, 1429)</td><td>10min</td><td>Transaction</td></tr><tr><td>Market-Finance</td><td>405</td><td>$\{ {12},{24},{72},{144}\}$</td><td>(7045, 1429, 1429)</td><td>10min</td><td>Transaction</td></tr><tr><td>Market-Terminal</td><td>307</td><td>$\{ {12},{24},{72},{144}\}$</td><td>(7045, 1429, 1429)</td><td>10min</td><td>Transaction</td></tr><tr><td>Market-Payment</td><td>759</td><td>$\{ {12},{24},{72},{144}\}$</td><td>(7045, 1429, 1429)</td><td>10min</td><td>Transaction</td></tr><tr><td>Market-Customer</td><td>395</td><td>$\{ {12},{24},{72},{144}\}$</td><td>(7045, 1429, 1429)</td><td>10min</td><td>Transaction</td></tr></table>

<table><tbody><tr><td>数据集</td><td>维度</td><td>预测长度</td><td>数据集大小</td><td>频率</td><td>信息</td></tr><tr><td>ETTh1、ETTh2</td><td>7</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(8545, 2881, 2881)</td><td>每小时</td><td>电力</td></tr><tr><td>ETTm1、ETTm2</td><td>7</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(34465, 11521, 11521)</td><td>15分钟</td><td>电力</td></tr><tr><td>交换；交易</td><td>8</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(5120, 665, 1422)</td><td>每日</td><td>经济</td></tr><tr><td>天气</td><td>21</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(36792, 5271, 10540)</td><td>10分钟</td><td>天气</td></tr><tr><td>欧洲冠军联赛（ECL）</td><td>321</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(18317, 2633, 5261)</td><td>每小时</td><td>电力</td></tr><tr><td>交通</td><td>862</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(12185, 1757, 3509)</td><td>每小时</td><td>交通运输</td></tr><tr><td>太阳能</td><td>137</td><td>$\{ {96},{192},{336},{720}\}$</td><td>(36601, 5161, 10417)</td><td>10分钟</td><td>能源</td></tr><tr><td>PEMS03</td><td>358</td><td>$\{ {12},{24},{48},{96}\}$</td><td>(15617, 5135, 5135)</td><td>5分钟</td><td>交通运输</td></tr><tr><td>PEMS04</td><td>307</td><td>$\{ {12},{24},{48},{96}\}$</td><td>(10172, 3375, 3375)</td><td>5分钟</td><td>交通运输</td></tr><tr><td>PEMS07</td><td>883</td><td>$\{ {12},{24},{48},{96}\}$</td><td>(16911, 5622, 5622)</td><td>5分钟</td><td>交通运输</td></tr><tr><td>PEMS08</td><td>170</td><td>$\{ {12},{24},{48},{96}\}$</td><td>(10690, 3548, 3548)</td><td>5分钟</td><td>交通运输</td></tr><tr><td>市场 - 商户</td><td>285</td><td>$\{ {12},{24},{72},{144}\}$</td><td>(7045, 1429, 1429)</td><td>10分钟</td><td>交易</td></tr><tr><td>市场 - 财富</td><td>485</td><td>$\{ {12},{24},{72},{144}\}$</td><td>(7045, 1429, 1429)</td><td>10分钟</td><td>交易</td></tr><tr><td>市场 - 金融</td><td>405</td><td>$\{ {12},{24},{72},{144}\}$</td><td>(7045, 1429, 1429)</td><td>10分钟</td><td>交易</td></tr><tr><td>市场终端</td><td>307</td><td>$\{ {12},{24},{72},{144}\}$</td><td>(7045, 1429, 1429)</td><td>10分钟</td><td>交易</td></tr><tr><td>市场支付</td><td>759</td><td>$\{ {12},{24},{72},{144}\}$</td><td>(7045, 1429, 1429)</td><td>10分钟</td><td>交易</td></tr><tr><td>市场客户</td><td>395</td><td>$\{ {12},{24},{72},{144}\}$</td><td>(7045, 1429, 1429)</td><td>10分钟</td><td>交易</td></tr></tbody></table>

<!-- Media -->

All the experiments are implemented in PyTorch (Paszke et al. 2019) and conducted on a single NVIDIA P100 16GB GPU. We utilize ADAM (Kingma & Ba, 2015) with an initial learning rate in $\left\{  {{10}^{-3},5 \times  {10}^{-4},{10}^{-4}}\right\}$ and L2 loss for the model optimization. The batch size is uniformly set to 32 and the number of training epochs is fixed to 10 . We set the number of inverted Transformer blocks in our proposed model $L \in  \{ 2,3,4\}$ . The dimension of series representations $D$ is set from $\{ {256},{512}\}$ . All the compared baseline models that we reproduced are implemented based on the benchmark of TimesNet (Wu et al. 2023) Repository, which is fairly built on the configurations provided by each model's original paper or official code. We provide the pseudo-code of iTransformer in Algorithm 1 We also report the standard deviation of iTransformer performance under five runs with different random seeds in Table 5, which exhibits that the performance of iTransformer is stable.

所有实验均在PyTorch（帕兹克等人，2019年）中实现，并在单块英伟达（NVIDIA）P100 16GB GPU上进行。我们使用ADAM优化器（金马和巴，2015年），初始学习率为$\left\{  {{10}^{-3},5 \times  {10}^{-4},{10}^{-4}}\right\}$，并采用L2损失进行模型优化。批量大小统一设置为32，训练轮数固定为10。我们在提出的模型$L \in  \{ 2,3,4\}$中设置了反转Transformer块的数量。序列表示$D$的维度设置为$\{ {256},{512}\}$。我们复现的所有对比基线模型均基于TimesNet（吴等人，2023年）仓库的基准实现，该基准是根据每个模型原始论文或官方代码提供的配置合理构建的。我们在算法1中提供了iTransformer的伪代码。我们还在表5中报告了iTransformer在使用不同随机种子进行五次运行时性能的标准差，这表明iTransformer的性能是稳定的。

<!-- Media -->

Table 5: Robustness of iTransformer performance. The results are obtained from five random seeds.

表5：iTransformer性能的稳健性。结果由五个随机种子得出。

<table><tr><td rowspan="2">Dataset Horizon</td><td colspan="2">ECL</td><td colspan="2">ETTh2</td><td colspan="2">Exchange</td></tr><tr><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>96</td><td>${0.148} \pm  {0.000}$</td><td>${0.240} \pm  {0.000}$</td><td>${0.297} \pm  {0.002}$</td><td>${0.349} \pm  {0.001}$</td><td>${0.088} \pm  {0.001}$</td><td>${0.209} \pm  {0.001}$</td></tr><tr><td>192</td><td>${0.162} \pm  {0.002}$</td><td>${0.253} \pm  {0.002}$</td><td>${0.380} \pm  {0.001}$</td><td>${0.400} \pm  {0.001}$</td><td>${0.181} \pm  {0.001}$</td><td>${0.304} \pm  {0.001}$</td></tr><tr><td>336</td><td>${0.178} \pm  {0.000}$</td><td>${0.269} \pm  {0.001}$</td><td>${0.428} \pm  {0.002}$</td><td>${0.432} \pm  {0.001}$</td><td>${0.334} \pm  {0.001}$</td><td>${0.419} \pm  {0.001}$</td></tr><tr><td>720</td><td>${0.225} \pm  {0.006}$</td><td>${0.317} \pm  {0.007}$</td><td>${0.427} \pm  {0.004}$</td><td>${0.445} \pm  {0.002}$</td><td>${0.829} \pm  {0.012}$</td><td>${0.691} \pm  {0.005}$</td></tr><tr><td>Dataset</td><td colspan="2">Solar-Energy</td><td colspan="2">Traffic</td><td colspan="2">Weather</td></tr><tr><td>Horizon</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>96</td><td>${0.203} \pm  {0.002}$</td><td>${0.237} \pm  {0.002}$</td><td>${0.395} \pm  {0.001}$</td><td>${0.268} \pm  {0.001}$</td><td>${0.174} \pm  {0.000}$</td><td>${0.214} \pm  {0.000}$</td></tr><tr><td>192</td><td>${0.233} \pm  {0.002}$</td><td>${0.261} \pm  {0.001}$</td><td>${0.417} \pm  {0.002}$</td><td>${0.276} \pm  {0.001}$</td><td>${0.221} \pm  {0.002}$</td><td>${0.254} \pm  {0.001}$</td></tr><tr><td>336</td><td>${0.248} \pm  {0.000}$</td><td>${0.273} \pm  {0.000}$</td><td>${0.433} \pm  {0.004}$</td><td>${0.283} \pm  {0.000}$</td><td>${0.278} \pm  {0.002}$</td><td>${0.296} \pm  {0.001}$</td></tr><tr><td>720</td><td>${0.249} \pm  {0.001}$</td><td>${0.275} \pm  {0.000}$</td><td>${0.467} \pm  {0.003}$</td><td>${0.302} \pm  {0.000}$</td><td>${0.358} \pm  {0.000}$</td><td>${0.349} \pm  {0.000}$</td></tr></table>

<table><tbody><tr><td rowspan="2">数据集时间范围</td><td colspan="2">信用损失预期（ECL，Expected Credit Loss）</td><td colspan="2">ETTh2（无需翻译，保留原词）</td><td colspan="2">交易；兑换</td></tr><tr><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td></tr><tr><td>96</td><td>${0.148} \pm  {0.000}$</td><td>${0.240} \pm  {0.000}$</td><td>${0.297} \pm  {0.002}$</td><td>${0.349} \pm  {0.001}$</td><td>${0.088} \pm  {0.001}$</td><td>${0.209} \pm  {0.001}$</td></tr><tr><td>192</td><td>${0.162} \pm  {0.002}$</td><td>${0.253} \pm  {0.002}$</td><td>${0.380} \pm  {0.001}$</td><td>${0.400} \pm  {0.001}$</td><td>${0.181} \pm  {0.001}$</td><td>${0.304} \pm  {0.001}$</td></tr><tr><td>336</td><td>${0.178} \pm  {0.000}$</td><td>${0.269} \pm  {0.001}$</td><td>${0.428} \pm  {0.002}$</td><td>${0.432} \pm  {0.001}$</td><td>${0.334} \pm  {0.001}$</td><td>${0.419} \pm  {0.001}$</td></tr><tr><td>720</td><td>${0.225} \pm  {0.006}$</td><td>${0.317} \pm  {0.007}$</td><td>${0.427} \pm  {0.004}$</td><td>${0.445} \pm  {0.002}$</td><td>${0.829} \pm  {0.012}$</td><td>${0.691} \pm  {0.005}$</td></tr><tr><td>数据集</td><td colspan="2">太阳能</td><td colspan="2">交通</td><td colspan="2">天气</td></tr><tr><td>地平线</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td><td>均方误差（MSE，Mean Squared Error）</td><td>平均绝对误差（MAE，Mean Absolute Error）</td></tr><tr><td>96</td><td>${0.203} \pm  {0.002}$</td><td>${0.237} \pm  {0.002}$</td><td>${0.395} \pm  {0.001}$</td><td>${0.268} \pm  {0.001}$</td><td>${0.174} \pm  {0.000}$</td><td>${0.214} \pm  {0.000}$</td></tr><tr><td>192</td><td>${0.233} \pm  {0.002}$</td><td>${0.261} \pm  {0.001}$</td><td>${0.417} \pm  {0.002}$</td><td>${0.276} \pm  {0.001}$</td><td>${0.221} \pm  {0.002}$</td><td>${0.254} \pm  {0.001}$</td></tr><tr><td>336</td><td>${0.248} \pm  {0.000}$</td><td>${0.273} \pm  {0.000}$</td><td>${0.433} \pm  {0.004}$</td><td>${0.283} \pm  {0.000}$</td><td>${0.278} \pm  {0.002}$</td><td>${0.296} \pm  {0.001}$</td></tr><tr><td>720</td><td>${0.249} \pm  {0.001}$</td><td>${0.275} \pm  {0.000}$</td><td>${0.467} \pm  {0.003}$</td><td>${0.302} \pm  {0.000}$</td><td>${0.358} \pm  {0.000}$</td><td>${0.349} \pm  {0.000}$</td></tr></tbody></table>

<!-- Media -->

## B ABLATION STUDIES

## 消融研究

To elaborate on the rational business of Transformer components, we conduct detailed ablations covering replacing components (Replace) and removing components (w/o). Since the average results are listed in Table 3 due to the paper limit, we provide detailed results and analysis here.

为了详细阐述Transformer组件的合理设计，我们进行了详细的消融实验，包括替换组件（替换）和移除组件（无）。由于篇幅限制，表3中列出了平均结果，我们在此提供详细结果和分析。

As shown in Table 6, among various architectural designs, iTransformer generally exhibits superior performance, which learns multivariate correlations by self-attention and encodes series representations by FFN. Nevertheless, the arrangement of the vanilla Transformer can lead to degenerated performance, indicating the misuse of Transformer components on the time series modality. Based on the relatively poor results of the second (both attentions) and the third (the vanilla Transformer) designs, one of the reasons for that may lie in the attention module over the temporal tokens of the lagged time series, which we elaborate more with the datasets support in Section E.3

如表6所示，在各种架构设计中，iTransformer通常表现出卓越的性能，它通过自注意力机制学习多元相关性，并通过前馈网络（FFN）对序列表示进行编码。然而，原始Transformer的架构安排可能导致性能下降，这表明在时间序列模态上对Transformer组件的使用不当。基于第二种（两种注意力机制）和第三种（原始Transformer）设计的相对较差的结果，其中一个原因可能在于对滞后时间序列的时间标记的注意力模块，我们将在E.3节中结合数据集进行更详细的阐述。

It is also notable that applying FFN on both dimensions can also lead to fair performance on datasets with small variate numbers (such as Weather with 21 variates). Still, with the increasing of variate numbers in challenging multivariate forecasting tasks, the importance of capturing multivariate correlations is ever more highlighted. We note that the heterogeneity of variates can be hardly considered by the vanilla Transformer. During embedding, the variates are projected into indistinguishable channels, which ignores the inconsistent physical measurements and thus fails to maintain the independence of variates, let alone capture and utilize the multivariate correlation. Consequently, by incorporating the advanced attention module for the variate correlating, the first (iTransformer) and the fifth (attention on variates) designs perform more effectively in challenging multivariate datasets.

同样值得注意的是，在两个维度上应用前馈神经网络（FFN）在变量数量较少的数据集（例如有21个变量的气象数据集）上也能取得不错的性能。不过，在具有挑战性的多变量预测任务中，随着变量数量的增加，捕捉多变量相关性的重要性愈发凸显。我们注意到，普通的Transformer模型很难考虑到变量的异质性。在嵌入过程中，变量被投影到难以区分的通道中，这忽略了不一致的物理测量，因此无法保持变量的独立性，更不用说捕捉和利用多变量相关性了。因此，通过引入用于变量关联的先进注意力模块，第一种（iTransformer）和第五种（对变量的注意力）设计在具有挑战性的多变量数据集上表现得更为有效。

In a nutshell, both temporal dependencies and multivariate correlations are of importance for multivariate time series forecasting. The proposed iTransformer employing the self-attention module to disentangle the correlations between variate tokens proves to be more powerful and interpretable than feed-forward networks, thereby further boosting the performance on challenging multivariate datasets and enhancing the model capacity.

简而言之，时间依赖关系和多变量相关性对于多变量时间序列预测都很重要。所提出的iTransformer采用自注意力模块来解开变量标记之间的相关性，事实证明它比前馈网络更强大、更具可解释性，从而进一步提升了在具有挑战性的多变量数据集上的性能，并增强了模型能力。

## C HYPERPARAMETER SENSITIVITY

## C 超参数敏感性

We evaluate the hyperparameter sensitivity of iTransformer with respect to the following factors: the learning rate ${lr}$ ,the number of Transformer blocks $L$ ,and the hidden dimension $D$ of variate tokens. The results are shown in Figure 9. We find that the learning rate, as the most common influencing factor, should be carefully selected when the number of variates is large (ECL, Traffic). The block number and hidden dimension are not essentially favored to be as large as possible in iTransformer.

我们评估了iTransformer（一种模型）在以下因素方面的超参数敏感性：学习率${lr}$、Transformer块的数量$L$以及变量标记的隐藏维度$D$。结果如图9所示。我们发现，作为最常见的影响因素，当变量数量较多时（如电力负荷（ECL）、交通流量（Traffic）数据），应谨慎选择学习率。在iTransformer中，块的数量和隐藏维度并非越大越好。

<!-- Media -->

<!-- figureText: Learning Rate Block Number Hidden Dimension 0.40 0.35 0.25 0.15 256 512 1024 0.40 0.35 0.25 0.2 0.15 0.0005 0.0003 0.0001 -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_14.jpg?x=305&y=1443&w=1181&h=392&r=0"/>

Figure 9: Hyperparameter sensitivity with respect to the learning rate, the number of Transformer blocks, and the hidden dimension of variate tokens. The results are recorded with the lookback window length $T = {96}$ and the forecast window length $S = {96}$ .

图9：学习率、Transformer块的数量以及变量标记的隐藏维度的超参数敏感性。结果记录了回溯窗口长度$T = {96}$和预测窗口长度$S = {96}$。

<!-- Media -->

## D Model EFFICIENCY

## D 模型效率

We comprehensively compare the forecasting performance, training speed, and memory footprint of the following models: iTransformer, iTransformer with our efficient training strategy and iTransformer with the efficient flow attention module (Wu et al. 2022); linear models: DLinear (Zeng et al. 2023) and TiDE (Das et al., 2023); Transformers: Transformer (Vaswani et al., 2017), PatchTST (Nie et al. 2023), and Crossformer (Zhang & Yan 2023). The results are recorded with the official model configuration and the same batch size. In Figure 10, we compare the efficiency under two representative datasets (21 variates in Weather and 862 in Traffic) with 96 time steps for lookback.

我们全面比较了以下模型的预测性能、训练速度和内存占用情况：iTransformer、采用我们高效训练策略的iTransformer以及采用高效流注意力模块的iTransformer（吴等人，2022年）；线性模型：DLinear（曾等人，2023年）和TiDE（达斯等人，2023年）；Transformer模型：Transformer（瓦斯瓦尼等人，2017年）、PatchTST（聂等人，2023年）和Crossformer（张和严，2023年）。结果是在官方模型配置和相同批量大小的情况下记录的。在图10中，我们比较了两个具有代表性的数据集（气象数据集中有21个变量，交通数据集中有862个变量）在96个时间步长回溯情况下的效率。

<!-- Media -->

Table 6: Full results of the ablation on iTransformer. We apply different components on the respective dimension to learn multivariate correlations (Variate) and series representations (Temporal), in addition to removing the specific component of Transformer.

表6：iTransformer消融实验的完整结果。除了移除Transformer的特定组件外，我们还在各自的维度上应用不同的组件来学习多变量相关性（变量维度）和序列表示（时间维度）。

<table><tr><td rowspan="2">Design</td><td rowspan="2">Variate</td><td rowspan="2">Temporal</td><td rowspan="2">Prediction Lengths</td><td colspan="2">ECL</td><td colspan="2">Traffic</td><td colspan="2">Weather</td><td colspan="2">Solar-Energy</td></tr><tr><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="5">iTransformer</td><td rowspan="5">Attention</td><td rowspan="5">FFN</td><td>96</td><td>0.148</td><td>0.240</td><td>0.395</td><td>0.268</td><td>0.174</td><td>0.214</td><td>0.203</td><td>0.237</td></tr><tr><td>192</td><td>0.162</td><td>0.253</td><td>0.417</td><td>0.276</td><td>0.221</td><td>0.254</td><td>0.233</td><td>0.261</td></tr><tr><td>336</td><td>0.178</td><td>0.269</td><td>0.433</td><td>0.283</td><td>0.278</td><td>0.296</td><td>0.248</td><td>0.273</td></tr><tr><td>720</td><td>0.225</td><td>0.317</td><td>0.467</td><td>0.302</td><td>0.358</td><td>0.349</td><td>0.249</td><td>0.275</td></tr><tr><td>Avg</td><td>0.178</td><td>0.270</td><td>0.428</td><td>0.282</td><td>0.258</td><td>0.279</td><td>0.233</td><td>0.262</td></tr><tr><td rowspan="15">Replace</td><td rowspan="5">Attention</td><td rowspan="5">Attention</td><td>96</td><td>0.161</td><td>0.263</td><td>1.021</td><td>0.581</td><td>0.168</td><td>0.213</td><td>0.227</td><td>0.270</td></tr><tr><td>192</td><td>0.180</td><td>0.280</td><td>0.834</td><td>0.447</td><td>0.217</td><td>0.256</td><td>0.255</td><td>0.292</td></tr><tr><td>336</td><td>0.194</td><td>0.296</td><td>0.906</td><td>0.493</td><td>0.277</td><td>0.299</td><td>0.279</td><td>0.301</td></tr><tr><td>720</td><td>0.238</td><td>0.331</td><td>0.892</td><td>0.477</td><td>0.356</td><td>0.351</td><td>0.283</td><td>0.300</td></tr><tr><td>Avg</td><td>0.193</td><td>0.293</td><td>0.913</td><td>0.500</td><td>0.255</td><td>0.280</td><td>0.261</td><td>0.291</td></tr><tr><td rowspan="5">FFN</td><td rowspan="5">Attention</td><td>96</td><td>0.169</td><td>0.270</td><td>0.907</td><td>0.540</td><td>0.176</td><td>0.221</td><td>0.247</td><td>0.299</td></tr><tr><td>192</td><td>0.189</td><td>0.292</td><td>0.839</td><td>0.489</td><td>0.224</td><td>0.261</td><td>0.275</td><td>0.305</td></tr><tr><td>336</td><td>0.204</td><td>0.304</td><td>0.248</td><td>0.364</td><td>0.279</td><td>0.301</td><td>0.317</td><td>0.337</td></tr><tr><td>720</td><td>0.245</td><td>0.335</td><td>1.059</td><td>0.606</td><td>0.354</td><td>0.347</td><td>0.301</td><td>0.329</td></tr><tr><td>Avg</td><td>0.202</td><td>0.300</td><td>0.863</td><td>0.499</td><td>0.258</td><td>0.283</td><td>0.285</td><td>0.317</td></tr><tr><td rowspan="5">FFN</td><td rowspan="5">FFN</td><td>96</td><td>0.159</td><td>0.261</td><td>0.606</td><td>0.342</td><td>0.162</td><td>0.207</td><td>0.237</td><td>0.277</td></tr><tr><td>192</td><td>0.171</td><td>0.271</td><td>0.559</td><td>0.342</td><td>0.211</td><td>0.252</td><td>0.273</td><td>0.293</td></tr><tr><td>336</td><td>0.187</td><td>0.287</td><td>0.569</td><td>0.348</td><td>0.270</td><td>0.293</td><td>0.284</td><td>0.287</td></tr><tr><td>720</td><td>0.211</td><td>0.307</td><td>0.664</td><td>0.359</td><td>0.349</td><td>0.345</td><td>0.284</td><td>0.289</td></tr><tr><td>Avg</td><td>0.182</td><td>0.287</td><td>0.599</td><td>0.348</td><td>0.248</td><td>0.274</td><td>0.269</td><td>0.287</td></tr><tr><td rowspan="10">w/o</td><td rowspan="5">Attention</td><td rowspan="5">W/O</td><td>96</td><td>0.163</td><td>0.254</td><td>0.427</td><td>0.296</td><td>0.177</td><td>0.219</td><td>0.226</td><td>0.266</td></tr><tr><td>192</td><td>0.174</td><td>0.263</td><td>0.446</td><td>0.300</td><td>0.226</td><td>0.259</td><td>0.255</td><td>0.288</td></tr><tr><td>336</td><td>0.191</td><td>0.280</td><td>0.459</td><td>0.306</td><td>0.281</td><td>0.298</td><td>0.275</td><td>0.301</td></tr><tr><td>720</td><td>0.228</td><td>0.315</td><td>0.492</td><td>0.324</td><td>0.359</td><td>0.249</td><td>0.275</td><td>0.301</td></tr><tr><td>Avg</td><td>0.189</td><td>0.278</td><td>0.456</td><td>0.306</td><td>0.261</td><td>0.281</td><td>0.258</td><td>0.289</td></tr><tr><td rowspan="5">w/o</td><td rowspan="5">FFN</td><td>96</td><td>0.169</td><td>0.253</td><td>0.437</td><td>0.283</td><td>0.183</td><td>0.220</td><td>0.228</td><td>0.263</td></tr><tr><td>192</td><td>0.177</td><td>0.261</td><td>0.449</td><td>0.287</td><td>0.231</td><td>0.262</td><td>0.261</td><td>0.283</td></tr><tr><td>336</td><td>0.194</td><td>0.278</td><td>0.464</td><td>0.294</td><td>0.285</td><td>0.300</td><td>0.279</td><td>0.294</td></tr><tr><td>720</td><td>0.233</td><td>0.311</td><td>0.496</td><td>0.313</td><td>0.362</td><td>0.350</td><td>0.276</td><td>0.291</td></tr><tr><td>Avg</td><td>0.193</td><td>0.276</td><td>0.461</td><td>0.294</td><td>0.265</td><td>0.283</td><td>0.261</td><td>0.283</td></tr></table>

<table><tbody><tr><td rowspan="2">设计</td><td rowspan="2">变化</td><td rowspan="2">时间的</td><td rowspan="2">预测长度</td><td colspan="2">ECL（原文未明确含义，保留英文）</td><td colspan="2">交通</td><td colspan="2">天气</td><td colspan="2">太阳能</td></tr><tr><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td rowspan="5">iTransformer（iTransformer）</td><td rowspan="5">注意力机制</td><td rowspan="5">前馈网络（Feed - Forward Network，FFN）</td><td>96</td><td>0.148</td><td>0.240</td><td>0.395</td><td>0.268</td><td>0.174</td><td>0.214</td><td>0.203</td><td>0.237</td></tr><tr><td>192</td><td>0.162</td><td>0.253</td><td>0.417</td><td>0.276</td><td>0.221</td><td>0.254</td><td>0.233</td><td>0.261</td></tr><tr><td>336</td><td>0.178</td><td>0.269</td><td>0.433</td><td>0.283</td><td>0.278</td><td>0.296</td><td>0.248</td><td>0.273</td></tr><tr><td>720</td><td>0.225</td><td>0.317</td><td>0.467</td><td>0.302</td><td>0.358</td><td>0.349</td><td>0.249</td><td>0.275</td></tr><tr><td>平均值（Average）</td><td>0.178</td><td>0.270</td><td>0.428</td><td>0.282</td><td>0.258</td><td>0.279</td><td>0.233</td><td>0.262</td></tr><tr><td rowspan="15">替换</td><td rowspan="5">注意力机制</td><td rowspan="5">注意力机制</td><td>96</td><td>0.161</td><td>0.263</td><td>1.021</td><td>0.581</td><td>0.168</td><td>0.213</td><td>0.227</td><td>0.270</td></tr><tr><td>192</td><td>0.180</td><td>0.280</td><td>0.834</td><td>0.447</td><td>0.217</td><td>0.256</td><td>0.255</td><td>0.292</td></tr><tr><td>336</td><td>0.194</td><td>0.296</td><td>0.906</td><td>0.493</td><td>0.277</td><td>0.299</td><td>0.279</td><td>0.301</td></tr><tr><td>720</td><td>0.238</td><td>0.331</td><td>0.892</td><td>0.477</td><td>0.356</td><td>0.351</td><td>0.283</td><td>0.300</td></tr><tr><td>平均值（Average）</td><td>0.193</td><td>0.293</td><td>0.913</td><td>0.500</td><td>0.255</td><td>0.280</td><td>0.261</td><td>0.291</td></tr><tr><td rowspan="5">前馈网络（Feed - Forward Network，FFN）</td><td rowspan="5">注意力机制</td><td>96</td><td>0.169</td><td>0.270</td><td>0.907</td><td>0.540</td><td>0.176</td><td>0.221</td><td>0.247</td><td>0.299</td></tr><tr><td>192</td><td>0.189</td><td>0.292</td><td>0.839</td><td>0.489</td><td>0.224</td><td>0.261</td><td>0.275</td><td>0.305</td></tr><tr><td>336</td><td>0.204</td><td>0.304</td><td>0.248</td><td>0.364</td><td>0.279</td><td>0.301</td><td>0.317</td><td>0.337</td></tr><tr><td>720</td><td>0.245</td><td>0.335</td><td>1.059</td><td>0.606</td><td>0.354</td><td>0.347</td><td>0.301</td><td>0.329</td></tr><tr><td>平均值（Average）</td><td>0.202</td><td>0.300</td><td>0.863</td><td>0.499</td><td>0.258</td><td>0.283</td><td>0.285</td><td>0.317</td></tr><tr><td rowspan="5">前馈网络（Feed - Forward Network，FFN）</td><td rowspan="5">前馈网络（Feed - Forward Network，FFN）</td><td>96</td><td>0.159</td><td>0.261</td><td>0.606</td><td>0.342</td><td>0.162</td><td>0.207</td><td>0.237</td><td>0.277</td></tr><tr><td>192</td><td>0.171</td><td>0.271</td><td>0.559</td><td>0.342</td><td>0.211</td><td>0.252</td><td>0.273</td><td>0.293</td></tr><tr><td>336</td><td>0.187</td><td>0.287</td><td>0.569</td><td>0.348</td><td>0.270</td><td>0.293</td><td>0.284</td><td>0.287</td></tr><tr><td>720</td><td>0.211</td><td>0.307</td><td>0.664</td><td>0.359</td><td>0.349</td><td>0.345</td><td>0.284</td><td>0.289</td></tr><tr><td>平均值（Average）</td><td>0.182</td><td>0.287</td><td>0.599</td><td>0.348</td><td>0.248</td><td>0.274</td><td>0.269</td><td>0.287</td></tr><tr><td rowspan="10">没有（without）</td><td rowspan="5">注意力机制</td><td rowspan="5">没有（Without）</td><td>96</td><td>0.163</td><td>0.254</td><td>0.427</td><td>0.296</td><td>0.177</td><td>0.219</td><td>0.226</td><td>0.266</td></tr><tr><td>192</td><td>0.174</td><td>0.263</td><td>0.446</td><td>0.300</td><td>0.226</td><td>0.259</td><td>0.255</td><td>0.288</td></tr><tr><td>336</td><td>0.191</td><td>0.280</td><td>0.459</td><td>0.306</td><td>0.281</td><td>0.298</td><td>0.275</td><td>0.301</td></tr><tr><td>720</td><td>0.228</td><td>0.315</td><td>0.492</td><td>0.324</td><td>0.359</td><td>0.249</td><td>0.275</td><td>0.301</td></tr><tr><td>平均值（Average）</td><td>0.189</td><td>0.278</td><td>0.456</td><td>0.306</td><td>0.261</td><td>0.281</td><td>0.258</td><td>0.289</td></tr><tr><td rowspan="5">没有（without）</td><td rowspan="5">前馈网络（Feed - Forward Network，FFN）</td><td>96</td><td>0.169</td><td>0.253</td><td>0.437</td><td>0.283</td><td>0.183</td><td>0.220</td><td>0.228</td><td>0.263</td></tr><tr><td>192</td><td>0.177</td><td>0.261</td><td>0.449</td><td>0.287</td><td>0.231</td><td>0.262</td><td>0.261</td><td>0.283</td></tr><tr><td>336</td><td>0.194</td><td>0.278</td><td>0.464</td><td>0.294</td><td>0.285</td><td>0.300</td><td>0.279</td><td>0.294</td></tr><tr><td>720</td><td>0.233</td><td>0.311</td><td>0.496</td><td>0.313</td><td>0.362</td><td>0.350</td><td>0.276</td><td>0.291</td></tr><tr><td>平均值（Average）</td><td>0.193</td><td>0.276</td><td>0.461</td><td>0.294</td><td>0.265</td><td>0.283</td><td>0.261</td><td>0.283</td></tr></tbody></table>

<!-- figureText: Memory Footprint Weather (21 Variates) ${1.09}\mathrm{{GB}},{85}\mathrm{{ms}}$ DLinear iTransformer (Efficient) ${0.87}\mathrm{{GB}},{29}\mathrm{{ms}}$ iFlowformer ${0.89}\mathrm{{GB}},{30}\mathrm{{ms}}$ Crossformer ${1.18GB},{110}\mathrm{{ms}}$ 1.09GB, 31ms Training Time (ms/iter) 0.8 TiDE ${2.72}\mathrm{{GB}},{130}\mathrm{{ms}}$ 6.0GB 0.7 PatchTST ${8.58}\mathrm{{GB}},{635}\mathrm{\;{ms}}$ Flowformer 0.5 1.66GB, 91ms iTransformer ${7.50}\mathrm{{GB}},{265}\mathrm{\;{ms}}$ 9.74GB, 702ms Training Time (ms/iter) -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_15.jpg?x=311&y=1652&w=1174&h=405&r=0"/>

Figure 10: Model efficiency comparison under input-96-predict-96 of Weather and Traffic.

图10：气象和交通数据集在输入96步预测96步情况下的模型效率比较。

<!-- Media -->

In a nutshell, the efficiency of iTransformer exceeds other Transformers in datasets with a relatively small number of variates (Weather). In datasets with numerous variates (Traffic), the memory footprints are basically the same as Transformers variates, but iTransformer can be trained faster. Based on the complexity of $\mathcal{O}\left( {N}^{2}\right)$ of the attention module,where $N$ is the number of tokens, Transformer surpasses iTransformer on efficiency in this case because of $N = {96}$ for the temporal token and $N = {862}$ for the variate token. Meanwhile,iTransformer achieves better performance on numerous variates, since the multivariate correlations can be explicitly utilized. By adopting a linear-complexity attention (Wu et al. 2022) or the proposed efficient training strategy as mentioned in Figure 8 (trained on 20% variates and forecast all variates), iTransformer can enjoy a comparable speed and memory footprint with linear models. Also, the two strategies can be adopted together.

简而言之，在变量数量相对较少的数据集（气象数据集）中，iTransformer的效率超过了其他Transformer模型。在变量众多的数据集（交通数据集）中，其内存占用与其他Transformer模型基本相同，但iTransformer的训练速度更快。基于注意力模块复杂度公式$\mathcal{O}\left( {N}^{2}\right)$（其中$N$为标记数量），在这种情况下，由于时间标记的复杂度为$N = {96}$，变量标记的复杂度为$N = {862}$，Transformer在效率上超过了iTransformer。同时，由于iTransformer能够明确利用多变量之间的相关性，因此在处理大量变量时表现更优。通过采用线性复杂度注意力机制（Wu等人，2022年）或图8中提出的高效训练策略（在20%的变量上进行训练并预测所有变量），iTransformer可以达到与线性模型相当的速度和内存占用。此外，这两种策略可以结合使用。

## E SHOWCASES

## E 案例展示

### E.1 Visualization of Multivariate Correlations

### E.1 多变量相关性可视化

By using the attention mechanism on variate tokens, the resulting learned map becomes more interpretable. To present an intuitive understanding of the multivariate correlations, we provide three randomly chosen case visualizations of the time series from Solar-Energy in Figure 11 . We provide the Pearson Correlation coefficients of each variate of the raw series by the following equation:

通过对变量标记使用注意力机制，得到的学习映射变得更具可解释性。为了直观理解多变量相关性，我们在图11中提供了从太阳能（Solar - Energy）时间序列中随机选取的三个案例可视化。我们通过以下方程给出原始序列每个变量的皮尔逊相关系数：

$$
{\rho }_{xy} = \frac{\mathop{\sum }\limits_{i}\left( {{x}_{i} - \bar{x}}\right) \left( {{y}_{i} - \bar{y}}\right) }{\sqrt{\mathop{\sum }\limits_{i}{\left( {x}_{i} - \bar{x}\right) }^{2}}\sqrt{\mathop{\sum }\limits_{i}{\left( {y}_{i} - \bar{y}\right) }^{2}}},
$$

where ${x}_{i},{y}_{i} \in  \mathbb{R}$ run through all time points of the paired variates to be correlated. All the cases have distinct multivariate correlations in the lookback and forecast window because the dataset exhibits obvious seasonal changes in the daytime and night. On the second row of each case, we provide the learned pre-Softmax maps of the self-attention module in both the first and the last layers. As we observe in the shallow attention layer (left), we find that the learned map is similar to the correlations of the raw lookback series. As we go deeper into the layers (right), the learned map gradually becomes more similar to the correlations of the future series to be predicted. This demonstrates that the inverted operation allows for interpretable attention in correlating, and that encoding of the past and decoding for the future are conducted through series representations during layer stacking.

其中 ${x}_{i},{y}_{i} \in  \mathbb{R}$ 遍历待关联的成对变量的所有时间点。由于数据集在白天和夜晚呈现出明显的季节性变化，所有案例在回溯窗口和预测窗口中都具有不同的多元相关性。在每个案例的第二行，我们给出了第一层和最后一层自注意力模块学习到的 Softmax 前映射。正如我们在浅层注意力层（左图）中观察到的，我们发现学习到的映射与原始回溯序列的相关性相似。随着我们深入到更深的层（右图），学习到的映射逐渐变得更类似于待预测的未来序列的相关性。这表明，逆运算允许在关联时进行可解释的注意力操作，并且在层堆叠过程中，通过序列表示来进行过去信息的编码和未来信息的解码。

<!-- Media -->

<!-- figureText: Case 2 -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_16.jpg?x=321&y=1526&w=1167&h=416&r=0"/>

Figure 11: Multivariate correlations of the lookback series and future series and the learned score maps by inverted self-attention of different layers. Cases all come from the Solar-Energy dataset.

图 11：回溯序列和未来序列的多元相关性以及不同层的逆自注意力学习到的得分图。所有案例均来自太阳能数据集。

<!-- Media -->

We present another interesting observation in Figure 12 to show that the attention module of iTrans-former has enhanced interpretability. We provide randomly chosen multivariate time series from Market. In this dataset, each variate represents the monitored values of a service interface of a kind, and the service can be further grouped into refined application categories. We divide these variates into corresponding applications (as listed on the top bar App), such that adjacent variates belong to the same application and we reveal the application index by the top bar.

我们在图12中展示了另一个有趣的观察结果，以表明iTransformer的注意力模块具有更强的可解释性。我们提供了从市场（Market）随机选取的多变量时间序列。在这个数据集中，每个变量代表某类服务接口的监测值，并且这些服务可以进一步细分为不同的应用类别。我们将这些变量划分为相应的应用（如顶部栏“应用（App）”所示），使得相邻的变量属于同一应用，并通过顶部栏显示应用索引。

We visualize the time series of the variates and plot the learned multivariate correlations with the marks of specific correlations between variates. On the one hand, we observe clear partitioning in the multivariate correlations map, indicating the grouping of variates. On the one hand, the marked correlation values can reflect the correlation of the raw series, where the similarity of variates from the same application becomes closer than the pairs from the different groups. Therefore, highly correlated variate will be leveraged for the next interaction and thus benefit for multivariate forecasting.

我们将变量的时间序列可视化，并绘制学习到的多变量相关性，同时标记出变量之间的特定相关性。一方面，我们在多变量相关性图中观察到明显的分区，这表明了变量的分组情况。另一方面，标记的相关值可以反映原始序列的相关性，同一应用中的变量之间的相似度比不同组之间的变量对更接近。因此，高度相关的变量将用于后续的交互，从而有利于多变量预测。

<!-- Media -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_17.jpg?x=306&y=580&w=1182&h=323&r=0"/>

Figure 12: Visualization of the variates from the Market dataset and the learned multivariate correlations. Each variate represents the monitored interface values of an application, and the applications can be further grouped into refined categories. The color bar is shared with Figure 11,

图12：市场数据集的变量以及学习到的多元相关性的可视化。每个变量代表一个应用程序的监控接口值，并且这些应用程序可以进一步细分为更精确的类别。此颜色条与图11共用。

<!-- Media -->

### E.2 VISUALIZATION OF PREDICTION RESULTS

### E.2 预测结果可视化

To provide a clear comparison among different models, we list supplementary prediction showcases of four representative datasets in Figures 13-16, which are given by the following models: iTransfomrer, PatchTST (Nie et al., 2023), DLinear (Zeng et al., 2023), Crossformer (Zhang & Yan, 2023), Autoformer (Wu et al. 2021), Transformer (Vaswani et al. 2017). Among the various models, iTransformer predicts the most precise future series variations and exhibits superior performance.

为了清晰比较不同模型，我们在图13 - 16中列出了四个具有代表性数据集的补充预测展示，这些展示由以下模型给出：iTransfomrer、PatchTST（聂等人，2023年）、DLinear（曾等人，2023年）、Crossformer（张和严，2023年）、Autoformer（吴等人，2021年）、Transformer（瓦斯瓦尼等人，2017年）。在各种模型中，iTransformer对未来序列变化的预测最为精确，表现出卓越的性能。

<!-- Media -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_17.jpg?x=365&y=1328&w=1061&h=576&r=0"/>

Figure 13: Visualization of input-96-predict-96 results on the Traffic dataset.

图13：交通数据集上输入96步预测96步结果的可视化。

<!-- Media -->

### E.3 Risks of Embedding Multivariate Points of A Timestamp

### E.3 嵌入时间戳多元点的风险

As aforementioned, the embedding approach of the previous Transformer fuses multiple variates representing potentially delayed events and distinct physical measurements, which may fail to learn variate-centric representations and result in meaningless attention maps. We provide the visualization case of Traffic (Liu et al. 2022a), which is collected from sensors on Los Angeles city roads in different areas. As shown in Figure 17, we can observe a strong correlation between the multivariate time series of the dataset, while they also exhibit obvious phase offset, which is due to the systematical time lags in the road occupancy that each series describes. Since the sensors are installed in different areas of the highway, an event (such as a traffic jam) can affect road occupancy with different delays.

如前所述，先前Transformer的嵌入方法融合了多个代表潜在延迟事件和不同物理测量值的变量，这可能无法学习以变量为中心的表示，并导致注意力图无意义。我们提供了交通（Liu等人，2022a）的可视化案例，该数据是从洛杉矶市不同区域道路上的传感器收集的。如图17所示，我们可以观察到该数据集的多变量时间序列之间存在很强的相关性，同时它们也表现出明显的相位偏移，这是由于每个序列所描述的道路占有率存在系统性的时间滞后。由于传感器安装在高速公路的不同区域，一个事件（如交通堵塞）可能会以不同的延迟影响道路占有率。

<!-- Media -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_18.jpg?x=365&y=238&w=1064&h=566&r=0"/>

Figure 14: Visualization of input-96-predict-96 results on the ECL dataset.

图14：ECL数据集上输入96步预测96步结果的可视化。

<!-- figureText: Crossformer -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_18.jpg?x=362&y=882&w=1063&h=560&r=0"/>

Figure 15: Visualization of input-96-predict-96 results on the Weather dataset.

图15：天气数据集上输入96步预测96步结果的可视化。

<!-- figureText: iTransformer PatchTST Autoformer -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_18.jpg?x=367&y=1526&w=1059&h=567&r=0"/>

Figure 16: Visualization of input-96-predict-96 results on the PEMS dataset.

图16：PEMS数据集上输入96步预测96步结果的可视化。

<!-- figureText: Road occupancy Road occupancy Sensor 860 -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_19.jpg?x=305&y=425&w=1183&h=321&r=0"/>

Figure 17: Visualization of partial variates of Traffic. We can observe that several series exhibit strong synchronization (such as Sensor 2 and Sensor 4), and there also exist obvious delays and advances between series (such as Sensor 1 and Sensor 2, Sensor 859 and Sensor 861).

图17：交通部分变量的可视化。我们可以观察到，有几个序列呈现出很强的同步性（如传感器2和传感器4），并且序列之间还存在明显的延迟和提前现象（如传感器1和传感器2、传感器859和传感器861）。

<!-- Media -->

Besides, we observe the significantly declined performance on the second and third designs of Traffic in Table 6, which apply attention to temporal tokens. In our opinion, capturing temporal dependencies by attention is not a big problem. But it is based on the fact that the time points of each timestamp essentially reflect the same event to enclose a semantic representation. Since there are inherent delays between the time points, the performance can degrade a lot because of the meaningless attention map, unless the model has an enlarged respective field to learn about the decay or causal process.

此外，我们在表6中观察到，交通的第二种和第三种设计（对时间标记应用注意力机制）的性能显著下降。我们认为，通过注意力机制捕捉时间依赖关系并不是一个大问题。但这是基于每个时间戳的时间点本质上反映同一事件以形成语义表示这一事实。由于时间点之间存在固有的延迟，由于注意力图没有意义，性能可能会大幅下降，除非模型有更大的感受野来学习衰减或因果过程。

Other risks can be aroused from the distinct variate measurements, such as organizing together different meteorological indicators (the temperature and rainfall) in the Weather dataset (Wu et al., 2021), and the quantity and proportion of the same observation in ILI (Wu et al. 2023). Given these potential risks, iTransformer proposes a new paradigm that embeds the whole series as the variate token, which can be more robust to extensive real-world scenarios, such as delayed events, inconsistent measurements, irregular (unevenly spaced) time series, systematical delay of monitors, and the time interval of generating and recording different time series.

其他风险可能源于不同的变量测量，例如在天气数据集（Wu等人，2021年）中组合不同的气象指标（温度和降雨量），以及流感样疾病（ILI）中相同观测值的数量和比例（Wu等人，2023年）。鉴于这些潜在风险，iTransformer提出了一种新的范式，即将整个序列嵌入为变量标记，这对于广泛的现实场景（如延迟事件、测量不一致、不规则（间隔不均匀）的时间序列、监测器的系统性延迟以及不同时间序列生成和记录的时间间隔）具有更强的鲁棒性。

## F FULL RESULTS

## F 完整结果

### F.1 FULL PROMOTION RESULTS

### F.1 完整提升结果

We compare the performance of Transformer and iTransformer on all datasets in Table 7 Consistent and great promotions can be achieved, indicating that the attention and feed-forward network on the inverted dimensions greatly empower Transformers in multivariate time series forecasting, leaving an instructive direction to build up the foundation model of extensive time series data.

我们在表7中比较了Transformer和iTransformer在所有数据集上的性能。可以实现持续且显著的提升，这表明在倒置维度上的注意力机制和前馈网络极大地增强了Transformer在多变量时间序列预测中的能力，为构建广泛时间序列数据的基础模型指明了有指导意义的方向。

<!-- Media -->

Table 7: Full performance comparison between the vanilla Transformer and the proposed iTransformer. The results are averaged from all four prediction lengths.

表7：原始Transformer与所提出的iTransformer的完整性能比较。结果是四个预测长度的平均值。

<table><tr><td rowspan="2">Datasets Metric</td><td colspan="2">ETT</td><td colspan="2">ECL</td><td colspan="2">PEMS</td><td colspan="2">Solar-Energy</td><td colspan="2">Traffic</td><td colspan="2">Weather</td></tr><tr><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>Transformer</td><td>2.750</td><td>1.375</td><td>0.277</td><td>0.372</td><td>0.157</td><td>0.263</td><td>0.256</td><td>0.276</td><td>0.665</td><td>0.363</td><td>0.657</td><td>0.572</td></tr><tr><td>iTransformer</td><td>0.383</td><td>0.407</td><td>0.178</td><td>0.270</td><td>0.113</td><td>0.221</td><td>0.233</td><td>0.262</td><td>0.428</td><td>0.282</td><td>0.258</td><td>0.279</td></tr><tr><td>Promotion</td><td>86.1%</td><td>70.4%</td><td>35.6%</td><td>27.4%</td><td>28.0%</td><td>16.0%</td><td>9.0%</td><td>5.1%</td><td>35.6%</td><td>22.3%</td><td>60.2%</td><td>50.8%</td></tr></table>

<table><tbody><tr><td rowspan="2">数据集指标</td><td colspan="2">电力变压器温度数据集（ETT）</td><td colspan="2">电力负荷数据集（ECL）</td><td colspan="2">加州交通流量数据集（PEMS）</td><td colspan="2">太阳能数据集（Solar - Energy）</td><td colspan="2">交通数据集</td><td colspan="2">天气</td></tr><tr><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td></tr><tr><td>变换器（Transformer）</td><td>2.750</td><td>1.375</td><td>0.277</td><td>0.372</td><td>0.157</td><td>0.263</td><td>0.256</td><td>0.276</td><td>0.665</td><td>0.363</td><td>0.657</td><td>0.572</td></tr><tr><td>i变换器（iTransformer）</td><td>0.383</td><td>0.407</td><td>0.178</td><td>0.270</td><td>0.113</td><td>0.221</td><td>0.233</td><td>0.262</td><td>0.428</td><td>0.282</td><td>0.258</td><td>0.279</td></tr><tr><td>促销</td><td>86.1%</td><td>70.4%</td><td>35.6%</td><td>27.4%</td><td>28.0%</td><td>16.0%</td><td>9.0%</td><td>5.1%</td><td>35.6%</td><td>22.3%</td><td>60.2%</td><td>50.8%</td></tr></tbody></table>

<!-- Media -->

### F.2 FULL FRAMEWORK GENERALITY RESULTS

### F.2 全框架通用性结果

We apply the proposed inverting framework to Transformer and its variants: Transformer (Vaswani et al. 2017), Reformer (Kitaev et al., 2020), Informer (Li et al., 2021), Flowformer (Wu et al.,

我们将所提出的反演框架应用于Transformer及其变体：Transformer（瓦斯瓦尼等人，2017年）、Reformer（基塔耶夫等人，2020年）、Informer（李等人，2021年）、Flowformer（吴等人，

2022), Flashformer (Dao et al. 2022). The averaged results are shown in Table 2 due to the limited pages. We provide the supplementary forecasting results in Table 8 . The results demonstrate that our iTransformers framework can consistently promote these Transformer variants, and take advantage of the booming efficient attention mechanisms.

2022年）、Flashformer（道等人，2022年）。由于篇幅有限，平均结果见表2。我们在表8中提供了补充预测结果。结果表明，我们的iTransformers框架可以持续推动这些Transformer变体的发展，并利用蓬勃发展的高效注意力机制。

<!-- Media -->

Table 8: Full results of Transformers with our inverted framework. Flashformer means Transformer equipped with the hardware-accelerated FlashAttention (Dao et al. 2022).

表8：采用我们的反演框架的Transformer的完整结果。Flashformer指配备了硬件加速的FlashAttention（道等人，2022年）的Transformer。

<table><tr><td colspan="3">Models</td><td colspan="2">Transformer (2017)</td><td colspan="2">Reformer (2020)</td><td colspan="2">Informer (2021)</td><td colspan="2">Flowformer (2022)</td><td colspan="2">Flashformer (2022)</td></tr><tr><td colspan="3">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="10">ECL</td><td rowspan="5">Original</td><td>96</td><td>0.260</td><td>0.358</td><td>0.312</td><td>0.402</td><td>0.274</td><td>0.368</td><td>0.215</td><td>0.320</td><td>0.259</td><td>0.357</td></tr><tr><td>192</td><td>0.266</td><td>0.367</td><td>0.348</td><td>0.433</td><td>0.296</td><td>0.386</td><td>0.259</td><td>0.355</td><td>0.274</td><td>0.374</td></tr><tr><td>336</td><td>0.280</td><td>0.375</td><td>0.350</td><td>0.433</td><td>0.300</td><td>0.394</td><td>0.296</td><td>0.383</td><td>0.310</td><td>0.396</td></tr><tr><td>720</td><td>0.302</td><td>0.386</td><td>0.340</td><td>0.420</td><td>0.373</td><td>0.439</td><td>0.296</td><td>0.380</td><td>0.298</td><td>0.383</td></tr><tr><td>Avg</td><td>0.277</td><td>0.372</td><td>0.338</td><td>0.422</td><td>0.311</td><td>0.397</td><td>0.267</td><td>0.359</td><td>0.285</td><td>0.377</td></tr><tr><td rowspan="5">+Inverted</td><td>96</td><td>0.148</td><td>0.240</td><td>0.182</td><td>0.275</td><td>0.190</td><td>0.286</td><td>0.183</td><td>0.267</td><td>0.178</td><td>0.265</td></tr><tr><td>192</td><td>0.162</td><td>0.253</td><td>0.192</td><td>0.286</td><td>0.201</td><td>0.297</td><td>0.192</td><td>0.277</td><td>0.189</td><td>0.276</td></tr><tr><td>336</td><td>0.178</td><td>0.269</td><td>0.210</td><td>0.304</td><td>0.218</td><td>0.315</td><td>0.210</td><td>0.295</td><td>0.207</td><td>0.294</td></tr><tr><td>720</td><td>0.225</td><td>0.317</td><td>0.249</td><td>0.339</td><td>0.255</td><td>0.347</td><td>0.255</td><td>0.332</td><td>0.251</td><td>0.329</td></tr><tr><td>Avg</td><td>0.178</td><td>0.270</td><td>0.208</td><td>0.301</td><td>0.216</td><td>0.311</td><td>0.210</td><td>0.293</td><td>0.206</td><td>0.291</td></tr><tr><td rowspan="10">Traffic</td><td rowspan="5">Original</td><td>96</td><td>0.647</td><td>0.357</td><td>0.732</td><td>0.423</td><td>0.719</td><td>0.391</td><td>0.691</td><td>0.393</td><td>0.641</td><td>0.348</td></tr><tr><td>192</td><td>0.649</td><td>0.356</td><td>0.733</td><td>0.420</td><td>0.696</td><td>0.379</td><td>0.729</td><td>0.419</td><td>0.648</td><td>0.358</td></tr><tr><td>336</td><td>0.667</td><td>0.364</td><td>0.742</td><td>0.420</td><td>0.777</td><td>0.420</td><td>0.756</td><td>0.423</td><td>0.670</td><td>0.364</td></tr><tr><td>720</td><td>0.697</td><td>0.376</td><td>0.755</td><td>0.432</td><td>0.864</td><td>0.472</td><td>0.825</td><td>0.449</td><td>0.673</td><td>0.354</td></tr><tr><td>Avg</td><td>0.665</td><td>0.363</td><td>0.741</td><td>0.422</td><td>0.764</td><td>0.416</td><td>0.750</td><td>0.421</td><td>0.658</td><td>0.356</td></tr><tr><td rowspan="5">+Inverted</td><td>96</td><td>0.395</td><td>0.268</td><td>0.617</td><td>0.356</td><td>0.632</td><td>0.367</td><td>0.493</td><td>0.339</td><td>0.464</td><td>0.320</td></tr><tr><td>192</td><td>0.417</td><td>0.276</td><td>0.629</td><td>0.361</td><td>0.641</td><td>0.370</td><td>0.506</td><td>0.345</td><td>0.479</td><td>0.326</td></tr><tr><td>336</td><td>0.433</td><td>0.283</td><td>0.648</td><td>0.370</td><td>0.663</td><td>0.379</td><td>0.526</td><td>0.355</td><td>0.501</td><td>0.337</td></tr><tr><td>720</td><td>0.467</td><td>0.302</td><td>0.694</td><td>0.394</td><td>0.713</td><td>0.405</td><td>0.572</td><td>0.381</td><td>0.524</td><td>0.350</td></tr><tr><td>Avg</td><td>0.428</td><td>0.282</td><td>0.647</td><td>0.370</td><td>0.662</td><td>0.380</td><td>0.524</td><td>0.355</td><td>0.492</td><td>0.333</td></tr><tr><td rowspan="10">Weather</td><td rowspan="5">Original</td><td>96</td><td>0.395</td><td>0.427</td><td>0.689</td><td>0.596</td><td>0.300</td><td>0.384</td><td>0.182</td><td>0.233</td><td>0.388</td><td>0.425</td></tr><tr><td>192</td><td>0.619</td><td>0.560</td><td>0.752</td><td>0.638</td><td>0.598</td><td>0.544</td><td>0.250</td><td>0.288</td><td>0.619</td><td>0.560</td></tr><tr><td>336</td><td>0.689</td><td>0.594</td><td>0.639</td><td>0.596</td><td>0.578</td><td>0.523</td><td>0.309</td><td>0.329</td><td>0.698</td><td>0.600</td></tr><tr><td>720</td><td>0.926</td><td>0.710</td><td>1.130</td><td>0.792</td><td>1.059</td><td>0.741</td><td>0.404</td><td>0.385</td><td>0.930</td><td>0.711</td></tr><tr><td>Avg</td><td>0.657</td><td>0.572</td><td>0.803</td><td>0.656</td><td>0.634</td><td>0.548</td><td>0.286</td><td>0.308</td><td>0.659</td><td>0.574</td></tr><tr><td rowspan="5">+Inverted</td><td>96</td><td>0.174</td><td>0.214</td><td>0.169</td><td>0.225</td><td>0.180</td><td>0.251</td><td>0.183</td><td>0.223</td><td>0.177</td><td>0.218</td></tr><tr><td>192</td><td>0.221</td><td>0.254</td><td>0.213</td><td>0.265</td><td>0.244</td><td>0.318</td><td>0.231</td><td>0.262</td><td>0.229</td><td>0.261</td></tr><tr><td>336</td><td>0.278</td><td>0.296</td><td>0.268</td><td>0.317</td><td>0.282</td><td>0.343</td><td>0.286</td><td>0.301</td><td>0.283</td><td>0.300</td></tr><tr><td>720</td><td>0.358</td><td>0.349</td><td>0.340</td><td>0.361</td><td>0.377</td><td>0.409</td><td>0.363</td><td>0.352</td><td>0.359</td><td>0.251</td></tr><tr><td>Avg</td><td>0.258</td><td>0.279</td><td>0.248</td><td>0.292</td><td>0.271</td><td>0.330</td><td>0.266</td><td>0.285</td><td>0.262</td><td>0.282</td></tr></table>

<table><tbody><tr><td colspan="3">模型</td><td colspan="2">Transformer模型（2017年）</td><td colspan="2">Reformer模型（2020年）</td><td colspan="2">Informer模型（2021年）</td><td colspan="2">Flowformer模型（2022年）</td><td colspan="2">Flashformer模型（2022年）</td></tr><tr><td colspan="3">指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td rowspan="10">欧几里得聚类损失（ECL）</td><td rowspan="5">原始的</td><td>96</td><td>0.260</td><td>0.358</td><td>0.312</td><td>0.402</td><td>0.274</td><td>0.368</td><td>0.215</td><td>0.320</td><td>0.259</td><td>0.357</td></tr><tr><td>192</td><td>0.266</td><td>0.367</td><td>0.348</td><td>0.433</td><td>0.296</td><td>0.386</td><td>0.259</td><td>0.355</td><td>0.274</td><td>0.374</td></tr><tr><td>336</td><td>0.280</td><td>0.375</td><td>0.350</td><td>0.433</td><td>0.300</td><td>0.394</td><td>0.296</td><td>0.383</td><td>0.310</td><td>0.396</td></tr><tr><td>720</td><td>0.302</td><td>0.386</td><td>0.340</td><td>0.420</td><td>0.373</td><td>0.439</td><td>0.296</td><td>0.380</td><td>0.298</td><td>0.383</td></tr><tr><td>平均值</td><td>0.277</td><td>0.372</td><td>0.338</td><td>0.422</td><td>0.311</td><td>0.397</td><td>0.267</td><td>0.359</td><td>0.285</td><td>0.377</td></tr><tr><td rowspan="5">+反向</td><td>96</td><td>0.148</td><td>0.240</td><td>0.182</td><td>0.275</td><td>0.190</td><td>0.286</td><td>0.183</td><td>0.267</td><td>0.178</td><td>0.265</td></tr><tr><td>192</td><td>0.162</td><td>0.253</td><td>0.192</td><td>0.286</td><td>0.201</td><td>0.297</td><td>0.192</td><td>0.277</td><td>0.189</td><td>0.276</td></tr><tr><td>336</td><td>0.178</td><td>0.269</td><td>0.210</td><td>0.304</td><td>0.218</td><td>0.315</td><td>0.210</td><td>0.295</td><td>0.207</td><td>0.294</td></tr><tr><td>720</td><td>0.225</td><td>0.317</td><td>0.249</td><td>0.339</td><td>0.255</td><td>0.347</td><td>0.255</td><td>0.332</td><td>0.251</td><td>0.329</td></tr><tr><td>平均值</td><td>0.178</td><td>0.270</td><td>0.208</td><td>0.301</td><td>0.216</td><td>0.311</td><td>0.210</td><td>0.293</td><td>0.206</td><td>0.291</td></tr><tr><td rowspan="10">交通</td><td rowspan="5">原始的</td><td>96</td><td>0.647</td><td>0.357</td><td>0.732</td><td>0.423</td><td>0.719</td><td>0.391</td><td>0.691</td><td>0.393</td><td>0.641</td><td>0.348</td></tr><tr><td>192</td><td>0.649</td><td>0.356</td><td>0.733</td><td>0.420</td><td>0.696</td><td>0.379</td><td>0.729</td><td>0.419</td><td>0.648</td><td>0.358</td></tr><tr><td>336</td><td>0.667</td><td>0.364</td><td>0.742</td><td>0.420</td><td>0.777</td><td>0.420</td><td>0.756</td><td>0.423</td><td>0.670</td><td>0.364</td></tr><tr><td>720</td><td>0.697</td><td>0.376</td><td>0.755</td><td>0.432</td><td>0.864</td><td>0.472</td><td>0.825</td><td>0.449</td><td>0.673</td><td>0.354</td></tr><tr><td>平均值</td><td>0.665</td><td>0.363</td><td>0.741</td><td>0.422</td><td>0.764</td><td>0.416</td><td>0.750</td><td>0.421</td><td>0.658</td><td>0.356</td></tr><tr><td rowspan="5">+反向</td><td>96</td><td>0.395</td><td>0.268</td><td>0.617</td><td>0.356</td><td>0.632</td><td>0.367</td><td>0.493</td><td>0.339</td><td>0.464</td><td>0.320</td></tr><tr><td>192</td><td>0.417</td><td>0.276</td><td>0.629</td><td>0.361</td><td>0.641</td><td>0.370</td><td>0.506</td><td>0.345</td><td>0.479</td><td>0.326</td></tr><tr><td>336</td><td>0.433</td><td>0.283</td><td>0.648</td><td>0.370</td><td>0.663</td><td>0.379</td><td>0.526</td><td>0.355</td><td>0.501</td><td>0.337</td></tr><tr><td>720</td><td>0.467</td><td>0.302</td><td>0.694</td><td>0.394</td><td>0.713</td><td>0.405</td><td>0.572</td><td>0.381</td><td>0.524</td><td>0.350</td></tr><tr><td>平均值</td><td>0.428</td><td>0.282</td><td>0.647</td><td>0.370</td><td>0.662</td><td>0.380</td><td>0.524</td><td>0.355</td><td>0.492</td><td>0.333</td></tr><tr><td rowspan="10">天气</td><td rowspan="5">原始的</td><td>96</td><td>0.395</td><td>0.427</td><td>0.689</td><td>0.596</td><td>0.300</td><td>0.384</td><td>0.182</td><td>0.233</td><td>0.388</td><td>0.425</td></tr><tr><td>192</td><td>0.619</td><td>0.560</td><td>0.752</td><td>0.638</td><td>0.598</td><td>0.544</td><td>0.250</td><td>0.288</td><td>0.619</td><td>0.560</td></tr><tr><td>336</td><td>0.689</td><td>0.594</td><td>0.639</td><td>0.596</td><td>0.578</td><td>0.523</td><td>0.309</td><td>0.329</td><td>0.698</td><td>0.600</td></tr><tr><td>720</td><td>0.926</td><td>0.710</td><td>1.130</td><td>0.792</td><td>1.059</td><td>0.741</td><td>0.404</td><td>0.385</td><td>0.930</td><td>0.711</td></tr><tr><td>平均值</td><td>0.657</td><td>0.572</td><td>0.803</td><td>0.656</td><td>0.634</td><td>0.548</td><td>0.286</td><td>0.308</td><td>0.659</td><td>0.574</td></tr><tr><td rowspan="5">+反向</td><td>96</td><td>0.174</td><td>0.214</td><td>0.169</td><td>0.225</td><td>0.180</td><td>0.251</td><td>0.183</td><td>0.223</td><td>0.177</td><td>0.218</td></tr><tr><td>192</td><td>0.221</td><td>0.254</td><td>0.213</td><td>0.265</td><td>0.244</td><td>0.318</td><td>0.231</td><td>0.262</td><td>0.229</td><td>0.261</td></tr><tr><td>336</td><td>0.278</td><td>0.296</td><td>0.268</td><td>0.317</td><td>0.282</td><td>0.343</td><td>0.286</td><td>0.301</td><td>0.283</td><td>0.300</td></tr><tr><td>720</td><td>0.358</td><td>0.349</td><td>0.340</td><td>0.361</td><td>0.377</td><td>0.409</td><td>0.363</td><td>0.352</td><td>0.359</td><td>0.251</td></tr><tr><td>平均值</td><td>0.258</td><td>0.279</td><td>0.248</td><td>0.292</td><td>0.271</td><td>0.330</td><td>0.266</td><td>0.285</td><td>0.262</td><td>0.282</td></tr></tbody></table>

<!-- Media -->

### F.3 FULL RESULTS OF VARIATE GENERALIZATION

### F.3 变量泛化的完整结果

We divide the variates of each dataset into five folders,train models with only ${20}\%$ of variates of one folder, and directly forecast all variates without fine-tuning. We adopt two strategies for Transformers to generalize on unseen variates: (1) CI-Transformers (Nie et al., 2023): Channel Independence regards each variate of time series as independent channels, and trains with a shared backbone. During inference, the model predicts variates one by one, but the procedure can be time-consuming. (2) iTransformers: with the flexibility of the attention mechanism that the number of input tokens can be dynamically changeable, the amount of variates as tokens is no longer restricted and thus feasible to vary from training and inference, and can even allow the model to be trained on arbitrary variates.

我们将每个数据集的变量划分为五个文件夹，仅使用一个文件夹中${20}\%$的变量来训练模型，并直接预测所有变量而不进行微调。我们采用两种策略让Transformer在未见变量上进行泛化：(1) CI - Transformer（聂等人，2023年）：通道独立性将时间序列的每个变量视为独立通道，并使用共享主干进行训练。在推理过程中，模型逐个预测变量，但该过程可能很耗时。(2) iTransformer：由于注意力机制具有输入令牌数量可以动态变化的灵活性，作为令牌的变量数量不再受限，因此在训练和推理时可以有所不同，甚至可以让模型在任意变量上进行训练。

As shown in Table 18, iTransformers can be naturally trained with 20% variates and accomplish forecast on all variates with the ability to learn transferable representations.

如表18所示，iTransformer可以自然地使用20%的变量进行训练，并凭借学习可迁移表示的能力对所有变量进行预测。

<!-- Media -->

<!-- figureText: 0.125 0.125 Solar-Energy 0.10 0.05 PEMS04 PEMS08 0.200 0.100 0.000 0.100 0.100 0.050 0.025 0.025 -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_21.jpg?x=305&y=225&w=1186&h=620&r=0"/>

Figure 18: Full performance of generalization on unseen variates, comparing the iTransformers with CI-Transfomers. We divide the variates of each dataset into five folders, train with 20% variates, and use the trained model to forecast all varieties. We plot the averaged results of all five folders.

图18：在未见变量上的全面泛化性能，比较iTransformers和CI-Transformers。我们将每个数据集的变量分为五个文件夹，用20%的变量进行训练，并使用训练好的模型对所有变量进行预测。我们绘制了所有五个文件夹的平均结果。

<!-- Media -->

### F.4 FULL FORECASTING RESULTS

### F.4 全面预测结果

The full multivariate forecasting results are provided in the following section due to the space limitation of the main text. We extensively evaluate competitive counterparts on challenging forecasting tasks. Table 9 contains the forecasting results on the four public subsets from PEMS (Liu et al., 2022a). Table 10 contains the detailed results of all prediction lengths of the nine well-acknowledged forecasting benchmarks. And Table 11 records the Market results for Alipay server load forecasting. The proposed model achieves comprehensive state-of-the-art in real-world forecasting applications.

由于正文篇幅有限，全面的多变量预测结果将在以下部分提供。我们在具有挑战性的预测任务上对有竞争力的对比模型进行了广泛评估。表9包含了来自加州交通传感器数据（PEMS）（Liu等人，2022a）的四个公共子集的预测结果。表10包含了九个公认的预测基准的所有预测长度的详细结果。表11记录了支付宝服务器负载预测的市场结果。所提出的模型在现实世界的预测应用中达到了全面的先进水平。

<!-- Media -->

Table 9: Full results of the PEMS forecasting task. We compare extensive competitive models under different prediction lengths following the setting of SCINet (2022a). The input length is set to 96 for all baselines. Avg means the average results from all four prediction lengths.

表9：加州交通传感器数据（PEMS）预测任务的全面结果。我们按照SCINet（2022a）的设置，在不同的预测长度下比较了大量有竞争力的模型。所有基线模型的输入长度均设置为96。Avg表示所有四个预测长度的平均结果。

<table><tr><td colspan="2">Models</td><td colspan="2">iTransformer (Ours)</td><td colspan="2">RLinear (2023)</td><td colspan="2">PatchTST (2023)</td><td colspan="2">Crossformer (2023)</td><td colspan="2">TiDE (2023)</td><td colspan="2">TimesNet (2023)</td><td colspan="2">DLinear (2023)</td><td colspan="2">SCINet (2022a)</td><td colspan="2">FEDformer (2022)</td><td colspan="2">Stationary (2022b)</td><td colspan="2">Autoformer (2021)</td></tr><tr><td/><td>Metric</td><td>MSE</td><td>MAE</td><td/><td>MSE MAE</td><td/><td>MSE MAE</td><td>MSE</td><td>MAE</td><td/><td>MSE MAE</td><td>MSE</td><td>MAE</td><td>MSE MAE</td><td/><td/><td>MSE MAE</td><td/><td>MSE MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="5">PEMS03</td><td>12</td><td>0.071</td><td>0.174</td><td/><td>0.126 0.236</td><td>0.099</td><td>0.216</td><td>0.090</td><td>0.203</td><td>0.178</td><td>0.305</td><td>0.085</td><td>0.192</td><td>0.122</td><td>0.243</td><td>0.066</td><td>0.172</td><td>0.126</td><td>0.251</td><td>0.081</td><td>0.188</td><td>0.272</td><td>0.385</td></tr><tr><td>24</td><td>$\underline{0.093}$</td><td>0.201</td><td>0.246</td><td>0.334</td><td>0.142</td><td>0.259</td><td>0.121</td><td>0.240</td><td>0.257</td><td>0.371</td><td>0.118</td><td>0.223</td><td>0.201</td><td>0.317</td><td>0.085</td><td>0.198</td><td>0.149</td><td>0.275</td><td>0.105</td><td>0.214</td><td>0.334</td><td>0.440</td></tr><tr><td>48</td><td>0.125</td><td>0.236</td><td>0.551</td><td>0.529</td><td>0.211</td><td>0.319</td><td>0.202</td><td>0.317</td><td>0.379</td><td>0.463</td><td>0.155</td><td>0.260</td><td>0.333</td><td>0.425</td><td>$\underline{0.127}$</td><td>0.238</td><td>0.227</td><td>0.348</td><td>0.154</td><td>0.257</td><td>1.032</td><td>0.782</td></tr><tr><td>96</td><td>0.164</td><td>0.275</td><td>1.057</td><td>0.787</td><td>0.269</td><td>0.370</td><td>0.262</td><td>0.367</td><td>0.490</td><td>0.539</td><td>0.228</td><td>0.317</td><td>0.457</td><td>0.515</td><td>$\underline{0.178}$</td><td>$\underline{0.287}$</td><td>0.348</td><td>0.434</td><td>0.247</td><td>0.336</td><td>1.031</td><td>0.796</td></tr><tr><td>Avg</td><td>0.113</td><td>0.221</td><td>0.495</td><td>0.472</td><td>0.180</td><td>0.291</td><td>0.169</td><td>0.281</td><td>0.326</td><td>0.419</td><td>0.147</td><td>0.248</td><td>0.278</td><td>0.375</td><td>$\underline{0.114}$</td><td>0.224</td><td>0.213</td><td>0.327</td><td>0.147</td><td>0.249</td><td>0.667</td><td>0.601</td></tr><tr><td rowspan="5">PEMS04</td><td>12</td><td>$\underline{0.078}$</td><td>$\underline{0.183}$</td><td>0.138</td><td>0.252</td><td>0.105</td><td>0.224</td><td>0.098</td><td>0.218</td><td>0.219</td><td>0.340</td><td>0.087</td><td>0.195</td><td>0.148</td><td>0.272</td><td>0.073</td><td>0.177</td><td>0.138</td><td>0.262</td><td>0.088</td><td>0.196</td><td>0.424</td><td>0.491</td></tr><tr><td>24</td><td>0.095</td><td>$\underline{0.205}$</td><td>0.258</td><td>0.348</td><td>0.153</td><td>0.275</td><td>0.131</td><td>0.256</td><td>0.292</td><td>0.398</td><td>0.103</td><td>0.215</td><td>0.224</td><td>0.340</td><td>0.084</td><td>0.193</td><td>0.177</td><td>0.293</td><td>0.104</td><td>0.216</td><td>0.459</td><td>0.509</td></tr><tr><td>48</td><td>$\underline{0.120}$</td><td>0.233</td><td>0.572</td><td>0.544</td><td>0.229</td><td>0.339</td><td>0.205</td><td>0.326</td><td>0.409</td><td>0.478</td><td>0.136</td><td>0.250</td><td>0.355</td><td>0.437</td><td>0.099</td><td>0.211</td><td>0.270</td><td>0.368</td><td>0.137</td><td>0.251</td><td>0.646</td><td>0.610</td></tr><tr><td>96</td><td>$\underline{0.150}$</td><td>$\underline{0.262}$</td><td>1.137</td><td>0.820</td><td>0.291</td><td>0.389</td><td>0.402</td><td>0.457</td><td>0.492</td><td>0.532</td><td>0.190</td><td>0.303</td><td>0.452</td><td>0.504</td><td>0.114</td><td>0.227</td><td>0.341</td><td>0.427</td><td>0.186</td><td>0.297</td><td>0.912</td><td>0.748</td></tr><tr><td>Avg</td><td>0.111</td><td>$\underline{0.221}$</td><td>0.526</td><td>0.491</td><td>0.195</td><td>0.307</td><td>0.209</td><td>0.314</td><td>0.353</td><td>0.437</td><td>0.129</td><td>0.241</td><td>0.295</td><td>0.388</td><td>0.092</td><td>0.202</td><td>0.231</td><td>0.337</td><td>0.127</td><td>0.240</td><td>0.610</td><td>0.590</td></tr><tr><td rowspan="5">PEMS07</td><td>12</td><td>0.067</td><td>0.165</td><td>0.118</td><td>0.235</td><td>0.095</td><td>0.207</td><td>0.094</td><td>0.200</td><td>0.173</td><td>0.304</td><td>0.082</td><td>0.181</td><td>0.115</td><td>0.242</td><td>0.068</td><td>0.171</td><td>0.109</td><td>0.225</td><td>0.083</td><td>0.185</td><td>0.199</td><td>0.336</td></tr><tr><td>24</td><td>0.088</td><td>0.190</td><td>0.242</td><td>0.341</td><td>0.150</td><td>0.262</td><td>0.139</td><td>0.247</td><td>0.271</td><td>0.383</td><td>0.101</td><td>0.204</td><td>0.210</td><td>0.329</td><td>0.119</td><td>0.225</td><td>0.125</td><td>0.244</td><td>0.102</td><td>0.207</td><td>0.323</td><td>0.420</td></tr><tr><td>48</td><td>0.110</td><td>0.215</td><td>0.562</td><td>0.541</td><td>0.253</td><td>0.340</td><td>0.311</td><td>0.369</td><td>0.446</td><td>0.495</td><td>0.134</td><td>0.238</td><td>0.398</td><td>0.458</td><td>$\underline{0.149}$</td><td>0.237</td><td>0.165</td><td>0.288</td><td>0.136</td><td>0.240</td><td>0.390</td><td>0.470</td></tr><tr><td>96</td><td>0.139</td><td>0.245</td><td>1.096</td><td>0.795</td><td>0.346</td><td>0.404</td><td>0.396</td><td>0.442</td><td>0.628</td><td>0.577</td><td>0.181</td><td>0.279</td><td>0.594</td><td>0.553</td><td>$\underline{0.141}$</td><td>0.234</td><td>0.262</td><td>0.376</td><td>0.187</td><td>0.287</td><td>0.554</td><td>0.578</td></tr><tr><td>Avg</td><td>0.101</td><td>0.204</td><td>0.504</td><td>0.478</td><td>0.211</td><td>0.303</td><td>0.235</td><td>0.315</td><td>0.380</td><td>0.440</td><td>0.124</td><td>0.225</td><td>0.329</td><td>0.395</td><td>0.119</td><td>$\underline{0.234}$</td><td>0.165</td><td>0.283</td><td>0.127</td><td>0.230</td><td>0.367</td><td>0.451</td></tr><tr><td rowspan="5">PEMS08</td><td>12</td><td>0.079</td><td>0.182</td><td>0.133</td><td>0.247</td><td>0.168</td><td>0.232</td><td>0.165</td><td>0.214</td><td>0.227</td><td>0.343</td><td>0.112</td><td>0.212</td><td>0.154</td><td>0.276</td><td>$\underline{0.087}$</td><td>$\underline{0.184}$</td><td>0.173</td><td>0.273</td><td>0.109</td><td>0.207</td><td>0.436</td><td>0.485</td></tr><tr><td>24</td><td>0.115</td><td>0.219</td><td>0.249</td><td>0.343</td><td>0.224</td><td>0.281</td><td>0.215</td><td>0.260</td><td>0.318</td><td>0.409</td><td>0.141</td><td>0.238</td><td>0.248</td><td>0.353</td><td>$\underline{0.122}$</td><td>0.221</td><td>0.210</td><td>0.301</td><td>0.140</td><td>0.236</td><td>0.467</td><td>0.502</td></tr><tr><td>48</td><td>0.186</td><td>0.235</td><td>0.569</td><td>0.544</td><td>0.321</td><td>0.354</td><td>0.315</td><td>0.355</td><td>0.497</td><td>0.510</td><td>0.198</td><td>0.283</td><td>0.440</td><td>0.470</td><td>0.189</td><td>0.270</td><td>0.320</td><td>0.394</td><td>0.211</td><td>0.294</td><td>0.966</td><td>0.733</td></tr><tr><td>96</td><td>0.221</td><td>0.267</td><td>1.166</td><td>0.814</td><td>0.408</td><td>0.417</td><td>0.377</td><td>0.397</td><td>0.721</td><td>0.592</td><td>0.320</td><td>0.351</td><td>0.674</td><td>0.565</td><td>$\underline{0.236}$</td><td>$\underline{0.300}$</td><td>0.442</td><td>0.465</td><td>0.345</td><td>0.367</td><td>1.385</td><td>0.915</td></tr><tr><td>Avg</td><td>0.150</td><td>0.226</td><td>0.529</td><td>0.487</td><td>0.280</td><td>0.321</td><td>0.268</td><td>0.307</td><td>0.441</td><td>0.464</td><td>0.193</td><td>0.271</td><td>0.379</td><td>0.416</td><td>$\underline{0.158}$</td><td>0.244</td><td>0.286</td><td>0.358</td><td>0.201</td><td>0.276</td><td>0.814</td><td>0.659</td></tr><tr><td colspan="3">${1}^{\text{st }}$ Count13</td><td>13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>7</td><td>7</td><td>0</td><td>0</td><td>│0</td><td>0</td><td>0</td><td>0</td></tr></table>

<table><tbody><tr><td colspan="2">模型</td><td colspan="2">iTransformer（我们的方法）</td><td colspan="2">RLinear（2023年）</td><td colspan="2">PatchTST（2023年）</td><td colspan="2">Crossformer（2023年）</td><td colspan="2">TiDE（2023年）</td><td colspan="2">时间网络（TimesNet，2023）</td><td colspan="2">深度线性模型（DLinear，2023）</td><td colspan="2">时空卷积网络（SCINet，2022a）</td><td colspan="2">联邦变换器（FEDformer，2022）</td><td colspan="2">平稳模型（Stationary，2022b）</td><td colspan="2">自动变换器（Autoformer，2021）</td></tr><tr><td></td><td>指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td></td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td rowspan="5">交通数据集PEMS03</td><td>12</td><td>0.071</td><td>0.174</td><td></td><td>0.126 0.236</td><td>0.099</td><td>0.216</td><td>0.090</td><td>0.203</td><td>0.178</td><td>0.305</td><td>0.085</td><td>0.192</td><td>0.122</td><td>0.243</td><td>0.066</td><td>0.172</td><td>0.126</td><td>0.251</td><td>0.081</td><td>0.188</td><td>0.272</td><td>0.385</td></tr><tr><td>24</td><td>$\underline{0.093}$</td><td>0.201</td><td>0.246</td><td>0.334</td><td>0.142</td><td>0.259</td><td>0.121</td><td>0.240</td><td>0.257</td><td>0.371</td><td>0.118</td><td>0.223</td><td>0.201</td><td>0.317</td><td>0.085</td><td>0.198</td><td>0.149</td><td>0.275</td><td>0.105</td><td>0.214</td><td>0.334</td><td>0.440</td></tr><tr><td>48</td><td>0.125</td><td>0.236</td><td>0.551</td><td>0.529</td><td>0.211</td><td>0.319</td><td>0.202</td><td>0.317</td><td>0.379</td><td>0.463</td><td>0.155</td><td>0.260</td><td>0.333</td><td>0.425</td><td>$\underline{0.127}$</td><td>0.238</td><td>0.227</td><td>0.348</td><td>0.154</td><td>0.257</td><td>1.032</td><td>0.782</td></tr><tr><td>96</td><td>0.164</td><td>0.275</td><td>1.057</td><td>0.787</td><td>0.269</td><td>0.370</td><td>0.262</td><td>0.367</td><td>0.490</td><td>0.539</td><td>0.228</td><td>0.317</td><td>0.457</td><td>0.515</td><td>$\underline{0.178}$</td><td>$\underline{0.287}$</td><td>0.348</td><td>0.434</td><td>0.247</td><td>0.336</td><td>1.031</td><td>0.796</td></tr><tr><td>平均值</td><td>0.113</td><td>0.221</td><td>0.495</td><td>0.472</td><td>0.180</td><td>0.291</td><td>0.169</td><td>0.281</td><td>0.326</td><td>0.419</td><td>0.147</td><td>0.248</td><td>0.278</td><td>0.375</td><td>$\underline{0.114}$</td><td>0.224</td><td>0.213</td><td>0.327</td><td>0.147</td><td>0.249</td><td>0.667</td><td>0.601</td></tr><tr><td rowspan="5">佩姆斯04（PEMS04）</td><td>12</td><td>$\underline{0.078}$</td><td>$\underline{0.183}$</td><td>0.138</td><td>0.252</td><td>0.105</td><td>0.224</td><td>0.098</td><td>0.218</td><td>0.219</td><td>0.340</td><td>0.087</td><td>0.195</td><td>0.148</td><td>0.272</td><td>0.073</td><td>0.177</td><td>0.138</td><td>0.262</td><td>0.088</td><td>0.196</td><td>0.424</td><td>0.491</td></tr><tr><td>24</td><td>0.095</td><td>$\underline{0.205}$</td><td>0.258</td><td>0.348</td><td>0.153</td><td>0.275</td><td>0.131</td><td>0.256</td><td>0.292</td><td>0.398</td><td>0.103</td><td>0.215</td><td>0.224</td><td>0.340</td><td>0.084</td><td>0.193</td><td>0.177</td><td>0.293</td><td>0.104</td><td>0.216</td><td>0.459</td><td>0.509</td></tr><tr><td>48</td><td>$\underline{0.120}$</td><td>0.233</td><td>0.572</td><td>0.544</td><td>0.229</td><td>0.339</td><td>0.205</td><td>0.326</td><td>0.409</td><td>0.478</td><td>0.136</td><td>0.250</td><td>0.355</td><td>0.437</td><td>0.099</td><td>0.211</td><td>0.270</td><td>0.368</td><td>0.137</td><td>0.251</td><td>0.646</td><td>0.610</td></tr><tr><td>96</td><td>$\underline{0.150}$</td><td>$\underline{0.262}$</td><td>1.137</td><td>0.820</td><td>0.291</td><td>0.389</td><td>0.402</td><td>0.457</td><td>0.492</td><td>0.532</td><td>0.190</td><td>0.303</td><td>0.452</td><td>0.504</td><td>0.114</td><td>0.227</td><td>0.341</td><td>0.427</td><td>0.186</td><td>0.297</td><td>0.912</td><td>0.748</td></tr><tr><td>平均值</td><td>0.111</td><td>$\underline{0.221}$</td><td>0.526</td><td>0.491</td><td>0.195</td><td>0.307</td><td>0.209</td><td>0.314</td><td>0.353</td><td>0.437</td><td>0.129</td><td>0.241</td><td>0.295</td><td>0.388</td><td>0.092</td><td>0.202</td><td>0.231</td><td>0.337</td><td>0.127</td><td>0.240</td><td>0.610</td><td>0.590</td></tr><tr><td rowspan="5">佩姆斯07（PEMS07）</td><td>12</td><td>0.067</td><td>0.165</td><td>0.118</td><td>0.235</td><td>0.095</td><td>0.207</td><td>0.094</td><td>0.200</td><td>0.173</td><td>0.304</td><td>0.082</td><td>0.181</td><td>0.115</td><td>0.242</td><td>0.068</td><td>0.171</td><td>0.109</td><td>0.225</td><td>0.083</td><td>0.185</td><td>0.199</td><td>0.336</td></tr><tr><td>24</td><td>0.088</td><td>0.190</td><td>0.242</td><td>0.341</td><td>0.150</td><td>0.262</td><td>0.139</td><td>0.247</td><td>0.271</td><td>0.383</td><td>0.101</td><td>0.204</td><td>0.210</td><td>0.329</td><td>0.119</td><td>0.225</td><td>0.125</td><td>0.244</td><td>0.102</td><td>0.207</td><td>0.323</td><td>0.420</td></tr><tr><td>48</td><td>0.110</td><td>0.215</td><td>0.562</td><td>0.541</td><td>0.253</td><td>0.340</td><td>0.311</td><td>0.369</td><td>0.446</td><td>0.495</td><td>0.134</td><td>0.238</td><td>0.398</td><td>0.458</td><td>$\underline{0.149}$</td><td>0.237</td><td>0.165</td><td>0.288</td><td>0.136</td><td>0.240</td><td>0.390</td><td>0.470</td></tr><tr><td>96</td><td>0.139</td><td>0.245</td><td>1.096</td><td>0.795</td><td>0.346</td><td>0.404</td><td>0.396</td><td>0.442</td><td>0.628</td><td>0.577</td><td>0.181</td><td>0.279</td><td>0.594</td><td>0.553</td><td>$\underline{0.141}$</td><td>0.234</td><td>0.262</td><td>0.376</td><td>0.187</td><td>0.287</td><td>0.554</td><td>0.578</td></tr><tr><td>平均值</td><td>0.101</td><td>0.204</td><td>0.504</td><td>0.478</td><td>0.211</td><td>0.303</td><td>0.235</td><td>0.315</td><td>0.380</td><td>0.440</td><td>0.124</td><td>0.225</td><td>0.329</td><td>0.395</td><td>0.119</td><td>$\underline{0.234}$</td><td>0.165</td><td>0.283</td><td>0.127</td><td>0.230</td><td>0.367</td><td>0.451</td></tr><tr><td rowspan="5">佩姆斯08（PEMS08）</td><td>12</td><td>0.079</td><td>0.182</td><td>0.133</td><td>0.247</td><td>0.168</td><td>0.232</td><td>0.165</td><td>0.214</td><td>0.227</td><td>0.343</td><td>0.112</td><td>0.212</td><td>0.154</td><td>0.276</td><td>$\underline{0.087}$</td><td>$\underline{0.184}$</td><td>0.173</td><td>0.273</td><td>0.109</td><td>0.207</td><td>0.436</td><td>0.485</td></tr><tr><td>24</td><td>0.115</td><td>0.219</td><td>0.249</td><td>0.343</td><td>0.224</td><td>0.281</td><td>0.215</td><td>0.260</td><td>0.318</td><td>0.409</td><td>0.141</td><td>0.238</td><td>0.248</td><td>0.353</td><td>$\underline{0.122}$</td><td>0.221</td><td>0.210</td><td>0.301</td><td>0.140</td><td>0.236</td><td>0.467</td><td>0.502</td></tr><tr><td>48</td><td>0.186</td><td>0.235</td><td>0.569</td><td>0.544</td><td>0.321</td><td>0.354</td><td>0.315</td><td>0.355</td><td>0.497</td><td>0.510</td><td>0.198</td><td>0.283</td><td>0.440</td><td>0.470</td><td>0.189</td><td>0.270</td><td>0.320</td><td>0.394</td><td>0.211</td><td>0.294</td><td>0.966</td><td>0.733</td></tr><tr><td>96</td><td>0.221</td><td>0.267</td><td>1.166</td><td>0.814</td><td>0.408</td><td>0.417</td><td>0.377</td><td>0.397</td><td>0.721</td><td>0.592</td><td>0.320</td><td>0.351</td><td>0.674</td><td>0.565</td><td>$\underline{0.236}$</td><td>$\underline{0.300}$</td><td>0.442</td><td>0.465</td><td>0.345</td><td>0.367</td><td>1.385</td><td>0.915</td></tr><tr><td>平均值</td><td>0.150</td><td>0.226</td><td>0.529</td><td>0.487</td><td>0.280</td><td>0.321</td><td>0.268</td><td>0.307</td><td>0.441</td><td>0.464</td><td>0.193</td><td>0.271</td><td>0.379</td><td>0.416</td><td>$\underline{0.158}$</td><td>0.244</td><td>0.286</td><td>0.358</td><td>0.201</td><td>0.276</td><td>0.814</td><td>0.659</td></tr><tr><td colspan="3">[乳胶代码0] 计数13（Count13）</td><td>13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>7</td><td>7</td><td>0</td><td>0</td><td>│0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table>

<!-- Media -->

Table 10: Full results of the long-term forecasting task. We compare extensive competitive models under different prediction lengths following the setting of TimesNet (2023). The input sequence length is set to 96 for all baselines. Avg means the average results from all four prediction lengths.

表10：长期预测任务的完整结果。我们按照TimesNet（2023年）的设置，在不同预测时长下对大量竞争模型进行了比较。所有基线模型的输入序列长度均设置为96。Avg表示四个预测时长的平均结果。

<!-- Media -->

<table><tr><td colspan="2">Models</td><td colspan="2">iTransformer (Ours)</td><td>RLinear (2023)</td><td colspan="2">PatchTST (2023)</td><td colspan="2">Crossformer (2023)</td><td colspan="2">TiDE (2023)</td><td colspan="2">TimesNet (2023)</td><td colspan="2">DLinear (2023)</td><td>SCINet FEDformer (2022a)</td><td colspan="2">Stationary (2022b)</td><td>Autoformer (2021)</td></tr><tr><td>(2022) MAE MSE 0.419 0.505 0.441 0.553 0.459 0.621 0.490 0.671 0.452 0.588 0.287 ).255 0.328 0.281 0.366 0.339 0.415 0.433 0.349 0.327 0.419 0.449 0.448 0.500 0.465 0.521 0.507 0.514 0.460 0.496 0.397 0.346 0.439 0.456 0.487 0.482 0.474 0.515 0.449 0.450 0.308 0.201 0.315 0.222 0.329 0.231 0.355 0.254 0.327 0.227 0.278 ).197 0.315 0.300 0.427 0.509 $\underline{0.695}$ 1.447 0.429 0.613 0.366 0.613 0.373 0.616 0.383 0.622 0.382 0.660 0.376 0.628 0.296 0.266 0.336 0.307 0.380 0.359 0.428 0.419 0.360 0.338 0.342 0.884 0.285 0.380 0.834 0.376 0.941 0.427 0.882 0.381 0.885 400</td><td>Metric</td><td>MSE</td><td>MAE</td><td>MSEMAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td/><td>MSEMAE</td><td/><td>MSEMAE</td><td>MSEMAEMSE</td><td/><td>MSEMAE</td><td>MAE</td></tr><tr><td rowspan="5"/><td>96</td><td>0.334</td><td>0.368</td><td>0.3550.376</td><td>0.329</td><td>0.367</td><td>0.404</td><td>0.426</td><td>0.364</td><td>0.387</td><td/><td>0.3380.375</td><td/><td>0.3450.372</td><td>0.4180.4380.379</td><td/><td>0.3860.398</td><td>0.475</td></tr><tr><td>192</td><td>0.377</td><td>0.391</td><td>0.3910.392</td><td>0.367</td><td>0.385</td><td>0.450</td><td>0.451</td><td>0.398</td><td>0.404</td><td/><td>0.3740.387</td><td/><td>0.3800.389</td><td>0.4390.4500.426</td><td/><td>0.4590.444</td><td/></tr><tr><td>336</td><td>0.426</td><td>0.420</td><td>0.4.4240.415</td><td>0.399</td><td>0.410</td><td>0.532</td><td>0.515</td><td>0.428</td><td>0.425</td><td/><td>$\underline{0.410}$$\underline{0.411}$</td><td/><td>0.4130.413</td><td>0.4900.4850.445</td><td/><td>0.4950.464</td><td>0.537</td></tr><tr><td>720</td><td>0.491</td><td>0.459</td><td>0.44870.450</td><td>0.454</td><td>0.439</td><td>0.666</td><td>0.589</td><td>0.487</td><td>0.461</td><td/><td>0.478$\underline{0.450}$</td><td/><td>$\underline{0.474}$0.453</td><td>0.5950.5500.543</td><td/><td>0.5850.516</td><td>0.561</td></tr><tr><td>Avg</td><td>0.407</td><td>0.410</td><td>0.407</td><td>0.387</td><td>0.400</td><td>0.513</td><td>0.496</td><td>0.419</td><td>0.419</td><td/><td>0.4000.406</td><td/><td>0.4030.407</td><td>0.485).4810.448</td><td/><td>0.4810.456</td><td>).517</td></tr><tr><td rowspan="5">2023-03</td><td>96</td><td>0.180</td><td>0.264</td><td>0.1820.265</td><td>0.175</td><td>0.259</td><td>0.287</td><td>0.366</td><td>0.207</td><td>0.305</td><td/><td>0.1870.267</td><td/><td>0.1930.292</td><td>0.2860.3770.203</td><td>0.1</td><td>.1920.274</td><td>.339</td></tr><tr><td>192</td><td>0.250</td><td>0.309</td><td>).2460.304</td><td>0.241</td><td>0.302</td><td>0.414</td><td>0.492</td><td>0.290</td><td>0.364</td><td/><td>0.2490.309</td><td/><td>0.2840.362</td><td>0.3990.4450.269</td><td/><td>0.2800.339</td><td>9.340</td></tr><tr><td>336</td><td>0.311</td><td>0.348</td><td>0.3070.342</td><td>0.305</td><td>0.343</td><td>0.597</td><td>0.542</td><td>0.377</td><td>0.422</td><td/><td>0.3210.351</td><td/><td>0.3690.427</td><td>0.6370.5910.325</td><td/><td>0.3340.361</td><td>372</td></tr><tr><td>720</td><td>0.412</td><td>0.407</td><td>0.4070.398</td><td>0.402</td><td>0.400</td><td>1.730</td><td>1.042</td><td>0.558</td><td>0.524</td><td/><td>0.4080.403</td><td/><td>0.5540.522</td><td>0.9600.7350.421</td><td/><td>0.4170.413</td><td/></tr><tr><td>Avg</td><td>0.288</td><td>0.332</td><td>0.286$\underline{0.327}$</td><td>0.281</td><td>0.326</td><td>0.757</td><td>0.610</td><td>0.358</td><td>0.404</td><td/><td>0.2910.333</td><td/><td>0.350</td><td>0.5710.5370.305</td><td/><td>0.3060.347</td><td>3.371</td></tr><tr><td rowspan="5"/><td>96</td><td>0.386</td><td>0.405</td><td>0.3860.395</td><td>0.414</td><td>0.419</td><td>0.423</td><td>0.448</td><td>0.479</td><td>0.464</td><td/><td>0.3840.402</td><td/><td>0.3860.400</td><td>0.6540.5990.376</td><td/><td>0.5130.491</td><td>.459</td></tr><tr><td>192</td><td>0.441</td><td>0.436</td><td>0.4370.424</td><td>0.460</td><td>0.445</td><td>0.471</td><td>0.474</td><td>0.525</td><td>0.492</td><td/><td>0.4360.429</td><td/><td>0.4370.432</td><td>0.7190.6310.420</td><td/><td>0.5340.504</td><td>482</td></tr><tr><td>336</td><td>0.487</td><td>0.458</td><td>0.4790.446</td><td>0.501</td><td>0.466</td><td>0.570</td><td>0.546</td><td>9.565</td><td>0.515</td><td/><td>0.4910.469</td><td/><td>0.4810.459</td><td>0.7780.6590.459</td><td/><td>0.5880.535</td><td>496</td></tr><tr><td>720</td><td>0.503</td><td>0.491</td><td>0.4810.470</td><td>0.500</td><td>0.488</td><td>0.653</td><td>0.621</td><td>0.594</td><td>0.558</td><td/><td>0.5210.500</td><td/><td>0.5190.516</td><td>0.8360.6990.506</td><td/><td>0.6430.616</td><td>0.512</td></tr><tr><td>Avg</td><td>0.454</td><td>$\underline{0.447}$</td><td>0.4460.434</td><td>0.469</td><td>0.454</td><td>0.529</td><td>0.522</td><td>0.541</td><td>0.507</td><td/><td>0.4580.450</td><td/><td>0.4560.452</td><td>0.7470.6470.440</td><td/><td>0.5700.537</td><td>487</td></tr><tr><td rowspan="5"/><td>96</td><td>0.297</td><td>0.349</td><td>0.2880.338</td><td>0.302</td><td>0.348</td><td>0.745</td><td>0.584</td><td>0.400</td><td>0.440</td><td/><td>0.3400.374</td><td/><td>0.3330.387</td><td>0.7070.6210.358</td><td/><td>0.4760.458</td><td>.388</td></tr><tr><td>192</td><td>0.380</td><td>0.400</td><td>0.3740.390</td><td>0.388</td><td>0.400</td><td>0.877</td><td>0.656</td><td>.528</td><td>0.509</td><td/><td>0.4020.414</td><td/><td>0.4770.476</td><td>0.8600.6890.429</td><td/><td>0.5120.493</td><td/></tr><tr><td>336</td><td>0.428</td><td>0.432</td><td>0.4150.426</td><td>0.426</td><td>0.433</td><td>1.043</td><td>0.731</td><td>0.643</td><td>0.571</td><td/><td>0.4520.452</td><td/><td>0.5940.541</td><td>1.0000.7440.496</td><td/><td>0.5520.551</td><td>4.486</td></tr><tr><td>720</td><td>0.427</td><td>$\underline{0.445}$</td><td>0.4200.440</td><td>0.431</td><td>0.446</td><td>1.104</td><td>0.763</td><td>0.874</td><td>0.679</td><td/><td>0.4620.468</td><td/><td>0.8310.657</td><td>1.2490.8380.463</td><td/><td>0.5620.560</td><td>5.511</td></tr><tr><td>Avg</td><td>$\underline{0.383}$</td><td>$\underline{0.407}$</td><td>9.3740.398</td><td>0.387</td><td>0.407</td><td>0.942</td><td>0.684</td><td>0.611</td><td>0.550</td><td/><td>0.4140.427</td><td/><td>0.5590.515</td><td>0.9540.7230.437</td><td/><td>0.5260.516</td><td>0.459</td></tr><tr><td rowspan="5">日出</td><td>96</td><td>0.148</td><td>0.240</td><td>0.2010.281</td><td>0.181</td><td>0.270</td><td>0.219</td><td>0.314</td><td>0.237</td><td>0.329</td><td/><td>0.1680.272</td><td/><td>0.1970.282</td><td>0.2470.3450.193</td><td/><td>0.1690.273</td><td>0.317</td></tr><tr><td>192</td><td>0.162</td><td>0.253</td><td>).2010.283</td><td>0.188</td><td>0.274</td><td>0.231</td><td>0.322</td><td>2,236</td><td>0.330</td><td/><td>0.1840.289</td><td/><td>0.1960.285</td><td>0.2570.3550.201</td><td/><td>0.1820.286</td><td>).334</td></tr><tr><td>336</td><td>0.178</td><td>0.269</td><td>0.2150.298</td><td>0.204</td><td>0.293</td><td>0.246</td><td>0.337</td><td>2,249</td><td>0.344</td><td/><td>0.1980.300</td><td/><td>0.2090.301</td><td>0.2690.3690.214</td><td/><td>0.2000.304</td><td>9.338</td></tr><tr><td>720</td><td>$\underline{0.225}$</td><td>0.317</td><td>0.2570.331</td><td>0.246</td><td>0.324</td><td>0.280</td><td>0.363</td><td>0.284</td><td>0.373</td><td/><td>0.2200.320</td><td/><td>0.2450.333</td><td>0.2990.3900.246</td><td/><td>0.2220.321</td><td>0.361</td></tr><tr><td>Avg</td><td>0.178</td><td>0.270</td><td>0.2190.298</td><td>0.205</td><td>0.290</td><td>0.244</td><td>0.334</td><td>0.251</td><td>0.344</td><td/><td>$\underline{0.192}$0.295</td><td/><td>0.2120.300</td><td>0.2680.3650.214</td><td/><td>0.1930.296</td><td>0.338</td></tr><tr><td rowspan="5"/><td>96</td><td>0.086</td><td>0.206</td><td>0.00930.217</td><td>0.088</td><td>0.205</td><td>0.256</td><td>0.367</td><td>0.094</td><td>0.218</td><td/><td>0.1070.234</td><td/><td>0.0880.218</td><td>0.2670.3960.148</td><td/><td>0.1110.237</td><td>0.323</td></tr><tr><td>192</td><td>0.177</td><td>0.299</td><td>0.1840.307</td><td>0.176</td><td>0.299</td><td>0.470</td><td>0.509</td><td>0.184</td><td>0.307</td><td/><td>0.2260.344</td><td/><td>0.1760.315</td><td>0.3510.4590.271</td><td/><td>0.2190.335</td><td>0.369</td></tr><tr><td>336</td><td>0.331</td><td>0.417</td><td>0.3510.432</td><td>0.301</td><td>0.397</td><td>1.268</td><td>0.883</td><td>0.349</td><td>0.431</td><td/><td>0.3670.448</td><td/><td>0.3130.427</td><td>1.3240.8530.460</td><td/><td>0.4210.476</td><td>0.524</td></tr><tr><td>720</td><td>$\underline{0.847}$</td><td>0.691</td><td>).8860.714</td><td>0.901</td><td>0.714</td><td>1.767</td><td>1.068</td><td>0.852</td><td>0.698</td><td/><td>0.9640.746</td><td/><td>0.8390.695</td><td>1.0580.7971.195</td><td/><td>1.0920.769</td><td>0.941</td></tr><tr><td>Avg</td><td>0.360</td><td>0.403</td><td>.3780.417</td><td>0.367</td><td>0.404</td><td>0.940</td><td>0.707</td><td>0.370</td><td>0.413</td><td/><td>0.4160.443</td><td/><td>0.3540.414</td><td>0.750).6260.519</td><td/><td>0.4610.454</td><td>0.539</td></tr><tr><td rowspan="5"/><td>96</td><td>0.395</td><td>0.268</td><td>).6490.389</td><td>0.462</td><td>0.295</td><td>0.522</td><td>0.290</td><td>0.805</td><td>0.493</td><td/><td>0.5930.321</td><td/><td>0.6500.396</td><td>0.7880.4990.587</td><td/><td>0.6120.338</td><td>3.388</td></tr><tr><td>192</td><td>0.417</td><td>0.276</td><td>0.6010.366</td><td>0.466</td><td>0.296</td><td>0.530</td><td>0.293</td><td>).756</td><td>0.474</td><td/><td>0.6170.336</td><td/><td>0.5980.370</td><td>0.7890.5050.604</td><td/><td>0.6130.340</td><td>).382</td></tr><tr><td>336</td><td>0.433</td><td>0.283</td><td>0.69,6090.369</td><td>0.482</td><td>0.304</td><td>0.558</td><td>0.305</td><td>).762</td><td>0.477</td><td/><td>0.6290.336</td><td/><td>0.6050.373</td><td>0.7970.5080.621</td><td/><td>0.6180.328</td><td>0.337</td></tr><tr><td>720</td><td>0.467</td><td>0.302</td><td>0.6470.387</td><td>0.514</td><td>$\underline{0.322}$</td><td>0.589</td><td>0.328</td><td>0.719</td><td>0.449</td><td/><td>0.6400.350</td><td/><td>0.6450.394</td><td>0.8410.5230.626</td><td/><td>0.6530.355</td><td>0.408</td></tr><tr><td>Avg</td><td>0.428</td><td>0.282</td><td>0.6260.378</td><td>0.481</td><td>0.304</td><td>0.550</td><td>0.304</td><td>0.760</td><td>0.473</td><td/><td>0.6200.336</td><td/><td>0.6250.383</td><td>0.610</td><td/><td>0.6240.340</td><td>).379</td></tr><tr><td rowspan="5"/><td>96</td><td>0.174</td><td>0.214</td><td>0.11920.232</td><td>0.177</td><td>0.218</td><td>0.158</td><td>0.230</td><td>0.202</td><td>0.261</td><td/><td>0.1720.220</td><td/><td>0.1960.255</td><td>0.2210.3060.217</td><td>0.1</td><td>1730.223</td><td>).336</td></tr><tr><td>192</td><td>0.221</td><td>0.254</td><td>0.22400.271</td><td>0.225</td><td>0.259</td><td>0.206</td><td>0.277</td><td>0.242</td><td>0.298</td><td/><td>0.2190.261</td><td/><td>0.2370.296</td><td>0.2610.3400.276</td><td/><td>).2450.285</td><td>).367</td></tr><tr><td>336</td><td>0.278</td><td>0.296</td><td>0.1.2920.307</td><td>0.278</td><td>0.297</td><td>0.272</td><td>0.335</td><td>0.287</td><td>0.335</td><td/><td>0.2800.306</td><td/><td>0.2830.335</td><td>0.3090.3780.339</td><td/><td>0.3210.338</td><td>.395</td></tr><tr><td>720</td><td>0.358</td><td>0.347</td><td>0.3640.353</td><td>0.354</td><td>$\underline{0.348}$</td><td>0.398</td><td>0.418</td><td>$\underline{0.351}$</td><td>0.386</td><td/><td>0.3650.359</td><td/><td>0.3450.381</td><td>0.3770.4270.403</td><td/><td>0.4140.410</td><td>0.428</td></tr><tr><td>Avg</td><td>0.258</td><td>0.278</td><td>3.2720.291</td><td>0.259</td><td>$\underline{0.281}$</td><td>0.259</td><td>0.315</td><td>0.271</td><td>0.320</td><td/><td>0.2590.287</td><td/><td>0.2650.317</td><td>0.2920.3630.309</td><td/><td>0.2880.314</td><td>0.382</td></tr><tr><td rowspan="5"/><td>96</td><td>0.203</td><td>0.237</td><td>0.3220.339</td><td>0.234</td><td>0.286</td><td>0.310</td><td>0.331</td><td>0.312</td><td>0.399</td><td/><td>0.2500.292</td><td/><td>0.2900.378</td><td>0.2370.3440.242</td><td/><td>0.2150.249</td><td>0.711</td></tr><tr><td>192</td><td>0.233</td><td>0.261</td><td>0.3590.356</td><td>0.267</td><td>0.310</td><td>0.734</td><td>0.725</td><td>0.3339</td><td>0.416</td><td/><td>0.2960.318</td><td/><td>0.3200.398</td><td>0.2800.3800.</td><td/><td>0.254).272</td><td>0.692</td></tr><tr><td>336</td><td>0.248</td><td>0.273</td><td>0.3970.369</td><td>0.290</td><td>0.315</td><td>0.750</td><td>0.735</td><td>368</td><td>0.430</td><td/><td>0.3190.330</td><td/><td>0.3530.415</td><td>0.3040.3890.282</td><td/><td>0.2900.296</td><td>0.723</td></tr><tr><td>720</td><td>0.249</td><td>0.275</td><td>0.3970.356</td><td>0.289</td><td>0.317</td><td>0.769</td><td>0.765</td><td>0.370</td><td>0.425</td><td/><td>0.3380.337</td><td/><td>0.3560.413</td><td>0.3080.3880.357</td><td/><td>0.2850.295</td><td>0.717</td></tr><tr><td>Avg</td><td>0.233</td><td>0.262</td><td>0.3690.356</td><td>0.270</td><td>$\underline{0.307}$</td><td>0.641</td><td>0.639</td><td>0.347</td><td>0.417</td><td/><td>0.3010.319</td><td/><td/><td>0.2820.291</td><td/><td>0.2610.381</td><td>0.711</td></tr><tr><td>${1}^{\text{st }}$</td><td>Count</td><td>16</td><td>22</td><td>612</td><td>$\underline{12}$</td><td>11</td><td>│3</td><td>0</td><td>0</td><td>0</td><td/><td>10</td><td/><td>30</td><td>00</td><td/><td>00</td><td>0</td></tr></table>

<table><tbody><tr><td colspan="2">模型</td><td colspan="2">iTransformer（我们的方法）</td><td>RLinear（2023年）</td><td colspan="2">PatchTST（2023年）</td><td colspan="2">Crossformer（2023年）</td><td colspan="2">TiDE（2023年）</td><td colspan="2">时间网络（TimesNet，2023）</td><td colspan="2">深度线性模型（DLinear，2023）</td><td>自校正网络联邦变换器（SCINet FEDformer，2022a）</td><td colspan="2">平稳模型（Stationary，2022b）</td><td>自动变换器（Autoformer，2021）</td></tr><tr><td>(2022年) 掩码自编码器（MAE）均方误差（MSE） 0.419 0.505 0.441 0.553 0.459 0.621 0.490 0.671 0.452 0.588 0.287 ).255 0.328 0.281 0.366 0.339 0.415 0.433 0.349 0.327 0.419 0.449 0.448 0.500 0.465 0.521 0.507 0.514 0.460 0.496 0.397 0.346 0.439 0.456 0.487 0.482 0.474 0.515 0.449 0.450 0.308 0.201 0.315 0.222 0.329 0.231 0.355 0.254 0.327 0.227 0.278 ).197 0.315 0.300 0.427 0.509 $\underline{0.695}$ 1.447 0.429 0.613 0.366 0.613 0.373 0.616 0.383 0.622 0.382 0.660 0.376 0.628 0.296 0.266 0.336 0.307 0.380 0.359 0.428 0.419 0.360 0.338 0.342 0.884 0.285 0.380 0.834 0.376 0.941 0.427 0.882 0.381 0.885 400</td><td>指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差与平均绝对误差（MSEMAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td></td><td>均方误差与平均绝对误差（MSEMAE）</td><td></td><td>均方误差与平均绝对误差（MSEMAE）</td><td>均方误差与平均绝对误差及均方误差（MSEMAEMSE）</td><td></td><td>均方误差与平均绝对误差（MSEMAE）</td><td>平均绝对误差（MAE）</td></tr><tr><td rowspan="5"></td><td>96</td><td>0.334</td><td>0.368</td><td>0.3550.376</td><td>0.329</td><td>0.367</td><td>0.404</td><td>0.426</td><td>0.364</td><td>0.387</td><td></td><td>0.3380.375</td><td></td><td>0.3450.372</td><td>0.4180.4380.379</td><td></td><td>0.3860.398</td><td>0.475</td></tr><tr><td>192</td><td>0.377</td><td>0.391</td><td>0.3910.392</td><td>0.367</td><td>0.385</td><td>0.450</td><td>0.451</td><td>0.398</td><td>0.404</td><td></td><td>0.3740.387</td><td></td><td>0.3800.389</td><td>0.4390.4500.426</td><td></td><td>0.4590.444</td><td></td></tr><tr><td>336</td><td>0.426</td><td>0.420</td><td>0.4.4240.415</td><td>0.399</td><td>0.410</td><td>0.532</td><td>0.515</td><td>0.428</td><td>0.425</td><td></td><td>$\underline{0.410}$$\underline{0.411}$</td><td></td><td>0.4130.413</td><td>0.4900.4850.445</td><td></td><td>0.4950.464</td><td>0.537</td></tr><tr><td>720</td><td>0.491</td><td>0.459</td><td>0.44870.450</td><td>0.454</td><td>0.439</td><td>0.666</td><td>0.589</td><td>0.487</td><td>0.461</td><td></td><td>0.478$\underline{0.450}$</td><td></td><td>$\underline{0.474}$0.453</td><td>0.5950.5500.543</td><td></td><td>0.5850.516</td><td>0.561</td></tr><tr><td>平均值</td><td>0.407</td><td>0.410</td><td>0.407</td><td>0.387</td><td>0.400</td><td>0.513</td><td>0.496</td><td>0.419</td><td>0.419</td><td></td><td>0.4000.406</td><td></td><td>0.4030.407</td><td>0.485).4810.448</td><td></td><td>0.4810.456</td><td>).517</td></tr><tr><td rowspan="5">2023-03</td><td>96</td><td>0.180</td><td>0.264</td><td>0.1820.265</td><td>0.175</td><td>0.259</td><td>0.287</td><td>0.366</td><td>0.207</td><td>0.305</td><td></td><td>0.1870.267</td><td></td><td>0.1930.292</td><td>0.2860.3770.203</td><td>0.1</td><td>.1920.274</td><td>.339</td></tr><tr><td>192</td><td>0.250</td><td>0.309</td><td>).2460.304</td><td>0.241</td><td>0.302</td><td>0.414</td><td>0.492</td><td>0.290</td><td>0.364</td><td></td><td>0.2490.309</td><td></td><td>0.2840.362</td><td>0.3990.4450.269</td><td></td><td>0.2800.339</td><td>9.340</td></tr><tr><td>336</td><td>0.311</td><td>0.348</td><td>0.3070.342</td><td>0.305</td><td>0.343</td><td>0.597</td><td>0.542</td><td>0.377</td><td>0.422</td><td></td><td>0.3210.351</td><td></td><td>0.3690.427</td><td>0.6370.5910.325</td><td></td><td>0.3340.361</td><td>372</td></tr><tr><td>720</td><td>0.412</td><td>0.407</td><td>0.4070.398</td><td>0.402</td><td>0.400</td><td>1.730</td><td>1.042</td><td>0.558</td><td>0.524</td><td></td><td>0.4080.403</td><td></td><td>0.5540.522</td><td>0.9600.7350.421</td><td></td><td>0.4170.413</td><td></td></tr><tr><td>平均值</td><td>0.288</td><td>0.332</td><td>0.286$\underline{0.327}$</td><td>0.281</td><td>0.326</td><td>0.757</td><td>0.610</td><td>0.358</td><td>0.404</td><td></td><td>0.2910.333</td><td></td><td>0.350</td><td>0.5710.5370.305</td><td></td><td>0.3060.347</td><td>3.371</td></tr><tr><td rowspan="5"></td><td>96</td><td>0.386</td><td>0.405</td><td>0.3860.395</td><td>0.414</td><td>0.419</td><td>0.423</td><td>0.448</td><td>0.479</td><td>0.464</td><td></td><td>0.3840.402</td><td></td><td>0.3860.400</td><td>0.6540.5990.376</td><td></td><td>0.5130.491</td><td>.459</td></tr><tr><td>192</td><td>0.441</td><td>0.436</td><td>0.4370.424</td><td>0.460</td><td>0.445</td><td>0.471</td><td>0.474</td><td>0.525</td><td>0.492</td><td></td><td>0.4360.429</td><td></td><td>0.4370.432</td><td>0.7190.6310.420</td><td></td><td>0.5340.504</td><td>482</td></tr><tr><td>336</td><td>0.487</td><td>0.458</td><td>0.4790.446</td><td>0.501</td><td>0.466</td><td>0.570</td><td>0.546</td><td>9.565</td><td>0.515</td><td></td><td>0.4910.469</td><td></td><td>0.4810.459</td><td>0.7780.6590.459</td><td></td><td>0.5880.535</td><td>496</td></tr><tr><td>720</td><td>0.503</td><td>0.491</td><td>0.4810.470</td><td>0.500</td><td>0.488</td><td>0.653</td><td>0.621</td><td>0.594</td><td>0.558</td><td></td><td>0.5210.500</td><td></td><td>0.5190.516</td><td>0.8360.6990.506</td><td></td><td>0.6430.616</td><td>0.512</td></tr><tr><td>平均值</td><td>0.454</td><td>$\underline{0.447}$</td><td>0.4460.434</td><td>0.469</td><td>0.454</td><td>0.529</td><td>0.522</td><td>0.541</td><td>0.507</td><td></td><td>0.4580.450</td><td></td><td>0.4560.452</td><td>0.7470.6470.440</td><td></td><td>0.5700.537</td><td>487</td></tr><tr><td rowspan="5"></td><td>96</td><td>0.297</td><td>0.349</td><td>0.2880.338</td><td>0.302</td><td>0.348</td><td>0.745</td><td>0.584</td><td>0.400</td><td>0.440</td><td></td><td>0.3400.374</td><td></td><td>0.3330.387</td><td>0.7070.6210.358</td><td></td><td>0.4760.458</td><td>.388</td></tr><tr><td>192</td><td>0.380</td><td>0.400</td><td>0.3740.390</td><td>0.388</td><td>0.400</td><td>0.877</td><td>0.656</td><td>.528</td><td>0.509</td><td></td><td>0.4020.414</td><td></td><td>0.4770.476</td><td>0.8600.6890.429</td><td></td><td>0.5120.493</td><td></td></tr><tr><td>336</td><td>0.428</td><td>0.432</td><td>0.4150.426</td><td>0.426</td><td>0.433</td><td>1.043</td><td>0.731</td><td>0.643</td><td>0.571</td><td></td><td>0.4520.452</td><td></td><td>0.5940.541</td><td>1.0000.7440.496</td><td></td><td>0.5520.551</td><td>4.486</td></tr><tr><td>720</td><td>0.427</td><td>$\underline{0.445}$</td><td>0.4200.440</td><td>0.431</td><td>0.446</td><td>1.104</td><td>0.763</td><td>0.874</td><td>0.679</td><td></td><td>0.4620.468</td><td></td><td>0.8310.657</td><td>1.2490.8380.463</td><td></td><td>0.5620.560</td><td>5.511</td></tr><tr><td>平均值</td><td>$\underline{0.383}$</td><td>$\underline{0.407}$</td><td>9.3740.398</td><td>0.387</td><td>0.407</td><td>0.942</td><td>0.684</td><td>0.611</td><td>0.550</td><td></td><td>0.4140.427</td><td></td><td>0.5590.515</td><td>0.9540.7230.437</td><td></td><td>0.5260.516</td><td>0.459</td></tr><tr><td rowspan="5">日出</td><td>96</td><td>0.148</td><td>0.240</td><td>0.2010.281</td><td>0.181</td><td>0.270</td><td>0.219</td><td>0.314</td><td>0.237</td><td>0.329</td><td></td><td>0.1680.272</td><td></td><td>0.1970.282</td><td>0.2470.3450.193</td><td></td><td>0.1690.273</td><td>0.317</td></tr><tr><td>192</td><td>0.162</td><td>0.253</td><td>).2010.283</td><td>0.188</td><td>0.274</td><td>0.231</td><td>0.322</td><td>2,236</td><td>0.330</td><td></td><td>0.1840.289</td><td></td><td>0.1960.285</td><td>0.2570.3550.201</td><td></td><td>0.1820.286</td><td>).334</td></tr><tr><td>336</td><td>0.178</td><td>0.269</td><td>0.2150.298</td><td>0.204</td><td>0.293</td><td>0.246</td><td>0.337</td><td>2,249</td><td>0.344</td><td></td><td>0.1980.300</td><td></td><td>0.2090.301</td><td>0.2690.3690.214</td><td></td><td>0.2000.304</td><td>9.338</td></tr><tr><td>720</td><td>$\underline{0.225}$</td><td>0.317</td><td>0.2570.331</td><td>0.246</td><td>0.324</td><td>0.280</td><td>0.363</td><td>0.284</td><td>0.373</td><td></td><td>0.2200.320</td><td></td><td>0.2450.333</td><td>0.2990.3900.246</td><td></td><td>0.2220.321</td><td>0.361</td></tr><tr><td>平均值</td><td>0.178</td><td>0.270</td><td>0.2190.298</td><td>0.205</td><td>0.290</td><td>0.244</td><td>0.334</td><td>0.251</td><td>0.344</td><td></td><td>$\underline{0.192}$0.295</td><td></td><td>0.2120.300</td><td>0.2680.3650.214</td><td></td><td>0.1930.296</td><td>0.338</td></tr><tr><td rowspan="5"></td><td>96</td><td>0.086</td><td>0.206</td><td>0.00930.217</td><td>0.088</td><td>0.205</td><td>0.256</td><td>0.367</td><td>0.094</td><td>0.218</td><td></td><td>0.1070.234</td><td></td><td>0.0880.218</td><td>0.2670.3960.148</td><td></td><td>0.1110.237</td><td>0.323</td></tr><tr><td>192</td><td>0.177</td><td>0.299</td><td>0.1840.307</td><td>0.176</td><td>0.299</td><td>0.470</td><td>0.509</td><td>0.184</td><td>0.307</td><td></td><td>0.2260.344</td><td></td><td>0.1760.315</td><td>0.3510.4590.271</td><td></td><td>0.2190.335</td><td>0.369</td></tr><tr><td>336</td><td>0.331</td><td>0.417</td><td>0.3510.432</td><td>0.301</td><td>0.397</td><td>1.268</td><td>0.883</td><td>0.349</td><td>0.431</td><td></td><td>0.3670.448</td><td></td><td>0.3130.427</td><td>1.3240.8530.460</td><td></td><td>0.4210.476</td><td>0.524</td></tr><tr><td>720</td><td>$\underline{0.847}$</td><td>0.691</td><td>).8860.714</td><td>0.901</td><td>0.714</td><td>1.767</td><td>1.068</td><td>0.852</td><td>0.698</td><td></td><td>0.9640.746</td><td></td><td>0.8390.695</td><td>1.0580.7971.195</td><td></td><td>1.0920.769</td><td>0.941</td></tr><tr><td>平均值</td><td>0.360</td><td>0.403</td><td>.3780.417</td><td>0.367</td><td>0.404</td><td>0.940</td><td>0.707</td><td>0.370</td><td>0.413</td><td></td><td>0.4160.443</td><td></td><td>0.3540.414</td><td>0.750).6260.519</td><td></td><td>0.4610.454</td><td>0.539</td></tr><tr><td rowspan="5"></td><td>96</td><td>0.395</td><td>0.268</td><td>).6490.389</td><td>0.462</td><td>0.295</td><td>0.522</td><td>0.290</td><td>0.805</td><td>0.493</td><td></td><td>0.5930.321</td><td></td><td>0.6500.396</td><td>0.7880.4990.587</td><td></td><td>0.6120.338</td><td>3.388</td></tr><tr><td>192</td><td>0.417</td><td>0.276</td><td>0.6010.366</td><td>0.466</td><td>0.296</td><td>0.530</td><td>0.293</td><td>).756</td><td>0.474</td><td></td><td>0.6170.336</td><td></td><td>0.5980.370</td><td>0.7890.5050.604</td><td></td><td>0.6130.340</td><td>).382</td></tr><tr><td>336</td><td>0.433</td><td>0.283</td><td>0.69,6090.369</td><td>0.482</td><td>0.304</td><td>0.558</td><td>0.305</td><td>).762</td><td>0.477</td><td></td><td>0.6290.336</td><td></td><td>0.6050.373</td><td>0.7970.5080.621</td><td></td><td>0.6180.328</td><td>0.337</td></tr><tr><td>720</td><td>0.467</td><td>0.302</td><td>0.6470.387</td><td>0.514</td><td>$\underline{0.322}$</td><td>0.589</td><td>0.328</td><td>0.719</td><td>0.449</td><td></td><td>0.6400.350</td><td></td><td>0.6450.394</td><td>0.8410.5230.626</td><td></td><td>0.6530.355</td><td>0.408</td></tr><tr><td>平均值</td><td>0.428</td><td>0.282</td><td>0.6260.378</td><td>0.481</td><td>0.304</td><td>0.550</td><td>0.304</td><td>0.760</td><td>0.473</td><td></td><td>0.6200.336</td><td></td><td>0.6250.383</td><td>0.610</td><td></td><td>0.6240.340</td><td>).379</td></tr><tr><td rowspan="5"></td><td>96</td><td>0.174</td><td>0.214</td><td>0.11920.232</td><td>0.177</td><td>0.218</td><td>0.158</td><td>0.230</td><td>0.202</td><td>0.261</td><td></td><td>0.1720.220</td><td></td><td>0.1960.255</td><td>0.2210.3060.217</td><td>0.1</td><td>1730.223</td><td>).336</td></tr><tr><td>192</td><td>0.221</td><td>0.254</td><td>0.22400.271</td><td>0.225</td><td>0.259</td><td>0.206</td><td>0.277</td><td>0.242</td><td>0.298</td><td></td><td>0.2190.261</td><td></td><td>0.2370.296</td><td>0.2610.3400.276</td><td></td><td>).2450.285</td><td>).367</td></tr><tr><td>336</td><td>0.278</td><td>0.296</td><td>0.1.2920.307</td><td>0.278</td><td>0.297</td><td>0.272</td><td>0.335</td><td>0.287</td><td>0.335</td><td></td><td>0.2800.306</td><td></td><td>0.2830.335</td><td>0.3090.3780.339</td><td></td><td>0.3210.338</td><td>.395</td></tr><tr><td>720</td><td>0.358</td><td>0.347</td><td>0.3640.353</td><td>0.354</td><td>$\underline{0.348}$</td><td>0.398</td><td>0.418</td><td>$\underline{0.351}$</td><td>0.386</td><td></td><td>0.3650.359</td><td></td><td>0.3450.381</td><td>0.3770.4270.403</td><td></td><td>0.4140.410</td><td>0.428</td></tr><tr><td>平均值</td><td>0.258</td><td>0.278</td><td>3.2720.291</td><td>0.259</td><td>$\underline{0.281}$</td><td>0.259</td><td>0.315</td><td>0.271</td><td>0.320</td><td></td><td>0.2590.287</td><td></td><td>0.2650.317</td><td>0.2920.3630.309</td><td></td><td>0.2880.314</td><td>0.382</td></tr><tr><td rowspan="5"></td><td>96</td><td>0.203</td><td>0.237</td><td>0.3220.339</td><td>0.234</td><td>0.286</td><td>0.310</td><td>0.331</td><td>0.312</td><td>0.399</td><td></td><td>0.2500.292</td><td></td><td>0.2900.378</td><td>0.2370.3440.242</td><td></td><td>0.2150.249</td><td>0.711</td></tr><tr><td>192</td><td>0.233</td><td>0.261</td><td>0.3590.356</td><td>0.267</td><td>0.310</td><td>0.734</td><td>0.725</td><td>0.3339</td><td>0.416</td><td></td><td>0.2960.318</td><td></td><td>0.3200.398</td><td>0.2800.3800.</td><td></td><td>0.254).272</td><td>0.692</td></tr><tr><td>336</td><td>0.248</td><td>0.273</td><td>0.3970.369</td><td>0.290</td><td>0.315</td><td>0.750</td><td>0.735</td><td>368</td><td>0.430</td><td></td><td>0.3190.330</td><td></td><td>0.3530.415</td><td>0.3040.3890.282</td><td></td><td>0.2900.296</td><td>0.723</td></tr><tr><td>720</td><td>0.249</td><td>0.275</td><td>0.3970.356</td><td>0.289</td><td>0.317</td><td>0.769</td><td>0.765</td><td>0.370</td><td>0.425</td><td></td><td>0.3380.337</td><td></td><td>0.3560.413</td><td>0.3080.3880.357</td><td></td><td>0.2850.295</td><td>0.717</td></tr><tr><td>平均值</td><td>0.233</td><td>0.262</td><td>0.3690.356</td><td>0.270</td><td>$\underline{0.307}$</td><td>0.641</td><td>0.639</td><td>0.347</td><td>0.417</td><td></td><td>0.3010.319</td><td></td><td></td><td>0.2820.291</td><td></td><td>0.2610.381</td><td>0.711</td></tr><tr><td>${1}^{\text{st }}$</td><td>计数；计算；总数</td><td>16</td><td>22</td><td>612</td><td>$\underline{12}$</td><td>11</td><td>│3</td><td>0</td><td>0</td><td>0</td><td></td><td>10</td><td></td><td>30</td><td>00</td><td></td><td>00</td><td>0</td></tr></tbody></table>

Table 11: Full results of the Market dataset. We compare extensive competitive models on the real-world transaction forecasting task. Avg means the average results from all prediction lengths.

表11：市场数据集的完整结果。我们在现实世界交易预测任务上比较了大量有竞争力的模型。Avg表示所有预测时长的平均结果。

<table><tr><td colspan="2">Models</td><td colspan="2">iTransformer (Ours)</td><td colspan="2">RLinear (2023)</td><td colspan="2">PatchTST 2023)</td><td colspan="2">Crossformer (2023)</td><td colspan="2">TiDE 2023)</td><td colspan="2">TimesNet (2023)</td><td colspan="2">DLinear (2023)</td><td colspan="2">SCINet (2022a)</td><td colspan="2">FEDformer (2022)</td><td colspan="2">Stationary 2022b)</td><td colspan="2">Autoformer (2021)</td></tr><tr><td/><td>Metric</td><td>MSE</td><td>MAE</td><td/><td>MSE MAE</td><td/><td>MSE MAE</td><td>MSE</td><td>MAE</td><td/><td>MSE MAE</td><td/><td>MSE MAE</td><td/><td>MSE MAE</td><td/><td>MSE MAE</td><td/><td>MSE MAE</td><td/><td>MSE MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="5">Jan-2020</td><td>12</td><td>0.058</td><td>0.126</td><td>0.139</td><td>0.232</td><td>0.072</td><td>0.155</td><td>0.068</td><td>0.141</td><td/><td>0.173 0.273</td><td>0.088</td><td>0.177</td><td>0.093</td><td>0.183</td><td>0.202</td><td>0.310</td><td>0.277</td><td>0.384</td><td>0.143</td><td>0.243</td><td>0.365</td><td>0.444</td></tr><tr><td>24</td><td>0.066</td><td>0.138</td><td>0.155</td><td>0.250</td><td>0.079</td><td>0.164</td><td>0.091</td><td>0.161</td><td>0.170</td><td>0.274</td><td>0.103</td><td>0.195</td><td>0.105</td><td>0.200</td><td>0.215</td><td>0.323</td><td>0.268</td><td>0.378</td><td>0.167</td><td>0.270</td><td>0.669</td><td>0.636</td></tr><tr><td>72</td><td>0.079</td><td>0.157</td><td>0.156</td><td>0.252</td><td>0.090</td><td>0.180</td><td>0.123</td><td>0.202</td><td>0.197</td><td>0.298</td><td>0.089</td><td>0.180</td><td>0.116</td><td>0.215</td><td>0.388</td><td>0.431</td><td>0.281</td><td>0.390</td><td>0.193</td><td>0.300</td><td>0.404</td><td>0.479</td></tr><tr><td>144</td><td>0.086</td><td>0.167</td><td>0.157</td><td>0.253</td><td>0.093</td><td>0.185</td><td>0.185</td><td>0.218</td><td/><td>2.208 0.311</td><td>$\underline{0.091}$</td><td>$\underline{0.183}$</td><td>0.124</td><td>0.225</td><td>0.459</td><td>9 0.477</td><td>0.359</td><td>0.453</td><td>0.183</td><td>0.294</td><td>0.536</td><td>0.566</td></tr><tr><td>Avg</td><td>0.072</td><td>0.147</td><td>0.152 0.247</td><td/><td>$\underline{0.084}\;\underline{0.171}$</td><td/><td>0.117</td><td>0.181</td><td/><td>0.1870.289</td><td>0.093</td><td>0.184</td><td/><td>0.110 0.206</td><td/><td>0.316 0.385</td><td/><td>0.296 0.401</td><td/><td>0.172 0.277</td><td>0.494 0.531</td><td/></tr><tr><td rowspan="5">Oct-2023</td><td>12</td><td>0.189</td><td>0.205</td><td>0.479</td><td>0.411</td><td>0.255</td><td>0.250</td><td>0.270</td><td>0.208</td><td/><td>0.486 0.427</td><td>0.275</td><td>0.277</td><td/><td>0.380 0.355</td><td>0.525</td><td>0.451</td><td>0.553</td><td>0.508</td><td>0.355</td><td>0.332</td><td>0.653</td><td>0.555</td></tr><tr><td>24</td><td>0.254</td><td>0.244</td><td>0.543</td><td>0.446</td><td>0.320</td><td>0.291</td><td>0.329</td><td>0.233</td><td>0.545</td><td>0.463</td><td>$\underline{0.300}$</td><td>0.285</td><td>0.456 (</td><td>0.397</td><td>0.583</td><td>0.479</td><td>0.567</td><td>0.514</td><td>0.430</td><td>0.377</td><td>0.761</td><td>0.611</td></tr><tr><td>72</td><td>0.421</td><td>0.327</td><td>0.634</td><td>0.481</td><td>0.459</td><td>0.360</td><td>0.484</td><td>0.324</td><td>0.651</td><td>0.510</td><td>0.384</td><td>0.326</td><td>0.555</td><td>0.438</td><td>0.761</td><td>0.558</td><td>0.636</td><td>0.548</td><td>0.573</td><td>0.454</td><td>0.857</td><td>0.658</td></tr><tr><td>144</td><td>0.517</td><td>0.379</td><td>0.683</td><td>0.504</td><td>0.541</td><td>0.404</td><td>0.633</td><td>0.388</td><td>0.698</td><td>0.526</td><td>0.481</td><td>$\underline{0.383}$</td><td>0.611</td><td>0.459</td><td>0.770</td><td>0.568</td><td>0.744</td><td>0.604</td><td>0.637</td><td>0.498</td><td>0.817</td><td>0.627</td></tr><tr><td>Avg</td><td>0.345</td><td>0.289</td><td>0.585 0.461</td><td/><td/><td>0.394 0.326</td><td>0.429</td><td>0.288</td><td>0.595 0.481</td><td/><td>0.360</td><td>0.318</td><td/><td>0.501 0.412</td><td/><td>0.660 0.514</td><td/><td>0.625 0.543</td><td/><td>0.499 0.415</td><td>0.772</td><td>0.612</td></tr><tr><td rowspan="5">HK\$’000</td><td>12</td><td>0.123</td><td>0.170</td><td>0.329</td><td>0.304</td><td>0.164</td><td>0.206</td><td>4.630</td><td>0.520</td><td/><td>0.512 0.350</td><td>0.465</td><td>0.291</td><td>0.321</td><td>0.271</td><td>1.865</td><td>0.602</td><td>1.537</td><td>0.538</td><td>0.537</td><td>0.384</td><td>1.651</td><td>0.593</td></tr><tr><td>24</td><td>0.158</td><td>0.197</td><td>0.386</td><td>0.332</td><td>$\underline{0.198}$</td><td>0.228</td><td>4.987</td><td>0.568</td><td>0.635</td><td>0.388</td><td>0.503</td><td>0.297</td><td>0.464</td><td>0.318</td><td>2.228</td><td>0.664</td><td>1.553</td><td>0.547</td><td>0.551</td><td>0.386</td><td>1.671</td><td>0.594</td></tr><tr><td>72</td><td>0.212</td><td>0.240</td><td>0.436</td><td>0.353</td><td>0.268</td><td>0.273</td><td>5.631</td><td>0.675</td><td>1.239</td><td>0.490</td><td>0.534</td><td>0.310</td><td>0.986</td><td>0.423</td><td>3.084</td><td>0.793</td><td>1.612</td><td>0.554</td><td>2.004</td><td>0.853</td><td>2.054</td><td>0.758</td></tr><tr><td>144</td><td>0.245</td><td>0.257</td><td>0.429</td><td>0.355</td><td>0.293</td><td>0.286</td><td>6.083</td><td>0.708</td><td>1.562</td><td>0.538</td><td>0.564</td><td>0.333</td><td>1.287</td><td>0.473</td><td>4.089</td><td>0.875</td><td>1.784</td><td>0.636</td><td>2.379</td><td>0.947</td><td>2.114</td><td>0.778</td></tr><tr><td>Avg</td><td>0.184</td><td>0.216</td><td>0.395</td><td>0.336</td><td>0.231</td><td>0.248</td><td>5.333</td><td>0.618</td><td/><td>0.987 0.442</td><td>0.516 0.308</td><td/><td/><td>0.765 0.372</td><td/><td>2.817 0.734</td><td>1.621</td><td/><td/><td>368 0.643</td><td>1.872</td><td>0.681</td></tr><tr><td rowspan="5">CO.</td><td>12</td><td>0.051</td><td>0.127</td><td>0.168</td><td>0.272</td><td>0.068 0.</td><td>0.164</td><td>0.055</td><td>0.140</td><td/><td>.212 0.304</td><td>0.074</td><td>9.169</td><td/><td>0.096 0.198</td><td>0.199</td><td>0.301</td><td>0.268</td><td>0.379</td><td>0.140</td><td>0.252</td><td>0.386</td><td>0.461</td></tr><tr><td>24</td><td>0.059</td><td>0.139</td><td>0.185</td><td>0.290</td><td>0.074</td><td>0.173</td><td>0.065</td><td>$\underline{0.155}$</td><td>0.201</td><td>0.301</td><td>0.081</td><td>0.178</td><td>0.105</td><td>0.209</td><td>0.225</td><td>0.325</td><td>0.256</td><td>0.370</td><td>0.174</td><td>0.289</td><td>0.708</td><td>0.644</td></tr><tr><td>72</td><td>0.071</td><td>0.160</td><td>0.183</td><td>0.291</td><td>0.081</td><td>0.187</td><td>0.077</td><td>$\underline{0.170}$</td><td>0.222</td><td>0.316</td><td>0.077</td><td>0.178</td><td>0.109</td><td>0.215</td><td>0.317</td><td>0.338</td><td>0.285</td><td>0.396</td><td>0.202</td><td>0.321</td><td>0.510</td><td>0.552</td></tr><tr><td>144</td><td>0.079</td><td>0.171</td><td>0.184</td><td>0.292</td><td>0.085</td><td>0.193</td><td>$\underline{0.085}$</td><td>$\underline{0.181}$</td><td>0.229</td><td>0.322</td><td>0.088</td><td>0.192</td><td>0.113</td><td>0.220</td><td>0.378</td><td>0.425</td><td>0.372</td><td>0.468</td><td>0.204</td><td>0.322</td><td>0.468</td><td>0.528</td></tr><tr><td>Avg</td><td>0.065</td><td>0.150</td><td/><td>1.180 0.286</td><td/><td>0.179</td><td>0.071</td><td>0.162</td><td>0.216 0.311</td><td/><td/><td>0.080 0.179</td><td/><td>0.106 0.210</td><td>0.280</td><td>880 0.360</td><td>0.295</td><td>5 0.403</td><td>0.180 (</td><td>80 0.296</td><td>0.518</td><td>0.547</td></tr><tr><td rowspan="5"/><td>12</td><td>0.050</td><td>0.121</td><td>0.123</td><td>0.230</td><td>0.065</td><td>0.156</td><td>0.152</td><td>0.145</td><td/><td>0.1840.265</td><td/><td>0.094 0.171</td><td/><td>0.090 0.180</td><td>0.164</td><td>4 0.249</td><td>0.272</td><td>0.349</td><td>0.129</td><td>0.229</td><td>0.382</td><td>0.437</td></tr><tr><td>24</td><td>0.062</td><td>0.135</td><td>0.144</td><td>0.249</td><td>0.077</td><td>0.167</td><td>0.178</td><td>0.165</td><td>0.183</td><td>0.266</td><td>0.099</td><td>0.178</td><td>0.108</td><td>0.196</td><td>0.216</td><td>0.280</td><td>0.265</td><td>0.343</td><td>0.157</td><td>0.266</td><td>0.345</td><td>0.412</td></tr><tr><td>72</td><td>0.082</td><td>0.155</td><td>0.151</td><td>0.251</td><td>0.094</td><td>0.184</td><td>0.236</td><td>0.193</td><td>0.226</td><td>0.287</td><td>0.111</td><td>0.189</td><td>0.129</td><td>0.209</td><td>0.360</td><td>0.370</td><td>0.284</td><td>0.360</td><td>0.183</td><td>0.291</td><td>0.437</td><td>0.471</td></tr><tr><td>144</td><td>0.093</td><td>0.166</td><td>0.154</td><td>0.251</td><td>0.101</td><td>0.190</td><td>0.260</td><td>0.214</td><td>0.240</td><td>0.294</td><td>0.115</td><td>$\underline{0.189}$</td><td>0.138</td><td>0.215</td><td>0.410</td><td>0.391</td><td>0.379</td><td>0.441</td><td>0.194</td><td>0.296</td><td>0.501</td><td>0.518</td></tr><tr><td>Avg</td><td>0.072</td><td>0.144</td><td>0.143</td><td>0.245</td><td>0.084</td><td/><td>0.207</td><td>0.179</td><td>0.208 (</td><td/><td>0.105</td><td/><td/><td>0.116 0.20</td><td>0.288</td><td/><td>0.300</td><td/><td/><td>0.166 0.271</td><td>0.417</td><td>0.460</td></tr><tr><td rowspan="5"/><td>12</td><td>0.065</td><td>0.129</td><td>0.191</td><td>0.247</td><td>0.091</td><td>0.160</td><td>0.243</td><td>$\underline{0.156}$</td><td>0.267</td><td>0.289</td><td>0.123</td><td>0.180</td><td>0.143</td><td>0.195</td><td>0.310</td><td>0.326</td><td>0.309</td><td>0.366</td><td>0.175</td><td>0.243</td><td>0.640</td><td>0.580</td></tr><tr><td>24</td><td>0.078</td><td>0.141</td><td>0.214</td><td>0.264</td><td>0.107</td><td>0.173</td><td>0.293</td><td>0.177</td><td>0.267</td><td>0.291</td><td>0.130</td><td>0.183</td><td>0.170</td><td>0.212</td><td>0.338</td><td>0.344</td><td>0.313</td><td>0.369</td><td>0.188</td><td>0.264</td><td>0.763</td><td>0.642</td></tr><tr><td>72</td><td>0.108</td><td>0.161</td><td>0.222</td><td>0.266</td><td>0.131</td><td>0.190</td><td>0.331</td><td>0.215</td><td>0.334</td><td>0.317</td><td>0.149</td><td>0.196</td><td>0.202</td><td>0.228</td><td>0.511</td><td>0.408</td><td>0.330</td><td>0.374</td><td>0.267</td><td>0.324</td><td>0.616</td><td>0.564</td></tr><tr><td>144</td><td>0.126</td><td>0.172</td><td>0.227</td><td>0.268</td><td>$\underline{0.141}$</td><td>0.195</td><td>0.368</td><td>0.226</td><td>0.363</td><td>0.332</td><td>0.166</td><td>0.206</td><td>0.222</td><td>0.239</td><td>0.687</td><td>0.461</td><td>0.450</td><td>0.456</td><td>0.336</td><td>0.373</td><td>0.658</td><td>0.586</td></tr><tr><td>Avg</td><td>0.094</td><td>0.150</td><td>0.214</td><td>0.261</td><td>$\underline{0.118}$</td><td>0.180</td><td>0.309</td><td>0.194</td><td>0.308</td><td>0.307</td><td>0.142</td><td>0.191</td><td>0.184</td><td>0.219</td><td>0.461</td><td>0.385</td><td>0.350</td><td>0.391</td><td>0.242</td><td>0.301</td><td>0.669</td><td>0.593</td></tr><tr><td/><td>Count</td><td>28</td><td>27</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>$\underline{3}$</td><td>0</td><td>0</td><td>$\underline{2}$</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></table>

<table><tbody><tr><td colspan="2">模型</td><td colspan="2">iTransformer（我们的方法）</td><td colspan="2">RLinear（2023年）</td><td colspan="2">PatchTST（2023年）</td><td colspan="2">Crossformer（2023年）</td><td colspan="2">TiDE（2023年）</td><td colspan="2">时间网络（TimesNet ，2023年）</td><td colspan="2">深度线性模型（DLinear ，2023年）</td><td colspan="2">科学计算网络（SCINet ，2022a）</td><td colspan="2">联邦变换器（FEDformer ，2022年）</td><td colspan="2">平稳模型（Stationary ，2022b）</td><td colspan="2">自动变换器（Autoformer ，2021年）</td></tr><tr><td></td><td>指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td></td><td>均方误差（MSE） 平均绝对误差（MAE）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td></tr><tr><td rowspan="5">2020年1月</td><td>12</td><td>0.058</td><td>0.126</td><td>0.139</td><td>0.232</td><td>0.072</td><td>0.155</td><td>0.068</td><td>0.141</td><td></td><td>0.173 0.273</td><td>0.088</td><td>0.177</td><td>0.093</td><td>0.183</td><td>0.202</td><td>0.310</td><td>0.277</td><td>0.384</td><td>0.143</td><td>0.243</td><td>0.365</td><td>0.444</td></tr><tr><td>24</td><td>0.066</td><td>0.138</td><td>0.155</td><td>0.250</td><td>0.079</td><td>0.164</td><td>0.091</td><td>0.161</td><td>0.170</td><td>0.274</td><td>0.103</td><td>0.195</td><td>0.105</td><td>0.200</td><td>0.215</td><td>0.323</td><td>0.268</td><td>0.378</td><td>0.167</td><td>0.270</td><td>0.669</td><td>0.636</td></tr><tr><td>72</td><td>0.079</td><td>0.157</td><td>0.156</td><td>0.252</td><td>0.090</td><td>0.180</td><td>0.123</td><td>0.202</td><td>0.197</td><td>0.298</td><td>0.089</td><td>0.180</td><td>0.116</td><td>0.215</td><td>0.388</td><td>0.431</td><td>0.281</td><td>0.390</td><td>0.193</td><td>0.300</td><td>0.404</td><td>0.479</td></tr><tr><td>144</td><td>0.086</td><td>0.167</td><td>0.157</td><td>0.253</td><td>0.093</td><td>0.185</td><td>0.185</td><td>0.218</td><td></td><td>2.208 0.311</td><td>$\underline{0.091}$</td><td>$\underline{0.183}$</td><td>0.124</td><td>0.225</td><td>0.459</td><td>9 0.477</td><td>0.359</td><td>0.453</td><td>0.183</td><td>0.294</td><td>0.536</td><td>0.566</td></tr><tr><td>平均值</td><td>0.072</td><td>0.147</td><td>0.152 0.247</td><td></td><td>$\underline{0.084}\;\underline{0.171}$</td><td></td><td>0.117</td><td>0.181</td><td></td><td>0.1870.289</td><td>0.093</td><td>0.184</td><td></td><td>0.110 0.206</td><td></td><td>0.316 0.385</td><td></td><td>0.296 0.401</td><td></td><td>0.172 0.277</td><td>0.494 0.531</td><td></td></tr><tr><td rowspan="5">2023年10月</td><td>12</td><td>0.189</td><td>0.205</td><td>0.479</td><td>0.411</td><td>0.255</td><td>0.250</td><td>0.270</td><td>0.208</td><td></td><td>0.486 0.427</td><td>0.275</td><td>0.277</td><td></td><td>0.380 0.355</td><td>0.525</td><td>0.451</td><td>0.553</td><td>0.508</td><td>0.355</td><td>0.332</td><td>0.653</td><td>0.555</td></tr><tr><td>24</td><td>0.254</td><td>0.244</td><td>0.543</td><td>0.446</td><td>0.320</td><td>0.291</td><td>0.329</td><td>0.233</td><td>0.545</td><td>0.463</td><td>$\underline{0.300}$</td><td>0.285</td><td>0.456 (</td><td>0.397</td><td>0.583</td><td>0.479</td><td>0.567</td><td>0.514</td><td>0.430</td><td>0.377</td><td>0.761</td><td>0.611</td></tr><tr><td>72</td><td>0.421</td><td>0.327</td><td>0.634</td><td>0.481</td><td>0.459</td><td>0.360</td><td>0.484</td><td>0.324</td><td>0.651</td><td>0.510</td><td>0.384</td><td>0.326</td><td>0.555</td><td>0.438</td><td>0.761</td><td>0.558</td><td>0.636</td><td>0.548</td><td>0.573</td><td>0.454</td><td>0.857</td><td>0.658</td></tr><tr><td>144</td><td>0.517</td><td>0.379</td><td>0.683</td><td>0.504</td><td>0.541</td><td>0.404</td><td>0.633</td><td>0.388</td><td>0.698</td><td>0.526</td><td>0.481</td><td>$\underline{0.383}$</td><td>0.611</td><td>0.459</td><td>0.770</td><td>0.568</td><td>0.744</td><td>0.604</td><td>0.637</td><td>0.498</td><td>0.817</td><td>0.627</td></tr><tr><td>平均值</td><td>0.345</td><td>0.289</td><td>0.585 0.461</td><td></td><td></td><td>0.394 0.326</td><td>0.429</td><td>0.288</td><td>0.595 0.481</td><td></td><td>0.360</td><td>0.318</td><td></td><td>0.501 0.412</td><td></td><td>0.660 0.514</td><td></td><td>0.625 0.543</td><td></td><td>0.499 0.415</td><td>0.772</td><td>0.612</td></tr><tr><td rowspan="5">港元（千）</td><td>12</td><td>0.123</td><td>0.170</td><td>0.329</td><td>0.304</td><td>0.164</td><td>0.206</td><td>4.630</td><td>0.520</td><td></td><td>0.512 0.350</td><td>0.465</td><td>0.291</td><td>0.321</td><td>0.271</td><td>1.865</td><td>0.602</td><td>1.537</td><td>0.538</td><td>0.537</td><td>0.384</td><td>1.651</td><td>0.593</td></tr><tr><td>24</td><td>0.158</td><td>0.197</td><td>0.386</td><td>0.332</td><td>$\underline{0.198}$</td><td>0.228</td><td>4.987</td><td>0.568</td><td>0.635</td><td>0.388</td><td>0.503</td><td>0.297</td><td>0.464</td><td>0.318</td><td>2.228</td><td>0.664</td><td>1.553</td><td>0.547</td><td>0.551</td><td>0.386</td><td>1.671</td><td>0.594</td></tr><tr><td>72</td><td>0.212</td><td>0.240</td><td>0.436</td><td>0.353</td><td>0.268</td><td>0.273</td><td>5.631</td><td>0.675</td><td>1.239</td><td>0.490</td><td>0.534</td><td>0.310</td><td>0.986</td><td>0.423</td><td>3.084</td><td>0.793</td><td>1.612</td><td>0.554</td><td>2.004</td><td>0.853</td><td>2.054</td><td>0.758</td></tr><tr><td>144</td><td>0.245</td><td>0.257</td><td>0.429</td><td>0.355</td><td>0.293</td><td>0.286</td><td>6.083</td><td>0.708</td><td>1.562</td><td>0.538</td><td>0.564</td><td>0.333</td><td>1.287</td><td>0.473</td><td>4.089</td><td>0.875</td><td>1.784</td><td>0.636</td><td>2.379</td><td>0.947</td><td>2.114</td><td>0.778</td></tr><tr><td>平均值</td><td>0.184</td><td>0.216</td><td>0.395</td><td>0.336</td><td>0.231</td><td>0.248</td><td>5.333</td><td>0.618</td><td></td><td>0.987 0.442</td><td>0.516 0.308</td><td></td><td></td><td>0.765 0.372</td><td></td><td>2.817 0.734</td><td>1.621</td><td></td><td></td><td>368 0.643</td><td>1.872</td><td>0.681</td></tr><tr><td rowspan="5">公司</td><td>12</td><td>0.051</td><td>0.127</td><td>0.168</td><td>0.272</td><td>0.068 0.</td><td>0.164</td><td>0.055</td><td>0.140</td><td></td><td>.212 0.304</td><td>0.074</td><td>9.169</td><td></td><td>0.096 0.198</td><td>0.199</td><td>0.301</td><td>0.268</td><td>0.379</td><td>0.140</td><td>0.252</td><td>0.386</td><td>0.461</td></tr><tr><td>24</td><td>0.059</td><td>0.139</td><td>0.185</td><td>0.290</td><td>0.074</td><td>0.173</td><td>0.065</td><td>$\underline{0.155}$</td><td>0.201</td><td>0.301</td><td>0.081</td><td>0.178</td><td>0.105</td><td>0.209</td><td>0.225</td><td>0.325</td><td>0.256</td><td>0.370</td><td>0.174</td><td>0.289</td><td>0.708</td><td>0.644</td></tr><tr><td>72</td><td>0.071</td><td>0.160</td><td>0.183</td><td>0.291</td><td>0.081</td><td>0.187</td><td>0.077</td><td>$\underline{0.170}$</td><td>0.222</td><td>0.316</td><td>0.077</td><td>0.178</td><td>0.109</td><td>0.215</td><td>0.317</td><td>0.338</td><td>0.285</td><td>0.396</td><td>0.202</td><td>0.321</td><td>0.510</td><td>0.552</td></tr><tr><td>144</td><td>0.079</td><td>0.171</td><td>0.184</td><td>0.292</td><td>0.085</td><td>0.193</td><td>$\underline{0.085}$</td><td>$\underline{0.181}$</td><td>0.229</td><td>0.322</td><td>0.088</td><td>0.192</td><td>0.113</td><td>0.220</td><td>0.378</td><td>0.425</td><td>0.372</td><td>0.468</td><td>0.204</td><td>0.322</td><td>0.468</td><td>0.528</td></tr><tr><td>平均值</td><td>0.065</td><td>0.150</td><td></td><td>1.180 0.286</td><td></td><td>0.179</td><td>0.071</td><td>0.162</td><td>0.216 0.311</td><td></td><td></td><td>0.080 0.179</td><td></td><td>0.106 0.210</td><td>0.280</td><td>880 0.360</td><td>0.295</td><td>5 0.403</td><td>0.180 (</td><td>80 0.296</td><td>0.518</td><td>0.547</td></tr><tr><td rowspan="5"></td><td>12</td><td>0.050</td><td>0.121</td><td>0.123</td><td>0.230</td><td>0.065</td><td>0.156</td><td>0.152</td><td>0.145</td><td></td><td>0.1840.265</td><td></td><td>0.094 0.171</td><td></td><td>0.090 0.180</td><td>0.164</td><td>4 0.249</td><td>0.272</td><td>0.349</td><td>0.129</td><td>0.229</td><td>0.382</td><td>0.437</td></tr><tr><td>24</td><td>0.062</td><td>0.135</td><td>0.144</td><td>0.249</td><td>0.077</td><td>0.167</td><td>0.178</td><td>0.165</td><td>0.183</td><td>0.266</td><td>0.099</td><td>0.178</td><td>0.108</td><td>0.196</td><td>0.216</td><td>0.280</td><td>0.265</td><td>0.343</td><td>0.157</td><td>0.266</td><td>0.345</td><td>0.412</td></tr><tr><td>72</td><td>0.082</td><td>0.155</td><td>0.151</td><td>0.251</td><td>0.094</td><td>0.184</td><td>0.236</td><td>0.193</td><td>0.226</td><td>0.287</td><td>0.111</td><td>0.189</td><td>0.129</td><td>0.209</td><td>0.360</td><td>0.370</td><td>0.284</td><td>0.360</td><td>0.183</td><td>0.291</td><td>0.437</td><td>0.471</td></tr><tr><td>144</td><td>0.093</td><td>0.166</td><td>0.154</td><td>0.251</td><td>0.101</td><td>0.190</td><td>0.260</td><td>0.214</td><td>0.240</td><td>0.294</td><td>0.115</td><td>$\underline{0.189}$</td><td>0.138</td><td>0.215</td><td>0.410</td><td>0.391</td><td>0.379</td><td>0.441</td><td>0.194</td><td>0.296</td><td>0.501</td><td>0.518</td></tr><tr><td>平均值</td><td>0.072</td><td>0.144</td><td>0.143</td><td>0.245</td><td>0.084</td><td></td><td>0.207</td><td>0.179</td><td>0.208 (</td><td></td><td>0.105</td><td></td><td></td><td>0.116 0.20</td><td>0.288</td><td></td><td>0.300</td><td></td><td></td><td>0.166 0.271</td><td>0.417</td><td>0.460</td></tr><tr><td rowspan="5"></td><td>12</td><td>0.065</td><td>0.129</td><td>0.191</td><td>0.247</td><td>0.091</td><td>0.160</td><td>0.243</td><td>$\underline{0.156}$</td><td>0.267</td><td>0.289</td><td>0.123</td><td>0.180</td><td>0.143</td><td>0.195</td><td>0.310</td><td>0.326</td><td>0.309</td><td>0.366</td><td>0.175</td><td>0.243</td><td>0.640</td><td>0.580</td></tr><tr><td>24</td><td>0.078</td><td>0.141</td><td>0.214</td><td>0.264</td><td>0.107</td><td>0.173</td><td>0.293</td><td>0.177</td><td>0.267</td><td>0.291</td><td>0.130</td><td>0.183</td><td>0.170</td><td>0.212</td><td>0.338</td><td>0.344</td><td>0.313</td><td>0.369</td><td>0.188</td><td>0.264</td><td>0.763</td><td>0.642</td></tr><tr><td>72</td><td>0.108</td><td>0.161</td><td>0.222</td><td>0.266</td><td>0.131</td><td>0.190</td><td>0.331</td><td>0.215</td><td>0.334</td><td>0.317</td><td>0.149</td><td>0.196</td><td>0.202</td><td>0.228</td><td>0.511</td><td>0.408</td><td>0.330</td><td>0.374</td><td>0.267</td><td>0.324</td><td>0.616</td><td>0.564</td></tr><tr><td>144</td><td>0.126</td><td>0.172</td><td>0.227</td><td>0.268</td><td>$\underline{0.141}$</td><td>0.195</td><td>0.368</td><td>0.226</td><td>0.363</td><td>0.332</td><td>0.166</td><td>0.206</td><td>0.222</td><td>0.239</td><td>0.687</td><td>0.461</td><td>0.450</td><td>0.456</td><td>0.336</td><td>0.373</td><td>0.658</td><td>0.586</td></tr><tr><td>平均值</td><td>0.094</td><td>0.150</td><td>0.214</td><td>0.261</td><td>$\underline{0.118}$</td><td>0.180</td><td>0.309</td><td>0.194</td><td>0.308</td><td>0.307</td><td>0.142</td><td>0.191</td><td>0.184</td><td>0.219</td><td>0.461</td><td>0.385</td><td>0.350</td><td>0.391</td><td>0.242</td><td>0.301</td><td>0.669</td><td>0.593</td></tr><tr><td></td><td>数量</td><td>28</td><td>27</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>$\underline{3}$</td><td>0</td><td>0</td><td>$\underline{2}$</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table>

<!-- Media -->

## G DISCUSSIONS AND FURTHER IMPROVEMENT

## G 讨论与进一步改进

### G.1 DISCUSSIONS ON ARCHITECTURE-FREE METHODS

### G.1 无架构方法的讨论

Channel Independence (CI) (Nie et al. 2023), regarding variates of time series independently and adopting the shared backbone, have gained increasing popularity in forecasting with performance promotions as an architecture-free method. Recent works (Han et al. 2023; Li et al. 2023) found that while Channel Dependence (CD) benefits from a higher capacity ideally, CI can greatly boost the performance because of sample scarcity, since most of the current forecasting benchmarks are not large enough. We think it is essential to make variates independent, especially when there are potential risks of embedding as mentioned in Appendix E.3, inducing the ideal model capacity of CD limited by the excessively localized receptive field. However, the essence of CI, regarding multivariate time series univariately, can lead to time-consuming training and inference and become an obstacle to scalability. Still, multivariate correlations can not be explicitly utilized. Perpendicular to these works, iTransformer repurposes an architecture with the native Transformer modules to tackle the issues.

通道独立性（CI）（聂等人，2023年）将时间序列的变量独立看待并采用共享主干，作为一种无架构方法，在预测中越来越受欢迎，且能提升性能。近期研究（韩等人，2023年；李等人，2023年）发现，虽然通道依赖性（CD）理论上受益于更高的容量，但由于样本稀缺，CI可以极大地提升性能，因为目前大多数预测基准数据集不够大。我们认为使变量相互独立至关重要，特别是如附录E.3中所述存在嵌入潜在风险时，这会导致CD的理想模型容量受过度局部化的感受野限制。然而，CI将多变量时间序列视为单变量的本质会导致训练和推理耗时，并成为可扩展性的障碍。此外，多变量相关性无法得到明确利用。与这些研究不同，iTransformer重新利用具有原生Transformer模块的架构来解决这些问题。

RevIN (Kim et al. 2021) and Stationarization (Liu et al. 2022b) have been widely applied for the distribution shift (non-stationarity) as architecture-free techniques. These works strive to reveal the temporal dependency better. This is accomplished by layer normalization in iTransformer and still leaves further improvement for us to tackle the distribution shift.

RevIN（金等人，2021年）和平稳化处理（刘等人，2022b）作为无架构技术，已被广泛应用于处理分布偏移（非平稳性）问题。这些研究致力于更好地揭示时间依赖关系。在iTransformer中通过层归一化实现了这一点，但在应对分布偏移方面仍有进一步改进的空间。

### G.2 DISCUSSIONS ON LINEAR FORECASTERS

### G.2 关于线性预测器的讨论

Linear forecasters have natural advantages in modeling temporal dependencies. The dense weighting (Zeng et al. 2023; Li et al. 2023) can reveal measurement-free relationships among the time points of the same variate. More advanced linear forecasters focus on structural point-wise modeling (Oreshkin et al. 2019; Liu et al. 2022a; 2023). By contrast, iTransformer is particularly good at forecasting high-dimensional time series (numerous variates with complicated correlations, which can be common and realistic for practitioners in real forecasting applications). For variate correlating, the embedding keeps the variate independent and the attention module can be applied to dig it out. Under univariate scenarios, iTransformer actually becomes a stackable linear forecaster (attention degradation), which leaves further enhancement to exploit the temporal dependency better.

线性预测器在对时间依赖关系进行建模方面具有天然优势。密集加权法（曾等人，2023年；李等人，2023年）可以揭示同一变量各时间点之间无需测量的关系。更先进的线性预测器专注于结构性逐点建模（奥列什金等人，2019年；刘等人，2022a；2023年）。相比之下，iTransformer特别擅长预测高维时间序列（具有复杂相关性的众多变量，这在实际预测应用中对从业者来说是常见且现实的情况）。对于变量相关性，嵌入操作使变量保持独立，并且可以应用注意力模块来挖掘这种相关性。在单变量场景下，iTransformer实际上变成了一个可堆叠的线性预测器（注意力退化），这为更好地挖掘时间依赖关系留下了进一步改进的空间。

### G.3 DISCUSSIONS ON TRANSFORMERS

### G.3 关于Transformer的讨论

We emphasize that iTransformer actually proposes a new perspective to think about the multivariate time series modality, specifically, how to consider the variates and the tokenization. We list several representatives in Figure 19. Transformer treats time series as the natural language but the time-aligned embedding may bring about risks in multi-dimensional series. The problem can be alleviated by expanding the receptive field. Although it is believed that Patching (Zhang & Yan, 2023, Nie et al. 2023) can be more fine-grained, it also brings higher computational complexity and the potential interaction noise between time-unaligned patches. If the current embedding (implemented by MLP) is enhanced with more inductive bias (such as TCN), it may handle more robust cases with the variate token paradigm and enjoy the flexibility of Transformer with changeable numbers of tokens.

我们强调，iTransformer实际上提出了一个思考多变量时间序列模态的新视角，具体来说，就是如何考虑变量和Token化（Tokenization）。我们在图19中列出了几个代表性方法。Transformer将时间序列视为自然语言，但时间对齐嵌入可能会在多维序列中带来风险。可以通过扩大感受野来缓解这个问题。尽管人们认为补丁化方法（Patching，Zhang & Yan，2023；Nie等人，2023）可以更细粒度，但它也带来了更高的计算复杂度以及时间未对齐补丁之间的潜在交互噪声。如果用更多归纳偏置（如时间卷积网络，TCN）来增强当前的嵌入（由多层感知机，MLP实现），那么它可能会在变量Token范式下处理更稳健的情况，并享受Transformer在Token数量可变时的灵活性。

We believe the capability and scalability of Transformer have stood the test by extensive fields, but there is still improvement room to elaborately design components based on the inverted architecture, such as efficient attention for multivariate correlation, structural temporal dependency modeling under distribution shift, fine-grained variate tokenization and well-designed embedding mechanisms.

我们认为，Transformer的能力和可扩展性已经在广泛领域中经受住了考验，但基于倒置架构精心设计组件仍有改进空间，例如用于多变量相关性的高效注意力机制、分布偏移下的结构化时间依赖建模、细粒度的变量Token化以及设计良好的嵌入机制。

<!-- Media -->

<!-- figureText: Temporal Token Variate Token iTransformer View Transformer View -->

<img src="https://cdn.noedgeai.com/01957f8c-fd84-7dae-9479-878754c6dda9_24.jpg?x=327&y=1826&w=1127&h=255&r=0"/>

Figure 19: Tokenizations for multivariate time series modality of representative Transformers.

图19：代表性Transformer的多变量时间序列模态的Token化方法。

<!-- Media -->