# OneSparse: A Unified System for Multi-index Vector Search 


#### Abstract

Multi-index vector search has become the cornerstone for many applications, such as recommendation systems. Efficient search in such a multi-modal hybrid vector space is challenging since no single index design performs well for all kinds of vector data. Existing approaches to processing multi-index hybrid queries either suffer from algorithmic limitations or processing inefficiency. In this paper, we propose OneSparse, a unified multi-vector index query system that incorporates multiple posting-based vector indices, which enables highly efficient retrieval of multi-modal data-sets. OneSparse introduces a novel multi-index query engine design of inter-index intersection push-down. It also optimizes the vector posting format to expedite multi-index queries. Our experiments show OneSparse achieves more than $6 \times$ search performance improvement while maintaining comparable accuracy. OneSparse has already been integrated into Microsoft online web search and advertising systems with $5 \times+$ latency gain for Bing web search and $2.0 \%$ Revenue Per Mille (RPM) gain for Bing sponsored search.

## 1 INTRODUCTION

In the past few years, data mining and machine learning techniques have converted an astronomical number of unstructured data (e.g. images, videos, documents) into high-dimensional vectors. Different kinds of vectors have their own unique characteristics in encoding different types of features on an unstructured data-set. For example, dense vectors are particularly well-suited for extracting semantic information while sparse vectors are suitable for keyword matching task. Therefore, multi-index hybrid queries, such as multimodal queries [31] and multi-model ensemble queries [28, 33], are widely adopted. These queries run joint search on multiple vector indices, such as finding similar items in a hybrid data-set [29, 32], collaborative filtering with a hybrid of sparse and dense features [9], and etc. Multi-index hybrid queries are proven to be highly effective in boosting query result accuracy [28,33].

---

However, multi-index joint retrieval is challenging since intersection among multiple vector indices cannot be directly pushed down due to the special traversal manner of vector indices. Many vector indices are built for processing approximate Top $K$ queries efficiently, which can return approximate $K$ results close to the optimal within milliseconds $[1,8]$. They are prohibitively expensive to return results in a monotonic way since they cannot sort all vectors beforehand without query vectors. These indices traverse in a manner where it first approaches the target region and then steadily departs away approximately [35]. Therefore, it remains uncertain whether a result returned from one index will be retrieved from another index during joint traversal. As a result, early intersection is not applicable. Intersection can only be done after searching different indices separately, so existing solutions either perform multi-index search in an isolated way [14, 28, 31] or fuse multiple vectors into one large vector and then perform single-index search [22, 23, 33].

---

Vector fusion [22, 23, 33] concatenates multiple vectors into one hybrid vector, over which a single vector index is built. However, increasing vector dimensions caused by vector fusion will bring additional cost, such as storage, latency, and bandwidth. Besides, vector fusion is limited in real applications because the vector similarity function needs to be decomposable, such as inner product.

---

Isolated search [14, 28, 31], on the other hand, has no limitation on vector distance metrics but it encounters issues of processing inefficiency. As Figure 1(a) illustrated, this kind of approach builds separate indices for different vectors independently (e.g. inverted index for sparse vectors and ANN index for dense vectors), and runs queries on each of them. Then, the candidates recalled from these indices are aggregated via data IDs and ranked to produce final Top $K$ results. However, such an isolated solution has two major shortcomings that conduce to low processing efficiency:

- It is difficult to determine the optimal number of candidates that each index needs to return (i.e. $K^{\prime}$ ), which minimizes search latency and achieves relatively high recall simultaneously. More results than $K^{\prime}$ lead to long latency; while fewer results than $K^{\prime}$ lead to low search accuracy. $K^{\prime}$ is dynamic per query, therefore isolated search can not predetermine a fixed $K^{\prime}$ to achieve high search accuracy and high efficiency in all situations. Milvus [31] addresses this issue by iteratively enlarging $K^{\prime}$ if the number of remaining candidates after intersection is less than the required K. However, each iteration produces a significant amount of redundant calculations, leading to even longer latency.
- Intersection and ranking can only be done after all isolated indices complete their individual $\mathrm{Top} K^{\prime}$ searches. This could lead to a significant waste of score computation and disk I/Os to rank Top $K^{\prime}$ candidates per index because a large portion of candidates will be eventually discarded after the final inter-index intersection. Elasticsearch [14] alleviates this problem by using Top $K^{\prime}$ results returned from HNSW to shortlist matched inverted lists from inverted index, reducing lots of BM25 calculations. However, ANN distance computation is not reduced since ANN search is still completed before intersection. Besides, it also faces the selection problem of $K^{\prime}$.

---

Recent trend of high-performance vector index designs shows similarity in vector data organization and query processing, which sheds new opportunities in optimizing isolated search of multiindex queries. Inverted index [27] for sparse vectors and state-of-the-art ANN index for billion scale dense vectors [8] all organize vectors as posting lists. During Top $K$ search, these indices first find matched posting lists, and then scan these posting lists to compute the Top $K$ vectors.

---

Based on this common design of vector indices, we observed a key design optimization of intersection push-down, which can effectively resolve the two major shortcomings in conventional isolated search of multiple indices. After narrowing down the search to a small number of nearest posting lists, instead of intersecting vector candidates after each index's Top $K^{\prime}$ operation, we propose to bypass each index's $\operatorname{Top} K^{\prime}$ operation, and intersect vector candidates before the final stage Top $K$ as shown in Figure 1(b). This approach allows the system to early filter low quality data points, therefore saving a significant amount of computations and I/Os. Because we eliminate individual $\operatorname{Top} K^{\prime}$ operations, the difficulty of identifying the optimal $K^{\prime}$ in the conventional methods is also completely resolved.

---

Based on this key observation of intersection push-down, we propose OneSparse, a unified index system for multi-index vector search. OneSparse is capable of running multi-index hybrid queries, and generates the optimal posting merging plan on-the-fly to enable fast inter-index intersection and intra-index union before score calculating and ranking. OneSparse unifies sparse and dense indices into one inverted index and re-arranging all posting lists according to doc IDs. Therefore, during the fine-grained traversal in candidate posting lists, when one index scans to a certain ID, candidates smaller than this ID in the other index can skip their BM25 score and Euclidean distance calculations, which solves the difficulty of identifying whether a result can be filtered out during joint traversal.

---

In addition, OneSparse applies compression optimization for posting lists of dense vectors, which greatly reduces I/Os as well as distance computations. Instead of storing full vectors, dense vectors in a posting are represented by their centroid, assuming the posting list stores a tight cluster with a small radius. In this way, only one vector is stored per posting and thus, heavy vector distance calculation can be reduced significantly.

---

In short, OneSparse makes the following contributions:

- We propose a novel multi-index search design that efficiently merge-and-rank vector results via intersection push-down. This enables a great reduction of computation and I/O for heavy Top $K$ calculations, thus greatly boosting the query performance.
- We further compress the posting lists for dense vectors to reduce high computational and I/O cost for dense vector distance calculations.
- We implement OneSparse by incorporating two vector search algorithms, SPANN for dense vectors, and inverted index for sparse vectors. Our experiments show that OneSparse has more than $6 \times$ latency improvement while achieving comparable recall accuracy.

---

OneSparse has proven to be effective in speeding multi-index vector queries in the real world. It has been successfully integrated into Microsoft's product lines, serving various online web search and advertising systems with 5X+ latency gain for Bing web search and $2.0 \%$ Revenue Per Mile (RPM) gain for Bing sponsored search.


## 2 BACKGROUND AND RELATED WORK

### 2.1 Multi-Index Search

The prosperity of deep learning has spawned a large number of neural network models that transform unstructured data (e.g. textual data) into dense vectors (e.g. embeddings with hundreds of dimensions), such as word2vec [25], Bert [12], and GPT-3 [6]. Different representations of the data-set (i.e., sparse bag-of-words and dense vectors) show features at different aspects. Dense vectors is particularly well-suited for extracting semantic information, such as when seeking docs pertaining to a specific topic, while sparse vectors is well-suited for keyword matching task, like querying docs associated with specific names or locations. Consequently, multi-index hybrid queries [9, 22, 23, 28] which leverage multiple kinds of features extracted from a single data-set has been increasingly used in many scenarios and have been proven to improve search accuracy [28, 33].

---

**Problem Definition.** Given a data-set consisting of $N$ data points, we extract $m$ different kinds of features on it, denoted as a set of vectors $X=\left\{X^{1}, X^{2}, \cdots, X^{m}\right\}$, where $X^{i} \in \mathbb{R}^{N \times n_{i}}$ is the collections of the $i^{t h}$ feature of the data-set with $n_{i}$ dimensions. Similarly, a query $q$ can be written as $\left\{q^{1}, q^{2}, \cdots, q^{m}\right\}$, where $q^{i} \in \mathbb{R}^{n_{i}}$. For each $X^{i}$, we build an index to accelerate the search process. Multi-index vector search is defined as finding Top $K$ data items retrieved by all indices and possessed the highest score based on the aggregate function $f$ :
$$
\begin{equation*}
f\left(X_{i}, q\right)=f\left(g_{1}\left(X_{i}^{1}, q^{1}\right), g_{2}\left(X_{i}^{2}, q^{2}\right), \cdots, g_{m}\left(X_{i}^{m}, q^{m}\right)\right) \tag{1}
\end{equation*}
$$

where $g_{1}, \cdots, g_{m}$ are similarity functions which can represent the relevance between $X_{i}$ and $q$. Similarity functions can be chosen according to the properties of feature vectors. For dense vectors like feature embeddings, euclidean distance, cosine distance, and inner product are widely used. For sparse representations like bag-of-words, BM25 $[18,19]$ is a popular choice, which can evaluate the relevance between queries and documents.

---

To accelerate the search process, indices are utilized on vectors. Index algorithms are quite different based on the different sparsity of vectors. For sparse data, inverted index [27] are widely used, taking advantage of sparsity per vector to optimize retrieval speed and memory usage. Inverted index also provides intra-index posting list intersection/union to avoid unnecessary computation and I/O cost for duplication introduced by the index itself as well as lowquality candidates which will not appear in the final results. For dense vectors, due to the curse of dimensionality [10], dense vector indices only offer approximate results with some query accuracy (i.e. recall). These Approximate Nearest Neighbor (ANN) indices are either organized as neighborhood graphs [13,24] or partitions (tree-based [5, 26, 30], hash-based [11, 20] and clustering-based [1, $4,8,15])$. To support super large scale data-sets, SPANN [8] achieves state-of-the-art performance, which has already been widely used in real production to support billion-scale vector search. SPANN partitions the data into a large number of posting lists (clusters) and builds a tree-graph hybrid index called SPTAG [7] for the centroids of posting lists to accelerate the search process of the nearest centroids.

---

These indices pose a considerable computational cost when attempting to retrieve results in a monotonous order, primarily because they lack the capability to pre-sort all vectors without access to query vectors. Instead, they follow a traversal manner wherein they initially approach the target region and then progressively move away approximately [35]. Consequently, it remains uncertain whether a result obtained from one index can be retrieved from another index during joint traversal and thus unifying multi-index joint search by simply pushing-down intersection is not applicable.

### 2.2 Traditional Approaches

Performing multi-index hybrid vector search is challenging since no single algorithm can perform well for all kinds of data. Traditional approaches for multi-index hybrid vector search can be divided into two categories, vector fusion and isolated search.

---

Vector fusion [22, 23, 33]. This kind of method assumes that the similarity function is decomposable such as inner product and the aggregate function is additive such as summation. By concatenating multiple vectors together into one hybrid vector, it conducts multi-index hybrid search on this vector using a single index built on the hybrid vectors. This kind of method is simple but has special algorithmic limitations on similarity function and aggregate function, which limits its usage in the real world. Besides, when fusing multiple vectors together, the resultant increase in vector dimensionality leads to escalating costs in terms of storage, latency, and bandwidth. Moreover, the extremely high dimensionality of sparse vectors makes it difficult to apply directly in vector fusion frameworks.

---

Isolated Search [14, 28, 31]. This approach uses separate algorithms and data structures to index and search multiple features
respectively. Taking sparse and dense hybrid search as an example, isolated methods utilize inverted index to manage sparse data (i.e., bag-of-words) and use BM25 scores to determine the similarity between documents and queries. Meanwhile, they build ANN index such as HNSW [24] or FAISS [15] on dense data (i.e., embeddings). When a query comes, it will firstly search in two indices separately and recall $\operatorname{Top} K^{\prime}$ candidates from inverted index and Top $K^{\prime \prime}$ candidates from ANN index. After that, these candidates will be merged and ranked by their ANN distance and BM25 score to gain TopK final results. Figure 1(a) shows the above process.

---

However, predicting the size of candidates returned by the two separate indices to minimize search latency while preserving accuracy is a challenging task. Let's consider $S_{1}$ and $S_{2}$ as the candidate sets retrieved from the inverted index and ANN index, respectively. If the size of their intersection, $\left|S_{1} \cap S_{2}\right|$, is less than the desired number of results, $K$, some candidates will not have both ANN distance and BM25 score available for the final ranking stage, leading to lower accuracy. On the other hand, if $\left|S_{1} \cap S_{2}\right|$ is greater than $K$, the results will be more accurate, but it will come at the cost of extra index traversal. Since different queries have unique characteristics, it is not possible to set a fixed size of $K^{\prime}$ and $K^{\prime \prime}$ for all scenarios. Milvus [31] addresses this issue by iteratively executing isolated search and enlarging $K^{\prime}$ if the number of remaining candidates after intersection is less than required $K$. However, each iteration results in excessive vector access and distance computation, leading to even longer latency.

---

Another problem of isolated search is that intersection and ranking can only be done after all of the isolated indices finish their Top $K^{\prime}$ candidate search. Since many candidates traversed in each index scan may not appear in the final inter-indices intersection, this will lead to large unnecessary computation and I/O cost. Elasticsearch [14] alleviates this problem by pushing down part of the intersection process. It firstly generates $K^{\prime}$ candidates through ANN search and then uses them to shortlist matched inverted lists from inverted index, reducing many BM25 calculations. However, ANN distance computation is not reduced since ANN search is still completed before intersection, and thus candidates that get removed after the intersection still have their distances calculated. Besides, it also faces the selection problem of $K^{\prime}$.

## 3 SYSTEM DESIGN

In this section, we will propose the architecture of OneSparse, introduce the key innovation of intersection push-down and optimization we adopt to accelerate the search process and finally explain the reason of OneSparse's superior performance.

### 3.1 OneSparse Architecture

To support multi-index hybrid queries, OneSparse builds a unified index system by managing all kinds of data through posting lists. The architecture of one typical scenario (one sparse index + one dense index) of OneSparse is shown in Figure 2. OneSparse requires that all vector indices store vectors in a uniform posting-list based format.

---

For sparse data, OneSparse maintains one dimension of the sparse vectors (i.e., term) per inverted posting list, which allows fast lookup to all relevant documents of a word in a query. The values stored in an inverted posting list are pairs of ID and a singledimensional feature (e.g., term frequency). For dense vectors, OneSpArse clusters them into several posting lists by SPANN. Besides, it builds a SPTAG in-memory ANN index on cluster centroids to quickly navigate to the nearest SPANN posting lists. The values stored in a SPANN posting list are pairs of ID and dense vector in this cluster. All inverted posting lists and SPANN posting lists are saved on disk. The left two columns of Table 1 summarize the comparison of traditional inverted posting lists and SPANN posting lists.

---

OneSparse's solution is general and flexible. Although our implementation of OneSparse incorporates sparse and dense indices (Figure 2), the design of OneSparse is also able to support multiple sparse indices, multiple dense indices and other index combinations as long as they conform to the same posting-based format.

### 3.2 Intersection Push-down

By leveraging OneSparse's unified architecture, we decompose the Top $K$ interface of each index and push the intersection operation down to the posting list traversal process. As Figure 1 illustrates, compared with traditional isolated methods that intersecting the results of individual Top $K^{\prime}$ searches of all indices, OneSparse intersects vector candidates among inter-index posting lists during posting list traversal. Therefore, we can bypass the score computation (i.e., ANN distance calculation, BM25 score calculation, etc.) of data points which do not show up in candidate posting lists of all indices, enabling a great reduction of computation and I/O for heavy $\mathrm{Top} K$ calculations, and thus boosting query performance tremendously. Besides, by pushing-down intersection, we eliminate individual Top $K$ operations of traditional isolated methods, and thus the difficulty of identifying the optimal $K^{\prime}$ in the conventional method is also completely resolved.

---

However, it is hard to determine whether a data point can be discarded or not during the joint index traversal, since it remains uncertain whether a result located within one index will be retrieved from another index or not because the traversal patterns of ANN index and inverted index are not monotonous. Based on the feature of traditional inverted index which can perform intra-index intersection/union during the inverted lists traversal, we address the above problem by sorting elements in SPANN posting lists based on their IDs, which aligns with traditional inverted index (see Table 1). This allows us to perform fast multi-way inter-index posting list intersection together with intra-index union simultaneously during multi-index traversal, where when one index scans to a certain ID, BM25 score and ANN distance calculations for candidates smaller than this ID in the other index can be skipped, which results in significant computational savings. We call this fast multi-way merge algorithm.

---

Figure 3 illustrate the execution process of the fast multi-way merge algorithm. In Figure 3(a), OneSparse has four posting lists from two different indices. We take the union of list $_{11}$ and list $_{12}$ and the union of list $_{21}$ and list $_{22}$ (intra-index union), and meanwhile conduct intersection among the union results from two indices (inter-intersection). Since the IDs in each posting list are ordered, we can compute intra-index union and inter-index intersection on four lists simultaneously. For example, we assign a pointer to each candidate posting list, starting from the first column. Assuming IDs pointed by four pointers denoted as $c_{11}, c_{12}, c_{21}, c_{22}$, we compare the minimum pointed IDs from each index, which are $c_{1}=\min \left(c_{11}, c_{12}\right)$ and $c_{2}=\min \left(c_{21}, c_{22}\right)$.

---

If $c_{1}=c_{2}$, it means this candidate is recalled by all indices and can be considered as high-quality. Then, we compute its ranking score according to similarity functions and aggregate function and insert it into the a heap (marked with yellow in Figure 3). After that, we shift all pointers to this ID backward. As Figure 3(b) shows, we find that $c_{1}=c_{2}=1$, therefore we compute the score of the candidate whose ID $=1$ and put it into a heap. Then, move the pointers of list $_{11}$, list $_{12}$ and list $_{22}$ to the next candidate.

---

If $c_{1} \neq c_{2}$, we choose the maximum value of $c_{1}$ and $c_{2}$, denoted as $c_{\max }=\max \left(c_{1}, c_{2}\right)$. Then, all pointers can jump directly to the nearest candidate whose ID is no less than $c_{\max }$. We build skip lists on each posting list to accelerate this seek process. In this way, we can skip many pages read from the disk, reducing lots of I/O costs. As Figure 3(c) shows, we find that $c_{1}=2$ while $c_{2}=3$, therefore, we just shift pointers of list $_{11}$ and list $_{12}$ to the nearest candidate that its ID is no less than 3 , which are 10 and 5 , respectively.

---

Repeat the above process until there is an index that all pointers of its posting lists finish the traversal. Then, we can stop the search process, returning the final Top- $K$ results from the heap. As we can see, the execution time of OneSparse is associated with the index whose size of the union of its posting lists is the shortest. In contrast, isolated methods must wait until all indices returned Top $K^{\prime}$ results even though fast indices finished searching early.

### 3.3 Posting-list Compression

We find that if a SPANN posting list represents a small neighborhood, the centroid of this posting list can represent all dense vectors in it sufficiently and accurately enough in terms of calculating distances to the query vector. Therefore, to further accelerate the search process, OneSparse uses centroids to represent the original full-size vectors in the corresponding posting lists to greatly reduce disk usage. Besides, since we do not need to maintain original vectors in the index, the size of each posting list can be compressed significantly, which also reduces disk I/Os when traversing indices. More importantly, the distance between the query vector and the original data vector can be replaced by using the distance between the query vector and the centroid. This means that distance only needs to be calculated once per posting list regardless of the number of elements per posting list, which further saves computations significantly.

---

Interestingly, according to our experiments, using the default settings of SPANN with replication_count $=8$, the search accuracy drops significantly after compression (e.g. recall@100 drops from $91 \%$ to $84 \%$ ). This is because the replication leads to the growth of the cluster radius, which reduces the representative of the centroid. Therefore, we eliminate this side effect by setting replication_count $=1$ and increasing the number of centroids when building SPANN index. In this way, compression would only slightly affect search accuracy, but it reduces query latency tremendously. We will show these results in Section 5.2.

### 3.4 Efficiency of OneSparse

In this section, we will analyze the efficiency of OneSparse sparse and dense hybrid search based on the computation data flow graph in Figure 4. This will showcase the design benefits that contribute to OneSparse's superior performance compared with SPANN + Inverted Index isolated approach.

---

In a conventional isolated search solution (Figure 4(a)), to compute Top $K$ nearest results towards query $q$ in sparse and dense hybrid data-set, one first needs to locate $n_{1}$ dense-vector posting lists via SPTAG, and $n_{2}$ sparse-vector posting lists via term matching. Then, every element in posting lists is traversed and their scores will be computed as either ANN distances or BM25 scores. In total, there are $r_{1}$ ANN distance calculations and $r_{2}$ BM25 score calculations. After that, sorting is performed on $r_{1}$ and $r_{2}$ candidates respectively based on their scores, which produces Top $K^{\prime}$ and Top $K^{\prime \prime}$ results. Finally, the system intersects the $K^{\prime}$ and $K^{\prime \prime}$ candidates and sorts them again by the aggregate function, producing the final Top $K$ results. The computation cost of this process is shown in Equation 2.

$$
\begin{align*}
t_{1}= & T_{\text {locate }}+r_{1} \times T_{\text {distance }}+r_{2} \times T_{\text {bm25 }} \\
& +T_{\text {sort }}\left(r_{1}\right)+T_{\text {sort }}\left(r_{2}\right)  \tag{2}\\
& +T_{\text {intersect }}\left(K^{\prime}, K^{\prime \prime}\right)+T_{\text {sort }}\left(K^{\prime \prime \prime}\right)
\end{align*}
$$

where $T_{\text {locate }}$ is the SPTAG ANN centroid search and term matching cost, $T_{\text {distance }}$ is an ANN distance calculation cost, $T_{b m 25}$ is a BM25 score calculation cost, $T_{\text {sort }}(x)$ is the cost of sorting $x$, $T_{\text {intersect }}(x, y)$ is the cost of intersecting $x$ and $y$.

---

Figure 4(b) shows OneSparse's computation flow graph of sparse and dense hybrid search. First, OneSparse locates the same $n_{1}$ SPANN posting lists and $n_{2}$ inverted posting lists just like the conventional solution. Then it filters vectors from $n_{1}+n_{2}$ posting lists by fast multi-way merge algorithm introduced in 3.2 , which produces $m$ high-quality vector candidates. Then, it computes ANN distance and BM25 score of these $m$ candidates for the final ranking phase, returning final $K$ results. If we adopted compression optimization introduced in 3.3, then the number of ANN distance calculations would be further reduced to zero, since the distance between query and the centroid has already been computed during locating the nearest centroids by SPTAG. The computation cost of the above process is shown in Equation 3.

$$
\begin{aligned}
t_2= & T_{\text {locate }}+T_{\text {multi-way-merge }}\left(r_1, r_2\right) \\
& +m \times T_{\text {bm } 25}+T_{\text {sort }}(m)
\end{aligned}
$$

where $T_{\text {multi-way-merge }}\left(r_{1}, r_{2}\right)$ is the cost of fast multi-way merge algorithm of $r_{1}$ and $r_{2}$.

---

Since ANN distance and BM25 score calculation are expensive due to the high dimensionality, we can ignore the cheap intersection cost. Besides, after pre-filtering low-quality data by intersection push-down, OneSparse reduces the amount of the candidates needed to conduct ANN distance and BM25 score calculation by up to $99 \%$ according to our experiments, where is $r_{1} \gg m, r_{2} \gg m$. Therefore, we have:
$$
\left\{\begin{array}{l}
T_{\text {sort }}\left(r_{1}\right)+T_{\text {sort }}\left(r_{2}\right)+T_{\text {sort }}\left(K^{\prime \prime \prime}\right) \gg T_{\text {sort }}(m)  \tag{4}\\
r_{1} \times T_{\text {distance }}+r_{2} \times T_{\text {bm } 25} \gg m \times T_{\text {bm } 25}
\end{array}\right.
$$

Thus, $t_{2} \ll t_{1}$, which proves the superiority of OneSparse over traditional isolated methods in multi-index search performance.

## 4 IMPLEMENTATION

We implement OneSparse in our internal system and an open source system (i.e., Elasticsearch).

### 4.1 Index Building

For sparse textual data, we firstly perform a number of operations after tokenizing such as removing punctuation, lowercasing, stemming and stop word removal. These pre-processing operations enhance the quality and efficiency of the inverted index. After that, these tokens are inserted into inverted index successively like Lucene [16]. OneSparse supports two kinds of similarity function for sparse vectors, BM25 and IDFSum (i.e., the sum of IDF). When choosing BM25 as similarity function, we need to store the term frequency of tokens in the inverted lists. When choosing IDFSum score, however, there is no need to store it, which reduces the consumption of disk. Meanwhile, compared with BM25 score, IDFSum score also reduces the number of multiplications during the score computation, leading to sightly better search latency.

---

For dense data, SPANN [8] is applied to cluster vectors into several posting lists. Then, we parse the original SPANN posting lists and sort elements in each posting list by their IDs. If enabling compression, the original vectors will be discarded. Besides, the SPTAG index built during the construction of SPANN index is maintained in memory to accelerate the nearest posting lists search.

### 4.2 Query Processing

The query procedure can be divided into two steps. Firstly, it narrows down the search to a small number of nearest posting lists by matching terms via suffix tree search and SPANN posting lists by finding the nearest centroids via SPTAG. Then, during the finegrained traversal in candidate posting lists, fast multi-way merge algorithm will be performed.Since SPANN posting lists in OneSPARSE are formatted into the same pattern as inverted lists, the union and intersection process can be implemented by AND/OR operators natively supported by the original inverted index. During this stage, candidate that appears in both inverted lists and posting lists will be scored based on the given aggregate function and then push into the heap according to its aggregate score. Final TopK results will be popped from the heap as long as the intersection process is completed.

### 4.3 Implement in Elasticsearch

We also implement the compression version of OneSparse in Elasticsearch [14], a widely-used text search engine library utilizing inverted index to serve documents, which is built on Lucene [16].

---

For index building, Elasticsearch supports sparse data via inverted index natively, therefore, what we need to do is to transform dense data into posting lists so that we can use inverted index to serve it. We do this by firstly applying SPANN to cluster dense vectors into several posting lists, obtaining the cluster ID to which each dense vector belongs. We then insert this cluster information along with the corresponding sparse textual data into Elasticsearch, where they can be indexed using the inverted index.

---

During the search process, we initially utilize the in-memory SPTAG index to identify several nearest clusters. This step involves retrieving the cluster IDs and calculating the ANN distance between the cluster centroids and the dense vector of the query. Subsequently, we generate a boolean query considering the information obtained from the previous step, as well as the sparse constraint. This query incorporates the relevant cluster IDs, corresponding ANN distance and textual data to refine the search results further. Appendix A shows the example code of the boolean query. Such a boolean query simulate ONESPARSE's intra-index union and interindex intersection manner. After executing by Elasticsearch, we can get the final Top $K$ results.

## 5 EVALUATION

In this section, we evaluate ONESPARSE in comparison with state-of-the-art ANN search algorithms and hybrid search systems based on MS MARCO [3] and Natural Questions (NQ) [21] and demonstrate OneSparse has superior performance on sparse and dense hybrid queries.

### 5.1 Experiment Setup

### 5.1.1 Evaluation Platform.

We conduct all the experiments on a Windows Server running Microsoft Windows Server 2019 Datacenter, which has an Intel Xeon E5-2673 v3 CPU at 2400 MHz with a total of 16 CPU cores, 128 GB memory, and 1.74 TB HDD.

### 5.1.2 Data-set. We use two different data-sets:

- MS MARCO [3], a passage ranking data-set with $8,841,823$ passages in total and we choose evaluation data as test data, with a total of 6,980 queries.
- Natural Questions (NQ) [21], a question answering data-set with 152,027 documents in total and we choose evaluation data as test data, with a total of 7,830 queries.

We utilize coCondensor [17] to extract semantic information of sparse textual data and generate dense vectors. Then, we follow [2] to order-preserving transform the original vectors extracted from coCondensor into euclidean distance space by adding one dimension in order to satisfy the triangle inequality. Therefore, the final dimension of each dense vector is 769. The ground truth of the queries is provided by the data-set itself which was generated by humans to label relevant passages. For each query, we return Top-100 results from each tested algorithm.

### 5.1.3 Evaluation Metrics. 

We evaluate both the search accuracy and performance. Recall is a commonly used metric to measure the accuracy of query results against the ground truth. Given the ground truth result set $S$ and the query results $S^{\prime}$, recall is defined as $\frac{\left|S \cap S^{\prime}\right|}{|S|}$. It is widely used in both sparse and dense vector search systems. We report recall@100 in all experiments. To evaluate the search performance, we measure the average, 50 th percentile, 90 th percentile, and 99th percentile latency from each execution of the tested algorithms.

### 5.1.4 Evaluation Systems. 

We compare OneSparse in two systems: our own internal system and an open-source system Elasticsearch 8.7.0 [14]. In our internal system, we evaluate Inverted Index, SPANN, Inverted Index + SPANN with different $K^{\prime}$ and $K^{\prime \prime}$ and ONESPARSE with compression or not as well as using BM25 or IDFSum All algorithms in internal system are written in C++. In Elasticsearch, we evaluate Inverted Index, HNSW and Inverted Index + HNSW with different $K^{\prime}$, which are natively supported by Elasticsearch. Besides, we also test the compression version of ONESPARSE in Elasticsearch as introduced in Section 4.3. The aggregation function for all hybrid search is:
$$
\begin{equation*}
\text { score }=\lambda \times \frac{1}{1+\text {l2-distance }^{2}}+\text { bm25-score } \tag{5}
\end{equation*}
$$

where we set $\lambda=15,000$. More details of the settings of each algorithm can be found in Appendix B.

### 5.2 Experiment Results

Table 2 and Table 3 show the results of different algorithms on MS MARCO and NQ data-sets.

**Hybrid Search vs. Single-index Search.** Compared with the results from single-index retrieval, recall usually can be improved by leveraging both sparse and dense representations. This is because sparse features help to bridge the gap between dense vectors and real semantics caused by neural model loss. Interestingly, we found an exception: SPANN + Inverted Index has worse recall than SPANN on MS MARCO. This is because some ground truth results have relatively low BM25 scores and since MS MARCO is large, retrieving 20000 candidates from inverted index is not enough to recall them. In contrast, HNSW + Inverted Index in Elasticsearch and OnESparse return all passages that match the query keywords and thus, they can achieve higher search accuracy. As for search speed, traditional isolated methods usually perform much slower than single index search. The extra time comes from two parts, longer traversing time due to returning more results, and the extra intersection and sort time. However, we observe that in the Elasticsearch, HNSW + Inverted Index performs faster than only Inverted Index on MS MARCO. This speed improvement can be attributed to the candidates retrieved from $H N S W$, which help filter out low-quality documents during the search process in the inverted index. By employing techniques such as the weak AND algorithm, the number of BM25 score calculations can be significantly reduced, leading to faster search performance. However, this optimization effect may not be as significant for smaller data-sets like NQ. Consequently, in such cases, HNSW + Inverted Index may not offer a substantial speed advantage over using just the Inverted Index.

---

**Traditional Isolated Search vs. OneSparse.** We can see that OneSPARSE is much faster than the isolated algorithms (HNSW + Inverted Index and SPANN + Inverted Index) in all situations while maintains similar or even higher recall in our experiments. For example, in the internal system, OneSparse without compression is more than $4 \times$ faster than SPANN + Inverted Index on MS MARCO and $2 \times$ faster than SPANN + Inverted Index on NQ. The main reason for the superior performance is that we filter out more than $99 \%$ low-quality candidates in average before conducting ANN distance and BM25 score calculation by pushing down intersection. When applying compression optimization, we can further reduce the search latency without losing much accuracy. This is mainly because dense vectors in OneSparse SPANN posting list are close enough and centroids can already represent the relevance between query vector and original dense vectors. In return, compression can reduce the substantial cost of computing ANN distances and disk I/Os, leading to better search performance. In Elasticsearch, the compression version of OneSparse is about $20 \%$ faster than HNSW + Inverted Index on MS MARCO and NQ. Meanwhile, in the internal system, after compression, OneSparse is more than $6 \times$ faster than SPANN + Inverted Index on MS MARCO and $3 \times$ faster than SPANN + Inverted Index on NQ, which is $30 \%$ faster than before compression. We can also see that using IDFSum score have little impact on search accuracy. Therefore, in real application scenario, we choice to replace BM25 with IDFSum since the latter consumes less memory and disk and helps to slightly decrease the search latency due to the reduced number of multiplications.

---

The selection problem of $K^{\prime}$. OneSparse's design also eliminates the selection problem of $K^{\prime}$. For traditional isolated approaches, choosing larger $K^{\prime}$ results in higher recall but will also increase the search latency, just like the experiment results of HNSW + Inverted Index with two different $K^{\prime}$. Even we can find a good $K^{\prime}$ manually that balances the search accuracy and performance for a certain data-set, changing the data-set may also cause the optimal $K^{\prime}$ to change. For example, for NQ data-set, setting $K^{\prime}=1000$ and $K^{\prime \prime}=10000$ in the SPANN + Inverted Index can already ensure higher recall than single-index search while controlling the latency. However, for MS MARCO, such setting is no longer applicable since the data-set size is much larger than NQ and we need to retrieve more intermediate results from two separate indices in order to get better search accuracy. OneSparse, in contrast, is no need to choose $K^{\prime}$ and can preserve high recall and low latency in all situations.

## 6 APPLICATION

OneSparse unified index and retrieval system has been successfully deployed in Microsoft Bing web search and sponsored search to serve as a hybrid retrieval channel satisfying both Term-Match and Embedding-Match with hybrid re-ranking to improve the recall quality and performance.

---

For sponsored search scenario, OneSparse has been integrated as an indispensable component in retrieval system for more than 2 years. Revenue Per Mille (RPM) and Bad Ratio are selected as measurements to respectively estimate the revenue gain and searched ads quality of online $A / B$ testing flight. In details, RPM means the revenue gained for every thousand search requests which is the core KPI in sponsored search scenario, and Bad Ratio means the ratio of irrelevant ad impressions which are labeled by human experts as a quality metric. Online $A / B$ testing showed that OneSparse has achieved $+2.0 \%$ RPM gain and $-3.84 \%$ Bad Ratio improvement, which is very significant as the original production system is already very strong, integrating many other advanced techniques, e.g. Uni-retriever [34], TextGNN [36], etc. These metrics were tracked hourly, and statistical significance tests were applied to ensure the reliability of the observed improvements.

---

For web search scenario, we have obtained $5 \times+$ online latency gain with result quality on-par compared with the traditional isolated search solution after deployment of OneSparse solution.

## 7 CONCLUSION

This paper introduces OneSparse, a novel unified index system designed for efficiently performing multi-index vector search. It unifies SPANN posting lists and inverted posting lists together, supporting multi-index queries and multi-model ensemble queries. By pushing down the intersection across all indices, OneSparse can pre-filter over $99 \%$ low-quality candidates and reduce unnecessary computation. Moreover, optimizations like compression of SPANN posting lists further reduces disk I/Os and accelerates the search process. We implement OneSparse in our internal system and integrated it with Elasticsearch. Through evaluation on two data-sets involving sparse and dense hybrid queries, we show a performance gain of over $6 \times$ compared to isolated methods. OneSpArse has also been integrated into Microsoft online web search and advertising systems with $5 \times+$ latency gain for Bing web search and $2.0 \%$ Revenue Per Mille (RPM) gain for Bing sponsored search. We hope that OneSparse enables more retrieval systems to operate on multi-modal hybrid data-sets much more practically.

## B DETAILS OF EVALUATION SYSTEM

We describe the settings of each evaluated algorithm introduced in Section 5.1.4.

Elasticsearch [14]. We conduct all experiments on Elasticsearch 8.7.0 and all data are inserted into one shard. To get better search performance, we merge all of the indices into one segment by force_merge API after inserting data.

- Inverted Index. We apply Elasticsearch to build an inverted index over sparse textual data and rank the documents through BM25 scores. The hyper-parameters of BM25 score are default. We choose it as the baseline which utilizes sparse data only.
- HNSW.Elasticsearch supports Hierarchical Navigable Small World graphs (HNSW) [24] as the algorithm to index and search dense vectors. We choose it as the baseline of graph-based dense data search. We set $m=16$, ef_construction $=100$ when building index and set num_ candidates $=500$ when searching queries.
- Inverted Index + HNSW. Elasticsearch also supports multi-index hybrid retrieval by isolated searching. It first retrieves Top $K^{\prime}$ candidates from HNSW and uses them to filter the matches from inverted index, where those who do not appear in the Top $K^{\prime}$ results from HNSW will skip the BM25 score computation. After that, candidates will be scored by their euclidean distance and BM25 score through aggregate Equation 5, returning final Top100 results. During the search, we set $K^{\prime}=1000,2000$.
- OneSparse. We implement the compression version of OneSparse in Elasticsearch as introduced in Section 4.3. The cluster information was generated by SPANN. As introduced in Section 3.3, we set the replication count to 1 and select $50 \%$ data as head to preserve the representative of the centroids. Besides, we set TPTNumber $=128$ and CEF $=2000$ to get better HeadIndex quality. During the search, we firstly search 256 nearest posting centroids by SPTAG. For better cluster search results, we set EnableBfs $=3$ and NumberOfInitialDynamicPivots $=100$. After getting the 256 nearest posting centroid IDs, we generate a boolean query as describe in Section 4.3. Finally, we send it to Elasticsearch and get the final Top100 results. The final score function in boolean query is the same as Equation 5.

---

Internal System. In order to eliminate the impact of differences in ANN search algorithm, we also conduct experiments in our own system. To ensure fairness, all of the algorithms in this system are written in $\mathrm{C}++$.

- Inverted Index. We re-test Inverted Index for sparse-data-only search in our own $\mathrm{C}++$ version.
- SPANN. It is one of the foundation algorithms in OneSparse. We evaluate it to show better search accuracy by leveraging both sparse and dense features without ANN algorithm impact. We use the hyper-parameter settings reported in the SPANN paper [8] except that the posting page limit is expanded to 96 and the number of nearest postings to be searched is set to 64 .
- Inverted Index + SPANN. We implement this isolated algorithm by firstly retrieving Top $K^{\prime}$ candidates from SPANN (using the same settings as SPANN-only-search introduced above) and then intersecting them with Top $K^{\prime \prime}$ candidates retrieved from inverted index. After merging the candidates together, we re-rank candidates through the same aggregate function as Equation 5, returning Top100 results. Here, we set $K^{\prime}=1000,2000$ and $K^{\prime \prime}=10000,20000$.
- OneSparse. The SPANN settings used here is the same as introduced in Elasticsearch OneSparse. Besides, The final aggregate function is also the same as Equation 5 except that we test both bm25_score and idf_sum as similarity function for sparse data.

We exclude the vector fusion solution because it only supports decomposable similarity functions such as inner product, which restricts its use in real-world applications.
