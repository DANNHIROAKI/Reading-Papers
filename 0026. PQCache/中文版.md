# 0. ABSTRACT  

随着大语言模型（LLMs）领域的不断发展，推理中的上下文长度正在稳步增长。Key-Value Cache（KVCache）作为LLM推理的关键组件，由于GPU内存限制，现已成为主要的内存瓶颈。当前的方法通过选择性地确定适合的键和值用于自注意力计算，试图解决这一问题。然而，这些方法要么难以保持模型质量，要么导致较高的服务延迟。受数据库领域先进嵌入检索技术的启发，我们将KVCache的存储与搜索视为一种典型的嵌入检索问题，提出了PQCache方案。PQCache采用产品量化（Product Quantization，PQ）来管理KVCache，在保证模型质量的同时实现较低的服务延迟。

在预填充阶段，我们对每个LLM层和头部的token键应用PQ。在自回归解码阶段，对于每个新生成的token，我们首先通过使用PQ码和中心点进行的最大内积搜索（Maximum Inner-Product Search，MIPS）识别重要token，然后提取相应的键值对用于自注意力计算。通过精心设计的重叠与缓存机制，我们最大限度地减少了在两个阶段中的额外计算和通信开销。

广泛的实验表明，PQCache在效率和效果之间实现了良好平衡。即使仅有五分之一的token参与注意力计算，PQCache仍能保持模型质量，同时达到了可接受的系统延迟水平。

# 1. INTRODUCTION  

随着ChatGPT [40] 的出现，大语言模型（LLMs）作为实现通用人工智能（AGI）的有力候选者，吸引了研究人员和工程师的广泛关注。LLMs 在“下一词预测”任务中表现出色：它们以一系列 token 作为输入（也称为提示，prompt），在推理过程中以自回归方式生成后续的 token。LLMs 由 Transformer 层构成，其核心机制是自注意力模块。在该模块中，每个 token 会计算其“查询”（query）、“键”（key）和“值”（value）的表示。每个 token 的查询通过与之前 token（包括自身）的键交互生成注意力权重，随后使用这些权重对之前 token 的值进行加权求和。图2展示了 Transformer 层中典型的自注意力模块。

------

为了适应日益增长的提示长度，LLMs 的最大输入长度显著扩展，从 2K-4K [50, 52] 增加到 32K [25, 51]、128K [15, 40]，甚至达到数百万 token [2, 9, 31]。如图2所示，LLM 推理过程包括两个阶段：预填充（prefilling）和解码（decoding）。在预填充阶段，LLMs 处理长度较长的输入，并为所有输入 token 计算键和值。在解码阶段，LLMs 生成下一个新 token，并计算其键和值。为了避免重复计算，前面 token 的键和值通常会被缓存到键值缓存（Key-Value Cache, KVCache）中，供后续 token 的注意力计算使用。然而，随着提示长度的增长，KVCache 的内存消耗已远远超过单个 GPU 的内存容量，即使是如图1(a)所示的 7B 和 13B 规模的 LLMs 也不例外。这对现代 LLM 推理提出了巨大的挑战。

---

鉴于特定 token 对生成的影响显著，即它们的注意力权重远大于其他 token [33, 63]，许多方法选择性地在注意力机制中纳入这些 token，同时排除其他 token。这种方法旨在解决 KVCache 带来的内存挑战，通常被称为选择性注意力（selective attention）[37]。相关方法可分为两类：KVCache 丢弃（KVCache dropping）[33, 58, 63] 和 KVCache 卸载（KVCache offloading）[46, 57]。

------

然而，这些方法要么依赖于不恰当的假设，要么在推理过程中引入显著的延迟，难以同时实现效果和效率。KVCache 丢弃方法通过丢弃不必要的键值对，假设不重要的 token 对后续生成没有影响。然而，如图1(b)中注意力得分示例所示，许多具有较低平均注意力权重的 token 仍然可能对后续生成的 token 有贡献。已有研究[10, 29]也指出了直接丢弃方法的不足之处。KVCache 卸载方法（如 InfLLM [57] 和 SPARQ [46]）将 KVCache 存储在 CPU 上，并根据易于计算的代理得分为每个新生成的 token 提取相关键值对。InfLLM 将 KVCache 组织为块，并使用每个块中的代表性 token 计算相关性。然而，如图1(b)所示，我们未观察到 InfLLM 中的空间连续性假设。SPARQ 通过识别查询中具有较大幅度的维度，仅从所有键中提取这些维度以确定最相关的 token。尽管在使用大量维度时表现出效果，其通信开销却过高，且序列化的计算-通信过程限制了系统优化（如预取）的可能性。总之，现有方法在长上下文 LLM 推理中难以同时实现效果与效率。

---

我们明确指出，选择性注意力计算需要根据查询与键的乘积找到最相关的 top-𝑘 键值对，这本质上是嵌入检索场景中的最大内积搜索（MIPS）问题。嵌入检索是数据库与数据管理领域的常见研究方向，包括许多知名方法，如产品量化（Product Quantization, PQ）[18, 24]、倒排索引（inverted index）[6, 7] 和基于图的方法 [23, 35]。这些方法主要涉及两项操作：（1）索引构建：将候选嵌入组织为结构化的索引；（2）检索：为给定的查询嵌入高效检索出最相关的 top-𝑘 嵌入。令人惊讶的是，我们发现 LLM 推理过程中选择性注意力计算的流程可以映射为上述两项操作。具体而言，预填充阶段生成大部分 KVCache 并构建索引，而解码阶段根据新生成的 token 检索相关键值并更新 KVCache。

------

受高级嵌入检索技术的启发，本文提出了 PQCache，以确保长上下文 LLM 推理中的效果与效率。在预填充阶段，我们生成 KVCache，将其存储在 CPU 内存中，并构建索引结构。在解码阶段，我们高效检索相关键值对以用于自注意力计算，并更新 KVCache。鉴于 LLM 推理的延迟需求，我们无法使用索引构建开销较大的方法，例如基于图的方法或复杂的倒排索引方法。我们利用嵌入检索中的低成本产品量化（PQ）[24, 23]，其基本思想是将嵌入分成子嵌入并进行聚类。PQCache 的关键思想是利用前序 token 的键构建 PQ 码本，并执行 MIPS 检索相关键值对以进行后续的自注意力计算。我们基于 PQ 提出了一种系统-算法协同设计方法，既利用其高召回潜力，也挖掘系统优化的机会。进一步的实验分析表明，与现有方法相比，PQCache 将 LongBench 分数提升了最高 6.21 分，同时实现了可接受的系统延迟。

---

据我们所知，这是首个将嵌入检索技术应用于解决 KVCache 内存挑战的研究工作。PQ 提供了一种对嵌入向量（及其内积）的良好近似，同时仅消耗少量内存。在预填充阶段，我们对每一层和每个头生成的键应用 PQ，并通过在 CPU 上的聚类生成 PQ 码和中心点。在每次自回归解码步骤中，我们对分区后的查询与 PQ 中心点进行内积运算，然后结合 PQ 码获取近似注意力权重。利用该近似值，我们从 CPU 内存中检索 top-𝑘 相关键值对用于自注意力计算，而无需访问整个 KVCache。

------

为了实现高效的 LLM 推理，我们精心设计了 PQCache 系统以减少延迟。我们尽可能多地实现预取和重叠操作：KVCache 卸载、PQ 构建以及 PQ 码和中心点的获取均与 LLM 计算重叠。此外，为了最大化 GPU 内存的利用率并最小化 CPU-GPU 通信，我们在 GPU 上引入了块级缓存，专门用于频繁访问的键值对。

------

我们总结了以下贡献：

- 我们将嵌入检索技术 PQ 融入 KVCache 管理，支持高效且有效的 LLM 推理。
- 我们提出了系统-算法协同设计方法 PQCache，通过精心设计的重叠和缓存机制，实现了对给定查询的 top-𝑘 相关键的近似检索。
- 我们通过广泛的实验评估了 PQCache。即使仅有五分之一的 token 参与注意力计算，PQCache 仍能保持模型质量，并实现了可接受的系统延迟。

# 2. PRELIMINARY  

In this section, we introduce fundamental concepts related to LLM, PQ, and the memory hierarchy.  

## 2.1. Large Language Model Inference  

LLM 推理的概述如图 2 所示。一个 LLM 由多层堆叠的 Transformer 层组成，同时包含用于输入的词汇嵌入和用于输出的 token 分类器。自注意力模块是 Transformer 层中的关键组件，能够在不同 token 之间实现交互和信息聚合。多头注意力（Multi-Head Attention, MHA）和分组查询注意力（Grouped-Query Attention, GQA）[3] 是自注意力模块的主要变体。按照表 1 中的符号表示，注意力模块接收的输入形状为 $(n, s, d)$。在 MHA 中，输入会分别投影并转置为查询（query）、键（key）和值（value），生成的形状为 $(n, h, s, d_h)$，其中通常满足 $d = h * d_h$ 的关系。不同的注意力头（head）旨在捕获不同的语义信息。注意力机制将查询和键相乘，应用下三角掩码（lower-triangular mask）以限制查询仅关注前面的键，然后执行 softmax 操作，得到形状为 $(n, h, s, s)$ 的注意力得分。注意力得分随后用于对值进行加权求和，生成形状为 $(n, h, s, d_h)$ 的输出，最后重塑为 $(n, s, d)$ 的形状。为减轻内存和计算负担，GQA 对键和值使用了较少的注意力头数 $h_{kv}$，使其形状变为 $(n, h_{kv}, s, d_h)$。在这种设置下，每对键值对应于多个查询。

------

在 LLM 推理过程中，模型以自回归的方式每次生成一个新 token。LLM 的首次遍历和后续遍历分别称为“预填充”（prefilling）和“解码”（decoding），如图 2 所示。在预填充阶段，自注意力模块计算所有输入 token 的查询、键和值，并将键值对存储为 KVCache 以备后续使用。在自回归解码阶段，注意力模块仅计算最后生成的 token 的查询、键和值，利用 KVCache 中存储的前序键和值，计算形状为 $(n, h, 1, s)$ 的注意力得分。同时，新生成的键和值会被添加到 KVCache 中。由此，KVCache 的内存消耗随序列长度线性增长，这在长上下文 LLM 推理场景中导致了内存瓶颈问题。

## 2.2. Product Quantization  

PQ [24] 被提出用于高效的近似最近邻搜索（Approximate Nearest Neighbor Search, ANNS），在给定查询嵌入的情况下，从大量候选嵌入中检索相关嵌入。最大内积搜索（MIPS）是 ANNS 的一种特例，它使用内积作为相似性度量。如图 3 所示，PQ 将每个候选嵌入划分为 $m$ 个分区，本质上是将原始嵌入空间分解为 $m$ 个独立的子空间。每个子空间通过 K-Means 聚类对子嵌入进行分组，生成 $2^b$ 个中心点。每个嵌入被分配 $m$ 个码，每个码占用 $b$ 比特，对应于这些中心点。这些紧凑的 PQ 码能够在减少内存需求的情况下重构近似嵌入。在 ANNS 过程中，查询嵌入计算与中心点的相似性，并利用 PQ 码聚合相似性，从而避免对所有嵌入进行完整的相似性计算。

------

PQ 对 ANNS 产生了深远影响，其原理已被整合到各种高效的 ANNS 方法中 [6, 23, 27]。PQ 有多个变体，包括优化 PQ（Optimized PQ）[18]、残差量化（Residual Quantization）[36] 和 SCaNN [19]。尽管 PQ 最初是为 ANNS 设计的，但其变体也被应用于各种学习任务 [30, 53, 61]，以实现有效的压缩和高效的计算。

## 2.3. GPU-CPU Memory Hierarchy  

现代深度学习任务高度依赖 GPU 来执行计算密集型操作。GPU-CPU 结构形成了典型的存储层次：成本更高的 GPU 内存为计算提供更快的内存 I/O 速度，而通过 PCIe 或 NVLink 连接的 CPU 内存则具有较低的带宽。随着模型参数的增长以及对中间结果（如 KVCache）存储需求的增加，CPU 常被用来分担内存负载。大量机器学习系统的研究提出将某些模型参数或激活值卸载到 CPU 内存 [39, 42, 45, 47]，从而提升以 GPU 为中心的深度学习任务的整体性能。此背景下的主要挑战是如何有效地调度内存 I/O（即 GPU-CPU 通信）与 GPU 计算协同进行，从而高效隐藏相关的开销。

# 3. PQCACHE  

在本节中，我们介绍了 **PQCache**，这是一种创新的系统-算法协同设计方法，旨在实现具有大规模 KVCache 的高效长上下文 LLM 推理。图 4 展示了 PQCache 的整体概览：在预填充阶段生成的 KVCache 首先被卸载到 CPU，并通过 PQ 进行压缩，然后在解码阶段通过 MIPS 按需提取。

## 3.1.  Overview  

我们设计了 **PQCache** 来将所有 KVCache 保存在 CPU 中，并选择性地提取相关的键值对用于自注意力计算。在长上下文推理场景中，整个 KVCache 对于注意力计算和内存层次结构中的 I/O 通信来说过于庞大。因此，一种常见的技术是仅对键值对的子集执行注意力计算，这一过程被称为“选择性注意力”。根据先前研究 [1, 17, 33, 44, 46, 55, 57, 59, 63]，注意力得分是衡量前序 token 重要性或相关性的一种合理指标。如图 5 所示，我们在 XSUM 数据集 [38] 的一个示例中随机选择了几个位置，并绘制了注意力得分的分布。这些注意力得分通常遵循幂律分布，表明少数 token 比大多数其他 token 更为重要。因此，我们可以仅包含得分较高的 token 进行自注意力计算。遵循之前的研究 [20, 58, 63]，我们还将初始 token 和最近的 token（称为局部 token）纳入注意力计算。

------

如 2.1 节详细所述，注意力得分是通过对当前查询与前序键的乘积应用 softmax 函数计算得出的。识别得分最高的 top-𝑘 键的过程本质上构成了一个最大内积搜索（MIPS）操作。因此，我们尝试利用嵌入检索技术实现有效的选择性注意力，并解决 KVCache 的内存问题。基于上述观察，我们设计了 **PQCache**，将所有 KVCache 卸载到 CPU，并在解码阶段仅提取相关 token 的键值对。计算所有前序 token 的精确注意力得分会涉及高昂的 I/O 通信成本，这在长上下文 LLM 推理中是不可接受的。受近似最近邻搜索（ANNS）[6, 23, 27] 的启发，我们采用轻量级的产品量化（PQ）方法 [24]，通过分区和 K-Means 聚类对向量进行压缩。尽管其他 ANNS 方法（例如基于图的方法 [13, 23, 35]）可以实现更好的召回性能，但它们在构建过程中的计算开销过高，可能会阻碍 LLM 推理的效率。

---

在 **PQCache** 中，我们在预填充阶段构建 PQ，并在解码阶段使用 PQ。在预填充阶段，我们需要为自注意力模块计算所有输入 token 的键和值。在获得形状为 $\left(n, h_{kv}, s, d_h\right)$ 的键之后，我们可以为每个样本和每个头部构建 PQ。具体来说，对于形状为 $\left(s, d_h\right)$ 的张量，我们进一步将维度 $d_h$ 划分为 $m$ 个子空间，每个子空间的维度为 $d_m$。对于划分后的向量 $\left(m, s, d_m\right)$（其中 $d_m = d_h / m$），我们分别对每组执行 K-Means 聚类，得到形状为 $\left(m, 2^b, d_m\right)$ 的中心点和形状为 $(s, m)$ 的 PQ 码。每个 PQ 码表示向量所属的聚类，仅消耗 $b$ 比特来存储。

------

在解码阶段，我们首先对查询向量和 PQ 中心点进行矩阵乘法运算，然后根据 PQ 码聚合所有 token 的结果。通过近似得分（softmax 前的得分），可以确定最相关的 top-𝑘 token。从 CPU 中提取近似的 top-𝑘 键值对后，自注意力计算继续使用检索到的 token。与普通的嵌入检索任务不同，在 LLM 推理中，新生成的键和值会添加到 KVCache 中。这些 token 首先被视为局部 token 并保存在 GPU 中。当它们从局部 token 的滑动窗口中被逐出时，会根据与最近中心点的距离分配 PQ 码。

## 3.2. Complexity Analysis  

在预填充阶段，我们不修改注意力计算，因此时间和内存的复杂度保持不变。额外的 K-Means 聚类过程的平均复杂度为 $O\left(s \cdot m \cdot 2^b \cdot T\right)$，其中 $T$ 是 K-Means 的迭代次数。我们利用空闲的 CPU 资源来执行 K-Means，这在第 3.3 节中有详细说明。在解码阶段，原始注意力计算的时间复杂度为 $O\left(s \cdot d + d^2\right)$。在 PQCache 中，我们首先对 PQ 中心点进行乘法运算，时间复杂度为 $O\left(s \cdot m + d^2\right)$，然后计算注意力，时间复杂度为 $O(k \cdot d)$。内存复杂度为 $O\left(s \cdot m + 2^b \cdot d\right)$，包括 PQ 中心点和 PQ 码。考虑到 $m \ll d$，$k \ll s$，且 $b$ 通常小于 10，时间复杂度 $O\left(s \cdot m + k \cdot d + d^2\right)$ 和内存复杂度都远小于原始复杂度。

------

为了促进高效的长上下文 LLM 推理，PQCache 的设计目标是提供与额外开销无关的服务。图 6 展示了启用 PQCache 的 LLM 推理中涉及的计算和通信，涵盖预填充和解码阶段。原始 LLM 的计算用蓝色表示，而 PQCache 引入的计算和通信分别用绿色和红色表示，可分为四个部分：(1) KVCache 卸载和 PQ 结构的获取；(2) 使用 K-Means 聚类构建 PQ；(3) 近似 top-𝑘 的计算；(4) top-𝑘 相关 token 的键值对提取过程。如图 6 所示，除了低成本的 top-𝑘 近似计算，我们采用了独特的系统设计来消除这些计算或通信的开销。具体的系统设计将在后续章节详细说明。

## 3.3. Prefilling Phase  

在预填充阶段，在每一层获取输入 token 的键和值后，它们可以同时用于注意力计算和卸载到 CPU。如图 7 所示，鉴于注意力计算时间复杂度随序列长度呈二次增长，而通信时间复杂度呈线性增长，在长上下文场景中，通信可以完全与计算重叠。

------

根据第 3.2 节所述，K-Means 聚类具有较高的复杂性。为了实现与开销无关的推理，我们旨在充分利用空闲的 CPU 资源进行聚类。然而，如图 7 所示，在 CPU 上进行聚类（包括为每层的所有头构建 PQ）的过程比 GPU 在预填充阶段进行单层 Transformer 的计算时间更长。这是因为过去几十年间 GPU 的计算能力增长迅速，而 CPU 并非专为计算密集型任务设计。为了解决这一问题，我们提出了一种自适应 K-Means 聚类过程，通过限制聚类迭代次数，确保聚类过程可以与 GPU 的计算重叠。对于任何给定的模型和设备，我们会分析不同序列长度下单层 Transformer 的计算时间和 K-Means 聚类时间的关系。通过建模计算时间与序列长度的关系，可以确定在任何给定序列长度下可以与计算重叠的最大 K-Means 迭代次数。在第 4.3.3 节中，我们对不同聚类迭代次数的效率与模型质量之间的权衡进行了实证研究。

## 3.4. Decoding Phase  

在解码阶段，构建好的 PQ 结构需要被每一层的注意力模块使用。当前一层的计算正在进行时，可以并行预取当前层的 PQ 中心点和 PQ 码。由于 PQ 结构的内存占用极低（见第 3.2 节），其通信可以直接与解码阶段的计算重叠。

------

在整个推理过程中，唯一无法重叠的通信是 top-𝑘 相关 token 的检索，因为它依赖于前一步的 PQ 近似计算。受先前研究 [33, 57, 63] 的启发，推理过程中存在一些始终重要的关键 token。因此，我们在 GPU 上为这些 token 保留一个缓存。参考 LM-Infinite [20] 和 StreamingLLM [58]，我们首先将初始 token 和局部 token 保存在缓存中。对于其余 token，我们采用块级缓存策略，缓存结构位于 CPU 上，而存储分配在 GPU 上。通过构建 token 块来减少缓存开销，并将这些块缓存在 GPU 中。在每一步解码中，我们识别包含 top-𝑘缓存命中 token 的块，并利用它们提取 token，同时更新缓存结构。我们采用异步更新机制，以避免额外的开销。第 4.3.4 节中的实验结果展示了缓存命中率，该命中率有助于减少整体通信量。

# 4. EXPERIMENTS  

In this section, we conduct experiments and compare PQCache with existing methods. We experimentally show that PQCache achieves both effectiveness and efficiency.  

## 4.1. Experimental Setup  

### 4.1.1 模型

我们使用两个具有代表性的开源 LLM 进行实验：LLaMA-2-7B-Chat [52] 和 Mistral-7B-Instructv0.2 [25]。前者采用多头注意力（MHA），支持 4K 的上下文长度；后者使用分组查询注意力（GQA），支持 32K 的上下文长度。两种模型具有相似的 LLM 架构。我们对两种模型均使用 FP16，这是 LLM 推理中的常见实践。

### 4.1.2 任务

我们在 LongBench [5] 上评估 PQCache，这是一种广泛使用的长上下文 LLM 推理基准。由于模型主要在英文数据上进行预训练，我们评估了基准中的所有英文任务。这些任务包括文档问答、摘要、少样本学习以及段落检索。LongBench 中的样本平均输入 token 长度为 8K。

------

我们还对两个额外任务进行了实验：Needle-in-a-Haystack [28] 和 GSM8k Chain-of-Thought (CoT) 推理 [56]。Needle-in-a-Haystack 测试评估了长上下文 LLM 的上下文检索能力，要求模型从一篇冗长的文档中检索一个随机事实或语句。在实验中，我们考虑的文档长度最高达到 30K。GSM8k 是一个数学推理数据集，包含 8K 个高质量、多样化的小学数学问题。其 CoT 变体是一个复杂的推理任务，需要模型关注广泛的上下文细节以获得准确答案，平均输入长度为 3.7K。

### 4.1.3 基线方法

我们选取 H2O [63]、SPARQ [46] 和 InfLLM [57] 作为基线方法。H2O 是最广泛使用的 KVCache 丢弃方法，并且是许多改进的基础。SPARQ 和 InfLLM 是 KVCache 卸载的最新方法。此外，我们还考虑了一种为每个头检索精确 top-𝑘 token 的方法，记为 Oracle。在实验中，我们对 Oracle、SPARQ、InfLLM 和 PQCache 对选择性注意力的 token 数量以及数据传输量进行了对齐，以确保公平比较。对于 H2O，我们允许其关注更多 token，以匹配其他方法中选定键值对的内存使用量和数据传输量，这与 SPARQ 的实验设置一致。我们将此基线称为 H2O(C)，其中 “C” 表示补偿（Compensation）。

### 4.1.4 硬件环境与超参数

我们在 NVIDIA A800 40GB GPU 卡上进行所有实验。大多数超参数是根据 token 数量和数据传输量确定的。默认情况下，我们对 PQ 使用 $m=2$ 和 $b=6$。对于其他超参数，我们与相关论文或开源代码中的设置保持一致。

## 4.2. Model Performance  

### 4.2.1. LongBench.  

两种 LLM 模型在 LongBench 上的实验结果如表 2 和表 3 所示。LongBench 针对每个数据集使用不同的指标，并通过计算平均分数来衡量整体性能。我们分别考虑在选择性注意力中包含输入 token 的 $1/5$ 和 $1/10$，并额外传输相当于 KVCache 内存 $1/128$ 的数据量：对于 PQCache，我们使用 $m=2$ 和 $b=6$，满足 $2 \times 6 / 16 / 128 < 1 / 128$；对于 SPARQ，考虑到 $d_h=128$，我们使用 $r=1$；对于 InfLLM，我们从每 128 个 token 中选取 1 个代表 token。$\mathrm{H} 2 \mathrm{O}(\mathrm{C})$ 按照第 4.1.3 节的介绍允许关注更多 token。除 Oracle 外，每种设置下的最佳结果用粗体标出。

------

总体来看，未压缩模型（记为 Full）由于几乎没有信息丢失，能够实现最佳结果（在 LongBench 中，当序列长度超过模型的最大上下文长度时，仅使用初始和最后的 token [5]，这会导致信息丢失）。PQCache 在大多数数据集上优于主要基线方法（即 H2O(C)、InfLLM 和 SPARQ）。虽然 PQCache 在少数情况下得分略低，但平均来看表现出显著的提升。具体而言，PQCache 在 Mistral-7B 上分别实现了 +3.88 和 +6.21 的提升，在 LLaMa2-7B 上分别实现了 +1.61 和 +1.60 的提升。需要注意的是，基于卸载的对比方法 InfLLM 和 SPARQ 平均表现最差，这主要是由于为了最小化延迟而减少了额外通信量。相比之下，PQCache 在相同约束下表现良好，验证了我们工作的优势。

------

Oracle 是一种理想方法，能够为选择性注意力精确检索 top-𝑘 token，在大多数情况下表现出色。然而，我们观察到在“1/5#Tokens”的情况下，PQCache 甚至优于 Oracle，并与未压缩方法的分数持平。这表明聚类可能帮助 PQCache 发掘 KVCache 潜在空间中的内在结构，从而带来出色的结果。此外，尽管通常预期 token 数量减少会导致性能下降，但在某些情况下出现了相反的结果。这可能是因为并非所有 token 对生成新 token 都有用，去除不必要的 token 反而可能增强推理性能。

### 4.2.2. Needle-in-a-Haystack.  

我们在此测试中使用 Mistral-7B-inst-v0.2，并采用常见的设置 [4]：将保罗·格雷厄姆（Paul Graham）的文章作为“干草堆”（haystack），将句子“The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.” 作为“针”（needle）。在每次实验中，我们在选择性注意力中包含 $1/5$ 的 token 数量，并传输额外 $1/128$ 的数据量。结果如图 8 所示，其中 $x$ 轴表示“干草堆”的长度，$y$ 轴表示“针”隐藏的位置。颜色越绿表示准确率越高。

------

在所有方法中，PQCache 达到了最佳性能，几乎在所有场景中成功定位“针”。然而，主要的基线方法在大量情况下未能检索到“针”。特别是 InfLLM，在大多数情况下难以找到“针”，这可能是因为其依赖于块划分，而“针”未被视为代表性 token。只有当“针”位于初始或局部 token 中时（我们将其纳入注意力范围），它才能成功定位“针”。

### 4.2.3. GSM8k CoT Reasoning.  

我们在该任务中使用 Mistral-7B-inst-v0.2 模型。我们的提示策略包括每个样本 8 个带有 9 步推理的问题，以及 2 个带有 8 步推理的问题，这是长上下文推理的常见设置 [14]。我们使用 $1 / 128$ 的额外通信量。如图 9(a) 所示，在不同的 token 数量下，PQCache 始终优于 H2O、SPARQ 和 InfLLM。在某些结果中，PQCache 的表现甚至超过了未压缩模型，这表明仅使用部分 token 可能会带来性能提升。与以往研究 [29] 的发现相反，我们观察到 H2O 在高级 Mistral 模型中表现良好。当使用 $1 / 10$ 的 token 数量时，$\mathrm{H} 2 \mathrm{O}(\mathrm{C})$ 的表现优于 PQCache，因为它被允许访问更多的 token。

### 4.2.4. Impact of Extra Communication.  

我们研究了额外通信量对模型在 HotPotQA 数据集上的性能影响，如图 9(b) 所示。在固定使用 $1/10$ 的 token 的情况下，随着额外通信量从 KVCache 内存的 $1/128$ 增加到 $1/16$，InfLLM 和 PQCache 的性能相对稳定，而 SPARQ 的性能则稳步提升。PQCache 始终保持高分，当通信量不超过 KVCache 的 $1/32$ 时，其表现优于其他方法。正如第 4.3.2 节所示，在 $1/128$ 的通信情况下，SPARQ 已经带来了显著的延迟，即使在 $1/16$ 的情况下表现良好，其延迟也变得越来越不可接受。在低通信场景中（更适合实际应用），PQCache 实现了最佳的模型性能。

### 4.2.5. Impact of PQ Configuration  

我们评估了 PQ 配置的效果。由于 PQ 的内存消耗为 $O(s \cdot m \cdot b + 2^b \cdot d_h)$，其中第二项可以忽略不计，我们在保持其乘积（通信量）几乎不变的情况下调整 $m$ 和 $b$ 的值。图 9(c) 展示了在 HotPotQA 和 Qasper 数据集上的结果，在选择性注意力中使用 $1/10$ 的 token，图例表示 $m \times b$。所有配置均表现良好。其中，$m = 2$ 的配置结果最为稳定，提供了足够数量的中心点。尽管 $b = 8$ 的配置表现更优，但我们在实践中发现，$b = 6$ 提供了更稳定的延迟和更低的内存使用，同时仍然能实现令人满意的模型性能。因此，我们选择 $b = 6$ 作为默认配置。

## 4.3. Efficiency  

### 4.3.1. Prefilling 

在 **PQCache** 中，K-Means 聚类与 GPU 计算并行进行。虽然它不会影响第一个 token 的生成，但后续 token 的生成依赖于聚类结果。为评估系统优化效果，我们使用 **Time To 2nd Token (TT2T)** 作为指标，考量从查询进入到 LLM 输出的时间以及 KVCache 管理的开销。如图 10(a) 所示，通过重叠和自适应聚类，PQCache 实现了最低的 TT2T。所有基线方法都存在显著的开销。

---

由于 H2O 在预填充阶段收集注意力得分，它无法利用 FlashAttention 进行加速，并在处理较长输入时遇到 OOM（内存不足）问题。SPARQ 没有预填充开销，但其解码过程较慢（见第 4.3.2 节）。InfLLM 因为需要为块级 KVCache 管理进行设置而产生时间开销。按照每分钟 250 字（≈333 个 token）[64] 的速率，H2O 避免了额外的通信开销，而 InfLLM 和 PQCache 都利用了系统优化来加速解码。InfLLM 的块级 token 管理允许它高效地从 CPU 收集数据；然而，这种块级假设对模型的整体质量产生了负面影响。PQCache 结合了预取和缓存机制，在不降低模型质量的情况下实现了可接受的 TT2T。

### 4.3.2. Decoding  

**每个输出 token 的时间 (Time Per Output Token, TPOT)** 用于衡量每一步解码的时间。我们在图 10(b) 中比较了 H2O、SPARQ、InfLLM 和 PQCache 的 TPOT。在实验中，我们在选择性注意力中使用了 $1/5$ 的 token 数量，并设置了 4096-token 的 GPU 缓存。由于 SPARQ 的计算和通信是顺序执行的，其通信开销随输入序列长度线性增长，因此其延迟最高。其他方法的每 token 延迟均快于人类的阅读速度（大约为 250 字/分钟，即约 333 token/分钟）。

### 4.3.3. Trade-off between Time and Accuracy.  

在第 3.3 节中，我们设计了一种自适应的 K-Means 聚类策略以消除延迟。为研究该策略对模型准确率的影响，我们在 HotpotQA 数据集上进行了实验，实验中选择性注意力涉及 $1/10$ 的 token。如图 10(c) 所示，自适应策略在保持足够模型质量的同时，实现了最低的聚类时间。尽管增加聚类迭代次数可以提高得分，但随之而来的推理延迟显著增加。在时间与准确率的权衡下，自适应策略是实际应用中最为实用的选择。此外，我们提供了一个接口，允许用户自行设置聚类迭代次数，从而根据其具体需求在模型性能与延迟之间取得平衡。

### 4.3.4. Cache Hit-rate  

我们评估了最近最少使用（Least Recently Used, LRU）和最少频率使用（Least Frequently Used, LFU）策略在不同数量的 top-$k_{\text{cache}}$ 块参与解码过程中的缓存命中率。实验在 HotpotQA 数据集上进行，选择性注意力涉及 $1/10$ 的 token，GPU 缓存中包含 4096 个 token（每块 128 个 token）。如图 10(d) 所示，LRU 和 LFU 的性能相似，在不同的块数下均达到了约 0.5 的命中率。随着块数的增加，命中率最初会因更多 token 能够在块中被找到而上升。然而，当块中较少命中的内容更新缓存结构时，命中率最终会下降，从而扰乱正常的缓存逻辑。在实际应用中，我们将块数设置为 32，此时命中率约为 0.6，可减少 60% 的通信量。

# 5. RELATED WORK  

**KVCache 的选择性注意力**
 为了消除内存密集型 KVCache 的影响，一些方法在 LLM 推理过程中仅包含必要的 token 进行注意力计算。一种方式是丢弃不必要的 token。例如，LM-Infinite [20] 和 Streaming-LLM [58] 仅保留初始 token 和最近的 token。H2O [63] 和 Scissorhands [33] 利用注意力得分识别重要的 token。后续研究 [1, 17, 44, 55] 探索了自适应的 token 选择和额外指标以提高模型准确性。LLMLingua 系列 [26, 41] 利用一个辅助的小模型判断哪些 token 是必要的。由于 token 级压缩以贪婪方式剔除 token，在后续解码阶段的信息丢失可能导致模型性能下降。另一种方式是在解码阶段按需提取相关的 token。SPARQ [46] 和 InfLLM [57] 将 KVCache 卸载到 CPU，并为每次注意力计算选择性地提取相关的键值对。PQCache 也属于这一类别，与现有技术相比，展示了高效且有效的 LLM 推理性能。

------

**KVCache 的量化**
 量化可直接应用于整个 KVCache [11, 21, 34]，这种直接的方法能保持较好的模型质量。其他压缩技术也可以用于解决量化引入的残差问题 [29]。值得注意的是，量化与 token 重要性是正交的，最近的研究已经探索了将两者结合的方法 [59]。

------

**KVCache 的调度**
 解决 KVCache 内存挑战的另一种方式是精细调度 KVCache 在内存层次结构中的位置。FlexGen [47] 使用线性规划调度通信，寻找存储和访问张量的高效模式。AttentionScore [16] 维护了一个分层 KV 缓存系统，支持在多轮对话中高效复用 KVCache。另一个相关的研究方向是用于 LLM 服务的 KVCache 流式处理 [32, 49]，它涉及在更复杂的内存层次中处理多个请求。

------

**嵌入管理**
 嵌入管理是数据库和数据管理领域的一个常见研究方向，包括嵌入压缩 [48, 60, 62]、嵌入检索 [22, 54] 和键值存储 [8, 43]。我们的工作为将经典的嵌入管理方法集成到 LLM 生态中提供了一个潜在的研究方向。

# 6. CONCLUSION  

在本文中，我们提出了 **PQCache**，一种系统与算法协同设计的方法，用于高效且有效的长上下文 LLM 推理。我们引入了嵌入检索技术 PQ，减少了内存和计算负担，并利用 PQ 码和中心点实现了注意力模块中重要 token 的高效 MIPS。通过精心设计的重叠与缓存机制，我们将开销降至可忽略的水平。我们在广泛的实验中评估了 PQCache，结果表明，PQCache 即使仅使用 1/5 的 token 参与注意力计算，也能够有效保持模型质量，同时实现了可接受的系统延迟。

---

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241206010334790.png" alt="image-20241206010334790" style="zoom:60%;" /> 

---

**图 1：观察结果**
 左图显示了 LLM 的 KVCache 内存消耗情况（关于 MHA 和 GQA 的含义，请参见第 2.1 节）。右图展示了 MultiNews 数据集 [12] 上注意力得分的一个示例，颜色越深表示得分越高。

**图 2：LLM 推理概览**
 左侧部分展示了自注意力模块的计算过程，其中“Q”、“K”、“V”、“AS”和“O”分别表示查询（query）、键（key）、值（value）、注意力得分（attention score）和输出（output）。右侧部分描绘了 LLM 的推理过程，包括预填充阶段和解码阶段，其中“Attn”和“FFN”分别表示注意力层和前馈网络层。数学符号的详细定义见表 1。

**表 2：Mistral-7B-inst-v0.2 GQA 模型（32K 上下文长度）的 LongBench 评估**
 InfLLM、SPARQ 和 PQCache 均涉及额外通信，通信量为 KVCache 内存的 1/128，用于预计算相关性；H2O 关注的 token 更多，其内存使用量等于其他方法选择的 token 和传输数据量。

**表 3：LLaMA-2-7B-Chat MHA 模型（4K 上下文长度）的 LongBench 评估**
 InfLLM、SPARQ 和 PQCache 均涉及额外通信，通信量为 KVCache 内存的 1/128，用于预计算相关性；H2O 关注的 token 更多，其内存使用量等于其他方法选择的 token 和传输数据量。