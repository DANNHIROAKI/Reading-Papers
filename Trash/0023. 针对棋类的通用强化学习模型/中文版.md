# 0. Abstract

围棋是人工智能历史上研究最久的领域之一。最强大的围棋程序是==基于复杂的搜索技术==、==特定领域的适应性==以及==经过人类专家数十年优化的手工==评价函数的结合。然而，与之形成对比的是，AlphaGo Zero程序最近通过自我对弈的强化学习，在围棋中达到了超人水平。在本文中，我们将这种方法推广为一个通用的AlphaZero算法，该算法在许多具有挑战性的游戏中可以实现超人表现。从随机对弈开始，仅给定游戏规则的基础知识，AlphaZero在国际象棋和将棋（日本象棋）以及围棋的比赛中都击败了世界冠军程序。



# 正文

计算机国际象棋的研究与计算机科学一样悠久。查尔斯·巴贝奇、阿兰·图灵、克劳德·香农和约翰·冯·诺伊曼发明了硬件、算法和理论来分析和下国际象棋。此后，国际象棋成为了==一代人工智能研究者的重大挑战任务==，最终产生了超人水平的高性能计算机国际象棋程序。然而，这些系统==高度依赖特定领域==，若要推广到其他游戏领域则需要大量的人力投入；相比之下，==通用游戏系统的表现仍然相对较弱==。

---

人工智能的一个长期目标是创建可以从基础原理中自我学习的程序。最近，AlphaGo Zero算法通过深度卷积神经网络表示围棋知识，仅通过自我对弈的强化学习，取得了超人水平的围棋表现。在本文中，我们介绍了AlphaZero，这是==AlphaGo Zero算法的更通用版本==，可适应更广泛的游戏规则。我们将AlphaZero应用于国际象棋、将棋和围棋，通过在所有三种游戏中使用相同的算法和网络架构。我们的研究结果表明，一个通用的强化学习算法可以在没有人类特定领域知识或数据的情况下从零学习，并在多个具有挑战性的游戏中达到超人表现。

---

1997年，深蓝击败了人类国际象棋世界冠军，这是人工智能领域的一个里程碑 (1)。在接下来的二十年中，计算机国际象棋程序持续稳定地进步，超越了人类水平。这些程序通过使用==手工设计的特征==和经过==精心调试的权重==来评估棋局，这些特征和权重==由强大的人类棋手和程序员构建==，并结合了高性能的alpha-beta搜索算法，通过大量巧妙的启发式方法和特定领域的适应性扩展了庞大的搜索树。在文献 (10) 中，我们描述了这些增强措施，特别聚焦于2016年国际象棋引擎锦标赛（TCEC）第9赛季的世界冠军Stockfish (11)；其他强大的国际象棋程序，包括深蓝，采用了非常相似的架构 (1, 12)。

---

从博弈树复杂性方面来看，==将棋的难度远高于国际象棋== (13, 14)：它在==更大的棋盘==上进行，==棋子种类更多==；任何被俘的对方棋子都会转换为己方棋子，并可被放置在棋盘上的任意位置。最强的将棋程序，如2017年计算机将棋协会（CSA）世界冠军Elmo，直到最近才击败了人类冠军 (15)。这些程序使用与计算机国际象棋程序类似的算法，仍然基于高度优化的alpha-beta搜索引擎，并带有许多特定领域的适应性调整。

---

AlphaZero用深度神经网络、通用的强化学习算法和通用的树搜索算法，取代了传统游戏程序中使用的手工知识和特定领域的增强。

---

AlphaZero不再使用手工设计的评价函数和着法排序启发式方法，而是使用一个带参数$\theta$的深度神经网络$(\mathbf{p}, v)=f_\theta(s)$。该神经网络$f_\theta(s)$将棋盘位置$s$作为输入，输出一个着法概率向量$\mathbf{p}$，其中每个动作$a$对应的概率为$p_a=\operatorname{Pr}(a \mid s)$，以及一个标量值$v$，用于估计从位置$s$开始的游戏预期结果$z$，即$v \approx \mathbb{E}[z \mid s]$。AlphaZero完全通过自我对弈来学习这些着法概率和价值估计，并将其用于指导未来的搜索。

---

AlphaZero不再使用带有特定领域增强的alpha-beta搜索，而是使用通用的蒙特卡罗树搜索（MCTS）算法。每次搜索都包括一系列自我对弈的模拟游戏，这些模拟游戏从根状态$s_{\text{root}}$开始，直到达到叶子状态。每次模拟通过在每个状态$s$选择一个具有低访问次数（之前未被频繁探索）、高着法概率以及高价值（在选择$a$自$s$后的模拟叶子状态的平均值）的着法$a$，根据当前的神经网络$f_\theta$进行。搜索返回一个表示着法概率分布的向量$\pi$，其中$\pi_a=\operatorname{Pr}\left(a \mid s_{\text{root}}\right)$。

---

AlphaZero中的深度神经网络参数$\theta$通过自我对弈的强化学习进行训练，初始时参数$\theta$为随机初始化。在每一回合$t$的当前棋盘位置$s_{\text{root}}=s_t$上运行MCTS，然后基于根状态的访问次数选择一个着法$a_t \sim \pi_t$，或按比例（用于探索）或贪婪（用于利用）进行选择。在游戏结束时，根据游戏规则对终局位置$s_T$进行评分，以计算游戏结果$z$：失败为-1，平局为0，胜利为+1。神经网络参数$\theta$通过最小化预测结果$v_t$与游戏结果$z$之间的误差，并最大化策略向量$\mathbf{p}_t$与搜索概率$\pi_t$的相似性来进行更新。具体来说，参数$\theta$通过梯度下降对损失函数$l$进行调整，该损失函数包含均方误差和交叉熵损失的和：

- $(\mathbf{p}, v)=f_\theta(s), l=(z-v)^2-\pi^{\mathrm{T}} \log \mathbf{p}+c\|\theta\|^2$ 

其中$c$是控制$L_2$权重正则化水平的参数。更新后的参数将在后续的自我对弈游戏中使用。

---

本文描述的AlphaZero算法与原始的AlphaGo Zero算法在多个方面有所不同（参见文献 (10) 获取伪代码）。  

---

AlphaGo Zero通过利用围棋具有二元胜负结果的特点，估计并优化获胜的概率。然而，国际象棋和将棋可能会以平局结束；据信，国际象棋的最优解可能是平局 (16–18)。AlphaZero则估计并优化预期的比赛结果。  

---

围棋的规则在旋转和平移时不变。AlphaGo和AlphaGo Zero利用了这一特性。首先，训练数据通过为每个位置生成八种对称性进行增强。其次，在MCTS过程中，棋盘位置在由神经网络评估之前会通过随机选择的旋转或反射进行转换，以便蒙特卡罗评估能在不同的偏差上进行平均。为了适应更广泛的游戏类别，AlphaZero不假设对称性；国际象棋和将棋的规则是非对称的（例如，兵只能向前移动，王车易位在王翼和后翼是不同的）。AlphaZero不进行训练数据的增强，也不在MCTS过程中对棋盘位置进行转换。  

---

在AlphaGo Zero中，自我对弈游戏由所有前一迭代中表现最好的玩家生成。每次迭代训练后，新的玩家与最佳玩家对战以评估其表现；如果新玩家的胜率达到55%，则替换最佳玩家。相比之下，AlphaZero仅维护一个持续更新的神经网络，而不等待迭代完成。自我对弈游戏始终使用该神经网络的最新参数生成。  

---

与AlphaGo Zero一样，棋盘状态由基于每个游戏基本规则的空间平面编码。行动则通过空间平面或一个扁平向量编码，仍然只依赖于每个游戏的基本规则 (10)。  

---

AlphaGo Zero使用了一种特别适合围棋的卷积神经网络架构：围棋规则具有平移不变性（匹配卷积网络的权重共享结构），并且规则定义依赖于棋盘上相邻点之间的气（匹配卷积网络的局部结构）。相比之下，国际象棋和将棋的规则则是位置相关的（例如，兵可以从第二排向前移动两步，并在第八排升变），并包含远程交互（例如，后可以在一步之内遍历棋盘）。尽管存在这些差异，AlphaZero在国际象棋、将棋和围棋中使用了与AlphaGo Zero相同的卷积网络架构。

---

AlphaGo Zero的超参数通过贝叶斯优化进行了调整。在AlphaZero中，我们对所有游戏复用了相同的超参数、算法设置和网络架构，而无需针对具体游戏进行调整。唯一的例外是探索噪声和学习率调度（详情请参见文献 (10)）。  

---

我们为国际象棋、将棋和围棋分别训练了AlphaZero的独立实例。训练从随机初始化的参数开始，进行了70万步（每批次包含4096个训练位置）。在训练期间，仅用于生成自我对弈游戏的设备包括5000个第一代张量处理单元（TPUs）（19），用于训练神经网络的设备包括16个第二代TPUs。训练大约持续了9小时（国际象棋）、12小时（将棋）和13天（围棋）（见表S3）(20)。训练过程的进一步详细信息见文献 (10)。  

---

图1展示了AlphaZero在自我对弈强化学习期间的表现，以Elo评分 (21) 为尺度，作为训练步数的函数。在国际象棋中，AlphaZero在仅4小时（300,000步）后首次超越Stockfish；在将棋中，AlphaZero在2小时（110,000步）后首次超越Elmo；而在围棋中，AlphaZero在30小时（74,000步）后首次超越AlphaGo Lee (9)。该训练算法在所有独立运行中均实现了类似的性能（见图S3），表明AlphaZero训练算法的高性能是可重复的。  

- 图1. AlphaZero训练70万步。Elo评分是根据不同玩家之间的对局计算的，每个玩家每步有1秒的思考时间。
  - (A) AlphaZero在国际象棋中的表现，与2016年TCEC世界冠军程序Stockfish相比。  
  - (B) AlphaZero在将棋中的表现，与2017年CSA世界冠军程序Elmo相比。  
  - (C) AlphaZero在围棋中的表现，与AlphaGo Lee和AlphaGo Zero（20块计算块，3天训练）相比。  

---

我们对完全训练的AlphaZero实例在国际象棋、将棋和围棋中分别对阵了Stockfish、Elmo和之前版本的AlphaGo Zero。每个程序均在其设计的硬件上运行 (23)：Stockfish和Elmo使用44个中央处理器（CPU）核心（与TCEC世界锦标赛中的设置相同），而AlphaZero和AlphaGo Zero使用了1台配备4个第一代TPUs和44个CPU核心的机器 (24)。国际象棋比赛是与2016年TCEC（第9赛季）世界冠军Stockfish对阵的（详情参见文献 (10)）。将棋比赛是与2017年CSA世界冠军版本的Elmo对阵的 (10)。围棋比赛是与之前发布的AlphaGo Zero版本对阵的[同样训练了70万步 (25)]。所有比赛的时间控制为每局3小时，每步加15秒。

---

在围棋中，AlphaZero击败了AlphaGo Zero（9），赢得了61%的比赛。这表明，一种通用方法能够恢复利用棋盘对称性来生成八倍数据量的算法的表现（见图S1）。

---

在国际象棋中，AlphaZero击败了Stockfish，在1000局比赛中赢得155局，输了6局（见图2）。为了验证AlphaZero的鲁棒性，我们进行了额外的比赛，这些比赛从常见的人类开局开始（见图3）。AlphaZero在每个开局中都击败了Stockfish，表明AlphaZero已经掌握了广泛的国际象棋玩法。图3中的频率图和图S2中的时间线显示，AlphaZero在自我对弈训练中独立发现并频繁使用了常见的人类开局。我们还进行了从2016年TCEC世界锦标赛开局位置开始的比赛；AlphaZero也在这场比赛中赢得了压倒性的胜利（26）（见图S4）。我们还与Stockfish的最新开发版本（27）和使用强大开局书的Stockfish变体（28）进行了额外的比赛。AlphaZero在所有比赛中都赢得了大比分胜利（见图2）。

- 图2. 与专业程序的对比。
  - (A) AlphaZero在国际象棋、将棋和围棋中的锦标赛评估，分别与Stockfish、Elmo和先前发布的版本AlphaGo Zero（AG0，训练3天）对局。在上方的条形图中，AlphaZero执白；在下方的条形图中，AlphaZero执黑。每个条形图显示从AlphaZero的角度看的结果：胜（W；绿色）、平（D；灰色）或负（L；红色）。
  - (B) AlphaZero在与Stockfish和Elmo对比时的可扩展性，考虑了思考时间的影响。Stockfish和Elmo始终获得全部时间（每局3小时加每步15秒）；AlphaZero的时间按图中所示缩放。
  - (C) AlphaZero在国际象棋中与当时最新版本的Stockfish（27）及使用强大开局书的Stockfish（28）进行的额外评估。AlphaZero在将棋中的额外评估与另一个强大的将棋程序Aperyqhapaq（29）进行了全时控制对局，并与Elmo在2017年CSA世界锦标赛时间控制下（每局10分钟加每步10秒）进行了对局。
  - (D) 从不同开局位置开始的国际象棋比赛的平均结果，包括常见的人类开局（见图3）或2016年TCEC世界锦标赛的开局位置（见图S4），以及从常见人类开局开始的将棋比赛的平均结果（见图3）。CSA世界锦标赛的比赛从初始棋盘位置开始。比赛条件总结在表S8和S9中。

- 图3. 从最流行的人工开局开始的比赛。AlphaZero分别与
  - (A) Stockfish进行国际象棋对局，以及
  - (B) Elmo进行将棋对局。在左侧的条形图中，AlphaZero执白，从给定位置开始；在右侧的条形图中，AlphaZero执黑。每个条形图显示从AlphaZero的角度看的结果：胜（绿色）、平（灰色）或负（红色）。AlphaZero在自我对弈训练中选择该开局的百分比频率图，按训练时间（小时）绘制。

---

表S6展示了AlphaZero在对阵Stockfish的比赛中进行的20局国际象棋比赛。在几局比赛中，AlphaZero为获得长期的战略优势而牺牲棋子，这表明其棋盘位置的评估比之前的基于规则的国际象棋程序更灵活，更依赖上下文。  

---

在将棋中，AlphaZero击败了Elmo，执黑时赢得了98.2%的比赛，整体胜率为91.2%。我们还在2017年CSA世界锦标赛所用的更快时间控制下以及与另一款最先进的将棋程序(29)进行了比赛；AlphaZero在两场比赛中均以大比分获胜（见图2）。  

---

表S7展示了AlphaZero在对阵Elmo的比赛中进行的10局将棋比赛。图3中的频率图和图S2中的时间线显示，AlphaZero经常使用两种最常见的人类开局之一，但很少使用另一种，并在第一步上就偏离了常规。  

---

在国际象棋和将棋中，AlphaZero每秒仅搜索约60,000个位置，而Stockfish每秒搜索6000万个位置，Elmo每秒搜索2500万个位置（见表S4）。AlphaZero可能通过利用其深度神经网络更有选择性地关注最有前景的变化，从而弥补较低的搜索次数（图4展示了AlphaZero在对阵Stockfish的比赛中的一个示例）——这种搜索方式可以说更接近人类的方法，最初由香农提出 (30)。在AlphaZero获得对手1/10的思考时间时（即搜索约1/10000的棋位数量）仍然击败了Stockfish，并在对阵Elmo时在仅有对手1/100的思考时间时（即搜索约1/40000的棋位数量）赢得了46%的比赛（见图2）。AlphaZero在使用MCTS时的高性能对普遍认为的alpha-beta搜索在这些领域本质上更优的观点提出了质疑 (31, 32)。  

---

几十年来，国际象棋一直是人工智能研究的巅峰领域。最先进的程序基于强大的引擎，能够搜索数百万个位置，利用精心设计的领域专业知识和复杂的领域适应性。AlphaZero是一种通用的强化学习和搜索算法——最初为围棋设计——在仅有几小时的训练中实现了超越性的结果，搜索的棋位数量仅为1/1000，且除国际象棋规则外没有其他领域知识。此外，同样的算法在无需修改的情况下应用于更具挑战性的将棋游戏，同样在短短几小时内超越了最先进的程序。这些结果使我们离人工智能的一个长期目标更近了一步 (3)：一个可以学习掌握任何游戏的通用博弈系统。  

---

图4. AlphaZero的搜索过程。该搜索过程以AlphaZero（白）对阵Stockfish（黑）的第1局（见表S6）中29... Qf8后的位置（插图）为例。AlphaZero的MCTS内部状态在102到106次模拟后进行了汇总。每个汇总显示了访问次数最多的10个状态。每个状态的估计值以白方的视角显示，范围为[0, 100]。每个状态的访问次数相对于该树的根状态，以边框圆圈的粗细表示。AlphaZero考虑了30.c6，但最终选择了30.d5。.  