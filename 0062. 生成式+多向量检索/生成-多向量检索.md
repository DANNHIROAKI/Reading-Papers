# Generative Retrieval as Multi-Vector Dense Retrieval

# 生成式检索作为多向量密集检索

Shiguang Wu

吴时光

Shandong University

山东大学

Qingdao, China

中国青岛

shiguang.wu@mail.sdu.edu.cn

Wenda Wei

魏文达

Shandong University

山东大学

Qingdao, China

中国青岛

weiwenda@mail.sdu.edu.cn

Mengqi Zhang

张梦琪

Shandong University

山东大学

Qingdao, China

中国青岛

mengqi.zhang@sdu.edu.cn

Zhumin Chen

陈竹敏

Shandong University

山东大学

Qingdao, China

中国青岛

chenzhumin@sdu.edu.cn

Jun Ma

马军

Shandong University

山东大学

Qingdao, China

中国青岛

majun@sdu.edu.cn

Zhaochun Ren

任兆春

Leiden University

莱顿大学

Leiden, The Netherlands

荷兰莱顿

z.ren@liacs.leidenuniv.nl

Maarten de Rijke

马腾·德·里克

University of Amsterdam

阿姆斯特丹大学

Amsterdam, The Netherlands

荷兰阿姆斯特丹

m.derijke@uva.nl

Pengjie Ren*

任鹏杰*

Shandong University

山东大学

Qingdao, China

中国青岛

jay.ren@outlook.com

## ABSTRACT

## 摘要

For a given query generative retrieval generates identifiers of relevant documents in an end-to-end manner using a sequence-to-sequence architecture. The relation between generative retrieval and other retrieval methods, especially those based on matching within dense retrieval models, is not yet fully comprehended. Prior work has demonstrated that generative retrieval with atomic identifiers is equivalent to single-vector dense retrieval. Accordingly, generative retrieval exhibits behavior analogous to hierarchical search within a tree index in dense retrieval when using hierarchical semantic identifiers. However, prior work focuses solely on the retrieval stage without considering the deep interactions within the decoder of generative retrieval.

对于给定的查询，生成式检索使用序列到序列架构以端到端的方式生成相关文档的标识符。生成式检索与其他检索方法，特别是基于密集检索模型内匹配的方法之间的关系，尚未得到充分理解。先前的研究表明，使用原子标识符的生成式检索等同于单向量密集检索。因此，当使用分层语义标识符时，生成式检索在密集检索中表现出类似于树索引内分层搜索的行为。然而，先前的工作仅关注检索阶段，而未考虑生成式检索解码器内的深度交互。

In this paper, we fill this gap by demonstrating that generative retrieval and multi-vector dense retrieval share the same framework for measuring the relevance to a query of a document. Specifically, we examine the attention layer and prediction head of generative retrieval, revealing that generative retrieval can be understood as a special case of multi-vector dense retrieval. Both methods compute relevance as a sum of products of query and document vectors and an alignment matrix. We then explore how generative retrieval applies this framework, employing distinct strategies for computing document token vectors and the alignment matrix. We have conducted experiments to verify our conclusions and show that both paradigms exhibit commonalities of term matching in their alignment matrix.

在本文中，我们通过证明生成式检索和多向量密集检索在衡量文档与查询的相关性方面共享相同的框架来填补这一空白。具体而言，我们研究了生成式检索的注意力层和预测头，揭示了生成式检索可以被理解为多向量密集检索的一种特殊情况。这两种方法都将相关性计算为查询向量、文档向量与对齐矩阵的乘积之和。然后，我们探讨了生成式检索如何应用这一框架，采用不同的策略来计算文档标记向量和对齐矩阵。我们进行了实验以验证我们的结论，并表明这两种范式在其对齐矩阵中都表现出术语匹配的共性。

Our findings apply to many generative retrieval identifier designs and provide possible explanations on how generative retrieval can express query-document relevance. As multi-vector dense retrieval is the state-of-the-art dense retrieval method currently, understanding the connection between generative retrieval and multi-vector dense retrieval is crucial for shedding light on the underlying mechanisms of generative retrieval and for developing, and understanding the potential of, new retrieval models.

我们的研究结果适用于许多生成式检索标识符设计，并为生成式检索如何表达查询 - 文档相关性提供了可能的解释。由于多向量密集检索是目前最先进的密集检索方法，因此理解生成式检索与多向量密集检索之间的联系对于揭示生成式检索的潜在机制、开发和理解新检索模型的潜力至关重要。

## CCS CONCEPTS

## 计算机与信息科学概念（CCS CONCEPTS）

- Information systems $\rightarrow$ Retrieval models and ranking.

- 信息系统 $\rightarrow$ 检索模型与排序。

## KEYWORDS

## 关键词

Generative Retrieval; Dense Retrieval; Multi-Vector Dense Retrieval

生成式检索（Generative Retrieval）；密集检索（Dense Retrieval）；多向量密集检索（Multi-Vector Dense Retrieval）

## ACM Reference Format:

## ACM引用格式：

Shiguang Wu, Wenda Wei, Mengqi Zhang, Zhumin Chen, Jun Ma, Zhaochun Ren, Maarten de Rijke, and Pengjie Ren. 2024. Generative Retrieval as Multi-Vector Dense Retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24), July 14-18, 2024, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3626772.3657697

吴世光、魏文达、张梦琪、陈竹敏、马军、任兆春、马腾·德·里克（Maarten de Rijke）和任鹏杰。2024年。将生成式检索视为多向量密集检索。收录于第47届ACM信息检索研究与发展国际会议（SIGIR '24）论文集，2024年7月14 - 18日，美国华盛顿特区。美国纽约州纽约市ACM协会，共11页。https://doi.org/10.1145/3626772.3657697

## 1 INTRODUCTION

## 1 引言

In recent years, the advent of pre-trained language models has catalyzed the popularity of neural-based retrieval models within the information retrieval community $\left\lbrack  {{13},{14},{29},{32},{38}}\right\rbrack$ .

近年来，预训练语言模型的出现促使基于神经网络的检索模型在信息检索领域广受欢迎 $\left\lbrack  {{13},{14},{29},{32},{38}}\right\rbrack$。

Neural-based retrieval models. One family of effective neural-based retrieval methods, dense retrieval (DR), has achieved the state-of-the-art ranking performance on multiple benchmarks [13, 14, 29]. Several approaches have been proposed to use multiple vectors to represent documents or queries, a.k.a., multi-vector dense retrieval (MVDR) [14, 32, 48].

基于神经网络的检索模型。一类有效的基于神经网络的检索方法，即密集检索（Dense Retrieval，DR），在多个基准测试中取得了最先进的排序性能 [13, 14, 29]。已经提出了几种使用多个向量来表示文档或查询的方法，即多向量密集检索（Multi-Vector Dense Retrieval，MVDR） [14, 32, 48]。

Recently, generative retrieval (GR) has emerged as a new paradigm in information retrieval. It aims to generate identifiers of relevant documents for a given query directly and parametrizes the indexing, retrieval, and ranking process in dense retrieval systems into a single model. GR adopts a sequence-to-sequence architecture model and is trained to directly map queries to their relevant document identifiers.

最近，生成式检索（Generative Retrieval，GR）已成为信息检索领域的一种新范式。它旨在直接为给定查询生成相关文档的标识符，并将密集检索系统中的索引、检索和排序过程参数化为单一模型。GR采用序列到序列的架构模型，并经过训练以直接将查询映射到其相关文档的标识符。

---

<!-- Footnote -->

*Corresponding author.

*通信作者。

<!-- Footnote -->

---

Generative retrieval vs. dense retrieval. Dense retrieval models typically employ encoders, e.g., BERT [6], for encoding both queries and documents, while the generative retrieval model adopts an encoder for query encoding and a decoder for identifier generation. Despite their superficial differences, dense retrieval and generative retrieval share key characteristics in their query-document relevance computations. When the two methods use document identifiers such as sub-strings, titles, or semantic IDs as representations for documents, both methods compute relevance to a query of a document as the dot product of two vectors. Dense retrieval involves using the direct product of the query vectors and document vectors as the relevance, while generative retrieval leverages the product of the last latent state from the decoder at each position with the prediction head, a.k.a., the word embedding lookup table. Consequently, a natural question that arises in this context:

生成式检索与密集检索。密集检索模型通常使用编码器（例如BERT [6]）对查询和文档进行编码，而生成式检索模型采用编码器进行查询编码，并使用解码器生成标识符。尽管存在表面差异，但密集检索和生成式检索在查询 - 文档相关性计算方面具有关键的共同特征。当这两种方法使用子字符串、标题或语义ID等文档标识符来表示文档时，它们都将文档与查询的相关性计算为两个向量的点积。密集检索使用查询向量和文档向量的直接乘积作为相关性，而生成式检索利用解码器在每个位置的最后潜在状态与预测头（即词嵌入查找表）的乘积。因此，在这种情况下自然会产生一个问题：

How is generative retrieval related to dense retrieval?

生成式检索与密集检索有何关系？

Although GR has shown promising results in various benchmarks as a new end-to-end retrieval paradigm $\left\lbrack  {{18},{24},{36},{39},{42}}\right\rbrack$ ,relatively few publications have closely examined how GR models work. Nguyen and Yates [28] have shown that GR using atomic identifiers can be viewed as a variant of bi-encoders for dense retrieval because the word embedding lookup table in generative retrieval works exactly the same as the flat index in dense retrieval. Thus, we can partially respond to the above question that GR with atomic identifiers is single-vector dense retrieval. Although atomic identifiers are considered non-mainstream in GR, it offers an insightful perspective on the matter. Nguyen and Yates [28] also discuss that GR with hierarchical semantic identifiers exhibits behavior similar to hierarchical search within a tree index in dense retrieval. However, their discussion focuses only on the retrieval stage without rigorously considering deep interactions within the decoder.

尽管生成式检索（GR）作为一种新的端到端检索范式在各种基准测试中显示出了有前景的结果 $\left\lbrack  {{18},{24},{36},{39},{42}}\right\rbrack$，但相对较少的出版物深入研究了生成式检索模型的工作原理。阮（Nguyen）和耶茨（Yates）[28] 表明，使用原子标识符的生成式检索可以被视为用于密集检索的双编码器的一种变体，因为生成式检索中的词嵌入查找表与密集检索中的平面索引的工作方式完全相同。因此，我们可以部分回答上述问题，即使用原子标识符的生成式检索是单向量密集检索。尽管原子标识符在生成式检索中被认为是非主流的，但它为这个问题提供了一个有洞察力的视角。阮和耶茨 [28] 还讨论了使用分层语义标识符的生成式检索表现出与密集检索中树索引内的分层搜索类似的行为。然而，他们的讨论仅集中在检索阶段，而没有严格考虑解码器内的深度交互。

Generative retrieval as multi-vector dense retrieval. In this work, we connect generative retrieval to a state-of-the-art dense retrieval method, multi-vector dense retrieval, in a rigorous way. We illustrate that these two methods exhibit commonalities in their training targets and a shared focus on semantic matching. We first examine the attention layer and the prediction head of GR and show that the logits in the loss function can be reformulated to a product of document word embeddings, query token vectors, and attention matrix in Section 4. This corresponds to the unified MVDR framework introduced in [17, 32]. In Section 5 we explore the distinct document encoding and alignment strategy in GR. Specifically, our discussion includes (i) its simple document encoding and how prefix-aware weight-adaptive (PAWA) decoding [39] and non-parametric (NP)-decoding [16] apply to our framework (Section 5.1), and (ii) the distinct alignment strategy employed by GR compared to MVDR and its properties (Section 5.2).

生成式检索作为多向量密集检索。在这项工作中，我们以严谨的方式将生成式检索（Generative retrieval）与最先进的密集检索方法——多向量密集检索（multi-vector dense retrieval）联系起来。我们表明，这两种方法在训练目标上有共同点，并且都专注于语义匹配。我们首先研究了生成式检索（GR）的注意力层和预测头，并在第4节中表明，损失函数中的对数几率可以重新表述为文档词嵌入、查询标记向量和注意力矩阵的乘积。这与文献[17, 32]中引入的统一多向量密集检索（MVDR）框架相对应。在第5节中，我们探讨了生成式检索（GR）中独特的文档编码和对齐策略。具体而言，我们的讨论包括：（i）其简单的文档编码，以及前缀感知权重自适应（PAWA）解码[39]和非参数（NP）解码[16]如何应用于我们的框架（第5.1节）；（ii）与多向量密集检索（MVDR）相比，生成式检索（GR）采用的独特对齐策略及其特性（第5.2节）。

Our discovery provides reliable explanations of how GR can express query-document relevance. By explaining how the GR method models query-document relevance, we can further understand how GR is fundamentally different from dense retrieval methods and adds to the spectrum of neural-based retrieval models. The connection we present provides the variants of GR methods with a theoretical foundation for further improvement.

我们的发现为广义相关性（GR）如何表达查询 - 文档相关性提供了可靠的解释。通过解释GR方法如何对查询 - 文档相关性进行建模，我们可以进一步理解GR与密集检索方法在本质上的不同之处，以及它如何丰富了基于神经网络的检索模型体系。我们所建立的联系为GR方法的各种变体提供了进一步改进的理论基础。

Contributions. Our main contributions in this paper are:

贡献。本文的主要贡献如下：

(1) We offer new insights into GR from the perspective of MVDR by showing that these methods share the same framework for measuring query-document relevance.

（1）我们从最小方差无失真响应（MVDR）的角度为GR提供了新的见解，表明这些方法在衡量查询 - 文档相关性方面共享相同的框架。

(2) We explore how GR applies this framework, employing distinct strategies for document encoding and the alignment matrix.

（2）我们探讨了GR如何应用这一框架，采用不同的策略进行文档编码和构建对齐矩阵。

(3) We also conduct extensive analytical experiments based on the framework to verify our conclusions and illustrate the term-matching phenomenon and properties of different alignment directions in both paradigms.

（3）我们还基于该框架进行了广泛的分析实验，以验证我们的结论，并阐明两种范式中术语匹配现象以及不同对齐方向的特性。

## 2 RELATED WORK

## 2 相关工作

Multi-vector dense retrieval (MVDR) can be seen as a generalization of single-vector dual encoder models [13, 14]. Instead of encoding the complete content of both query and documents into a single low-dimensional vector, MVDR uses fine-grained token-level modeling for scoring. MVDR models such as ColBERT [14] compute query-document relevance by selecting the highest-scoring document token for each query token and aggregating the scores. The postponed token-level interactions allow us to efficiently apply the model for retrieval, benefiting the effectiveness of modeling fine-grained interactions. MVDR overcomes the limited expressivity of single-vector retrieval and achieves significantly better results across various benchmarks [13, 14, 17, 23, 29, 32]. However, due to the cost of storing vectors for each document token, it is challenging to scale the approach to large collections $\left\lbrack  {9,{14},{17},{32}}\right\rbrack$ .

多向量密集检索（Multi-vector dense retrieval，MVDR）可被视为单向量双编码器模型的泛化形式 [13, 14]。与将查询和文档的完整内容编码为单个低维向量不同，MVDR 使用细粒度的词元级建模进行评分。像 ColBERT [14] 这样的 MVDR 模型通过为每个查询词元选择得分最高的文档词元并汇总得分来计算查询 - 文档相关性。延迟的词元级交互使我们能够高效地应用该模型进行检索，有利于对细粒度交互进行有效建模。MVDR 克服了单向量检索表达能力有限的问题，在各种基准测试中取得了显著更好的结果 [13, 14, 17, 23, 29, 32]。然而，由于需要为每个文档词元存储向量，将该方法扩展到大规模数据集具有挑战性 $\left\lbrack  {9,{14},{17},{32}}\right\rbrack$。

Generative retrieval (GR) is an emerging paradigm in information retrieval $\left\lbrack  {{18},{31},{34},{38},{39},{45}}\right\rbrack$ . It leverages generative models to directly generate identifiers of relevant documents. This approach originated with $\left\lbrack  {2,{38}}\right\rbrack$ and has garnered considerable attention [see, e.g., 37]. Currently, all implementations of the generative retrieval paradigm adhere to an encoder-decoder transformer architecture, e.g., T5 [33] and BART [19]. In this method, documents are initially associated with a concise token sequence that serves as an identifier. The model is then trained to predict this token sequence autoregressively, using conventional cross-entropy loss.

生成式检索（Generative retrieval，GR）是信息检索领域的一种新兴范式 $\left\lbrack  {{18},{31},{34},{38},{39},{45}}\right\rbrack$。它利用生成式模型直接生成相关文档的标识符。这种方法起源于 $\left\lbrack  {2,{38}}\right\rbrack$，并引起了广泛关注 [例如，参见文献 37]。目前，生成式检索范式的所有实现都遵循编码器 - 解码器的Transformer架构，例如T5 [33] 和BART [19]。在这种方法中，文档最初与一个简洁的标记序列相关联，该序列用作标识符。然后，使用传统的交叉熵损失对模型进行训练，以自回归方式预测该标记序列。

One notable advantage of the generative retrieval model is its streamlined end-to-end architecture, which requires significantly less disk storage space compared to other retrieval methods. However, it is important to note that due to the limited supervision of each token, the generative retrieval may not achieve comparable performance when compared to dense retrieval [31, 44].

生成式检索模型的一个显著优点是其简化的端到端架构，与其他检索方法相比，它所需的磁盘存储空间要少得多。然而，需要注意的是，由于每个标记的监督有限，与密集检索相比，生成式检索可能无法取得相当的性能 [31, 44]。

Connecting dense retrieval and generative retrieval. Nguyen and Yates [28] show that GR with atomic identifiers is equivalent to single-vector dense retrieval. They compare the inferential processes of DR with a tree index and GR with hierarchical identifiers. However, the former is just an optimized version of the original DR without changing the semantic matching method, while the latter also considers the predicted IDs and the query in each generation step, which greatly affects how GR would express the relevance, but this is ignored in [28]. Yuan et al. [44] empirically analyze the error rate at each generation step of GR and identify the problem of poor memory accuracy for fine-grained features compared with DR. They integrate GR and DR into a new coarse-to-fine retrieval paradigm, combining their respective strengths, but circumvented an in-depth discussion of the connection.

连接密集检索和生成式检索。阮（Nguyen）和耶茨（Yates）[28]表明，带有原子标识符的生成式检索（GR）等同于单向量密集检索。他们比较了带有树索引的密集检索（DR）和带有分层标识符的生成式检索的推理过程。然而，前者只是原始密集检索的优化版本，并未改变语义匹配方法，而后者在每个生成步骤中还会考虑预测的标识符和查询，这极大地影响了生成式检索表达相关性的方式，但[28]中忽略了这一点。袁（Yuan）等人[44]对生成式检索每个生成步骤的错误率进行了实证分析，并指出与密集检索相比，生成式检索对细粒度特征的记忆准确性较差的问题。他们将生成式检索和密集检索整合到一种新的由粗到细的检索范式中，结合了它们各自的优势，但回避了对两者联系的深入讨论。

In this work, we address the limitations listed above by showing that GR expresses query-document relevance in the same way as MVDR. This connection is rigorously derived from the decoder of GR and can be applied to many identifiers.

在这项工作中，我们通过证明生成式检索与多向量密集检索（MVDR）以相同的方式表达查询 - 文档相关性，解决了上述局限性。这种联系是从生成式检索的解码器中严格推导出来的，并且可以应用于许多标识符。

## 3 PRELIMINARIES

## 3 预备知识

In this section, we formulate our task and introduce key notation and the mainstream framework of MVDR models.

在本节中，我们阐述我们的任务，并介绍关键符号和多向量密集检索模型的主流框架。

Task definition. We formulate the retrieval task as ranking by relevance score. Given a query $q$ ,we aim to retrieve relevant documents $d$ in $\mathcal{D}$ by ranking them by their relevance $\operatorname{rel}\left( {d,q}\right)$ to $q$ .

任务定义。我们将检索任务表述为按相关性得分进行排序。给定一个查询 $q$，我们的目标是通过根据文档 $d$ 与 $q$ 的相关性 $\operatorname{rel}\left( {d,q}\right)$ 对其进行排序，从 $\mathcal{D}$ 中检索相关文档。

Notation. Table 1 lists the main notation used in the paper. We denote the word embedding lookup table in the decoder as $\mathbf{E}$ and the vocabulary set as $\mathcal{V}$ . Each document $d$ comprises $M$ tokens. To ensure uniform length, padding tokens are added or excess tokens are truncated from each document. The word embedding matrix of $d$ is denoted as ${\mathbf{E}}_{d} \mathrel{\text{:=}} \left\lbrack  {{\mathbf{e}}_{{d}_{1}},\ldots ,{\mathbf{e}}_{{d}_{M}}}\right\rbrack   \in  {\mathbb{R}}^{d \times  M}$ ,and the latent token vector matrix after encoding is $\mathbf{D} \mathrel{\text{:=}} \left\lbrack  {{\mathbf{d}}_{1},\ldots ,{\mathbf{d}}_{M}}\right\rbrack   \in  {\mathbb{R}}^{d \times  M}$ . Each query $q$ with $N$ tokens has token vectors $\mathbf{Q} \mathrel{\text{:=}} \left\lbrack  {{\mathbf{q}}_{1},\ldots ,{\mathbf{q}}_{N}}\right\rbrack   \in$ ${\mathbb{R}}^{d \times  N}$ after encoding,similar to the documents.

符号说明。表1列出了本文使用的主要符号。我们将解码器中的词嵌入查找表表示为$\mathbf{E}$，将词汇集表示为$\mathcal{V}$。每个文档$d$由$M$个词元组成。为确保长度一致，会为每个文档添加填充词元或截断多余的词元。$d$的词嵌入矩阵表示为${\mathbf{E}}_{d} \mathrel{\text{:=}} \left\lbrack  {{\mathbf{e}}_{{d}_{1}},\ldots ,{\mathbf{e}}_{{d}_{M}}}\right\rbrack   \in  {\mathbb{R}}^{d \times  M}$，编码后的潜在词元向量矩阵为$\mathbf{D} \mathrel{\text{:=}} \left\lbrack  {{\mathbf{d}}_{1},\ldots ,{\mathbf{d}}_{M}}\right\rbrack   \in  {\mathbb{R}}^{d \times  M}$。每个包含$N$个词元的查询$q$在编码后具有词元向量$\mathbf{Q} \mathrel{\text{:=}} \left\lbrack  {{\mathbf{q}}_{1},\ldots ,{\mathbf{q}}_{N}}\right\rbrack   \in$${\mathbb{R}}^{d \times  N}$，与文档类似。

<!-- Media -->

Table 1: Main notation used in this work.

表1：本文使用的主要符号（notation）。

<table><tr><td>Symbol</td><td>Description</td></tr><tr><td>$E$</td><td>word embedding lookup table</td></tr><tr><td>${\mathbf{E}}_{d}$</td><td>document word embedding matrix</td></tr><tr><td>$D,Q$</td><td>document / query token vector matrix</td></tr><tr><td>${d}_{i},{q}_{j}$</td><td>document / query tokens</td></tr><tr><td>${d}_{i},{q}_{j}$</td><td>encoded document / query token vector</td></tr></table>

<table><tbody><tr><td>符号</td><td>描述</td></tr><tr><td>$E$</td><td>词嵌入查找表</td></tr><tr><td>${\mathbf{E}}_{d}$</td><td>文档词嵌入矩阵</td></tr><tr><td>$D,Q$</td><td>文档/查询词元向量矩阵</td></tr><tr><td>${d}_{i},{q}_{j}$</td><td>文档/查询词元</td></tr><tr><td>${d}_{i},{q}_{j}$</td><td>编码后的文档/查询词向量</td></tr></tbody></table>

<!-- Media -->

Framework for MVDR. Following [17, 32], MVDR methods can be represented as a unified framework, in which the relevance to query $q$ of document $d$ is given by:

最小方差无畸变响应（MVDR）框架。参照文献[17, 32]，MVDR方法可以用一个统一的框架来表示，其中文档$d$与查询$q$的相关性由以下公式给出：

$$
\operatorname{rel}\left( {d,q}\right)  = \frac{1}{Z}\operatorname{sum}\left( {{\mathbf{D}}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)  = \frac{1}{Z}\mathop{\sum }\limits_{{i,j}}{\mathbf{d}}_{i}^{\top }{\mathbf{q}}_{j}{A}_{ij}, \tag{1}
$$

where $\mathbf{A}$ is the alignment matrix that controls whether a document and query token pair can be matched and contribute to the relevance,and $Z = \mathop{\sum }\limits_{{ij}}{A}_{ij}$ is used for normalization and is dropped in many MVDR methods.

其中$\mathbf{A}$是对齐矩阵，用于控制文档和查询词对是否可以匹配并对相关性产生贡献，而$Z = \mathop{\sum }\limits_{{ij}}{A}_{ij}$用于归一化，在许多MVDR方法中会被舍弃。

Alignment strategy. Different MVDR models adopt different alignment strategies,and,thus,a different alignment matrix $\mathbf{A}$ . It is often computed using heuristic algorithms, such as lexical exact match [11], top-1 relevant token match [14], single-vector alignment $\left\lbrack  {{13},{25}}\right\rbrack$ ,or sparse unary salience [32].

对齐策略。不同的MVDR模型采用不同的对齐策略，因此会有不同的对齐矩阵$\mathbf{A}$。它通常使用启发式算法来计算，例如词法精确匹配[11]、排名第一的相关词匹配[14]、单向量对齐$\left\lbrack  {{13},{25}}\right\rbrack$或稀疏一元显著性[32]。

Contrastive loss used in MVDR. MVDR methods usually use contrastive loss as the training target, where negative documents are used. For a query $q$ and target document $d$ ,the loss is computed as

MVDR中使用的对比损失。MVDR方法通常使用对比损失作为训练目标，其中会使用负样本文档。对于查询$q$和目标文档$d$，损失的计算方式如下

$$
\mathcal{L}\left( {d,q}\right)  =  - \log \frac{\exp \operatorname{rel}\left( {d,q}\right) }{\mathop{\sum }\limits_{{{d}^{ - } \in  {\mathcal{D}}^{ - }}}\exp \operatorname{rel}\left( {{d}^{ - },q}\right) }, \tag{2}
$$

where ${\mathcal{D}}^{ - }$ is the collected negative set.

其中 ${\mathcal{D}}^{ - }$ 是收集到的负样本集。

## 4 IN-DEPTH ANALYSIS OF GENERATIVE RETRIEVAL

## 4 生成式检索的深入分析

To address the question posed in Section 1, this section conducts a detailed analysis of GR. Specifically, we first illustrate the model architecture and training loss of the GR (Section 4.1). Subsequently, we derive that the training target of GR falls into the framework of MVDR (Section 4.2):

为了解决第1节提出的问题，本节对生成式检索（GR）进行详细分析。具体而言，我们首先阐述GR的模型架构和训练损失（4.1节）。随后，我们推导出GR的训练目标属于最小方差无失真响应（MVDR）框架（4.2节）：

$$
\mathcal{L}\left( {d,q}\right)  \propto  \operatorname{sum}\left( {{\widetilde{\mathbf{E}}}_{d}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right) , \tag{3}
$$

where ${\widetilde{\mathbf{E}}}_{d},\mathbf{Q}$ and $\mathbf{A}$ correspond to $\mathbf{D},\mathbf{Q}$ and $\mathbf{A}$ in Eq. (1).

其中 ${\widetilde{\mathbf{E}}}_{d},\mathbf{Q}$ 和 $\mathbf{A}$ 分别对应公式(1)中的 $\mathbf{D},\mathbf{Q}$ 和 $\mathbf{A}$。

### 4.1 Model architecture and training loss

### 4.1 模型架构和训练损失

Model architecture. We focus on the transformer sequence-to-sequence architecture used in GR, more precisely, the encoder-decoder structure. Within this framework, the encoder primarily targets processing the input query, while the decoder is tasked with predicting document identifiers.

模型架构。我们关注GR中使用的Transformer序列到序列架构，更确切地说是编码器 - 解码器结构。在这个框架内，编码器主要负责处理输入查询，而解码器则负责预测文档标识符。

The decoder component consists of stacks of self-attention, cross-attention, and feed-forward layers. We particularly underscore the significance of the cross-attention layers, as they facilitate interaction between query tokens and document tokens.

解码器组件由多层自注意力层、交叉注意力层和前馈层组成。我们特别强调交叉注意力层的重要性，因为它们促进了查询标记（query tokens）和文档标记（document tokens）之间的交互。

To predict the document token ${d}_{i}$ at the $i$ -th position,we compute the cross attention weights between query token vectors $\mathbf{Q}$ and ${\widehat{\mathbf{d}}}_{i - 1}$ from the previous attention layers at position(i - 1)as follows:

为了预测第 $i$ 个位置的文档标记 ${d}_{i}$，我们按如下方式计算查询标记向量 $\mathbf{Q}$ 与来自前一个注意力层中位置 (i - 1) 的 ${\widehat{\mathbf{d}}}_{i - 1}$ 之间的交叉注意力权重：

$$
{\mathbf{\alpha }}_{i} = \operatorname{softmax}\left( {{\mathbf{Q}}^{\top }\mathbf{W}{\widehat{\mathbf{d}}}_{i - 1}}\right) , \tag{4}
$$

where softmax $\left( \cdot \right)$ denotes the column-wise softmax function, $\mathbf{W} \in$ ${\mathbb{R}}^{d \times  d}$ is the product of the attention matrices ${\mathbf{W}}_{K}$ and ${\mathbf{W}}_{Q}$ ,i.e., $\mathbf{W} = {\mathbf{W}}_{K}^{\top }{\mathbf{W}}_{Q}$ ,and ${\mathbf{\alpha }}_{i} \in  {\mathbb{R}}^{N}$ .

其中softmax $\left( \cdot \right)$ 表示按列进行的softmax函数，$\mathbf{W} \in$ ${\mathbb{R}}^{d \times  d}$ 是注意力矩阵 ${\mathbf{W}}_{K}$ 和 ${\mathbf{W}}_{Q}$ 的乘积，即 $\mathbf{W} = {\mathbf{W}}_{K}^{\top }{\mathbf{W}}_{Q}$ ，且 ${\mathbf{\alpha }}_{i} \in  {\mathbb{R}}^{N}$ 。

Consequently, the output of the cross-attention layer is

因此，交叉注意力层的输出为

$$
{\mathbf{h}}_{i} = {\mathbf{W}}_{V}\mathbf{Q}{\mathbf{\alpha }}_{i} \in  {\mathbb{R}}^{d}. \tag{5}
$$

For simplicity, we ignore the non-linear activation functions, and the linear maps in the feedforward layers can be absorbed in attention weights,e.g., ${\mathbf{W}}_{V}$ . Therefore, ${\mathbf{h}}_{i}$ serves as the prediction head for generating the next token.

为简单起见，我们忽略非线性激活函数，并且前馈层中的线性映射可以被吸收到注意力权重中，例如 ${\mathbf{W}}_{V}$ 。因此，${\mathbf{h}}_{i}$ 用作生成下一个标记的预测头。

Training loss. The loss function to minimize at position $i$ is formulated as:

训练损失。在位置 $i$ 处要最小化的损失函数定义为：

$$
{\mathcal{L}}_{i}\left( {d,q}\right)  =  - \log \left( \frac{\exp {\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i}}{\mathop{\sum }\limits_{{v \in  \mathcal{V}}}\exp {\mathbf{e}}_{v}^{\top }{\mathbf{h}}_{i}}\right)  \tag{6}
$$

$$
 =  - {\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i} + \log \mathop{\sum }\limits_{{v \in  \mathcal{V}}}\exp {\mathbf{e}}_{v}^{\top }{\mathbf{h}}_{i}. \tag{7}
$$

### 4.2 GR has the same framework as MVDR

### 4.2 GR（广义旁瓣相消器，Generalized Sidelobe Canceler）与MVDR（最小方差无失真响应，Minimum Variance Distortionless Response）具有相同的框架

Next, we demonstrate that GR shares a similar framework with MVDR, namely, that the logits within the loss function can be reformulated as a product of document word embeddings, query token vectors, and attention matrix. This formulation corresponds to Eq. (1).

接下来，我们证明广义回归（GR）与最小方差无失真响应（MVDR）具有相似的框架，即损失函数中的对数几率可以重新表示为文档词嵌入、查询标记向量和注意力矩阵的乘积。这种表示形式对应于公式（1）。

In particular, as we employ teacher-forcing supervision, ground-truth document identifiers are directly fed into the decoder, and token vectors at all positions are computed simultaneously. Based on this configuration, the overall loss is given by:

具体而言，由于我们采用了教师强制监督，真实文档标识符会直接输入到解码器中，并且会同时计算所有位置的标记向量。基于此配置，总体损失由以下公式给出：

$$
\mathcal{L}\left( {d,q}\right)  = \mathop{\sum }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}\log p\left( {{d}_{i} \mid  {d}_{i - 1},\ldots ,{d}_{0},q}\right)  \tag{8}
$$

$$
 = \mathop{\sum }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}{\mathcal{L}}_{i}\left( {d,q}\right)  \tag{9}
$$

$$
 = \mathop{\sum }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}\left( {-{\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i} + \log \mathop{\sum }\limits_{{v \in  \mathcal{V}}}\exp {\mathbf{e}}_{v}^{\top }{\mathbf{h}}_{i}}\right) , \tag{10}
$$

where ${d}_{0}$ could be some special token such as [BOS] or the [CLS] token vector from the query.

其中 ${d}_{0}$ 可以是一些特殊标记，例如 [BOS] 或查询中的 [CLS] 标记向量。

When using the sampled softmax loss, which involves employing several negative tokens instead of the entire set of tokens in the lookup embedding table, the loss exhibits a similar structure to the contrastive loss used in DR and MVDR. Consequently, we treat the dot product of embedding ${\mathbf{e}}_{{d}_{i}}$ and token vector ${\mathbf{h}}_{i}$ ,i.e., ${\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i}$ , as the final relevance score at position $i$ . Further insights into the reason are elaborated in Appendix A. We plug in the dot product with the computation of ${\mathbf{h}}_{i}$ from Eq. (5) and obtain:

当使用采样的softmax损失时（该损失涉及使用几个负标记而非查找嵌入表中的整个标记集），该损失的结构与文档重排序（DR）和多视图文档重排序（MVDR）中使用的对比损失类似。因此，我们将嵌入向量${\mathbf{e}}_{{d}_{i}}$与标记向量${\mathbf{h}}_{i}$的点积，即${\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i}$，视为位置$i$处的最终相关性得分。关于此原因的更多见解在附录A中详细阐述。我们将点积代入式(5)中${\mathbf{h}}_{i}$的计算，得到：

$$
\operatorname{rel}\left( {d,q}\right)  = \mathop{\sum }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}{\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i} \tag{11}
$$

$$
 = \mathop{\sum }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}{\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{W}}_{V}\mathbf{Q}{\mathbf{\alpha }}_{i} \tag{12}
$$

$$
 = \mathop{\sum }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}\mathop{\sum }\limits_{{j \in  \left\lbrack  N\right\rbrack  }}{\widetilde{\mathbf{e}}}_{{d}_{i}}^{\top }{\mathbf{q}}_{j}{\alpha }_{ij} \tag{13}
$$

$$
 = \operatorname{sum}\left( {{\widetilde{\mathbf{E}}}_{d}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right) , \tag{14}
$$

where ${\widetilde{\mathbf{e}}}_{{d}_{i}}^{\top } = {\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{W}}_{V},{\widetilde{\mathbf{E}}}_{d}^{\top } = {\mathbf{E}}_{d}^{\top }{\mathbf{W}}_{V},\mathbf{A} = {\left\lbrack  {\mathbf{\alpha }}_{1},\ldots ,{\mathbf{\alpha }}_{M}\right\rbrack  }^{\top } \in$ ${\mathbb{R}}^{M \times  N}$ ,and $\odot$ is the element-wise matrix product operation.

其中 ${\widetilde{\mathbf{e}}}_{{d}_{i}}^{\top } = {\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{W}}_{V},{\widetilde{\mathbf{E}}}_{d}^{\top } = {\mathbf{E}}_{d}^{\top }{\mathbf{W}}_{V},\mathbf{A} = {\left\lbrack  {\mathbf{\alpha }}_{1},\ldots ,{\mathbf{\alpha }}_{M}\right\rbrack  }^{\top } \in$ ${\mathbb{R}}^{M \times  N}$ ，并且 $\odot$ 是逐元素矩阵乘积运算。

Further, we have a more detailed computation

此外，我们有更详细的计算

$$
\operatorname{rel}\left( {d,q}\right)  = \operatorname{sum}\left( {{\widetilde{\mathbf{E}}}_{d}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)  \tag{15}
$$

$$
 = \operatorname{sum}\left( {{\widetilde{\mathbf{E}}}_{d}^{\top }\mathbf{Q} \odot  \operatorname{softmax}\left( {{\widehat{\mathbf{D}}}_{-1}^{\top }{\mathbf{W}}^{\top }\mathbf{Q}}\right) }\right) , \tag{16}
$$

where ${\widehat{\mathbf{D}}}_{-1} = \left\lbrack  {{\widehat{\mathbf{d}}}_{0},{\widehat{\mathbf{d}}}_{1},\ldots ,{\widehat{\mathbf{d}}}_{M - 1}}\right\rbrack$ is the output from the previous layer with the right-shifted document tokens as model input.

其中 ${\widehat{\mathbf{D}}}_{-1} = \left\lbrack  {{\widehat{\mathbf{d}}}_{0},{\widehat{\mathbf{d}}}_{1},\ldots ,{\widehat{\mathbf{d}}}_{M - 1}}\right\rbrack$ 是上一层的输出，使用右移后的文档标记作为模型输入。

In conclusion, for GR we observe a similar framework as for MVDR, $\operatorname{rel}\left( {d,q}\right)  = \operatorname{sum}\left( {{\widetilde{\mathbf{E}}}_{d}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)$ ,where relevance is represented by an interaction of multiple "token vectors," i.e., ${\widetilde{\mathbf{E}}}_{d}$ and $\mathbf{Q}$ ,from both query and document and aligned by a matrix $\mathbf{A}$ . We summarize our derivation and conclusion in Figure 1.

总之，对于广义回归（GR），我们观察到与最小方差无失真响应（MVDR）类似的框架，$\operatorname{rel}\left( {d,q}\right)  = \operatorname{sum}\left( {{\widetilde{\mathbf{E}}}_{d}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)$，其中相关性由多个“标记向量”（即来自查询和文档的 ${\widetilde{\mathbf{E}}}_{d}$ 和 $\mathbf{Q}$）的交互表示，并通过矩阵 $\mathbf{A}$ 进行对齐。我们在图 1 中总结了我们的推导和结论。

## 5 COMPARISON BETWEEN MVDR AND GR

## 5 最小方差无失真响应（MVDR）与广义回归（GR）的比较

To further explore how GR is related to MVDR, we build upon the unified framework of relevance computation for GR and MVDR derived in the previous section. We conduct a comprehensive analysis of both methods, focusing specifically on their similarities and disparities in terms of the document encoding and the design of the alignment matrix. A summary of the comparison between the two methods is shown in Table 2.

为了进一步探究广义相关性（GR）与最小方差无失真响应（MVDR）之间的关系，我们基于上一节推导出的GR和MVDR相关性计算统一框架展开研究。我们对这两种方法进行了全面分析，特别关注它们在文档编码和对齐矩阵设计方面的异同。两种方法的比较总结见表2。

<!-- Media -->

MVDR relevance computation

最小方差无失真响应（MVDR）相关性计算

<img src="https://cdn.noedgeai.com/01958ba3-6408-790f-88f5-651c789dda86_3.jpg?x=961&y=277&w=668&h=110&r=0"/>

GR training loss

广义相关性（GR）训练损失

<!-- figureText: : doc embed vec $\mathcal{L}\left( {d,q}\right)  \propto  \sum {\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i} = \mathop{\sum }\limits_{i}\left( {{\mathbf{\Phi }}_{{d}_{i}}^{\top } \times  \left| {\mathbf{q}}_{1}\right| {\mathbf{q}}_{2}\cdots {\mathbf{q}}_{N} \times  {\mathbf{\alpha }}_{i}}\right)$ cross attention -->

<img src="https://cdn.noedgeai.com/01958ba3-6408-790f-88f5-651c789dda86_3.jpg?x=928&y=457&w=718&h=251&r=0"/>

Figure 1: Summary of our derivation and conclusion. The logits of GR can be reformulated as $\operatorname{sum}\left( {{\mathbf{E}}_{d}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)$ ,which corresponds to the framework $\operatorname{sum}\left( {{\mathbf{D}}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)$ of MVDR.

图1：我们的推导和结论总结。GR的对数几率可以重新表示为$\operatorname{sum}\left( {{\mathbf{E}}_{d}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)$，这对应于MVDR的框架$\operatorname{sum}\left( {{\mathbf{D}}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)$。

Table 2: Summary of our comparison between MVDR and $\mathbf{{GR}.}$

表2：我们对最小方差无失真响应（MVDR）和$\mathbf{{GR}.}$的比较总结

<table><tr><td rowspan="2">Component in $\operatorname{sum}\left( {{\mathbf{D}}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)$</td><td colspan="2">Model</td><td rowspan="2">Comp- arison</td></tr><tr><td>MVDR (Sect. 3)</td><td>GR (Sect. 4)</td></tr><tr><td>$D$ doc token</td><td>$\mathbf{D}$ (token vec.)</td><td>${\widetilde{\mathbf{E}}}_{d}$ (embed. vec.)</td><td>Sect. 5.1</td></tr><tr><td>$Q$ query token</td><td>$Q$ (token vec.)</td><td>$Q$ (token vec.)</td><td>-</td></tr><tr><td>$\mathbf{A}$ alignment matrix</td><td>sparse query-to-doc</td><td>dense and learned doc-to-query</td><td>Sect. 5.2</td></tr></table>

<table><tbody><tr><td rowspan="2">$\operatorname{sum}\left( {{\mathbf{D}}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)$中的组件</td><td colspan="2">模型</td><td rowspan="2">比较</td></tr><tr><td>最小方差无失真响应（MVDR，第3节）</td><td>广义瑞利商（GR，第4节）</td></tr><tr><td>$D$文档标记</td><td>$\mathbf{D}$（词元向量 (token vec.)）</td><td>${\widetilde{\mathbf{E}}}_{d}$（嵌入向量 (embed. vec.)）</td><td>第5.1节</td></tr><tr><td>$Q$查询词元 (query token)</td><td>$Q$（词元向量 (token vec.)）</td><td>$Q$（词元向量 (token vec.)）</td><td>-</td></tr><tr><td>$\mathbf{A}$对齐矩阵 (alignment matrix)</td><td>稀疏查询到文档</td><td>密集且经过学习的文档到查询</td><td>第5.2节</td></tr></tbody></table>

<!-- Media -->

### 5.1 Document encoding

### 5.1 文档编码

One of the noticeable differences between GR and MVDR is in the document encoding. As depicted in Figure 1, MVDR uses more expressive contextualized token vectors $\mathbf{D} = \left\lbrack  {{\mathbf{d}}_{1},\ldots ,{\mathbf{d}}_{M}}\right\rbrack$ for each position. In contrast, GR only attends each query token to a simple word embedding ${\mathbf{e}}_{{d}_{i}}$ that does not hold any contextual information about the document. This was considered a severe compromise for the extremely lightweight modeling and storage of GR. To address this imbalance in modeling capacity, several studies [16, 39] have proposed novel decoding methods. Wang et al. [39] introduce the prefix-aware weight-adaptive (PAWA) decoding method, while Lee et al. [16] propose the non-parametric (NP) decoding. We incorporate these methods into our framework and show how they fundamentally improve the encoding compared with MVDR in Table 3.

广义检索（GR）和最大信干噪比（MVDR）之间一个显著的区别在于文档编码。如图1所示，最大信干噪比（MVDR）为每个位置使用更具表现力的上下文词元向量$\mathbf{D} = \left\lbrack  {{\mathbf{d}}_{1},\ldots ,{\mathbf{d}}_{M}}\right\rbrack$。相比之下，广义检索（GR）仅将每个查询词元与一个简单的词嵌入${\mathbf{e}}_{{d}_{i}}$关联起来，该词嵌入不包含文档的任何上下文信息。对于广义检索（GR）极其轻量级的建模和存储而言，这被认为是一个严重的折衷方案。为了解决建模能力的这种不平衡，多项研究[16, 39]提出了新颖的解码方法。王等人[39]引入了前缀感知权重自适应（PAWA）解码方法，而李等人[16]提出了非参数（NP）解码方法。我们将这些方法纳入我们的框架，并在表3中展示与最大信干噪比（MVDR）相比，它们如何从根本上改进编码。

<!-- Media -->

Table 3: Document encoding comparison between GR and MVDR. PAWA and NP-decoding either multiply or replace the simple embedding vectors ${\widetilde{\mathbf{E}}}_{d}$ with contextualized token vectors $\widetilde{D}$ .

表3：GR和MVDR之间的文档编码比较。PAWA和NP解码要么将简单嵌入向量${\widetilde{\mathbf{E}}}_{d}$与上下文相关的标记向量$\widetilde{D}$相乘，要么用后者替换前者。

<table><tr><td>Model</td><td>Document encoding</td></tr><tr><td>MVDR (Sect. 3)</td><td>$\mathbf{D}$ (token vec.)</td></tr><tr><td>GR (Sect. 4)</td><td>${\widetilde{\mathbf{E}}}_{d}$ (embed. vec.)</td></tr><tr><td>- w/ PAWA</td><td>${\widetilde{\mathbf{E}}}_{d} \rightarrow  {\widetilde{\mathbf{E}}}_{d}{\widetilde{\mathbf{D}}}^{\prime }$ (embed. &token vec.)</td></tr><tr><td>- w/ NP-dec.</td><td>${\mathbf{E}}_{d} \rightarrow  \mathbf{D}$ (token vec.)</td></tr></table>

<table><tbody><tr><td>模型</td><td>文档编码</td></tr><tr><td>最小方差无畸变响应（MVDR）（第3节）</td><td>$\mathbf{D}$（词向量）</td></tr><tr><td>广义瑞利商（GR）（第4节）</td><td>${\widetilde{\mathbf{E}}}_{d}$（嵌入向量）</td></tr><tr><td>- 搭配PAWA（PAWA）</td><td>${\widetilde{\mathbf{E}}}_{d} \rightarrow  {\widetilde{\mathbf{E}}}_{d}{\widetilde{\mathbf{D}}}^{\prime }$（嵌入与词元向量（embed. &token vec.））</td></tr><tr><td>- 搭配NP解码（NP-dec.）</td><td>${\mathbf{E}}_{d} \rightarrow  \mathbf{D}$（词元向量（token vec.））</td></tr></tbody></table>

<!-- Media -->

PAWA enhances the document encoding from ${\widetilde{E}}_{d}$ to ${\widetilde{E}}_{d}{\widetilde{D}}^{\prime }$ . PAWA [39] aims to improve the embedding modeling for distinguishing different semantics of a token ID at different positions. Unlike the standard transformer, which uses a static embedding lookup table for every position, PAWA generates different embedding tables at each generation step. PAWA consists of a transformer decoder and an adaptive projection layer $\mathrm{E} \in  {\mathbb{R}}^{M \times  \left| \mathcal{V}\right|  \times  d \times  d}$ . The projection matrix of token $v$ at the $i$ -th position is

PAWA（位置自适应词嵌入增强器）将文档编码从${\widetilde{E}}_{d}$提升到${\widetilde{E}}_{d}{\widetilde{D}}^{\prime }$。PAWA [39]旨在改进嵌入建模，以区分标记ID在不同位置的不同语义。与标准Transformer（变换器）不同，标准Transformer对每个位置使用静态嵌入查找表，而PAWA在每个生成步骤生成不同的嵌入表。PAWA由一个Transformer解码器和一个自适应投影层$\mathrm{E} \in  {\mathbb{R}}^{M \times  \left| \mathcal{V}\right|  \times  d \times  d}$组成。标记$v$在第$i$个位置的投影矩阵是

$$
{\mathbf{E}}_{i,v} = \mathbf{E}\left\lbrack  {i,v, : , : }\right\rbrack   \in  {\mathbb{R}}^{d \times  d}.
$$

Here, $\mathrm{E}$ can be seen as a generalized version of the embedding lookup table that uses a matrix ${\mathbf{E}}_{i,v}$ to represent each token $v$ . To get the generated embedding vector for token $v$ at the $i$ -th position, PAWA decoder first uses the transformer decoder to process the document into a set of latent vectors ${\mathbf{D}}^{\prime } = \left\lbrack  {{\mathbf{d}}_{1}^{\prime },\ldots ,{\mathbf{d}}_{M}^{\prime }}\right\rbrack   \in  {\mathbb{R}}^{d \times  M}$ . Then it multiplies the projection matrix ${\mathbf{E}}_{i,v}$ with the latent vector ${\mathbf{d}}_{i}^{\prime }$ and gets the final embedding vector ${\mathbf{e}}_{i,v} = {\mathbf{E}}_{i,v}{\mathbf{d}}_{i}^{\prime }$ .

在这里，$\mathrm{E}$ 可以被视为嵌入查找表（embedding lookup table）的广义版本，它使用矩阵 ${\mathbf{E}}_{i,v}$ 来表示每个标记 $v$。为了得到第 $i$ 个位置的标记 $v$ 的生成嵌入向量，PAWA 解码器首先使用变压器解码器（transformer decoder）将文档处理成一组潜在向量 ${\mathbf{D}}^{\prime } = \left\lbrack  {{\mathbf{d}}_{1}^{\prime },\ldots ,{\mathbf{d}}_{M}^{\prime }}\right\rbrack   \in  {\mathbb{R}}^{d \times  M}$。然后，它将投影矩阵 ${\mathbf{E}}_{i,v}$ 与潜在向量 ${\mathbf{d}}_{i}^{\prime }$ 相乘，得到最终的嵌入向量 ${\mathbf{e}}_{i,v} = {\mathbf{E}}_{i,v}{\mathbf{d}}_{i}^{\prime }$。

Therefore,we have the logit ${\mathbf{e}}_{v}^{\top }{\mathbf{h}}_{i}$ in loss Eq. (10) replaced by ${\mathbf{e}}_{i,v}^{\top }{\mathbf{h}}_{i} = {\mathbf{d}}_{i}^{\prime \top }{\mathbf{E}}_{i,v}^{\top }{\mathbf{h}}_{i} :$

因此，我们将损失方程（10）中的对数几率${\mathbf{e}}_{v}^{\top }{\mathbf{h}}_{i}$替换为${\mathbf{e}}_{i,v}^{\top }{\mathbf{h}}_{i} = {\mathbf{d}}_{i}^{\prime \top }{\mathbf{E}}_{i,v}^{\top }{\mathbf{h}}_{i} :$

$$
\mathcal{L}\left( {d,q}\right)  = \mathop{\sum }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}\left\lbrack  {-{\mathbf{d}}_{i}^{\prime \top }{\mathbf{E}}_{i,{d}_{i}}^{\top }{\mathbf{h}}_{i} + \log \mathop{\sum }\limits_{{v \in  \mathcal{V}}}\exp {\mathbf{d}}_{i}^{\prime \top }{\mathbf{E}}_{i,v}^{\top }{\mathbf{h}}_{i}}\right\rbrack  . \tag{17}
$$

With a similar derivation as in Section 4.2, the relevance can be established as

通过与4.2节类似的推导，可以建立如下相关性

$$
\operatorname{rel}\left( {d,q}\right)  = \operatorname{sum}\left( {{\widetilde{\mathbf{D}}}^{\prime \top }{\widetilde{\mathbf{E}}}_{d}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right) , \tag{18}
$$

where

其中

$$
{\widetilde{\mathbf{D}}}^{\prime } = \left\lbrack  \begin{matrix} {\mathbf{d}}_{1}^{\prime } & \mathbf{0} & \cdots & \mathbf{0} \\  \mathbf{0} & {\mathbf{d}}_{2}^{\prime } & \cdots & \mathbf{0} \\  \vdots & \vdots &  \ddots  & \vdots \\  \mathbf{0} & \mathbf{0} & \cdots & {\mathbf{d}}_{M}^{\prime } \end{matrix}\right\rbrack   \in  {\mathbb{R}}^{\left( {d \times  M}\right)  \times  M}, \tag{19}
$$

and ${\widetilde{\mathbf{E}}}_{d} = {\mathbf{W}}_{V}^{\top }\left\lbrack  {{\mathbf{E}}_{1,{d}_{1}},\ldots ,{\mathbf{E}}_{M,{d}_{M}}}\right\rbrack   \in  {\mathbb{R}}^{d \times  \left( {d \times  M}\right) }$ .

且${\widetilde{\mathbf{E}}}_{d} = {\mathbf{W}}_{V}^{\top }\left\lbrack  {{\mathbf{E}}_{1,{d}_{1}},\ldots ,{\mathbf{E}}_{M,{d}_{M}}}\right\rbrack   \in  {\mathbb{R}}^{d \times  \left( {d \times  M}\right) }$ 。

As we can see,PAWA multiplies the term ${\widetilde{\mathbf{E}}}_{d}^{\top }\mathbf{Q}$ with contex-tualized document token vectors ${\widetilde{\mathbf{D}}}^{\prime }$ ,which greatly improves the expressivity of document encoding.

正如我们所见，PAWA（概率注意力加权聚合）将项 ${\widetilde{\mathbf{E}}}_{d}^{\top }\mathbf{Q}$ 与上下文文档词元向量 ${\widetilde{\mathbf{D}}}^{\prime }$ 相乘，这极大地提高了文档编码的表达能力。

NP-decoding directly replaces ${\widetilde{E}}_{d}$ with $D$ . Lee et al. [16] employ an approach akin to contextualized sparse retrieval methods, leveraging token vectors encoded by the Contextualized Embedding Encoder [CE Encoder, 16], referred to as contextualized token em-beddings. This set of vectors,written as $\mathbf{D}$ in our notation,serves as the embedding table for the decoder. Both the BASE and ASYNC nonparametric decoding methods in [16] can be reformulated within our framework as:

非参数解码（NP - decoding）直接用 $D$ 替换 ${\widetilde{E}}_{d}$。Lee 等人 [16] 采用了一种类似于上下文稀疏检索方法的方式，利用由上下文嵌入编码器 [CE 编码器，16] 编码的词元向量，即上下文词元嵌入。在我们的表示法中，这组向量记为 $\mathbf{D}$，用作解码器的嵌入表。[16] 中的 BASE 和 ASYNC 非参数解码方法都可以在我们的框架内重新表述为：

$$
\operatorname{rel}\left( {d,q}\right)  = \operatorname{sum}\left( {{\mathbf{D}}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right) , \tag{20}
$$

where $\mathbf{D}$ are the token vectors of documents either pre-computed (as done by the pre-trained T5 model and frozen in the BASE method) or gradually updated (by the encoder of the GR model every $N$ epochs in the ASYNC method) during the training of the GR model.

其中 $\mathbf{D}$ 是文档的标记向量（token vectors），这些向量要么是预先计算好的（如预训练的 T5 模型所做的那样，并在 BASE 方法中固定下来），要么是在 GR 模型训练期间逐步更新的（在 ASYNC 方法中，GR 模型的编码器每 $N$ 个 epoch 更新一次）。

While the NP-decoding method shares the same document encoding with MVDR, two significant differences exist:

虽然 NP 解码方法与 MVDR 共享相同的文档编码，但存在两个显著差异：

(1) In NP-decoding, $\mathbf{D}$ is mostly frozen and detached from training, causing training imbalance compared to MVDR. $\mathbf{Q}$ and $\mathbf{A}$ in NP-decoding are fully trained,while $\mathbf{D}$ remains frozen.

(1) 在 NP 解码中，$\mathbf{D}$ 大多是固定的，并且与训练分离，与 MVDR 相比会导致训练不平衡。NP 解码中的 $\mathbf{Q}$ 和 $\mathbf{A}$ 会进行充分训练，而 $\mathbf{D}$ 保持固定。

(2) Due to GR computing logits for the entire vocabulary in each generation step, there is a need to reduce the large storage footprint of $\mathbf{D}$ to save computation. NP-decoding methods address this by using clustering to compress token vectors. MVDR, on the other hand, achieves lower inference time through a sparse alignment strategy.

(2) 由于 GR 在每个生成步骤中都会为整个词汇表计算对数几率（logits），因此需要减少 $\mathbf{D}$ 的大量存储占用以节省计算资源。NP 解码方法通过使用聚类来压缩标记向量来解决这个问题。另一方面，MVDR 通过稀疏对齐策略实现了更低的推理时间。

### 5.2 Alignment strategy

### 5.2 对齐策略

In addition to document encoding, the alignment matrix represents a crucial distinction between GR and MVDR methods. This matrix plays a decisive role in shaping the divergent inference procedures employed in retrieval. In this section, we analyze the alignment matrix,denoted as $\mathbf{A}$ within our unified framework,in terms of sparsity, alignment direction, and some common properties.

除文档编码外，对齐矩阵（alignment matrix）代表了广义回归（GR）方法和最小方差无失真响应（MVDR）方法之间的一个关键区别。该矩阵在塑造检索中采用的不同推理过程方面起着决定性作用。在本节中，我们从稀疏性、对齐方向和一些常见属性的角度分析对齐矩阵，在我们的统一框架中用 $\mathbf{A}$ 表示。

5.2.1 The concept of "alignment" in both methods. The concept of "alignment" has garnered significant attention in MVDR [8, 17, 22, 32]. We will briefly introduce the alignment problem in MVDR models, and claim that the alignment matrix of the GR method, as asserted in our framework Eq. (16), indeed exhibits similar alignment functionality to MVDR models.

5.2.1 两种方法中的“对齐”概念。“对齐”概念在最小方差无失真响应（MVDR）方法中受到了广泛关注 [8, 17, 22, 32]。我们将简要介绍最小方差无失真响应（MVDR）模型中的对齐问题，并声称在我们的框架方程（16）中所断言的广义回归（GR）方法的对齐矩阵，实际上表现出与最小方差无失真响应（MVDR）模型类似的对齐功能。

Token alignment involves determining whether tokens from the query and document should be matched lexically or semantically. It essentially represents another formulation of the "term mismatch problem" $\left\lbrack  {9,{14},{32},{47}}\right\rbrack$ . The prevailing strategy considered optimal at present is the all-to-all soft alignment strategy in MVDR models [14, 32], which eliminates the lexical form match restriction.

Token对齐（Token alignment）涉及确定查询和文档中的Token是应进行词法匹配还是语义匹配。它本质上代表了“术语不匹配问题” $\left\lbrack  {9,{14},{32},{47}}\right\rbrack$ 的另一种表述。目前被认为最优的主流策略是最小方差无失真响应（MVDR）模型中的全对全软对齐策略 [14, 32]，该策略消除了词法形式匹配的限制。

GR methods leverage the transformer architecture that originated in NLP, and the concept of "alignment" has been extensively discussed in the domain of neural machine translation $\left\lbrack  {3,5,{20},{21}}\right\rbrack$ , focusing on the alignment between tokens in source and target sentences. The attention mechanism, as a core component, computes the alignment matrix and proves highly effective in capturing alignment between source and target sentences $\left\lbrack  {5,{12},{43},{46}}\right\rbrack$ . Theoretical work $\left\lbrack  {7,{30}}\right\rbrack$ has validated the phenomenon of copying behavior, forming a foundational basis for the alignment ability.

图表示（GR）方法利用了起源于自然语言处理（NLP）的Transformer架构，并且“对齐”的概念在神经机器翻译领域得到了广泛讨论$\left\lbrack  {3,5,{20},{21}}\right\rbrack$，主要关注源句子和目标句子中标记之间的对齐。注意力机制作为核心组件，计算对齐矩阵，并在捕捉源句子和目标句子之间的对齐方面被证明非常有效$\left\lbrack  {5,{12},{43},{46}}\right\rbrack$。理论研究$\left\lbrack  {7,{30}}\right\rbrack$验证了复制行为现象，为对齐能力奠定了基础。

We conclude that, in GR methods, the attention matrix is able to capture the alignment between the query and the document, akin to the alignment matrix observed in MVDR methods.

我们得出结论，在图表示（GR）方法中，注意力矩阵能够捕捉查询与文档之间的对齐，类似于最小方差无失真响应（MVDR）方法中观察到的对齐矩阵。

5.2.2 Different sparsity and learnability: sparse vs. dense and learned alignment matrices. The alignment matrices of MVDR and GR differ in sparsity and learnability. MVDR typically employs a sparse alignment matrix for maximum efficiency during inference. In contrast, GR uses a dense and fully learnable alignment matrix derived from the computationally intensive attention mechanism.

5.2.2 不同的稀疏性和可学习性：稀疏与密集以及可学习的对齐矩阵。MVDR（最小方差无失真响应）和GR（此处未明确，可能是某种方法缩写）的对齐矩阵在稀疏性和可学习性方面存在差异。MVDR通常采用稀疏对齐矩阵，以在推理过程中实现最高效率。相比之下，GR使用从计算密集型注意力机制派生而来的密集且完全可学习的对齐矩阵。

For MVDR methods, the sparse alignment matrix is often computed using heuristic algorithms [11, 14, 32]. Taking ColBERT [14] as an example, it selects the most relevant document token for each query token. The relevance score between document $d$ and query $q$ is computed as

对于MVDR方法，稀疏对齐矩阵通常使用启发式算法进行计算[11, 14, 32]。以ColBERT [14]为例，它为每个查询词元选择最相关的文档词元。文档$d$和查询$q$之间的相关性得分计算如下

$$
\operatorname{rel}\left( {d,q}\right)  = \operatorname{sum}\left( {{\mathbf{D}}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)  = \mathop{\sum }\limits_{{i,j}}{\mathbf{d}}_{i}^{\top }\mathbf{q}{A}_{ij}
$$

$$
 = \mathop{\sum }\limits_{{j \in  \left\lbrack  N\right\rbrack  }}\mathop{\max }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}{\mathbf{d}}_{i}^{\top }{\mathbf{q}}_{j} = \mathop{\sum }\limits_{{j \in  \left\lbrack  N\right\rbrack  }}\operatorname{rel}\left( {d,{q}_{j}}\right) , \tag{21}
$$

where $\mathbf{A}$ is a sparse alignment matrix with only one non-zero element for each column $\left( {{A}_{ij} = 1}\right.$ if ${\mathbf{d}}_{i}^{\top }{\mathbf{q}}_{j} = \mathop{\max }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}{\mathbf{d}}_{i}^{\top }{\mathbf{q}}_{j}$ ; ${A}_{ij} = 0$ otherwise). The sum-max operation is highly parallelizable, ensuring efficiency during inference.

其中 $\mathbf{A}$ 是一个稀疏对齐矩阵（sparse alignment matrix），对于每一列 $\left( {{A}_{ij} = 1}\right.$，若 ${\mathbf{d}}_{i}^{\top }{\mathbf{q}}_{j} = \mathop{\max }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}{\mathbf{d}}_{i}^{\top }{\mathbf{q}}_{j}$，则该列仅有一个非零元素；否则 ${A}_{ij} = 0$）。求和 - 最大化操作具有高度的并行性，可确保推理过程的效率。

For GR methods, the alignment matrix is computed through the attention mechanism, considering all possible pairs of query and document tokens, as shown in Eq. (16).

对于GR方法（GR methods），对齐矩阵是通过注意力机制（attention mechanism）计算得出的，该机制会考虑查询词和文档词元的所有可能组合，如公式（16）所示。

The dense alignment matrix is highly expressive and trainable. While not suitable for exact relevance score computation in inference for each query-document pair, efficient approximate algorithms such as greedy search or beam search can be used to retrieve the top- $k$ documents. These decoding algorithms rely on the following decomposition:

密集对齐矩阵（dense alignment matrix）具有很强的表达能力且可训练。虽然它不适用于在推理时为每一对查询 - 文档精确计算相关性得分，但可以使用诸如贪心搜索（greedy search）或束搜索（beam search）等高效的近似算法来检索前 $k$ 个文档。这些解码算法依赖于以下分解：

$$
\operatorname{rel}\left( {d,q}\right)  = \mathop{\sum }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}\operatorname{sum}\left( {{\widetilde{\mathbf{e}}}_{{d}_{i}}^{\top }\mathbf{Q}{\mathbf{\alpha }}_{i}}\right)  = \mathop{\sum }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}\operatorname{rel}\left( {{d}_{i},q}\right) , \tag{22}
$$

where $\operatorname{rel}\left( {{d}_{i},q}\right)$ is conditioned on ${d}_{0},\ldots ,{d}_{i - 1}$ ,approximating the search for the most relevant document $d$ to finding the most relevant token ${d}_{i}$ at each position $i$ .

其中 $\operatorname{rel}\left( {{d}_{i},q}\right)$ 以 ${d}_{0},\ldots ,{d}_{i - 1}$ 为条件，将寻找最相关文档 $d$ 的问题近似为在每个位置 $i$ 寻找最相关的标记 ${d}_{i}$ 的问题。

5.2.3 Different alignment directions: query-to-document vs. document-to-query alignment. Beyond differences in the sparsity and learnability of the alignment matrix, MVDR and GR exhibit distinctions in their alignment directions.

5.2.3 不同的对齐方向：查询到文档对齐与文档到查询对齐。除了对齐矩阵在稀疏性和可学习性方面的差异外，MVDR（最小方差无失真响应）和 GR（此处未明确，可能是某种方法）在对齐方向上也表现出区别。

Eq. (21) reveals that MVDR's relevance score can be decomposed into the sum of relevance scores for each query token and its aligned document token. In this context, we consider the alignment matrix in MVDR as query-to-document alignment. Each query token individually aligns to a document token, seeking the optimal match during retrieval. Mathematically, the alignment matrix is computed column-wise and represents a one-hot vector for each column.

公式 (21) 表明，MVDR 的相关性得分可以分解为每个查询标记及其对齐的文档标记的相关性得分之和。在这种情况下，我们将 MVDR 中的对齐矩阵视为查询到文档的对齐。每个查询标记单独与一个文档标记对齐，在检索过程中寻求最优匹配。从数学角度来看，对齐矩阵是按列计算的，并且每列表示一个独热向量。

Conversely, the relevance score of GR, as depicted in Eq. (22), is the sum of relevance scores for each document token and its softly aligned query token. Here, we categorize the alignment matrix in GR as document-to-query alignment. Each document token is considered individually to focus attention on the most relevant query token. The alignment matrix is computed row-wise with a $\operatorname{softmax}\left( \cdot \right)$ operation to normalize attention weights in each row.

相反，如公式(22)所示，GR（全局相关性，Global Relevance）的相关性得分是每个文档标记及其软对齐查询标记的相关性得分之和。在这里，我们将GR中的对齐矩阵归类为文档到查询的对齐。单独考虑每个文档标记，以将注意力集中在最相关的查询标记上。对齐矩阵通过$\operatorname{softmax}\left( \cdot \right)$操作按行计算，以对每行中的注意力权重进行归一化。

Document-to-query alignment may seem counter-intuitive for a retrieval task, as we do not know the target documents while predicting. As a solution, GR pre-computes the alignment strategy for the document token ${d}_{i}$ (to be predicted) using previous document tokens ${d}_{0},\ldots ,{d}_{i - 1}$ and thus can retrieve the next token that best aligns with the desired next alignment strategy.

对于检索任务而言，文档到查询的对齐可能看起来有悖直觉，因为我们在预测时并不知道目标文档。作为一种解决方案，GR（全局相关性，Global Relevance）使用先前的文档标记${d}_{0},\ldots ,{d}_{i - 1}$预先计算文档标记${d}_{i}$（待预测）的对齐策略，从而可以检索出与所需的下一个对齐策略最匹配的下一个标记。

5.2.4 Low-rank nature of both alignment matrices. In analyzing the shared characteristics of the two alignment matrices, it is demonstrated that both matrices exhibit a low-rank property.

5.2.4 两个对齐矩阵的低秩性质。在分析这两个对齐矩阵的共同特征时，结果表明这两个矩阵都呈现出低秩特性。

MVDR models, e.g., ALIGNER [32], integrate the pairwise alignment matrix with unary salience, given by

最小方差无失真响应（MVDR）模型，例如ALIGNER模型 [32]，将成对对齐矩阵与一元显著性相结合，具体如下

$$
\mathbf{A} = \widetilde{\mathbf{A}} \odot  {\mathbf{u}}_{d}{\mathbf{u}}_{q}^{\top }. \tag{23}
$$

Here, $\widetilde{\mathbf{A}} \in  {\mathbb{R}}^{M \times  N}$ signifies the pairwise alignment matrix,determining the alignment of query and document tokens. The sparse token weights, ${\mathbf{u}}_{d} \in  {\mathbb{R}}^{M}$ and ${\mathbf{u}}_{q} \in  {\mathbb{R}}^{N}$ ,decide whether a token requires alignment. Notably,the alignment matrix $\mathbf{A}$ contains a low-rank component ${\mathbf{u}}_{d}{\mathbf{u}}_{q}^{\top }$ that influences the alignment strategy.

在此，$\widetilde{\mathbf{A}} \in  {\mathbb{R}}^{M \times  N}$ 表示成对对齐矩阵（pairwise alignment matrix），它决定了查询词和文档标记的对齐方式。稀疏标记权重 ${\mathbf{u}}_{d} \in  {\mathbb{R}}^{M}$ 和 ${\mathbf{u}}_{q} \in  {\mathbb{R}}^{N}$ 决定了一个标记是否需要对齐。值得注意的是，对齐矩阵 $\mathbf{A}$ 包含一个低秩分量 ${\mathbf{u}}_{d}{\mathbf{u}}_{q}^{\top }$，它会影响对齐策略。

In the case of GR methods, the alignment matrix is computed using an attention mechanism, which inherently results in a low-rank matrix. A lemma provides evidence of this low-rank property and is presented briefly here, with a detailed proof delegated to Wu et al. [41, Appendix B] due to space limitations.

对于GR方法，对齐矩阵是使用注意力机制计算得出的，这本质上会得到一个低秩矩阵。有一个引理证明了这种低秩特性，这里简要介绍该引理，由于篇幅限制，详细证明请参考Wu等人的文献 [41, 附录B]。

LEMMA 5.1. For a matrix $\mathbf{A} = \operatorname{softmax}\left( {{\mathbf{D}}^{\top }\mathbf{{WQ}}}\right)$ ,there exists a rank-one matrix $\mathbf{R}$ such that

引理5.1。对于矩阵$\mathbf{A} = \operatorname{softmax}\left( {{\mathbf{D}}^{\top }\mathbf{{WQ}}}\right)$，存在一个秩为1的矩阵$\mathbf{R}$，使得

$$
\parallel \mathbf{A} - \mathbf{R}\parallel  \leq  {4\gamma }\parallel \mathbf{W}\parallel , \tag{24}
$$

where the term $\gamma$ depends on the matrix entries.

其中项$\gamma$取决于矩阵元素。

From this lemma, we can conclude that both MVDR and GR methods reveal a rank-one component in their alignment matrices.

从这个引理，我们可以得出结论：MVDR和GR方法在它们的对齐矩阵中都揭示了一个秩为1的分量。

5.2.5 Decomposition of both relevance scores. In this subsection, we show that the relevance score computation in both MVDR and GR models can be decomposed into query and document components.

5.2.5 两种相关性得分的分解。在本小节中，我们表明MVDR和GR模型中的相关性得分计算可以分解为查询和文档分量。

The MVDR method employs a bi-encoder architecture, wherein query and document tokens are modeled separately. This architecture can easily be regarded as a decomposition of the relevance score between the query and document:

MVDR方法采用双编码器架构，其中查询和文档标记分别建模。这种架构可以很容易地被视为查询和文档之间相关性得分的分解：

$$
\operatorname{rel}\left( {d,q}\right)  = \operatorname{sum}\left( {{\mathbf{D}}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)  = \operatorname{sum}\left( {\operatorname{top} - 1\left( {{\mathbf{D}}^{\top }\mathbf{Q}}\right) }\right) ,
$$

where top-1 $\left( \cdot \right)$ is the operator that selects the maximum value in each column of the matrix.

其中top - 1 $\left( \cdot \right)$是选择矩阵每列中最大值的运算符。

In the subsequent lemma, we establish that the relevance score of GR cannot only be decomposed but also be kernelized, implying the existence of a kernel function capable of processing both query vectors and document vectors to compute the score (further details are delegated to Wu et al. [41, Appendix C] due to space limitations):

在接下来的引理中，我们证明了广义相关性（GR）的相关性得分不仅可以分解，还可以进行核化，这意味着存在一个核函数，能够同时处理查询向量和文档向量来计算得分（由于篇幅限制，更多细节请参考Wu等人的文献[41，附录C]）：

LEMMA 5.2. For simplicity,let $\operatorname{rel}\left( {d,q}\right)  = \operatorname{sum}\left( {{\mathbf{D}}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)$ ,where $\mathbf{A} = \operatorname{softmax}\left( {{\mathbf{D}}^{\top }\mathbf{Q}}\right)$ . It can be kernelized as

引理5.2。为简单起见，设$\operatorname{rel}\left( {d,q}\right)  = \operatorname{sum}\left( {{\mathbf{D}}^{\top }\mathbf{Q} \odot  \mathbf{A}}\right)$ ，其中$\mathbf{A} = \operatorname{softmax}\left( {{\mathbf{D}}^{\top }\mathbf{Q}}\right)$ 。它可以核化为

$$
\operatorname{rel}\left( {d,q}\right)  = \mathop{\sum }\limits_{{i,j}}{\mathbf{d}}_{i}^{\top }{\mathbf{q}}_{j}{A}_{ij} = \mathop{\sum }\limits_{{i,j}}{\mathbf{d}}_{i}^{\top }{\mathbf{q}}_{j}\operatorname{softmax}{\left( {\mathbf{d}}_{i}^{\top }\mathbf{Q}\right) }_{j}
$$

$$
 = \mathop{\sum }\limits_{{i,j}}\frac{1}{{p}_{ij}}\operatorname{tr}\left( {\mathbf{F}{\left( {\mathbf{d}}_{i}\right) }^{\top }\mathbf{F}\left( {\mathbf{q}}_{j}\right) }\right) , \tag{25}
$$

where $\mathbf{F}\left( \mathbf{x}\right)  = \mathbf{x}\mathbf{\phi }{\left( \mathbf{x}\right) }^{\top }$ ,and ${p}_{ij}$ is a term that depends on ${\mathbf{d}}_{i}$ and ${\mathbf{q}}_{j}$ . We choose $\operatorname{elu}\left( \cdot \right)$ as the kernel function $\phi \left( \cdot \right)$ .

其中 $\mathbf{F}\left( \mathbf{x}\right)  = \mathbf{x}\mathbf{\phi }{\left( \mathbf{x}\right) }^{\top }$ ，并且 ${p}_{ij}$ 是一个依赖于 ${\mathbf{d}}_{i}$ 和 ${\mathbf{q}}_{j}$ 的项。我们选择 $\operatorname{elu}\left( \cdot \right)$ 作为核函数 $\phi \left( \cdot \right)$ 。

Furthermore, by applying the trace inequality, we can approximately decompose the relevance score as

此外，通过应用迹不等式，我们可以近似地将相关性得分分解为

$$
\operatorname{rel}\left( {d,q}\right)  \leq  \mathop{\sum }\limits_{{i,j}}\frac{1}{{\widehat{p}}_{i}{\widehat{p}}_{j}}\sqrt{\operatorname{tr}\left( {\mathbf{F}{\left( {\mathbf{d}}_{i}\right) }^{\top }\mathbf{F}\left( {\mathbf{d}}_{i}\right) }\right) }\sqrt{\operatorname{tr}\left( {\mathbf{F}{\left( {\mathbf{q}}_{j}\right) }^{\top }\mathbf{F}\left( {\mathbf{q}}_{j}\right) }\right) }.
$$

From this lemma, we can conclude that both relevance scores in MVDR and GR methods can be decomposed. The decomposition of MVDR is more straightforward, and the kernelization of GR is more complicated. Both kernelizations would provide possibilities for new retrieval strategies.

从这个引理我们可以得出结论，最小方差无失真响应（MVDR）和广义回归（GR）方法中的相关性得分都可以进行分解。MVDR的分解更直接，而GR的核化更复杂。两种核化方法都为新的检索策略提供了可能性。

### 5.3 Upshot

### 5.3 总结

In summary, our findings indicate that certain studies enhance the modeling capacity of GR by employing more expressive document encoding, akin to MVDR. Furthermore, GR employs a distinct alignment direction, but it also exhibits similar low-rank and decomposition properties as MVDR.

综上所述，我们的研究结果表明，某些研究通过采用更具表现力的文档编码来增强广义回归（GR）的建模能力，这与最小方差无失真响应（MVDR）类似。此外，GR采用了不同的对齐方向，但它也表现出与MVDR相似的低秩和分解特性。

## 6 EXPERIMENTAL SETUP

## 6 实验设置

Next, we seek experimental confirmation that generative retrieval and multi-vector dense retrieval share the same framework for measuring relevance to a query of a document, as derived in Section 4.

接下来，我们寻求实验验证，以证实生成式检索和多向量密集检索在衡量文档与查询的相关性方面共享相同的框架，这一框架在第4节中已推导得出。

### 6.1 Datasets

### 6.1 数据集

We conduct experiments on two well-known datasets, NQ [15] and MS MARCO [27]. We use the same settings and processed datasets as Sun et al. [36], and we summarize the statistics of the datasets in Table 4.

我们在两个著名的数据集上进行了实验，即自然问答数据集（NQ）[15]和微软机器阅读理解数据集（MS MARCO）[27]。我们采用了与Sun等人[36]相同的设置和处理后的数据集，并在表4中总结了这些数据集的统计信息。

<!-- Media -->

Table 4: Statistics of datasets used in our experiments.

表4：我们实验中使用的数据集的统计信息。

<table><tr><td>Dataset</td><td>#Docs</td><td>#Test queries</td><td>#Train pairs</td></tr><tr><td>NQ320K</td><td>109,739</td><td>7,830</td><td>307,373</td></tr><tr><td>MS MARCO</td><td>323,569</td><td>5,187</td><td>366,235</td></tr></table>

<table><tbody><tr><td>数据集</td><td>文档数量</td><td>测试查询数量</td><td>训练对数量</td></tr><tr><td>NQ320K（原词不变）</td><td>109,739</td><td>7,830</td><td>307,373</td></tr><tr><td>微软机器阅读理解数据集（MS MARCO）</td><td>323,569</td><td>5,187</td><td>366,235</td></tr></tbody></table>

<!-- Media -->

$\mathrm{{NQ}}{320}\mathrm{k}$ . NQ320 $\mathrm{K}$ is a popular dataset for evaluating retrieval models $\left\lbrack  {{13},{24},{38},{39}}\right\rbrack$ . It is based on the Natural Questions (NQ) dataset [15]. NQ320k consists of ${320}\mathrm{k}$ query-document pairs,where the documents are gathered from Wikipedia pages, and the queries are natural language questions.

$\mathrm{{NQ}}{320}\mathrm{k}$ . NQ320 $\mathrm{K}$是用于评估检索模型的常用数据集$\left\lbrack  {{13},{24},{38},{39}}\right\rbrack$。它基于自然问题（Natural Questions，NQ）数据集[15]。NQ320k由${320}\mathrm{k}$查询 - 文档对组成，其中文档从维基百科页面收集而来，查询则是自然语言问题。

MS MARCO. The MS MARCO document retrieval dataset is a collection of queries and web pages from Bing searches. Like NQ320k and following [36], we sample a subset of documents from the labeled documents and use their corresponding queries for training. We evaluate the models on the queries of the MS MARCO dev set and retrieval on the sampled document subset.

MS MARCO。MS MARCO文档检索数据集是一个包含来自必应搜索的查询和网页的集合。与NQ320k类似，并且遵循文献[36]的做法，我们从标注文档中抽样出一个文档子集，并使用它们对应的查询进行训练。我们在MS MARCO开发集的查询上评估模型，并在抽样的文档子集上进行检索评估。

### 6.2 Base models

### 6.2 基础模型

As we aim to provide a new perspective on GR as MVDR, we consider representative models from both paradigms, i.e., SEAL [1] for GR and ColBERT [14] for MVDR. For a fair comparison, we reproduce both methods using the T5 architecture [33]. We have made several changes to adapt ColBERT and SEAL to their T5 variants: ${}^{1}$

由于我们旨在从多向量文档检索（MVDR）的角度为全局检索（GR）提供新视角，我们从这两种范式中选取了具有代表性的模型，即用于全局检索的SEAL [1]和用于多向量文档检索的ColBERT [14]。为了进行公平比较，我们使用T5架构 [33]重现了这两种方法。我们对ColBERT和SEAL进行了一些修改，以使其适应T5变体：${}^{1}$

- T5-ColBERT. We use in-batch negative samples instead of the pair-wise samples in the official ColBERTv1 implementation. We set the batch size to 256 and train 5 epochs. Due to space limitations, details of our T5 variant ColBERT are delegated to Wu et al. [41, Appendix D].

- T5-ColBERT。我们使用批次内负样本，而非官方ColBERTv1实现中的成对样本。我们将批次大小设置为256，并训练5个轮次。由于篇幅限制，我们的T5变体ColBERT的详细信息请参考Wu等人的文章 [41，附录D]。

- T5-SEAL. We use the Huggingface transformers library [40] to train the model. We use the constructed query-to-span data for training and each span has a length of 10 sampled according to Bevilacqua et al. [1]. The learning rate is set to 1e-3 and the batch size is 256.

- T5-SEAL。我们使用Huggingface的transformers库 [40]来训练模型。我们使用构建的查询到片段（query-to-span）数据进行训练，每个片段的长度为10，采样方法参考了Bevilacqua等人的文章 [1]。学习率设置为1e - 3，批次大小为256。

### 6.3 Inference settings

### 6.3 推理设置

We consider two inference settings: end-to-end and re-ranking.

我们考虑两种推理设置：端到端和重排序。

End-to-end retrieval setting. Both methods can perform an end-to-end retrieval on the corpus for a given query.

端到端检索设置。两种方法都可以针对给定查询在语料库上进行端到端检索。

- T5-ColBERT maintains a large vector pool of all document token vectors after training. During inference, it first retrieves for each query token vector,the $k$ -nearest document token vectors in the vector pool,resulting in $N \times  k$ retrieved vectors. These vectors are from at most $N \times  k$ different documents which are used as candidates. It then computes the exact relevance score for each candidate document and performs the final re-ranking.

- T5-ColBERT（一种模型）在训练后会维护一个包含所有文档词元向量的大型向量池。在推理过程中，它首先为每个查询词元向量在向量池中检索出$k$个最近邻的文档词元向量，从而得到$N \times  k$个检索到的向量。这些向量最多来自$N \times  k$个不同的文档，这些文档被用作候选文档。然后，它会为每个候选文档计算精确的相关性得分，并进行最终的重排序。

- T5-SEAL directly uses its generative style inference with the help of constrained beam search to predict valid document identifiers, i.e., n-grams from the documents.

- T5-SEAL（一种模型）借助受限束搜索直接使用其生成式推理来预测有效的文档标识符，即文档中的n元语法。

Re-ranking setting. Since we are focusing on relevance computing in the training target, we introduce a re-ranking setting that removes the influence of different approximated inference strategies. As stated in some previous work $\left\lbrack  {{17},{24}}\right\rbrack$ ,both MVDR and GR have discrepancies between training and inference. The approximated retrieval methods are largely different from the training target and may decrease the performance of the trained retrievers. In the re-ranking setting, we collect 100 documents retrieved by BM25 [35] together with the ground-truth document as the candidate set for each query. As in the training stage, we take both the query and each candidate document as the input of the model and use the relevance computing in Section 3 and 4.

重排序设置。由于我们在训练目标中专注于相关性计算，因此引入了一种重排序设置，以消除不同近似推理策略的影响。正如一些先前的工作$\left\lbrack  {{17},{24}}\right\rbrack$所述，最小方差无失真响应（MVDR，Minimum Variance Distortionless Response）和贪婪检索（GR，Greedy Retrieval）在训练和推理之间都存在差异。近似检索方法与训练目标有很大不同，可能会降低训练好的检索器的性能。在重排序设置中，我们将由BM25 [35]检索到的100篇文档与真实文档一起收集起来，作为每个查询的候选集。与训练阶段一样，我们将查询和每个候选文档都作为模型的输入，并使用第3节和第4节中的相关性计算方法。

## 7 EXPERIMENTAL ANALYSES

## 7 实验分析

### 7.1 Performance of different alignment directions

### 7.1 不同对齐方向的性能

As described in Section 5.2.3, MVDR and GR exhibit different alignment directions, i.e., query-to-document and document-to-query alignment. We aim to look at how alignment directions affect retrieval performance. We first conduct experiments in the re-ranking setting to show the performance gap between MVDR and GR. As shown in Table 5, MVDR with the original alignment strategy, which is indicated as MVDR $\left( {\mathrm{q} \rightarrow  \mathrm{d}}\right)$ ,has a much better performance than GR. To compare the alignment directions of MVDR and GR, we have designed a model MVDR (q $\leftarrow$ d) that integrates the features of both, i.e., expressive document encoding from MVDR and document-to-query alignment strategy from GR. From Table 5, we can see that the performance of the new model is roughly intermediate between the other two. Note that the designed experimental model MVDR $\left( {\mathrm{q} \leftarrow  \mathrm{d}}\right)$ can only be used in a re-ranking setting. We conclude that query-to-document alignment is preferred for re-ranking.

如5.2.3节所述，最大信干噪比（MVDR，Minimum Variance Distortionless Response）和广义回归（GR，Generalized Regression）呈现出不同的对齐方向，即查询到文档对齐和文档到查询对齐。我们旨在研究对齐方向如何影响检索性能。我们首先在重排序设置下进行实验，以展示MVDR和GR之间的性能差距。如表5所示，采用原始对齐策略的MVDR（表示为MVDR $\left( {\mathrm{q} \rightarrow  \mathrm{d}}\right)$）的性能比GR好得多。为了比较MVDR和GR的对齐方向，我们设计了一个模型MVDR (q $\leftarrow$ d)，该模型集成了两者的特征，即来自MVDR的富有表现力的文档编码和来自GR的文档到查询对齐策略。从表5中可以看出，新模型的性能大致介于另外两者之间。请注意，所设计的实验模型MVDR $\left( {\mathrm{q} \leftarrow  \mathrm{d}}\right)$仅可用于重排序设置。我们得出结论，查询到文档对齐更适合重排序。

### 7.2 Term matching in alignment

### 7.2 对齐中的词项匹配

As we have discussed in Section 5.2.1, alignment is essentially a term-matching problem. In this section, we design an experiment to observe the extent of term matching in the two methods, and we find that both methods exhibit a preference for exact term matching in their alignment.

正如我们在5.2.1节中所讨论的，对齐本质上是一个术语匹配问题。在本节中，我们设计了一个实验来观察这两种方法中术语匹配的程度，我们发现这两种方法在对齐时都倾向于精确的术语匹配。

Exact match of MVDR in query-to-document direction. We calculate the exact matching rate between document token IDs and each query token ID during the alignment process, which we refer to as the "hard exact match rate." We also define a "soft exact match rate" which is the alignment score corresponding to the exact match query-document token pairs. The alignment score is defined as the element in the alignment matrix. As MVDR uses a sparse alignment matrix,we apply column-wise softmax $\left( \cdot \right)$ to $\mathbf{A}$ and use the element as the alignment score. We average the rate over candidate documents for each query token and categorize the query tokens according to their IDF. We assume that IDF approximates the term importance as is done in [10]. From the results in Figure 2, we can see that MVDR chooses exactly matched document tokens in 11.4% on average. Also, we notice that rare query tokens have not received much attention during alignment. This observation suggests that MVDR may prioritize commonly occurring query tokens in its alignment process, potentially overlooking or underemphasizing the importance of rare query tokens.

查询到文档方向上MVDR（最小方差无畸变响应，Minimum Variance Distortionless Response）的精确匹配。在对齐过程中，我们计算文档标记ID与每个查询标记ID之间的精确匹配率，我们将其称为“硬精确匹配率”。我们还定义了一个“软精确匹配率”，它是与精确匹配的查询 - 文档标记对相对应的对齐分数。对齐分数定义为对齐矩阵中的元素。由于MVDR使用稀疏对齐矩阵，我们对$\left( \cdot \right)$到$\mathbf{A}$应用按列的softmax函数，并将该元素用作对齐分数。我们对每个查询标记的候选文档的匹配率进行平均，并根据查询标记的逆文档频率（IDF，Inverse Document Frequency）对其进行分类。我们假设IDF近似于术语的重要性，就像文献[10]中所做的那样。从图2的结果中，我们可以看到MVDR平均选择了11.4%的精确匹配文档标记。此外，我们注意到在对齐过程中，罕见的查询标记没有得到太多关注。这一观察结果表明，MVDR在其对齐过程中可能会优先考虑常见的查询标记，可能会忽略或低估罕见查询标记的重要性。

---

<!-- Footnote -->

${}^{1}$ Our code link is https://github.com/Furyton/GR-as-MVDR.

${}^{1}$ 我们的代码链接是https://github.com/Furyton/GR-as-MVDR。

<!-- Footnote -->

---

<!-- Media -->

Table 5: Comparison of MVDR and GR in the re-ranking setting. MVDR and GR are our reproduced T5-SEAL and T5- ColBERT. "R" denotes Recall, and "M" denotes MRR.

表5：重排序设置下MVDR（最小方差无失真响应）和GR（此处未明确，保留原词）的比较。MVDR和GR分别是我们复现的T5 - SEAL和T5 - ColBERT。“R”表示召回率（Recall），“M”表示平均倒数排名（MRR）。

<table><tr><td rowspan="2">Model</td><td colspan="3">NQ320K</td><td colspan="3">MS MARCO</td></tr><tr><td>R@1</td><td>R@10</td><td>M@10</td><td>R@1</td><td>R@10</td><td>M@10</td></tr><tr><td>MVDR $\left( {\mathrm{q} \rightarrow  \mathrm{d}}\right)$</td><td>61.3</td><td>91.9</td><td>72.0</td><td>46.5</td><td>84.5</td><td>58.9</td></tr><tr><td>MVDR (q←d)</td><td>53.2</td><td>90.1</td><td>65.7</td><td>34.8</td><td>78.8</td><td>48.4</td></tr><tr><td>GR</td><td>47.4</td><td>87.0</td><td>60.5</td><td>35.3</td><td>77.1</td><td>48.3</td></tr></table>

<table><tbody><tr><td rowspan="2">型号</td><td colspan="3">NQ320K</td><td colspan="3">微软机器阅读理解数据集（MS MARCO）</td></tr><tr><td>R@1</td><td>R@10</td><td>M@10</td><td>R@1</td><td>R@10</td><td>M@10</td></tr><tr><td>最小方差无失真响应（MVDR） $\left( {\mathrm{q} \rightarrow  \mathrm{d}}\right)$</td><td>61.3</td><td>91.9</td><td>72.0</td><td>46.5</td><td>84.5</td><td>58.9</td></tr><tr><td>最小方差无失真响应（MVDR） (查询词←文档)</td><td>53.2</td><td>90.1</td><td>65.7</td><td>34.8</td><td>78.8</td><td>48.4</td></tr><tr><td>广义回归（GR）</td><td>47.4</td><td>87.0</td><td>60.5</td><td>35.3</td><td>77.1</td><td>48.3</td></tr></tbody></table>

<!-- Media -->

Exact match of MVDR and GR in document-to-query direction. We have devised an experiment to investigate the alignment in the opposite direction, i.e., document-to-query, in Figure 3. As GR is not trained with hard alignment, we only examine the soft exact match rate of both methods. The computation of the exact match rate is similar except that it is computed for each document token. From the results, we have discerned a consistent trend: as the importance of tokens increases, the rate of exact matches also tends to rise. We think this is because it is hard for the rare query token to match among many common tokens since the document is much longer than the query. When we look at each document token, it will be easier to match among fewer query tokens. We also conduct experiments on MS MARCO and have similar results.

MVDR和GR在文档到查询方向上的精确匹配。我们设计了一个实验来研究相反方向（即文档到查询）的对齐情况，如图3所示。由于GR未经过硬对齐训练，我们仅考察两种方法的软精确匹配率。精确匹配率的计算方式类似，只是针对每个文档标记进行计算。从结果中，我们发现了一个一致的趋势：随着标记重要性的增加，精确匹配率也趋于上升。我们认为这是因为文档比查询长得多，罕见的查询标记很难在众多常见标记中匹配到。当我们查看每个文档标记时，在较少的查询标记中进行匹配会更容易。我们还在MS MARCO数据集上进行了实验，得到了类似的结果。

<!-- Media -->

<!-- figureText: 0.4 0.05 0.2 ), 3) [3, 5) [5, 8) [8, 10) -->

<img src="https://cdn.noedgeai.com/01958ba3-6408-790f-88f5-651c789dda86_7.jpg?x=154&y=1519&w=707&h=246&r=0"/>

Figure 2: Exact match rate of MVDR on NQ320k dataset in the query-to-document direction.

图2：MVDR在NQ320k数据集上在查询到文档方向的精确匹配率。

<!-- Media -->

### 7.3 Improved document encoding

### 7.3 改进的文档编码

In Section 5.1, we include two popular document encoding methods, PAWA and NP-decoding, into our framework. To demonstrate the improvement of these two methods, we compare the performance of GR with and without them in Table 6. PAWA is typically used in GR with short semantic identifiers due to its high computational complexity during generation. Thus, we compare it with DSI [38] with semantic identifiers. Note that all these models use the same architecture (T5-base [33]) and similar training procedures without data augmentation, e.g., synthetic query-doc pairs generation, etc. *-PAWA and *-NP can be seen as a naive approach to using the two enhancing methods to the base GR models. From Table 6, we see that both PAWA and NP-decoding can greatly improve the performance and achieve similar results compared with T5-ColBERT on Recall@1.However, there is still a large gap in terms of Recall@10.The implementation of the additional decoding modules is only an approximation for reducing the cost of time and storage as discussed in Section 5.1. This, together with the alignment direction, may be a cause of the performance gap between GR equipped with these document encodings and MVDR.

在5.1节中，我们将两种流行的文档编码方法，即PAWA和NP解码，纳入我们的框架。为了展示这两种方法的改进效果，我们在表6中比较了有无这两种方法时GR（生成式检索）的性能。由于PAWA在生成过程中的计算复杂度较高，它通常用于带有短语义标识符的GR中。因此，我们将其与带有语义标识符的DSI [38]进行比较。请注意，所有这些模型都使用相同的架构（T5-base [33]）和相似的训练程序，且没有进行数据增强，例如合成查询 - 文档对生成等。*-PAWA和*-NP可以看作是将这两种增强方法应用于基础GR模型的简单方法。从表6中可以看出，与T5 - ColBERT在Recall@1指标上相比，PAWA和NP解码都能显著提高性能并取得相似的结果。然而，在Recall@10指标上仍存在较大差距。如5.1节所述，额外解码模块的实现只是为了降低时间和存储成本的一种近似方法。这一点，再加上对齐方向，可能是配备这些文档编码的GR与MVDR之间性能存在差距的原因。

<!-- Media -->

<!-- figureText: 0.035 0.1 $\left\lbrack  {0,3)\;\lbrack 3,5)\;\lbrack 5,7)\;\lbrack 7,9)\;\lbrack 9,{11})}\right\rbrack$ 0.025 $\begin{matrix} \lbrack 0, & 3) & \left\lbrack  {3,}\right\rbrack  & \left\lbrack  {5,7}\right) & \left\lbrack  {7,9}\right) & \left\lbrack  {9,{11}}\right)  \end{matrix}$ token IDF -->

<img src="https://cdn.noedgeai.com/01958ba3-6408-790f-88f5-651c789dda86_7.jpg?x=934&y=243&w=707&h=248&r=0"/>

Figure 3: Soft exact match rate of MVDR and GR on NQ320k dataset in the document-to-query direction.

图3：MVDR和GR在NQ320k数据集上从文档到查询方向的软精确匹配率。

Table 6: Performance of GR models with different document encoding methods on NQ320k in end-to-end setting. The results of DSI-PAWA, DSI, and T5-GENRE-NP are from [16, 36, 39]. T5-GENRE is the T5 variant of GENRE [2] used by [16].

表6：在端到端设置下，采用不同文档编码方法的GR模型在NQ320k上的性能。DSI - PAWA、DSI和T5 - GENRE - NP的结果来自文献[16, 36, 39]。T5 - GENRE是文献[16]使用的GENRE [2]的T5变体。

<table><tr><td>Model</td><td>R@1</td><td>R@10</td></tr><tr><td>T5-ColBERT</td><td>61.1</td><td>88.4</td></tr><tr><td>DSI [38]</td><td>55.2</td><td>67.4</td></tr><tr><td>DSI-PAWA [39]</td><td>60.2</td><td>80.2</td></tr><tr><td>T5-SEAL</td><td>44.7</td><td>75.5</td></tr><tr><td>T5-GENRE [2]</td><td>53.7</td><td>64.7</td></tr><tr><td>T5-GENRE-NP [16]</td><td>62.2</td><td>78.8</td></tr></table>

<table><tbody><tr><td>模型</td><td>R@1</td><td>R@10</td></tr><tr><td>T5-科尔伯特（T5-ColBERT）</td><td>61.1</td><td>88.4</td></tr><tr><td>分布式语义索引（DSI） [38]</td><td>55.2</td><td>67.4</td></tr><tr><td>基于位置感知加权注意力的分布式语义索引（DSI-PAWA） [39]</td><td>60.2</td><td>80.2</td></tr><tr><td>T5-密封（T5-SEAL）</td><td>44.7</td><td>75.5</td></tr><tr><td>T5-流派（T5-GENRE） [2]</td><td>53.7</td><td>64.7</td></tr><tr><td>T5-体裁-名词短语（T5-GENRE-NP） [16]</td><td>62.2</td><td>78.8</td></tr></tbody></table>

<!-- Media -->

### 7.4 Low-rank nature of alignment matrix

### 7.4 对齐矩阵的低秩特性

In Section 5.2.4, we show that the alignment matrix in GR also has a low-rank property in Lemma 5.1. As MVDR using alignment matrix (23) already contains a low-rank component, we only conduct experiments to verify GR. Since the $\gamma$ in Lemma 5.1 is hard to attain, we illustrate the relation between $\parallel \mathbf{W}\parallel$ and $\parallel \mathbf{A} - \mathbf{R}\parallel$ in Figure 4(a). We can see that the inequality is loose and $\parallel \mathbf{A} - \mathbf{R}\parallel$ is much lower than $\parallel \mathbf{W}\parallel$ . We also show the relative error of the approximation of $\mathbf{R}$ in Figure 4(b). The error is relatively low on average,which indicates the low-rank nature of the alignment matrix of GR.

在第5.2.4节中，我们在引理5.1中证明了广义瑞利商（GR）中的对齐矩阵也具有低秩特性。由于使用对齐矩阵（23）的最小方差无失真响应（MVDR）方法已经包含了一个低秩分量，我们仅通过实验来验证广义瑞利商（GR）。由于引理5.1中的$\gamma$难以获得，我们在图4（a）中展示了$\parallel \mathbf{W}\parallel$和$\parallel \mathbf{A} - \mathbf{R}\parallel$之间的关系。我们可以看到，该不等式较为宽松，且$\parallel \mathbf{A} - \mathbf{R}\parallel$远低于$\parallel \mathbf{W}\parallel$。我们还在图4（b）中展示了$\mathbf{R}$近似的相对误差。平均而言，误差相对较低，这表明广义瑞利商（GR）的对齐矩阵具有低秩特性。

### 7.5 Case study of the alignment matrix

### 7.5 对齐矩阵的案例研究

We chose a specific case from the dataset NQ320k to show what the alignment matrix looks like in Figure 5. Since the document is too long for demonstration, we simplify and extract a sub-sentence containing the answer to the query. In Figure 5(a), we have observed a pronounced phenomenon of exact matches in MVDR. The song name and people's names are completely matched with high scores.

我们从数据集NQ320k中选取了一个特定案例，在图5中展示对齐矩阵的样子。由于文档过长，不便于演示，我们简化并提取了一个包含查询答案的子句。在图5(a)中，我们观察到了多视图文档重排（MVDR）中明显的精确匹配现象。歌曲名称和人名完全匹配，且得分很高。

<!-- Media -->

<!-- figureText: 100 (a) Relation of $\parallel \mathbf{W}\parallel$ and $\parallel \mathbf{A} - \mathbf{R}\parallel$ -->

<img src="https://cdn.noedgeai.com/01958ba3-6408-790f-88f5-651c789dda86_8.jpg?x=159&y=258&w=700&h=300&r=0"/>

Figure 4: Low-rank approximation of $R$ to alignment matrix $A$ in GR in MS MARCO dataset.

图4：MS MARCO数据集中广义相关性（GR）下$R$到对齐矩阵$A$的低秩近似。

<!-- figureText: Ringo (a) MVDR document-to-query direction (b) GR query-to-document direction -->

<img src="https://cdn.noedgeai.com/01958ba3-6408-790f-88f5-651c789dda86_8.jpg?x=153&y=658&w=710&h=355&r=0"/>

Figure 5: An example of alignment matrix in MVDR and GR. In Figure 5(b), this phenomenon is less obvious, but each document token has more attention on the people's name and song name.

图5：多视图文档重排（MVDR）和广义相关性（GR）中对齐矩阵的一个示例。在图5(b)中，这种现象不太明显，但每个文档标记对人名和歌曲名称的关注度更高。

<!-- Media -->

### 7.6 Upshot

### 7.6 总结

We verify the existence of "exact term match," a specific alignment scenario, in both paradigms. We also show the superiority of the alignment direction in MVDR. The improved document encoding and the low-rank nature of the alignment matrix are validated.

我们验证了两种范式中都存在“精确术语匹配”这一特定对齐场景。我们还展示了多视图文档重排（MVDR）中对齐方向的优越性。改进后的文档编码和对齐矩阵的低秩性质得到了验证。

## 8 LIMITATIONS

## 8 局限性

We have examined the training target of GR and have connected it with MVDR, but we have not discussed whether relevance computing can be generalized to the generative style inference. We have not considered the multi-layer interactions in the cross-attention between query and document for simplicity.

我们已经研究了生成式检索（GR）的训练目标，并将其与最小方差无失真响应（MVDR）联系起来，但尚未讨论相关性计算是否可以推广到生成式风格推理。为简单起见，我们没有考虑查询和文档之间交叉注意力中的多层交互。

Our framework does not discuss how query-generation augmentation reduces the discrepancy between training and inference [50]. We aim to study how different architectures and identifier designs will affect the alignment and generalization during inference in future work.

我们的框架没有讨论查询生成增强如何减少训练和推理之间的差异[50]。我们的目标是在未来的工作中研究不同的架构和标识符设计将如何影响推理过程中的对齐和泛化能力。

## 9 CONCLUSION

## 9 结论

In this paper, we have offered new insights into GR from the perspective of MVDR that both paradigms share the same frameworks for measuring the relevance between a query and a document. Both paradigms compute relevance as a sum of products of query and document vectors and an alignment matrix. We have explored how GR applies this framework and differs from MVDR. We have shown that GR has simpler document encoding and an alignment strategy with different sparsity and direction. They also share a low-rank property and can be decomposed into query and document components. We have conducted extensive experiments to verify our conclusions and found that both methods have commonalities of term matching in the alignment. We also found that query-to-document alignment direction has better performance than document-to-query.

在本文中，我们从最小方差无失真响应（MVDR）的角度对生成式检索（GR）提供了新的见解，这两种范式在衡量查询和文档之间的相关性时采用了相同的框架。这两种范式都将相关性计算为查询向量、文档向量与对齐矩阵的乘积之和。我们探讨了生成式检索（GR）如何应用此框架以及它与最小方差无失真响应（MVDR）的不同之处。我们表明，生成式检索（GR）具有更简单的文档编码和具有不同稀疏性和方向的对齐策略。它们还具有低秩特性，并且可以分解为查询和文档组件。我们进行了广泛的实验来验证我们的结论，发现这两种方法在对齐方面具有术语匹配的共性。我们还发现，从查询到文档的对齐方向比从文档到查询的对齐方向具有更好的性能。

Based on our findings, practitioners in the field may consider leveraging the shared frameworks highlighted in this study to understand and develop new GR methods, and pay more attention to the classic term matching problem underlying GR models.

基于我们的研究结果，该领域的从业者可以考虑利用本研究中强调的共享框架来理解和开发新的全局表示（GR）方法，并更多地关注全局表示模型背后的经典术语匹配问题。

As to future work, we will continue to study how multi-layer attention may affect the framework. The difference in the generalization properties for new documents between DR and GR [4, 26, ${28},{49}\rbrack$ base on our framework is also an important aspect deserving further investigation. We will continue to discover new relations in the GR paradigm and provide more insights into the methodology.

关于未来的工作，我们将继续研究多层注意力机制可能如何影响该框架。基于我们的框架，文档表示（DR）和全局表示（GR）在新文档泛化特性上的差异[4, 26, ${28},{49}\rbrack$]也是一个值得进一步研究的重要方面。我们将继续探索全局表示范式中的新关系，并为该方法提供更多见解。

## ACKNOWLEDGMENTS

## 致谢

This research was (partially) funded by the Natural Science Foundation of China (62102234, 62372275, 62272274, 62202271, T2293773, 62072279), the National Key R&D Program of China with grant No.2022YFC3303004, the Natural Science Foundation of Shandong Province (ZR2021QF129), the Hybrid Intelligence Center, a 10-year program funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research, https://hybrid-intelligence-centre.nl, project LESSEN with project number NWA. 1389.20.183 of the research program NWA ORC 2020/21, which is (partly) financed by the Dutch Research Council (NWO), project ROBUST with project number KICH3.LTP.- 20.006, which is (partly) financed by the Dutch Research Council (NWO), DPG Media, RTL, and the Dutch Ministry of Economic Affairs and Climate Policy (EZK) under the program LTP KIC 2020- 2023, and the FINDHR (Fairness and Intersectional Non-Discrimination in Human Recommendation) project that received funding from the European Union's Horizon Europe research and innovation program under grant agreement No 101070212.

本研究（部分）由中国自然科学基金（62102234、62372275、62272274、62202271、T2293773、62072279）、国家重点研发计划（项目编号2022YFC3303004）、山东省自然科学基金（ZR2021QF129）、混合智能中心（该中心是一项为期10年的项目，由荷兰教育、文化和科学部通过荷兰科学研究组织资助，网址：https://hybrid-intelligence-centre.nl）、研究项目“LESSEN”（项目编号NWA. 1389.20.183，属于研究计划NWA ORC 2020/21，部分经费由荷兰研究委员会（NWO）提供）、项目“ROBUST”（项目编号KICH3.LTP.- 20.006，部分经费由荷兰研究委员会（NWO）、DPG媒体、RTL以及荷兰经济事务和气候政策部（EZK）在LTP KIC 2020 - 2023计划下提供）以及“FINDHR（人类推荐中的公平性和交叉非歧视）”项目资助，该项目获得了欧盟“地平线欧洲”研究与创新计划的资助（资助协议编号101070212）。

All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.

所有内容仅代表作者的观点，其各自的雇主和/或赞助商不一定认同或支持这些观点。

## A FURTHER EXPLANATION OF THE RELEVANCE SCORE IN GR

## 对GR中相关性得分的进一步解释

Further insights into the reason for using $\mathop{\sum }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}{\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i}$ as the relevance can be elaborated as follows. Suppose we treat all other token embeddings ${\mathbf{e}}_{k}$ ,where $k \neq  {d}_{i}$ ,as fixed with respect to ${\mathbf{e}}_{{d}_{i}}$ , then at the early stage of the training, the loss can be expressed as:

关于使用$\mathop{\sum }\limits_{{i \in  \left\lbrack  M\right\rbrack  }}{\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i}$作为相关性的原因，可进一步阐述如下。假设我们将所有其他词元嵌入${\mathbf{e}}_{k}$（其中$k \neq  {d}_{i}$）视为相对于${\mathbf{e}}_{{d}_{i}}$固定不变的，那么在训练的早期阶段，损失函数可以表示为：

$$
{\mathcal{L}}_{i}\left( {d,q}\right)  =  - {\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i} + \log \left( {\exp {\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i} + \mathop{\sum }\limits_{{v \neq  {d}_{i}}}\exp {\mathbf{e}}_{v}^{\top }{\mathbf{h}}_{i}}\right)  \tag{26}
$$

$$
 =  - {\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i} + \log \left( {\exp {\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i} + C}\right)  \propto   - {\mathbf{e}}_{{d}_{i}}^{\top }{\mathbf{h}}_{i}. \tag{27}
$$

Due to space limitations,details of Appendix A, B, C, and D can be found online in [41].

由于篇幅限制，附录A、B、C和D的详细内容可在文献[41]的网络版中找到。

## REFERENCES

## 参考文献

[1] Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022. Autoregressive Search Engines: Generating Substrings as Document Identifiers. Advances in Neural Information Processing Systems 35 (2022), 31668-31683.

[1] 米歇尔·贝维拉夸（Michele Bevilacqua）、朱塞佩·奥塔维亚诺（Giuseppe Ottaviano）、帕特里克·刘易斯（Patrick Lewis）、斯科特·伊（Scott Yih）、塞巴斯蒂安·里德尔（Sebastian Riedel）和法比奥·彼得罗尼（Fabio Petroni）。2022 年。自回归搜索引擎：将子字符串生成为文档标识符。《神经信息处理系统进展》35 卷（2022 年），第 31668 - 31683 页。

[2] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2021. Autoregressive Entity Retrieval. arXiv:2010.00904 [cs.CL]

[2] 尼古拉·德·曹（Nicola De Cao）、高蒂埃·伊扎卡尔（Gautier Izacard）、塞巴斯蒂安·里德尔（Sebastian Riedel）和法比奥·彼得罗尼（Fabio Petroni）。2021 年。自回归实体检索。预印本 arXiv:2010.00904 [计算机科学 - 计算语言学（cs.CL）]

[3] Guanhua Chen, Yun Chen, and Victor O. K. Li. 2021. Lexically Constrained Neural Machine Translation with Explicit Alignment Guidance. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021. AAAI Press, 12630-12638. https://doi.org/10.1609/AAAI.V35I14.17496

[3] 陈冠华（Guanhua Chen）、陈韵（Yun Chen）和李Victor O. K.。2021 年。具有显式对齐引导的词汇约束神经机器翻译。收录于第三十五届人工智能协会会议（Thirty - Fifth AAAI Conference on Artificial Intelligence，AAAI 2021）、第三十三届人工智能创新应用会议（Thirty - Third Conference on Innovative Applications of Artificial Intelligence，IAAI 2021）、第十一届人工智能教育进展研讨会（The Eleventh Symposium on Educational Advances in Artificial Intelligence，EAAI 2021），虚拟会议，2021 年 2 月 2 日至 9 日。AAAI 出版社，第 12630 - 12638 页。https://doi.org/10.1609/AAAI.V35I14.17496

[4] Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Wei Chen, Yixing Fan, and Xueqi Cheng. 2023. Continual Learning for Generative Retrieval over Dynamic Corpora. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM '23). Association for Computing Machinery, New York, NY, USA, 306-315. https://doi.org/10.1145/3583780.3614821

[4] 陈剑贵、张汝清、郭佳峰、马腾·德·里克（Maarten de Rijke）、陈伟、范逸兴和程学旗。2023年。动态语料库上生成式检索的持续学习。见第32届ACM信息与知识管理国际会议论文集（CIKM '23）。美国计算机协会，美国纽约州纽约市，306 - 315页。https://doi.org/10.1145/3583780.3614821

[5] Yun Chen, Yang Liu, Guanhua Chen, Xin Jiang, and Qun Liu. 2020. Accurate Word Alignment Induction from Neural Machine Translation. In EMNLP (1). Association for Computational Linguistics, 566-576.

[5] 陈云、刘洋、陈冠华、蒋鑫和刘群。2020年。从神经机器翻译中准确诱导词对齐。见自然语言处理经验方法会议（EMNLP (1)）。计算语言学协会，566 - 576页。

[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT (1). Association for Computational Linguistics, 4171-4186.

[6] 雅各布·德夫林（Jacob Devlin）、张明伟（Ming-Wei Chang）、肯顿·李（Kenton Lee）和克里斯蒂娜·图托纳娃（Kristina Toutanova）。2019年。BERT：用于语言理解的深度双向变换器预训练。见北美计算语言学协会人类语言技术会议（NAACL - HLT (1)）。计算语言学协会，4171 - 4186页。

[7] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Das-Sarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021. A Mathematical Framework for Transformer Circuits. Transformer Circuits Thread. https://transformer-circuits.pub/2021/framework/index.html.

[7] 尼尔森·埃尔哈格（Nelson Elhage）、尼尔·南达（Neel Nanda）、凯瑟琳·奥尔森（Catherine Olsson）、汤姆·亨尼根（Tom Henighan）、尼古拉斯·约瑟夫（Nicholas Joseph）、本·曼（Ben Mann）、阿曼达·阿斯克尔（Amanda Askell）、白运涛（Yuntao Bai）、安娜·陈（Anna Chen）、汤姆·科纳利（Tom Conerly）、诺瓦·达斯 - 萨尔马（Nova Das-Sarma）、道恩·德雷恩（Dawn Drain）、迪普·冈古利（Deep Ganguli）、扎克·哈特菲尔德 - 多兹（Zac Hatfield-Dodds）、丹尼·埃尔南德斯（Danny Hernandez）、安迪·琼斯（Andy Jones）、杰克逊·克尔尼翁（Jackson Kernion）、莉安·洛维特（Liane Lovitt）、卡迈勒·恩杜塞（Kamal Ndousse）、达里奥·阿莫迪（Dario Amodei）、汤姆·布朗（Tom Brown）、杰克·克拉克（Jack Clark）、贾里德·卡普兰（Jared Kaplan）、山姆·麦坎德利什（Sam McCandlish）和克里斯·奥拉（Chris Olah）。2021 年。Transformer 电路的数学框架。Transformer 电路专题。https://transformer-circuits.pub/2021/framework/index.html。

[8] Zhen Fan, Luyu Gao, Rohan Jha, and Jamie Callan. 2023. COILcr: Efficient Semantic Matching Contextualized Exact Match Retrieval. In Advances in Information Retrieval: 45th European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2-6, 2023, Proceedings, Part I (Dublin, Ireland). Springer-Verlag, Berlin, Heidelberg, 298-312. https://doi.org/10.1007/978-3-031-28244-7_19

[8] 樊震（Zhen Fan）、高璐宇（Luyu Gao）、罗汉·贾（Rohan Jha）和杰米·卡伦（Jamie Callan）。2023 年。COILcr：高效语义匹配的上下文精确匹配检索。见《信息检索进展：第 45 届欧洲信息检索会议（ECIR 2023）论文集，第一部分》，2023 年 4 月 2 日至 6 日，爱尔兰都柏林。德国施普林格出版社，柏林，海德堡，298 - 312 页。https://doi.org/10.1007/978-3-031-28244-7_19

[9] Thibault Formal, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking. In SIGIR. ACM, 2288-2292.

[9] 蒂博·福尔马尔（Thibault Formal）、本杰明·皮沃瓦尔斯基（Benjamin Piwowarski）和斯特凡·克兰尚（Stéphane Clinchant）。2021年。SPLADE：用于第一阶段排序的稀疏词汇与扩展模型（SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking）。发表于信息检索研究与发展会议（SIGIR）。美国计算机协会（ACM），2288 - 2292页。

[10] Thibault Formal, Benjamin Piwowarski, and Stéphane Clinchant. 2021. A White Box Analysis of ColBERT. In Advances in Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021, Proceedings, Part II (Lecture Notes in Computer Science, Vol. 12657), Djoerd Hiemstra, Marie-Francine Moens, Josiane Mothe, Raffaele Perego, Martin Potthast, and Fabrizio Sebastiani (Eds.). Springer, 257-263. https://doi.org/10.1007/978-3-030- ${72240} - 1 \smallsetminus  {23}$

[10] 蒂博·福尔马尔（Thibault Formal）、本杰明·皮沃瓦尔斯基（Benjamin Piwowarski）和斯特凡·克兰尚（Stéphane Clinchant）。2021年。ColBERT的白盒分析（A White Box Analysis of ColBERT）。收录于《信息检索进展——第43届欧洲信息检索研究会议（ECIR 2021）论文集，第二部分》（Advances in Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021, Proceedings, Part II）（《计算机科学讲义》第12657卷），由乔尔德·希姆斯特拉（Djoerd Hiemstra）、玛丽 - 弗朗辛·莫恩斯（Marie - Francine Moens）、乔西安·莫特（Josiane Mothe）、拉斐尔·佩雷戈（Raffaele Perego）、马丁·波塔斯塔（Martin Potthast）和法布里齐奥·塞巴斯蒂亚尼（Fabrizio Sebastiani）编辑。施普林格出版社（Springer），257 - 263页。https://doi.org/10.1007/978 - 3 - 030 - ${72240} - 1 \smallsetminus  {23}$

[11] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List. In NAACL-HLT. Association for Computational Linguistics, 3030-3042.

[11] 高璐宇（Luyu Gao）、戴竹云（Zhuyun Dai）和杰米·卡兰（Jamie Callan）。2021 年。COIL：借助上下文倒排列表重新审视信息检索中的精确词汇匹配。发表于北美计算语言学协会人类语言技术会议（NAACL - HLT）。计算语言学协会，3030 - 3042。

[12] Zhuolin Jiang, Amro El-Jaroudi, William Hartmann, Damianos Karakos, and Lingjun Zhao. 2020. Cross-lingual Information Retrieval with BERT. In Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020), Kathy McKeown, Douglas W. Oard, Elizabeth, and Richard Schwartz (Eds.). European Language Resources Association, Marseille, France, 26-31. https://aclanthology.org/2020.clssts-1.5

[12] 蒋卓霖（Zhuolin Jiang）、阿姆罗·埃尔 - 贾鲁迪（Amro El - Jaroudi）、威廉·哈特曼（William Hartmann）、达米安奥斯·卡拉科斯（Damianos Karakos）和赵凌军（Lingjun Zhao）。2020 年。使用 BERT 进行跨语言信息检索。收录于跨语言文本与语音搜索和摘要研讨会（CLSSTS2020）会议论文集，由凯西·麦基翁（Kathy McKeown）、道格拉斯·W·奥尔德（Douglas W. Oard）、伊丽莎白（Elizabeth）和理查德·施瓦茨（Richard Schwartz）编辑。欧洲语言资源协会，法国马赛，26 - 31。https://aclanthology.org/2020.clssts - 1.5

[13] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In EMNLP. Association for Computational Linguistics, 6769-6781.

[13] 弗拉基米尔·卡尔普欣（Vladimir Karpukhin）、巴拉斯·奥古兹（Barlas Oguz）、闵世元（Sewon Min）、帕特里克·S. H. 刘易斯（Patrick S. H. Lewis）、莱德尔·吴（Ledell Wu）、谢尔盖·叶杜诺夫（Sergey Edunov）、陈丹琦（Danqi Chen）和易文涛（Wen-tau Yih）。2020 年。用于开放域问答的密集段落检索（Dense Passage Retrieval for Open-Domain Question Answering）。见《自然语言处理经验方法会议论文集》（EMNLP）。计算语言学协会，6769 - 6781。

[14] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In SIGIR. ACM, 39-48.

[14] 奥马尔·哈塔卜（Omar Khattab）和马泰·扎哈里亚（Matei Zaharia）。2020 年。ColBERT：通过基于 BERT 的上下文延迟交互实现高效有效的段落搜索（ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT）。见《信息检索研究与发展国际会议论文集》（SIGIR）。美国计算机协会，39 - 48。

[15] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: a Benchmark for Question Answering Research. Trans. Assoc. Comput. Linguistics 7 (2019), 452-466.

[15] 汤姆·夸特科夫斯基（Tom Kwiatkowski）、珍妮玛丽亚·帕洛马基（Jennimaria Palomaki）、奥利维亚·雷德菲尔德（Olivia Redfield）、迈克尔·柯林斯（Michael Collins）、安库尔·P. 帕里克（Ankur P. Parikh）、克里斯·阿尔贝蒂（Chris Alberti）、丹妮尔·爱泼斯坦（Danielle Epstein）、伊利亚·波洛苏欣（Illia Polosukhin）、雅各布·德夫林（Jacob Devlin）、肯顿·李（Kenton Lee）、克里斯蒂娜·图托纳娃（Kristina Toutanova）、利翁·琼斯（Llion Jones）、马修·凯尔西（Matthew Kelcey）、张明伟（Ming-Wei Chang）、安德鲁·M. 戴（Andrew M. Dai）、雅各布·乌斯佐雷（Jakob Uszkoreit）、勒·奎克（Quoc Le）和斯拉夫·彼得罗夫（Slav Petrov）。2019 年。自然问题：问答研究的基准（Natural Questions: a Benchmark for Question Answering Research）。《计算语言学协会汇刊》（Trans. Assoc. Comput. Linguistics）7（2019），452 - 466。

[16] Hyunji Lee, JaeYoung Kim, Hoyeon Chang, Hanseok Oh, Sohee Yang, Vladimir Karpukhin, Yi Lu, and Minjoon Seo. 2023. Nonparametric Decoding for Generative Retrieval. In ACL (Findings). Association for Computational Linguistics, 12642-12661.

[16] 李贤智（Hyunji Lee）、金在英（JaeYoung Kim）、张慧妍（Hoyeon Chang）、吴汉锡（Hanseok Oh）、杨素熙（Sohee Yang）、弗拉基米尔·卡尔普欣（Vladimir Karpukhin）、陆毅（Yi Lu）和徐民俊（Minjoon Seo）。2023年。生成式检索的非参数解码。收录于ACL（研究成果）。计算语言学协会，12642 - 12661。

[17] Jinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar Naim, Ming-Wei Chang, and Vincent Y. Zhao. 2023. Rethinking the Role of Token Retrieval in Multi-Vector Retrieval. CoRR abs/2304.01982 (2023). https://doi.org/ 10.48550/ARXIV.2304.01982 arXiv:2304.01982

[17] 李镇赫（Jinhyuk Lee）、戴竹云（Zhuyun Dai）、赛·梅赫尔·卡尔蒂克·杜杜（Sai Meher Karthik Duddu）、雷涛（Tao Lei）、伊夫特哈尔·奈姆（Iftekhar Naim）、张明伟（Ming - Wei Chang）和赵文森（Vincent Y. Zhao）。2023年。重新思考Token检索在多向量检索中的作用。计算机研究存储库（CoRR）论文编号abs/2304.01982（2023年）。https://doi.org/ 10.48550/ARXIV.2304.01982 arXiv:2304.01982

[18] Sunkyung Lee, Minjin Choi, and Jongwuk Lee. 2023. GLEN: Generative Retrieval via Lexical Index Learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6- 10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 7693-7704. https://aclanthology.org/2023.emnlp-main.477

[18] 李顺京（Sunkyung Lee）、崔敏珍（Minjin Choi）和李钟旭（Jongwuk Lee）。2023 年。GLEN：通过词法索引学习实现生成式检索。见《2023 年自然语言处理经验方法会议论文集》（Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing），EMNLP 2023，新加坡，2023 年 12 月 6 - 10 日，胡达·布阿穆尔（Houda Bouamor）、胡安·皮诺（Juan Pino）和卡利卡·巴利（Kalika Bali）（编）。计算语言学协会，7693 - 7704。https://aclanthology.org/2023.emnlp - main.477

[19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association for Computational Linguistics, 7871-7880. https://doi.org/10.18653/V1/2020.ACL-MAIN.703

[19] 迈克·刘易斯（Mike Lewis）、刘音涵（Yinhan Liu）、纳曼·戈亚尔（Naman Goyal）、马尔詹·加兹维尼贾德（Marjan Ghazvininejad）、阿卜杜勒拉赫曼·穆罕默德（Abdelrahman Mohamed）、奥默·利维（Omer Levy）、维塞林·斯托亚诺夫（Veselin Stoyanov）和卢克·泽特尔莫耶（Luke Zettlemoyer）。2020年。BART：用于自然语言生成、翻译和理解的去噪序列到序列预训练。见《计算语言学协会第58届年会论文集》，ACL 2020，线上会议，2020年7月5 - 10日，丹·朱拉夫斯基（Dan Jurafsky）、柴乔伊斯（Joyce Chai）、娜塔莉·施吕特（Natalie Schluter）和乔尔·R·特雷罗（Joel R. Tetreault）（编）。计算语言学协会，7871 - 7880。https://doi.org/10.18653/V1/2020.ACL - MAIN.703

[20] Bryan Li. 2022. Word Alignment in the Era of Deep Learning: A Tutorial. CoRR abs/2212.00138 (2022). https://doi.org/10.48550/ARXIV.2212.00138 arXiv:2212.00138

[20] 布莱恩·李（Bryan Li）。2022年。深度学习时代的词对齐：教程。计算机研究存储库（CoRR）论文编号abs/2212.00138（2022年）。https://doi.org/10.48550/ARXIV.2212.00138 arXiv:2212.00138

[21] Lei Li, Kai Fan, Hongjia Li, and Chun Yuan. 2022. Structural Supervision for Word Alignment and Machine Translation. In ACL (Findings). Association for Computational Linguistics, 4084-4094.

[21] 李雷、樊凯、李宏佳和袁春。2022年。用于词对齐和机器翻译的结构监督。见《计算语言学协会研究成果（ACL (Findings)）》。计算语言学协会，4084 - 4094。

[22] Minghan Li, Sheng-Chieh Lin, Barlas Oguz, Asish Ghoshal, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023. CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 11891-11907. https://doi.org/10.18653/V1/ 2023.ACL-LONG.663

[22] 李明翰、林圣杰、巴拉斯·奥古兹、阿西什·戈沙尔、吉米·林、亚沙尔·梅赫达德、易文涛和陈希伦。2023年。CITADEL：通过动态词法路由实现条件式Token交互以进行高效多向量检索。见《计算语言学协会第61届年会论文集（第1卷：长论文）》（Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)），ACL 2023，加拿大多伦多，2023年7月9 - 14日，安娜·罗杰斯、乔丹·L·博伊德 - 格雷伯和冈崎直明（编）。计算语言学协会，11891 - 11907。https://doi.org/10.18653/V1/ 2023.ACL - LONG.663

[23] Minghan Li, Sheng-Chieh Lin, Xueguang Ma, and Jimmy Lin. 2023. SLIM: Sparsi-fied Late Interaction for Multi-Vector Retrieval with Inverted Indexes. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1954-1959. https://doi.org/10.1145/3539618.3591977

[23] 李明翰（Minghan Li）、林圣杰（Sheng-Chieh Lin）、马学光（Xueguang Ma）和林吉米（Jimmy Lin）。2023 年。SLIM：用于基于倒排索引的多向量检索的稀疏化后期交互。收录于第 46 届 ACM 信息检索研究与发展国际会议论文集。1954 - 1959 页。https://doi.org/10.1145/3539618.3591977

[24] Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. 2023. Learning to Rank in Generative Retrieval. CoRR abs/2306.15222 (2023). https://doi.org/10.48550/ARXIV.2306.15222 arXiv:2306.15222

[24] 李永奇（Yongqi Li）、杨楠（Nan Yang）、王亮（Liang Wang）、魏富如（Furu Wei）和李文杰（Wenjie Li）。2023 年。生成式检索中的排序学习。计算机研究存储库（CoRR）论文编号 abs/2306.15222（2023 年）。https://doi.org/10.48550/ARXIV.2306.15222 arXiv:2306.15222

[25] Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse, Dense, and Attentional Representations for Text Retrieval. Trans. Assoc. Comput. Linguistics 9 (2021), 329-345.

[25] 栾毅（Yi Luan）、雅各布·艾森斯坦（Jacob Eisenstein）、克里斯蒂娜·图塔诺娃（Kristina Toutanova）和迈克尔·柯林斯（Michael Collins）。2021 年。用于文本检索的稀疏、密集和注意力表示。《计算语言学协会汇刊》（Trans. Assoc. Comput. Linguistics）第 9 卷（2021 年），329 - 345 页。

[26] Sanket Mehta, Jai Gupta, Yi Tay, Mostafa Dehghani, Vinh Tran, Jinfeng Rao, Marc Najork, Emma Strubell, and Donald Metzler. 2023. DSI++: Updating Transformer Memory with New Documents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 8198-8213. https://doi.org/10.18653/v1/2023.emnlp-main.510

[26] 桑凯特·梅塔（Sanket Mehta）、杰伊·古普塔（Jai Gupta）、伊·泰（Yi Tay）、莫斯塔法·德赫加尼（Mostafa Dehghani）、文·陈（Vinh Tran）、金峰·饶（Jinfeng Rao）、马克·纳约克（Marc Najork）、艾玛·斯特鲁贝尔（Emma Strubell）和唐纳德·梅茨勒（Donald Metzler）。2023 年。DSI++：用新文档更新Transformer内存。见《2023 年自然语言处理经验方法会议论文集》，胡达·布阿穆尔（Houda Bouamor）、胡安·皮诺（Juan Pino）和卡利卡·巴利（Kalika Bali）（编）。计算语言学协会，新加坡，8198 - 8213。https://doi.org/10.18653/v1/2023.emnlp - main.510

[27] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. In Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016 (CEUR Workshop Proceedings, Vol. 1773), Tarek Richard Besold, Antoine Bordes, Artur S. d'Avila Garcez, and Greg Wayne (Eds.). CEUR-WS.org. https://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf

[27] 特里·阮（Tri Nguyen）、米尔·罗森伯格（Mir Rosenberg）、宋霞（Xia Song）、高剑锋（Jianfeng Gao）、索拉布·蒂瓦里（Saurabh Tiwary）、兰甘·马朱姆德（Rangan Majumder）和李邓（Li Deng）。2016年。MS MARCO：一个人工生成的机器阅读理解数据集（MS MARCO: A Human Generated MAchine Reading COmprehension Dataset）。收录于《认知计算研讨会论文集：整合神经与符号方法（2016年）》（Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016），该研讨会与第30届神经信息处理系统年度会议（NIPS 2016）同期举办，于2016年12月9日在西班牙巴塞罗那举行（CEUR研讨会论文集，第1773卷），由塔里克·理查德·贝索尔德（Tarek Richard Besold）、安托万·博尔德斯（Antoine Bordes）、阿图尔·S·达维拉·加尔塞斯（Artur S. d'Avila Garcez）和格雷格·韦恩（Greg Wayne）编辑。CEUR - WS.org。https://ceur - ws.org/Vol - 1773/CoCoNIPS_2016_paper9.pdf

[28] Thong Nguyen and Andrew Yates. 2023. Generative Retrieval as Dense Retrieval. CoRR abs/2306.11397 (2023). https://doi.org/10.48550/ARXIV.2306.11397 arXiv:2306.11397

[28] 通·阮（Thong Nguyen）和安德鲁·耶茨（Andrew Yates）。2023年。生成式检索即密集检索（Generative Retrieval as Dense Retrieval）。计算机研究报告库（CoRR），编号abs/2306.11397（2023年）。https://doi.org/10.48550/ARXIV.2306.11397 arXiv:2306.11397

[29] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Ábrego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large Dual Encoders Are Generalizable Retrievers. In EMNLP. Association for Computational Linguistics, 9844-9855.

[29] 倪建墨（Jianmo Ni）、曲晨（Chen Qu）、卢静（Jing Lu）、戴竹云（Zhuyun Dai）、古斯塔沃·埃尔南德斯·阿夫雷戈（Gustavo Hernández Ábrego）、马骥（Ji Ma）、赵文森（Vincent Y. Zhao）、栾毅（Yi Luan）、基思·B·霍尔（Keith B. Hall）、张明伟（Ming-Wei Chang）和杨荫飞（Yinfei Yang）。2022年。大型双编码器是可泛化的检索器。见《自然语言处理经验方法会议论文集》（EMNLP）。计算语言学协会，9844 - 9855。

[30] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova Das-Sarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2022. In-context Learning and Induction Heads. Transformer Circuits Thread. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

[30] 凯瑟琳·奥尔森（Catherine Olsson）、纳尔逊·埃尔哈格（Nelson Elhage）、尼尔·南达（Neel Nanda）、尼古拉斯·约瑟夫（Nicholas Joseph）、诺瓦·达斯 - 萨尔马（Nova Das-Sarma）、汤姆·亨尼根（Tom Henighan）、本·曼（Ben Mann）、阿曼达·阿斯凯尔（Amanda Askell）、白运涛（Yuntao Bai）、陈安娜（Anna Chen）、汤姆·科纳利（Tom Conerly）、道恩·德雷恩（Dawn Drain）、迪普·甘古利（Deep Ganguli）、扎克·哈特菲尔德 - 多兹（Zac Hatfield-Dodds）、丹尼·埃尔南德斯（Danny Hernandez）、斯科特·约翰斯顿（Scott Johnston）、安迪·琼斯（Andy Jones）、杰克逊·克尔尼翁（Jackson Kernion）、莉安·洛维特（Liane Lovitt）、卡迈勒·恩杜塞（Kamal Ndousse）、达里奥·阿莫迪（Dario Amodei）、汤姆·布朗（Tom Brown）、杰克·克拉克（Jack Clark）、贾里德·卡普兰（Jared Kaplan）、山姆·麦坎德利什（Sam McCandlish）和克里斯·奥拉（Chris Olah）。2022年。上下文学习与归纳头。Transformer电路专题。https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html。

[31] Ronak Pradeep, Kai Hui, Jai Gupta, Ádám D. Lelkes, Honglei Zhuang, Jimmy Lin, Donald Metzler, and Vinh Q. Tran. 2023. How Does Generative Retrieval Scale to Millions of Passages?. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 1305-1321. https://aclanthology.org/2023.emnlp-main.83

[31] 罗纳克·普拉迪普（Ronak Pradeep）、凯·许（Kai Hui）、杰伊·古普塔（Jai Gupta）、亚当·D·莱克斯（Ádám D. Lelkes）、庄宏磊（Honglei Zhuang）、吉米·林（Jimmy Lin）、唐纳德·梅茨勒（Donald Metzler）和阮文魁（Vinh Q. Tran）。2023年。生成式检索如何扩展到数百万个段落？收录于《2023年自然语言处理经验方法会议论文集》（Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing），EMNLP 2023，新加坡，2023年12月6 - 10日，胡达·布阿莫尔（Houda Bouamor）、胡安·皮诺（Juan Pino）和卡利卡·巴利（Kalika Bali）（编）。计算语言学协会，第1305 - 1321页。https://aclanthology.org/2023.emnlp-main.83

[32] Yujie Qian, Jinhyuk Lee, Sai Meher Karthik Duddu, Zhuyun Dai, Siddhartha Brahma, Iftekhar Naim, Tao Lei, and Vincent Y. Zhao. 2022. Multi-Vector Retrieval as Sparse Alignment. arXiv:2211.01267 (November 2022). http://arxiv.org/abs/ 2211.01267

[32] 钱玉洁（Yujie Qian）、李晋赫（Jinhyuk Lee）、赛·梅赫尔·卡尔蒂克·杜杜（Sai Meher Karthik Duddu）、戴竹云（Zhuyun Dai）、悉达多·布拉马（Siddhartha Brahma）、伊夫泰哈尔·奈姆（Iftekhar Naim）、雷涛（Tao Lei）和赵文森（Vincent Y. Zhao）。2022年。多向量检索作为稀疏对齐。预印本arXiv:2211.01267（2022年11月）。http://arxiv.org/abs/ 2211.01267

[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21, 140 (2020), 1-67. http://jmlr.org/papers/v21/20-074.html

[33] 科林·拉菲尔（Colin Raffel）、诺姆·沙泽尔（Noam Shazeer）、亚当·罗伯茨（Adam Roberts）、凯瑟琳·李（Katherine Lee）、沙兰·纳朗（Sharan Narang）、迈克尔·马泰纳（Michael Matena）、周燕琪（Yanqi Zhou）、李伟（Wei Li）和彼得·J·刘（Peter J. Liu）。2020年。使用统一的文本到文本转换器探索迁移学习的极限。《机器学习研究杂志》（Journal of Machine Learning Research）21, 140 (2020), 1 - 67。http://jmlr.org/papers/v21/20 - 074.html

[34] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q. Tran, Jonah Samost, Maciej Kula, Ed H. Chi, and Maheswaran Sathiamoorthy. 2023. Recommender Systems with Generative Retrieval. CoRR abs/2305.05065 (2023). https://doi.org/10.48550/ ARXIV.2305.05065 arXiv:2305.05065

[34] 沙尚克·拉杰普特（Shashank Rajput）、尼基尔·梅塔（Nikhil Mehta）、阿尼玛·辛格（Anima Singh）、拉古南丹·胡利卡尔·凯沙万（Raghunandan Hulikal Keshavan）、陈忠（Trung Vu）、卢卡斯·赫尔德特（Lukasz Heldt）、洪丽婵（Lichan Hong）、泰毅（Yi Tay）、陈维·Q（Vinh Q. Tran）、乔纳·萨莫斯特（Jonah Samost）、马切伊·库拉（Maciej Kula）、埃德·H·池（Ed H. Chi）和马埃斯瓦兰·萨蒂亚莫尔蒂（Maheswaran Sathiamoorthy）。2023年。具有生成式检索功能的推荐系统。计算机研究报告库（CoRR）abs/2305.05065 (2023)。https://doi.org/10.48550/ ARXIV.2305.05065 arXiv:2305.05065

[35] Stephen E. Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (2009), 333-389. https://doi.org/10.1561/1500000019

[35] 斯蒂芬·E·罗伯逊（Stephen E. Robertson）和雨果·萨拉戈萨（Hugo Zaragoza）。2009年。概率相关性框架：BM25及其他。《信息检索趋势与基础》（Found. Trends Inf. Retr.）第3卷，第4期（2009年），333 - 389页。https://doi.org/10.1561/1500000019

[36] Weiwei Sun, Lingyong Yan, Zheng Chen, Shuaiqiang Wang, Haichao Zhu, Pengjie Ren, Zhumin Chen, Dawei Yin, Maarten de Rijke, and Zhaochun Ren. 2023. Learning to Tokenize for Generative Retrieval. In NeurIPS 2023: Thirty-seventh Conference on Neural Information Processing Systems. https://papers.nips.cc/paper_files/ paper/2023/file/91228b942a4528cdae031c1b68b127e8-Paper-Conference.pdf

[36] 孙薇薇、闫凌勇、陈政、王帅强、朱海潮、任鹏杰、陈竹敏、尹大为、马腾·德·里克（Maarten de Rijke）和任兆春。2023年。为生成式检索学习分词（Tokenize）方法。收录于《2023年神经信息处理系统大会（NeurIPS 2023）：第三十七届神经信息处理系统会议论文集》。https://papers.nips.cc/paper_files/ paper/2023/file/91228b942a4528cdae031c1b68b127e8-Paper-Conference.pdf

[37] Yubao Tang, Ruqing Zhang, Jiafeng Guo, and Maarten de Rijke. 2023. Recent Advances in Generative Information Retrieval. In SIGIR-AP. ACM, 294-297.

[37] 唐玉宝、张汝清、郭佳峰和马腾·德·里克（Maarten de Rijke）。2023年。生成式信息检索的最新进展。收录于《亚太信息检索大会（SIGIR - AP）》。美国计算机协会（ACM），294 - 297页。

[38] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Prakash Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022. Transformer Memory as a Differentiable Search Index. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/ hash/892840a6123b5ec99ebaab8be1530fba-Abstract-Conference.html

[38] 易·泰（Yi Tay）、阮文（Vinh Tran）、莫斯塔法·德赫加尼（Mostafa Dehghani）、倪建谟（Jianmo Ni）、达拉·巴里（Dara Bahri）、哈什·梅塔（Harsh Mehta）、秦震（Zhen Qin）、惠凯（Kai Hui）、赵哲（Zhe Zhao）、贾伊·普拉卡什·古普塔（Jai Prakash Gupta）、塔尔·舒斯特（Tal Schuster）、威廉·W·科恩（William W. Cohen）和唐纳德·梅茨勒（Donald Metzler）。2022年。将Transformer内存作为可微搜索索引。收录于《神经信息处理系统进展35：2022年神经信息处理系统年度会议（NeurIPS 2022）论文集》，会议于2022年11月28日至12月9日在美国路易斯安那州新奥尔良市举行，由桑米·科耶乔（Sanmi Koyejo）、S·穆罕默德（S. Mohamed）、A·阿加瓦尔（A. Agarwal）、丹妮尔·贝尔格雷夫（Danielle Belgrave）、K·赵（K. Cho）和A·吴（A. Oh）编辑。http://papers.nips.cc/paper_files/paper/2022/ hash/892840a6123b5ec99ebaab8be1530fba-Abstract-Conference.html

[39] Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, Xing Xie, Hao Sun, Weiwei Deng, Qi Zhang, and Mao Yang. 2022. A Neural Corpus Indexer for Document Retrieval. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/ a46156bd3579c3b268108ea6aca71d13-Abstract-Conference.html

[39] 王雨静、侯迎艳、王浩楠、苗子铭、吴世斌、陈琦、夏雨晴、迟成敏、赵国帅、刘政、谢星、孙浩、邓薇薇、张琦和杨旄。2022年。用于文档检索的神经语料索引器。《神经信息处理系统进展35：2022年神经信息处理系统年度会议（NeurIPS 2022）论文集》，美国路易斯安那州新奥尔良，2022年11月28日至12月9日，桑米·科耶霍（Sanmi Koyejo）、S·穆罕默德（S. Mohamed）、A·阿加瓦尔（A. Agarwal）、丹妮尔·贝尔格雷夫（Danielle Belgrave）、K·赵（K. Cho）和A·吴（A. Oh） 编。http://papers.nips.cc/paper_files/paper/2022/hash/ a46156bd3579c3b268108ea6aca71d13-Abstract-Conference.html

[40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. In EMNLP (Demos). Association for Computational Linguistics, 38-45.

[40] 托马斯·沃尔夫（Thomas Wolf）、利桑德尔·德比特（Lysandre Debut）、维克多·桑（Victor Sanh）、朱利安·肖蒙（Julien Chaumond）、克莱门特·德朗格（Clement Delangue）、安东尼·莫伊（Anthony Moi）、皮埃尔里克·西斯塔克（Pierric Cistac）、蒂姆·劳尔特（Tim Rault）、雷米·卢夫（Rémi Louf）、摩根·丰托维奇（Morgan Funtowicz）、乔·戴维森（Joe Davison）、山姆·施莱弗（Sam Shleifer）、帕特里克·冯·普拉滕（Patrick von Platen）、克拉拉·马（Clara Ma）、亚辛·杰尔尼（Yacine Jernite）、朱利安·普鲁（Julien Plu）、徐灿文（Canwen Xu）、特文·勒·斯考（Teven Le Scao）、西尔万·古格（Sylvain Gugger）、玛丽亚玛·德拉梅（Mariama Drame）、昆汀·勒霍斯特（Quentin Lhoest）和亚历山大·M·拉什（Alexander M. Rush）。2020年。《Transformer：最先进的自然语言处理技术》。收录于自然语言处理经验方法会议（EMNLP）（演示环节）。计算语言学协会，第38 - 45页。

[41] Shiguang Wu, Wenda Wei, Mengqi Zhang, Zhumin Chen, Jun Ma, Zhaochun Ren, Maarten de Rijke, and Pengjie Ren. 2024. Generative Retrieval as Multi-Vector Dense Retrieval. arXiv:2404.00684 [cs.IR]

[41] 吴时光（Shiguang Wu）、魏文达（Wenda Wei）、张梦琪（Mengqi Zhang）、陈珠敏（Zhumin Chen）、马军（Jun Ma）、任兆春（Zhaochun Ren）、马腾·德·里克（Maarten de Rijke）和任鹏杰（Pengjie Ren）。2024年。《生成式检索即多向量密集检索》。预印本编号：arXiv:2404.00684 [计算机科学：信息检索（cs.IR）]

[42] Tianchi Yang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, and Qi Zhang. 2023. Auto Search Indexer for End-to-End Document Retrieval. In EMNLP (Findings). Association for Computational Linguistics, 6955-6970.

[42] 杨天池、宋明辉、张梓涵、黄海珍、邓薇薇、孙峰和张琦。2023年。用于端到端文档检索的自动搜索索引器。收录于EMNLP（研究成果）。计算语言学协会，6955 - 6970。

[43] Puxuan Yu, Hongliang Fei, and Ping Li. 2021. Cross-lingual Language Model Pretraining for Retrieval. In WWW '21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021, Jure Leskovec, Marko Grobelnik, Marc Najork, Jie Tang, and Leila Zia (Eds.). ACM / IW3C2, 1029-1039. https://doi.org/ 10.1145/3442381.3449830

[43] 余普轩、费洪亮和平李。2021年。用于检索的跨语言模型预训练。收录于WWW '21：2021年万维网会议，虚拟会议/斯洛文尼亚卢布尔雅那，2021年4月19 - 23日，尤雷·莱斯科维奇、马尔科·格罗贝尼克、马克·纳约克、唐杰和莱拉·齐亚（编）。美国计算机协会/万维网联盟，1029 - 1039。https://doi.org/ 10.1145/3442381.3449830

[44] Peiwen Yuan, Xinglin Wang, Shaoxiong Feng, Boyuan Pan, Yiwei Li, Heda Wang, Xupeng Miao, and Kan Li. 2024. Generative Dense Retrieval: Memory Can Be a Burden. arXiv:2401.10487 [cs.IR]

[44] 袁培文（Peiwen Yuan）、王杏林（Xinglin Wang）、冯少雄（Shaoxiong Feng）、潘博远（Boyuan Pan）、李依伟（Yiwei Li）、王贺达（Heda Wang）、苗旭鹏（Xupeng Miao）和李侃（Kan Li）。2024年。生成式密集检索：记忆可能成为负担。预印本arXiv:2401.10487 [计算机科学：信息检索（cs.IR）]

[45] Hansi Zeng, Chen Luo, Bowen Jin, Sheikh Muhammad Sarwar, Tianxin Wei, and Hamed Zamani. 2023. Scalable and Effective Generative Information Retrieval. CoRR abs/2311.09134 (2023). https://doi.org/10.48550/ARXIV.2311.09134 arXiv:2311.09134

[45] 曾寒思（Hansi Zeng）、罗晨（Chen Luo）、金博文（Bowen Jin）、谢赫·穆罕默德·萨尔瓦尔（Sheikh Muhammad Sarwar）、魏天昕（Tianxin Wei）和哈米德·扎马尼（Hamed Zamani）。2023年。可扩展且有效的生成式信息检索。计算机研究报告CoRR abs/2311.09134 (2023)。https://doi.org/10.48550/ARXIV.2311.09134 预印本arXiv:2311.09134

[46] Fuwei Zhang, Zhao Zhang, Xiang Ao, Dehong Gao, Fuzhen Zhuang, Yi Wei, and Qing He. 2022. Mind the Gap: Cross-Lingual Information Retrieval with Hierarchical Knowledge Enhancement. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022. AAAI Press, 4345-4353. https://doi.org/10.1609/AAAI.V36I4.20355

[46] 张福伟、张钊、敖翔、高德宏、庄福振、魏毅和何清。2022 年。关注差距：基于分层知识增强的跨语言信息检索。收录于第三十六届人工智能协会会议（Thirty-Sixth AAAI Conference on Artificial Intelligence，AAAI 2022）、第三十四届人工智能创新应用会议（Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence，IAAI 2022）、第十二届人工智能教育进展研讨会（The Twelveth Symposium on Educational Advances in Artificial Intelligence，EAAI 2022）线上活动，2022 年 2 月 22 日至 3 月 1 日。AAAI 出版社，4345 - 4353。https://doi.org/10.1609/AAAI.V36I4.20355

[47] Le Zhao. 2012. Modeling and Solving Term Mismatch for Full-text Retrieval. SIGIR Forum 46, 2 (2012), 117-118.

[47] 赵乐。2012 年。全文检索中术语不匹配问题的建模与求解。《SIGIR 论坛》（SIGIR Forum）46 卷 2 期（2012 年），117 - 118。

[48] Giulio Zhou and Jacob Devlin. 2021. Multi-Vector Attention Models for Deep Re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 5452-5456. https://doi.org/10.18653/v1/2021.emnlp-main.443

[48] 朱利奥·周（Giulio Zhou）和雅各布·德夫林（Jacob Devlin）。2021年。用于深度重排序的多向量注意力模型。《2021年自然语言处理经验方法会议论文集》。计算语言学协会，线上会议及多米尼加共和国蓬塔卡纳，第5452 - 5456页。https://doi.org/10.18653/v1/2021.emnlp-main.443

[49] Yujia Zhou, Jing Yao, Zhicheng Dou, Ledell Wu, and Ji-Rong Wen. 2023. Dynami-cRetriever: A Pre-trained Model-based IR System Without an Explicit Index. Mach. Intell. Res. 20, 2 (April 2023), 276-288. https://doi.org/10.1007/s11633-022-1373-9

[49] 周宇佳（Yujia Zhou）、姚静（Jing Yao）、窦志成（Zhicheng Dou）、莱德尔·吴（Ledell Wu）和文继荣（Ji - Rong Wen）。2023年。动态检索器（Dynami - cRetriever）：一种无需显式索引的基于预训练模型的信息检索系统。《机器智能研究》（Mach. Intell. Res.）第20卷，第2期（2023年4月），第276 - 288页。https://doi.org/10.1007/s11633-022-1373-9

[50] Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuc-con, and Daxin Jiang. 2022. Bridging the Gap Between Indexing and Retrieval for Differentiable Search Index with Query Generation. ArXiv abs/2206.10128 (2022). https://api.semanticscholar.org/CorpusID:249890267

[50] 庄圣耀、任厚兴、寿林军、裴健、龚鸣、圭多·祖孔（Guido Zuc-con）和蒋大新。2022 年。通过查询生成弥合可微搜索索引的索引与检索之间的差距。预印本 arXiv:2206.10128 (2022)。https://api.semanticscholar.org/CorpusID:249890267