#### 摘要

近似最近邻搜索（ANNS）是许多应用程序中的基础和关键组件，包括推荐系统和基于大型语言模型的应用程序。随着多模态神经模型的发展，这些模型将来自不同模态的数据转换为共享的高维空间中的特征向量，跨模态ANNS的目标是使用来自一个模态（例如文本）的数据向量作为查询，从另一个模态（例如图像或视频）中检索最相似的项目。然而，不同模态的嵌入之间存在固有的分布差距，导致跨模态查询相对于基础数据成为分布外（OOD）数据。因此，最先进的ANNS方法在处理OOD工作负载时表现不佳。

------

在本文中，我们对OOD工作负载的特性进行了定量分析，以深入理解其ANNS效率。与单模态工作负载不同，我们揭示了OOD查询在空间上与基础数据存在偏差，并且OOD查询的k近邻在嵌入空间中彼此距离较远。这一特性破坏了现有ANNS方法的假设，并与它们为高效搜索而设计的结构不匹配。基于对OOD工作负载的洞察，我们提出了一种投影二分图（pRojected bipartite Graph, RoarGraph），这是一种在查询分布指导下构建的高效ANNS图索引。大量实验表明，RoarGraph在现代跨模态数据集上显著优于最先进的方法，对于OOD查询，在$90%$召回率下的搜索速度提高了最多$3.56 \times$倍。

## 1 引言

近似最近邻搜索（ANNS）是多个应用领域中基础且性能关键的组成部分，例如大规模信息检索 [49, 54, 78]、推荐系统 [11, 55] 和问答系统 [42, 62]。近年来，将检索增强生成（RAG）应用于大型语言模型（LLMs）的新兴应用同样利用向量数据库作为外部知识库，通过ANNS提升检索效率 [2, 46, 73]。这些应用需要对相似向量搜索提供快速且准确的响应，而ANNS可以高效地从数据库中为给定查询检索近似最近邻，从而避免执行不可行的精确k近邻搜索 [9, 19, 85]。为提升ANNS的性能，大量研究致力于设计高效的数据结构，包括基于分区的方法 [15, 65, 67, 81]、基于量化的方法 [4, 24, 26, 36, 77] 和基于哈希的方法 [13, 21, 31, 31, 86]，其中基于图的方法 $[20,52,56]$ 在许多数据集上表现出了最先进的性能。

==关于$\text{ANN}$==

:one:关于$\text{ANN}$

1. 应用领域：
   - 传统的：大规模信息检索$/$推荐系统$/$问答系统
   - 新兴的：$\text{RAG}$，即让大模型能检索外部知识
2. 实现方式：基于划分，基于量化，基于哈希，基于图

------

近年来，随着多模态数据表示技术的发展，跨模态检索逐渐引起了广泛关注。诸如CLIP [59]等为多模态任务训练的深度学习模型，能够将来自不同模态（如视觉和自然语言）的非结构化数据嵌入到语义保持的共享高维空间中，即嵌入空间。在跨模态向量检索中，一个模态的数据（例如文本）被用作查询，以检索来自另一模态（例如图像或视频）中语义最相似的数据 [32, 41, 47]。鉴于跨模态检索在多样化和关键应用场景中的广泛需求，提升跨模态检索性能的高效ANNS显得尤为重要 [7, 63, 66, 68, 74, 80, 83]。然而，现有的ANNS索引主要针对单模态场景设计，在跨模态查询中表现不佳。例如，在现代跨模态数据集LAION [61]上，使用HNSW（分层可导航小世界）[52]索引进行文本-图像搜索时，为了使recall@10达到0.95，需要访问14374个节点；相比之下，如果使用图像搜索图像，仅需访问1568个节点，显示出近10倍的效率下降。

==跨模态检索的发展和挑战==

:one:$\text{CLIP}$深度学习模型：将不同模态(图像$/$文本)映射到**同一**高维空间

:two:跨模态检索的概念：使用一个模态的数据作为$\text{query}$，但返回另一个模态中语义相似的内容

- 比如输入`Apple`然后返回一张苹果的图片

:three:现存的问题

1. $\text{ANN}$主要针对单模态设计
2. 跨模态场景下性能急剧下降，有研究说明图像-图像搜索的效率是文本-图像的十倍

---

跨模态检索的主要特征在于来自两种模态的向量数据分布存在显著差异。尽管多模态神经嵌入模型可以实现不同模态向量之间的相似度度量，但在跨模态表示学习中，两种模态嵌入之间仍然存在一致且固有的分布差距，这被称为模态差距 [45]。因此，在跨模态ANNS中，查询向量相对于数据库中的向量（基础数据）属于分布外（OOD）[34]。这与单模态任务形成鲜明对比，在单模态工作负载中，查询向量与基础数据属于相同分布（ID）。马哈拉诺比斯距离 [51] 显示，在跨模态数据集（例如Text-to-Image [14]、LAION [61] 和 WebVid [5]）中，来自另一模态的查询向量比ID查询与基础数据之间的距离远 $10 \sim 100 \times$。此外，通过深入实验和分析，我们发现分布外查询距离基础数据较远，其$k$近邻彼此之间也相距较远。这表明查询不仅偏离了基础数据，而且OOD查询的最近邻（真实标签）比ID查询的最近邻分布得更为分散。

==检索中的核心问题 - 模态差距(modal gap)== 

:one:模态差距的本质：即使不同模态映射到了同一空间，但他们在这个空间的分布差异巨大，且无法完全消除

:two:分布外($\text{OOD}$)和同分布($\text{ID}$)

1. 跨模态$\text{ANN}$：查询向量分布$\xleftrightarrow{\text{OOD}}$基础数据分布
2. 单模态$\text{ANN}$：查询向量分布$\xleftrightarrow{\text{ID}}$基础数据分布
3. 可以用马哈拉诺比斯距离量化这种差异
   - 马哈拉诺比斯距离：反映数据点在特定分布中的异常程度
   - 分布外查询点与数据集的马哈拉诺比斯距离，是同分布查询的$10-100$倍

:three:一些其它发现

1. $\text{OOD}$查询与基础数据很远，并且即使与其$k$近邻也很远
2. 这使得$\text{ANN}$的真实标签也很分散，所以效率底下

------

然而，当前最先进的ANNS索引主要是为ID查询设计的 $[20,52,56]$。这些方法假设查询出现在基础数据附近，并且查询的最近邻彼此靠近。在这一假设下，基于图的ANNS方法在索引构建阶段采用束搜索（n-贪婪搜索）来构建近似KNN图 [20, 35, 43, 50, 52]，其中距离较小的向量倾向于相互连接。此外，搜索阶段也使用束搜索，期望通过紧密连接的基础数据实现快速收敛。然而，这种索引设计在跨模态ANNS中面对OOD查询时表现不佳。由于跨模态查询和基础数据遵循不同的分布，并且OOD查询的真实标签分布较为分散，因此关于查询和基础数据分布的关键假设被破坏。结果是，这种基于图的跨模态搜索无法高效收敛，反而需要更多的图遍历跳跃操作。这正是现有ANNS方法在处理OOD工作负载时表现不佳的主要原因。

==为何$\text{ANN}$在跨模态下效果不好==

:one:现有$\text{ANN}$的基本假设：查询离基础数据近，查询的最近邻彼此靠近，查询是$\text{ID}$的

:two:图$\text{ANN}$的方法原理

1. 索引构建：应用$\text{BeamSearch}$构建$\text{KNN}$图，距离相近的点会被连接
2. 搜索阶段：使用$\text{BeamSearch}$收敛到目标区域

:three:为何跨模态$\text{ANN}$性能下降：

1. 查询对基础数据是$\text{OOD}$的，导致查询真实标签分散
2. 因此无法快速收敛，遍历跳跃数增加

---

我们提出了一种名为**投影二分图（pRojected bipartite Graph, RoarGraph）**的高效图索引，基于查询分布知识，专门用于跨模态近似最近邻搜索（ANNS）。我们的核心思想是将查询的最近邻分布向量映射为图索引中紧密连接的邻居。RoarGraph的索引构建过程如下：首先，通过精细的边选择，基于查询与基础数据之间的相似关系构建一个二分图，并将其映射到统一的图结构中。其次，将二分图投影到基础数据上，结合邻域感知投影机制，为空间上较远但从查询视角看较近的节点创建连接路径。最后，执行连接增强方案，以确保图的连通性以及所有基础数据向量的可达性。RoarGraph索引仅由基础数据组成，但能有效保留查询分布推导出的邻接关系。本论文的主要贡献总结如下：

- 我们通过深入实验揭示了跨模态ANNS的低效性，并提出洞见性分析，阐明当前最先进方法在跨模态ANNS中性能下降的根本原因。
- 我们提出了一种新型的图索引——RoarGraph，用于高效跨模态ANNS，充分利用查询分布来指导图索引构建。
- 我们在三个跨模态数据集上（包括文本、图像和视频帧）进行了广泛实验，结果表明RoarGraph显著提高了跨模态向量搜索的性能。

==介绍了论文的解决方案$\text{RoarGraph}$== 

:one:关于二分图

1. 概念：所有结点分为两个不交叉集合，同一集合内不能直接向量
2. 此处：两个集合即为查询向量集$/$基础数据集，边连接两个集合中的元素，以表示二者相似度

:one:核心思想

1. 将查询的最近邻的分散分布，映射为图中紧密连接的结构
2. 利用查询分布的知识，来指导索引构建
3. 最终构建的索引仅包含基础数据，但保留了查询分布的==邻接关系特征==

:two:构建过程

1. 构建二分图：精细选出一些边，连接查询$/$基础数据集中的点以表示相似度
2. 投影二分图：将二分图投影到基础数据上，让空间上很远但是查询上很近的点相连
   - 比如$d_1d_2$本来很远，在传统$\text{ANN}$构建中不会相连
   - 但假设有查询$q$离二者都很近，本文的方法反而就会连接$d_1d_2$ 
3. 连接增强：具体没说，看后面吧

------

RoarGraph通过减少搜索阶段的绕行路径和跳跃次数，加速跨模态向量搜索。这使其在现有图索引的基础上实现了显著性能提升，在三个跨模态数据集上搜索速度提高了$1.84 \times$到$3.56 \times$，召回率$@ \mathrm{k} \geq 0.9$，其中$\mathrm{k}=1, 10, 100$。特别是，RoarGraph还达到了现有方法无法实现的卓越召回率（recall@k $\geq 0.99$）。此外，我们方法的一个变体在NeurIPS 2023实用向量搜索（Big ANN）挑战赛的OOD赛道中获得了冠军。

==$\text{RoarGraph}$的实验效果== 

## 2 背景与动机

本节介绍了近似最近邻搜索（ANNS）和分布外（OOD）ANNS的背景知识。同时，我们对现有ANNS方法在跨模态数据集上的性能进行了定量评估和分析。

### 2.1 ANNS 背景

**2.1.1 ANNS定义。**
 近似最近邻搜索（ANNS）起源于k近邻搜索（KNNS），其目标是从数据库（基础数据）中找到与给定查询向量最接近的$k$个向量。接近度的度量通常采用余弦距离、$\ell_{2}$距离、内积等方法。在现代应用中，处理的大规模数据集向量维度可能增加到数百维 [66]。由于“维度灾难”带来的挑战 [33]，精确的k近邻搜索（KNNS）变得昂贵且不切实际。为此，ANNS方法为基础数据创建特定索引，以在搜索速度和精度之间实现权衡。

**定义1.** 给定数据库中的N个向量$\mathcal{X}=\left\{x_{1}, \ldots, x_{N} \right\} \in\mathbb{R}^{D}, q \in \mathbb{R}^{D}$和一个用于计算两个向量之间距离的函数$\delta(\cdot, \cdot)$。Top-$k(k \leq N)$ ANNS的目标是找到
$$
\begin{equation*}
S=\mathrm{k}-\arg \min _{i \in 1, \ldots, N} \delta\left(q, x_{i}\right) \tag{1}
\end{equation*}
$$

集合$S$需满足：$|S|=k$，对于所有$x \in S$和$x^{\prime} \in \mathcal{X} \backslash S$，有$\delta(q, x) \leq(1+\epsilon) \delta\left(x^{\prime}, q\right)$，其中$\epsilon \geq 0$是一个小常数，仅用于描述ANNS满足的近似特性，而不是直接使用。

ANN索引的搜索性能将通过搜索速度与召回率的权衡来评估。召回率使用公式recall@$k=|S \cap K N N(q)| / k$计算，其中$K N N(q)$表示查询$q$的精确k近邻（真值），而$S$是结果集且满足$|S|=k$。

:one:$\text{ANN}$定义：在大数据库中，找到与查询向量最接近的$k$个向量

:two:为何要近似：高维数据有维度灾难，精确最邻近太耗时

---

**2.1.2 最先进的ANNS方法。**

图基方法和倒排文件索引是两种常见的近似最近邻搜索（ANNS）方法。其中，图基方法是性能最优的ANNS方法家族，能够在搜索速度与召回率之间提供比其他方法更好的权衡 [20, 44, 50, 52, 56, 75]。这些方法将基础数据索引为图结构，其中每个节点表示一个数据向量。

**HNSW（Hierarchical Navigable Small World）[52]** 是一种广为人知的多层图结构。在构建过程中，所有基础向量首先插入到基础层（第0层）。随后，仅第$i$层的子集会以递减概率插入到第$i+1$层，层数越高，包含的节点越少（最顶层仅包含1个点）。每层中的节点会与其近似最近邻相连接。

在搜索阶段，HNSW针对给定查询首先在较高层进行贪婪搜索。来自较高层的最近点将作为下一层的入口点。在基础层，基于图执行束搜索。束搜索是贪婪搜索的一种变体，通过在有限队列中扩展最优候选元素来探索图 [57]，逐步收敛到查询的最近邻。队列的容量（称为束宽度）控制精度和搜索速度之间的权衡。

:one:图方法概述

1. 是所有类型中性能最好的
2. 基本思路：将基础数据构建成图结构

:two:关于$\text{HNSW}$

1. 结构特点：越往上结点越少

   ```bash
   第3层:     O (1个节点)
   第2层:    O O O (少量节点)
   第1层:   O O O O O (较多节点)
   第0层: O O O O O O O O (所有节点)
   ```

2. 搜索策略：

   - 从顶层开始搜索
   - 在每层$\text{GreedySearch}$出最近点
   - 把该层的结果作为下一层的起始
   - 到达最底层后，转而秩序$\text{BeamSearch}$(可视为$\text{GreedySearch}$变体)

------

倒排文件索引（IVF）方法是ANNS中另一种常见的索引类型，因其在范围最近邻搜索中操作便利且性能优越而受到欢迎。例如，IVF [67] 和 IMI [3] 等方法。

IVF首先使用K-means算法对基础数据进行聚类，得到$n$个质心，然后将基础数据中的每个向量分配到其最近的聚类中。在搜索阶段，IVF选择与查询最接近的nprobe个质心，并扫描这些nprobe聚类中的所有向量，从而获取前$k$个结果。

==不多说了，博客里都写了==

### 2.2 分布外ANNS（OODANNS）

分布外近似最近邻搜索（OODANNS）指的是查询的分布与基础数据的分布不同。在现代跨模态应用中，两种模态的向量之间始终存在固有的分布差距 [45]。例如，当使用文本作为查询来检索相关的视觉数据时，会出现分布差距，导致查询成为分布外数据。不幸的是，大多数ANNS索引算法主要针对单模态任务设计，例如图像-图像搜索。

:one:$\text{OOD-ANN}$：

1. 概念：查询向量和基础数据来自不同的分布，分布的差异固有且不可消除
2. 举例：用文本查询图像

------

跨模态向量搜索的OOD特性可以通过两种数学距离来定量描述：衡量两种分布的Wasserstein距离 [38, 72] 和衡量向量到分布的Mahalanobis距离 [51]。我们使用这两个指标来评估三个现代真实世界多模态数据集上的数据分布：Text-to-Image [14]、LAION [61] 和 WebVid [5]。这些数据集的特性如表1所示。

:one:对$\text{OOD}$的量化

1. $\text{Wasserstein}$距离：衡量两个分布间的差异，可理解为把一个分布搬到另一个的最小代价
2. $\text{Mahalanobis}$距离：衡量一个向量到一个分布的距离，可理解为一个点相对于一个分布的异常程度

------

为了用Wasserstein距离量化OOD特性，我们从基础数据中抽取两个互不相交的集合（$B_{1}, B_{2}$）以及一个来自查询向量的查询集合$(Q)$，每个集合包含100,000个向量。如表2所示，来自基础数据的两个样本在$W_{2}\left(B_{1}, B_{2}\right)$上表现出接近性。而查询分布相较于基础数据分布的偏离程度，在三个数据集上分别高出1.67倍、1.89倍和2.89倍。

:one:实验验证：用$\text{Wasserstein}$距离衡量$\text{OOD}$特性

1. 参与验证的集：从基础数据集中抽取的无交叉集$B_1/B_2$，查询集$Q$
2. 实验结果：$\text{Wasserstein}(B_1,Q)$和$\text{Wasserstein}(B_2,Q)$，大致是$\text{Wasserstein}(B_1,B_2)$两倍

------

除了整体分布差异外，如果查询的Mahalanobis距离与基础数据的距离显著不同于基础向量之间的距离 [34]，则认为该查询属于分布外数据。对于ID查询集中的每个$q_{id}$和查询集$Q$中的OOD查询$q_{ood}$，我们计算Mahalanobis距离来估计$d_{M}(q, P)$，其中$P$是基础数据分布。如图1所示，OOD查询显著偏离基础数据分布。特别是，来自LAION和WebVid的查询相较于Text-to-Image中的查询表现出更强的分布外特性。

:one:实验验证：用$\text{Mahalanobis}$距离衡量$\text{OOD}$特性

1. 参与验证的数据：基础数据满足分布$P$，来自$\text{ID}$查询集的$q_{id}$，来自$\text{OOD}$查询集的$q_{ood}$
2. 分别计算$q_{id}/q_{ood}$与$P$的$\text{Mahalanobis}$距离，结果显然是$\text{Mahalanobis}(q_{id},P)$更小

### 2.3 当前方法在OOD-ANNS中的低效性

**2.3.1 当前方法在OOD工作负载上的性能表现。**
 我们在表1所示的三个多模态数据集上对IVF和HNSW进行了评估。分布内（ID）查询来自与基础数据具有相同经验分布的原始大规模数据集，而分布外（OOD）查询则使用每个数据集的文本查询集。IVF索引通过Faiss库 [37] 构建，HNSW索引使用官方实现 [52] 构建，并采用推荐参数进行实验。

如图2所示，OOD工作负载上存在显著的性能下降。当使用IVF索引时，与ID查询相比，OOD查询需要搜索显著更多的聚类才能达到较高的召回率。在三个数据集中，ID查询在搜索最接近的50个聚类时，Recall@10均超过0.97；然而，OOD查询的Recall@10分别仅为$0.91, 0.20$和0.52。

同样，在HNSW索引中，OOD查询在图上的束搜索过程中也需要访问显著更多的节点，导致在三个数据集上搜索效率较低。在LAION数据集中，OOD查询需要超过500次跳跃才能达到Recall@10 $\geq 0.93$，而ID查询仅需要48次跳跃。这突显了由于OOD查询导致的搜索路径长度增加了约10倍，从而带来了效率低下的问题。

实验结果表明，现有索引在跨模态ANNS任务中表现不佳，性能大幅下降，这进一步强调了设计高效索引的迫切需求。

:one:$\text{HNSW}$和$\text{IVF}$在$\text{OOD}$查询的性能：实验证明$\text{IVF}$的$\text{Recall}$大幅下降，$\text{HNSW}$的跳数明显增加

---

**2.3.2 先前OOD-ANNS解决方案的局限性。**

针对OOD-ANNS问题设计的首个图索引是**RobustVamana**，其由OOD-DiskANN [34] 提出。RobustVamana的主要目标是利用查询向量在Vamana图 [35] 中添加边。在将基础数据连接后，还将查询插入到Vamana图中。随后，RobustVamana启动一个名为**RobustStitch**的互连过程，以在插入的查询关联的最近节点之间创建完整连接。

------

图3比较了RobustVamana与其原始设计Vamana在处理OOD工作负载时的性能表现。当召回率达到Recall@10=0.9时，RobustVamana相较于Vamana提供了$13% \sim 67%$的性能提升，但随着召回率的提高，这种提升变得越来越小。然而，在处理OOD查询时，RobustVamana平均比Vamana处理ID查询的速度分别慢了3.9倍、5.3倍和10.0倍（针对三个数据集）。

------

这些结果突显了现有OOD-ANNS解决方案在效率上的不足，并强调了设计一种新型索引以提升OOD工作负载性能的巨大潜力。

:one:$\text{RobustVamana}$的设计

1. 过程：
   - 基于$\text{Vamana}$图，先用基础数据建图
   - 再将查询插入图中，用$\text{RobustStitch}$连接新的边
2. 性能：相比$\text{Vamana}$在$\text{OOD-ANN}$上$\text{Reecall}$提升了$\text{40\%}$左右
3. 难题：处理$\text{OOD}$查询比$\text{ID}$查询慢很多

