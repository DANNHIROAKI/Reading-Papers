# 0. 摘要

自然语言理解（NLU）的最新进展正在快速推动信息检索（IR）的发展，主要归功于对深层语言模型（LMs）进行微调以用于文档排序。尽管效果显著，但基于这些语言模型的排序模型在计算成本上比先前的方法高出数个数量级，尤其是因为它们必须将每个查询-文档对输入到庞大的神经网络中以计算单一的相关性评分。为了解决这个问题，我们提出了ColBERT，一种新颖的排序模型，它通过深层语言模型（尤其是BERT）实现高效检索。ColBERT引入了一种后期交互架构，它使用BERT独立编码查询和文档，然后采用一种既便宜又强大的交互步骤来模拟它们的细粒度相似性。通过延迟但保留这种细粒度的交互，ColBERT能够利用深层语言模型的表达能力，同时获得离线预计算文档表示的能力，大大加快查询处理速度。除了降低对传统模型检索出的文档进行重排序的成本外，ColBERT的支持剪枝的交互机制还使其能够利用向量相似性索引直接从大型文档集合中进行端到端的检索。我们使用两个最新的段落搜索数据集对ColBERT进行了广泛评估。结果表明，ColBERT的效果与现有的基于BERT的模型竞争（并优于所有非BERT基准），同时查询执行速度提高了两个数量级，且每个查询所需的浮点运算量减少了四个数量级。

# 1. Intro

在过去的几年里，信息检索（IR）领域引入了大量的神经排序模型，包括DRMM [7]、KNRM [4, 36]和Duet [20, 22]。与依赖人工设计特征的传统排序学习方法不同，这些模型采用基于嵌入的查询和文档表示，并直接对其内容间的局部交互（即细粒度关系）进行建模。其中，一种新的方法出现了，它通过微调深度预训练语言模型（LMs），如ELMo [29]和BERT [5]，来估计相关性。通过计算查询-文档对的深度上下文语义表示，这些语言模型有助于弥合文档和查询之间普遍存在的词汇不匹配问题 [21, 42] [30]。确实，在短短几个月内，基于BERT的多个排序模型已在各种检索基准测试上取得了最新的效果 [3,18,25,39]，并被Google${ }^1$和Bing${ }^2$专有地适配用于部署。

---

然而，这些语言模型所带来的显著效果提升是以急剧增加的计算成本为代价的。Hofstätter等人 [9] 和MacAvaney等人 [18] 观察到，文献中的基于BERT的模型比之前的模型在计算上高出100-1000倍——其中一些模型起初也并不便宜 [13]。图1总结了这一质量-成本权衡，该图将两个基于BERT的排序器 [25, 27] 与一组代表性的排序模型进行了比较。该图使用了MS MARCO Ranking [24]，这是一个包含900万段落和100万查询的最新集合，来自Bing的日志。它报告了在官方验证集上的检索效果（MRR@10）以及平均查询延迟（对数刻度），使用的是一台高端服务器，每个查询为神经重排序模型配备一块Tesla V100 GPU。按照MS MARCO的重排序设置，ColBERT（重排序）、神经匹配模型和深层语言模型对每个查询的官方前1000个文档进行重排序。其他方法，包括ColBERT（全检索），则直接从整个集合中检索前1000个结果。

---

如图所示，BERT显著提高了搜索精度，使MRR@10比最佳的先前方法提高了近7%；同时，即使使用高端GPU，它的延迟也增加到数万毫秒。这带来了一个棘手的权衡问题，因为查询响应时间的增加即使只有100毫秒，也会影响用户体验，甚至可测量地减少收入 [17]。为了解决这个问题，最近的研究开始探索使用自然语言理解（NLU）技术来增强传统的检索模型，如BM25 [32]。例如，Nogueira等人 [26, 28] 在使用BM25评分进行索引之前，通过NLU生成的查询扩展文档，而Dai和Callan [2] 则用NLU估计的词项重要性替代了BM25的词频。尽管这些方法成功地减少了延迟，但相较于BERT，精度通常显著降低。

---

为了在信息检索中平衡效率和上下文处理，我们提出了ColBERT，这是一种基于BERT的上下文化后期交互的排序模型。顾名思义，ColBERT提出了一种新的后期交互范式，用于估计查询$q$与文档$d$之间的相关性。在后期交互中，$q$和$d$被分别编码为两组上下文嵌入，并通过这两组嵌入之间的便捷且适合剪枝的计算来评估相关性——即快速计算，这样的计算可以在不穷尽所有候选项的情况下实现排序。

---

图2对比了我们提出的后期交互方法与现有的神经匹配范式。在左侧，图2(a)展示了基于表示的排序模型，这些模型分别为$q$和$d$独立计算一个嵌入，并将相关性估计为两个向量之间的单一相似度分数 [12, 41]。向右移动，图2(b)展示了典型的基于交互的排序模型。与将$q$和$d$总结为单个嵌入不同，这些排序模型在$q$和$d$之间建模词和短语级别的关系，并使用深度神经网络（例如CNN/MLP [22]或核函数 [36]）进行匹配。在最简单的情况下，它们将一个交互矩阵输入到神经网络中，该矩阵反映了$q$和$d$之间每对词的相似性。再向右，图2(c)展示了一种更强大的基于交互的范式，如BERT的transformer架构 [25]，该范式在建模$q$和$d$之间的词级交互的同时，也建模了其内部的词级交互。

---

这些日益富有表现力的架构之间存在一定的紧张关系。虽然基于交互的模型（即图2 (b)和(c)）在信息检索任务中往往表现更优 [8, 21]，但基于表示的模型——通过隔离$q$和$d$之间的计算——可以预先离线计算文档表示，从而大大降低每个查询的计算负担 [41]。在这项工作中，我们观察到，基于交互的模型的细粒度匹配能力和基于表示的模型的文档表示预计算功能可以通过保留但延迟查询-文档交互来结合。图2(d)展示了这样一种架构。正如所示，每个查询嵌入通过MaxSim运算符与所有文档嵌入进行交互，该运算符计算最大相似度（例如，余弦相似度），这些运算符的标量输出在查询项之间累加。此范式允许ColBERT在利用基于深度语言模型的表示的同时，将文档编码的成本转移到离线进行，并将查询编码的成本一次性分摊到所有排序文档上。此外，它使ColBERT能够利用向量相似性检索索引（例如，[1,15]）直接从大型文档集合中检索前$k$个结果，从而显著提升了相对于仅重排序基于词项检索输出的模型的召回率。

---

如图1所示，ColBERT可以在几十到几百毫秒内响应查询。例如，当用于重排序（如“ColBERT (re-rank)”）时，相较于现有的基于BERT的模型，它实现了超过$170 \times$的加速（并且所需的浮点运算减少了$14,000 \times$），同时效果优于所有非BERT基准 (§4.2和4.3)。ColBERT的索引构建（唯一需要将文档输入BERT的步骤）也具有实际操作性：它可以在大约3小时内使用配备四块GPU的单台服务器索引9百万篇MS MARCO段落集合 (§4.5)，并能在空间占用仅几十GiB的情况下保持其有效性。我们广泛的消融研究 (§4.4) 显示，后期交互、其通过MaxSim操作的实现以及我们基于BERT的编码器中的关键设计选择对于ColBERT的有效性都至关重要。

---

我们的主要贡献如下：

(1) 我们提出了后期交互 (§3.1) 作为一种高效且有效的神经排序范式。

(2) 我们介绍了ColBERT (§3.2和3.3)，一种高度有效的模型，它在后期交互范式下使用了新颖的基于BERT的查询和文档编码器。

(3) 我们展示了如何利用ColBERT既可用于基于词项的检索模型上的重排序 (§3.5)，也可用于使用向量相似性索引对完整集合进行搜索 (§3.6)。

(4) 我们在MS MARCO和TREC CAR两个最新的段落搜索集合上对ColBERT进行了评估。