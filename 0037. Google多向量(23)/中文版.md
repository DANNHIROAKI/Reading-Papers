# 多向量检索作为稀疏对齐


#### 摘要

多向量检索模型在许多信息检索任务上优于单向量双编码器。在本文中，我们将多向量检索问题转化为查询和文档标记之间的稀疏对齐。我们提出了ALIGNER，一种新颖的多向量检索模型，该模型学习查询和文档标记之间的稀疏成对对齐（例如“狗”与“小狗”），以及反映它们在检索中相对重要性的每个标记的单元素显著性。我们展示了控制成对标记对齐的稀疏性通常能带来显著的性能提升。虽然大多数集中于文档特定部分的事实型问题需要较少的对齐，但其他需要更广泛理解文档的问题则倾向于更多的对齐。另一方面，单元素显著性决定了一个标记是否需要在检索中与其他标记对齐（例如“什么类型的货币在新西兰使用”中的“类型”）。通过稀疏化的单元素显著性，我们能够修剪大量的查询和文档标记向量，并提高多向量检索的效率。我们使用熵正则化的线性规划学习稀疏的单元素显著性，这种方法在实现稀疏性方面优于其他方法。在零样本设置中，ALIGNER在BEIR基准测试的13个任务中获得了51.1的nDCG@10分数，实现了新的仅检索器的最先进水平。此外，通过少量示例（≤8）调整成对对齐，进一步将论点检索任务的性能提高了多达15.7个nDCG@10分数。ALIGNER的单元素显著性帮助我们仅保留20%的文档标记表示，同时将性能损失降至最低。我们进一步表明，我们的模型通常生成可解释的对齐，并在从更大的语言模型初始化时显著提高其性能。

## 1 引言

神经信息检索（IR）已成为改进传统IR系统的一个有前景的研究方向。最常用的方法称为双编码器，其通过将每个查询和文档表示为单个密集向量来操作。在有足够标注的情况下，双编码器直接学习向量之间的任务驱动相似性，并且在复杂任务（如问答）上通常超越传统IR系统（Lee等，2019；Karpukhin等，2020；Ni等，2021）。然而，由于单向量的表示能力有限，这些模型在跨领域数据集（Thakur等，2021）和/或以实体为中心的问题（Sciavolino等，2021）上可能难以泛化。作为一种补救措施，多向量检索模型（Khattab & Zaharia，2020；Luan等，2021；Gao等，2021）转而使用多个向量（通常是上下文化的标记向量）来表示文本。这些模型大幅提高了模型的表达能力，并表现出比单向量模型更强的性能和鲁棒性。

---

现有的多向量检索模型（如ColBERT，Khattab & Zaharia，2020）通过为每个查询标记选择得分最高的文档标记并聚合得分来计算查询-文档相似性。这种“最大和”方法有两个主要局限性。首先，将选择限制为单个文档标记对于某些检索任务可能非常次优。正如我们将在实验中展示的那样，通过放宽这一约束，检索性能可以提高超过16个nDCG@10分数。其次，这种方法还会导致较大的搜索索引和昂贵的计算成本。具体来说，检索和存储成本与查询和文档长度成线性比例，这使得多向量检索模型在效率要求高的应用中成为次优选择。我们直接应对这些挑战，以构建更快、更准确的模型。

---

多向量检索的表示学习问题可以表述为优化标记级别的对齐。具体来说，我们使用稀疏对齐矩阵来聚合标记级别的相似性，其中每个元素表示一对标记的对齐。从这个角度来看，我们能够以统一的方式表述不同的检索模型（图1），并识别出现有模型的缺点。

---

基于我们的公式，我们提出了Aligner，一种新颖的多向量检索模型，它由成对对齐和单元素显著性组成。成对对齐构成了ALIGNER的基础，其中查询和文档标记对基于它们的上下文表示进行稀疏对齐。研究发现，改变对齐的稀疏性可以显著影响检索任务的性能。例如，事实型问题通常倾向于少量对齐，因为它们通常集中在文档的一小部分。然而，其他任务（例如论点检索和事实核查）的查询需要更多的对齐以更广泛地理解文档。我们的发现也支持Dai等人（2022b）的论点，即具有不同意图的检索任务应该以不同的方式建模。

---

ALIGNER还学习单元素显著性，它决定每个标记是否需要在检索中与任何其他标记对齐。这对应于屏蔽对齐矩阵的整行或整列，而不是单个标记对齐。为了稀疏化整行或整列，我们引入了一种算法，该算法生成稀疏标记显著性，并且基于熵正则化线性规划的新公式是端到端可微的。稀疏化的单元素显著性使我们能够修剪大量的文档和查询标记表示，使多向量检索成为一种更高效和经济的解决方案。

---

我们在BEIR基准测试（Thakur等，2021）上评估了ALIGneR，该基准测试涵盖了多个领域的多种检索任务。在零样本设置中，我们展示了简单地扩展我们的模型即可达到最先进的性能，优于之前的神经检索器，而无需对比预训练、基于模型的硬负样本挖掘或蒸馏。通过使用目标任务的少量示例调整成对对齐——类似于Dai等人（2022b）的设置——ALIGNER在论点检索任务上可以进一步提高多达15.7个nDCG@10分数。同时，使用我们的单元素显著性进行修剪可以减少50%的查询标记以提高运行时效率，并减少80%的文档标记以改善存储占用，而nDCG@10的下降不到1分。成对对齐和单元素显著性也具有高度可解释性，因此它们通常作为检索的简明理由。

## 2 多向量检索作为稀疏对齐

给定一个查询 $Q$ 和一个包含 $N$ 个文档的集合 $\mathscr{C}=\left\{D^{(1)}, \ldots, D^{(N)}\right\}$，检索中的一个关键问题是如何表示这些文本输入以便于高效搜索。为此，一种方法是使用稀疏的词袋表示进行词汇检索；另一种方法是密集检索，这也是本文的重点。密集检索模型学习一个参数化函数，将查询和文档分别编码为查询表示 $\boldsymbol{q}$ 和文档表示 $\left\{\boldsymbol{d}^{(1)}, \ldots, \boldsymbol{d}^{(N)}\right\}$。通常，每个表示是一个单一的 $d$ 维向量。在检索中，相似性函数通常定义为 $\operatorname{sim}\left(Q, D^{(i)}\right)=\boldsymbol{q}^{\top} \boldsymbol{d}^{(i)}$，并且检索与查询具有高相似性得分的文档。

### 2.1 多向量检索

与将每个查询和文档表示为单个固定长度向量不同，多向量检索使用多个标记向量来表示它们，主要是为了提高固定长度表示的有限容量。具体来说，查询 $Q=\left\{q_{1}, \ldots, q_{n}\right\}$ 和文档 $D=\left\{d_{1}, \ldots, d_{m}\right\}$ 被编码为一组向量 $\left\{\boldsymbol{q}_{1}, \ldots, \boldsymbol{q}_{n}\right\}$ 和 $\left\{\boldsymbol{d}_{1}, \ldots, \boldsymbol{d}_{m}\right\}$。多向量检索重新定义了查询和文档之间的相似性函数。例如，ColBERT（Khattab & Zaharia 2020）设计了如下相似性函数：

$$
\operatorname{sim}(Q, D)=\sum_{i=1}^{n} \max _{j=1 \ldots m} \boldsymbol{q}_{i}^{\top} \boldsymbol{d}_{j}
$$

在检索中，多向量检索不是索引 $N$ 个文档向量，而是预先计算 $N \times \bar{m}$ 个文档标记向量，其中 $\bar{m}$ 是文档的平均长度。然后，它使用最大内积搜索（MIPS）为每个查询标记向量检索 $K$ 个文档标记向量，得到 $n \times K$ 个候选文档标记。检索到的标记用于追溯原始文档（Lee等，2021a），通常随后是一个最终的精炼阶段，该阶段使用每个文档和查询的所有标记表示来评分相似性 $\operatorname{sim}(Q, D)$（Khattab & Zaharia，2020）。我们在实验中采用了与ColBERT相同的做法。

### 2.2 稀疏对齐公式化

检索模型的一个关键设计问题是以一种平衡模型表达能力和推理成本的方式定义相似性函数。为了便于讨论，我们将之前方法中使用的相似性形式化为一类稀疏对齐函数。这种公式化还导致了现有工作的原则性扩展，我们将在 $\S 3$ 中描述。

---

我们首先定义一个相似性矩阵 $\boldsymbol{S} \in \mathbb{R}^{n \times m}$，该矩阵由所有查询和文档标记对计算得出，其中 $\boldsymbol{S}_{i, j}=\boldsymbol{q}_{i}^{\top} \boldsymbol{d}_{j}$。然后，我们使用一个对齐矩阵 $\boldsymbol{A} \in[0,1]^{n \times m}$ 来计算 $Q$ 和 $D$ 之间的相似性，如下所示：

$$
\begin{equation*}
\operatorname{sim}(Q, D)=\frac{1}{Z} \sum_{i=1}^{n} \sum_{j=1}^{m} \boldsymbol{S}_{i, j} \boldsymbol{A}_{i, j} \tag{1}
\end{equation*}
$$

其中 $Z$ 是一个归一化项，定义为 $Z=\sum_{i, j} \boldsymbol{A}_{i, j}$。对齐矩阵 $\boldsymbol{A}$ 可以直接从 $S$ 导出，或者作为 $Q$ 和 $D$ 的函数计算得出。

---

在我们的公式基础上，对齐矩阵 $\boldsymbol{A}$ 被约束为稀疏激活：$\|\boldsymbol{A}\|_{0} \leq \sigma$，其中 $\|\cdot\|_{0}$ 表示矩阵中非零元素的数量。稀疏激活假设只有少数查询-文档标记匹配对检索至关重要，这一假设受到传统检索方法的启发。事实上，大多数现有的密集检索模型已经通过各自的启发式方法实现了稀疏对齐。图 1 展示了不同模型在我们的公式下的描述：

- **Dense Passage Retriever (DPR; Karpukhin 等, 2020)** 使用单个 [CLS] 向量表示每个查询和文档。这相当于设置 $A_{1,1}=1$，其余为 0，因此 $\|\boldsymbol{A}\|_{0}=1$。
- **ME-BERT (Luan 等, 2021)** 使用前 $k$ 个文档标记向量来表示文档，但查询仍使用单个向量。其相似性函数为 $\max _{j=1 \ldots k} \boldsymbol{q}_{1}^{\top} \boldsymbol{d}_{j}$，这相当于当 $\boldsymbol{S}_{1, j}$ 在 $\boldsymbol{S}_{1,1}$ 到 $\boldsymbol{S}_{1, k}$ 中最大时设置 $A_{1, j}=1$，否则为 0。对齐稀疏性为 $\|\boldsymbol{A}\|_{0}=1$。
- **ColBERT** 使用“最大和”相似性函数 $\sum_{i=1}^{n} \max _{j=1 \ldots m} \boldsymbol{S}_{i, j}$，这相当于设置对齐矩阵以选择 $\boldsymbol{S}$ 每行中的最大元素，即当 $\boldsymbol{S}_{i, j}$ 在 $\boldsymbol{S}_{i,:}$ 中最大时设置 $\boldsymbol{A}_{i, j}=1$，此时 $\|\boldsymbol{A}\|_{0}=n$。
- **COIL (Gao 等, 2021)** 与 ColBERT 类似，也从 $\boldsymbol{S}$ 的每行中选择最大元素，但要求所选对在词汇上完全匹配，即当 $\boldsymbol{S}_{i, j}$ 在 $\left\{\boldsymbol{S}_{i, j^{\prime}} \mid q_{i}=d_{j^{\prime}}\right\}$ 中最大时设置 $\boldsymbol{A}_{i, j}=1$，此时 $\|\boldsymbol{A}\|_{0} \leq n$。

---

相似性和稀疏性的选择对模型的容量和效率有很大影响。例如，ColBERT 比 DPR（Thakur 等, 2021）更具表达能力和鲁棒性，但其检索和存储成本也更高。我们的工作旨在进一步推进表达能力，同时保持较高的效率。我们将在下一节中描述我们的方法。

## 3 Aligner

在本节中，我们介绍基于稀疏对齐公式的 Aligner。Aligner 将对齐矩阵分解为成对对齐和单元素显著性：

$$
\begin{equation*}
\boldsymbol{A}=\tilde{\boldsymbol{A}} \odot\left(\boldsymbol{u}^{q} \otimes \boldsymbol{u}^{d}\right) \tag{2}
\end{equation*}
$$

其中 $\odot$ 表示哈达玛积（逐元素相乘），$\otimes$ 表示两个向量的外积。成对对齐 $\tilde{A} \in \mathbb{R}^{n \times m}$ 决定哪些查询和文档标记应对齐，其稀疏性约束根据下游任务进行调整（见 $\S 3.1$）。单元素显著性 $\boldsymbol{u}^{q} \in \mathbb{R}^{n}$ 和 $\boldsymbol{u}^{d} \in \mathbb{R}^{m}$ 是稀疏的标记权重，决定一个标记是否需要参与对齐（见 $\S 3.2$）。

---

这种分解基于两个关键假设。首先，对齐的最优稀疏性可能依赖于任务。与 ColBERT 中的 top-1 约束不同，为某些任务激活多个对齐可以增强检索性能。例如，在我们的分析中，我们观察到仅关注文档特定部分的事实型问题需要少量对齐，而其他查询（如事实核查）则需要更多对齐以更广泛地理解文档。我们探索了成对对齐矩阵 $\tilde{\boldsymbol{A}}$ 的不同搜索空间，以便为每个下游任务实现更好的检索性能。其次，对齐仅对极少数标记是必要的。例如，在我们的初步研究中分析了 2000 个最常检索的文档，发现只有 $12.8 \%$ 的文档标记被至少一个查询检索到。直观上，无信息的标记不需要对齐和存储，这对应于 $\boldsymbol{A}$ 的整行或整列的稀疏激活。Aligner 直接学习行和列的稀疏性作为单元素显著性，并利用它们来提高检索效率。

### 3.1 调整成对对齐

查询和文档的分布可能各不相同。例如，查询可以是单个实体、自然问题或几句话，而文档的范围可以从短段落到长文章。此外，任务的搜索意图也会发生变化（Dai 等，2022b）。这些变化可能导致不同的最优对齐策略。我们探索了以下稀疏对齐变体，超越了现有工作中常用的 top-1 策略：

- **Top-k**：每个查询标记与相似性得分最高的 $k$ 个文档标记对齐。具体来说，当第 $j$ 个标记在行 $\boldsymbol{S}_{i}$ 的前 $k$ 个时，$\tilde{\boldsymbol{A}}_{i, j}=1$。当 $k=1$ 时，它与 ColBERT 等价。
- **Top-$p$**：该策略与 top-$k$ 类似，但不是每个查询标记恰好与 $k$ 个标记对齐，而是使对齐数量与文档长度成比例，即每个查询标记与 $\max (\lfloor p \cdot m\rfloor, 1)$ 个标记对齐，其中 $m$ 是文档长度，$p \in[0,1]$ 是对齐比例。

---

尽管这些变体简单，但它们确实可以在诸如论点检索等任务上显著提高检索准确性。更重要的是，虽然可以为不同的对齐变体训练单独的模型，但我们感兴趣的是使用单个共享模型进行快速测试时适应，因为许多重要的检索任务缺乏足够的训练数据（Thakur 等，2021）。具体来说，我们首先在源域中使用固定的对齐策略（如 top-1）训练 Aligner，然后在目标任务中调整对齐策略，而不改变模型参数。我们使用以下少样本对齐适应方法：给定语料库 $\left\{D^{(1)}, \ldots, D^{(N)}\right\}$ 以及目标任务中的少量相关性标注的查询-文档对 $\left\{\left(Q^{1}, D_{+}^{1}\right), \ldots\left(Q^{K}, D_{+}^{K}\right)\right\}$，我们首先使用学习到的标记表示检索候选文档，然后根据标注数据上的排序性能决定成对对齐策略。这种适应可以高效执行，因为对齐仅涉及精炼阶段中相似性得分（公式 1）的计算。在实践中，对于某些任务，我们能够找到合适的对齐策略，并使用少至 8 个标注样本提高检索性能。

### 3.2 学习单元素显著性

ALIGnER 从标记表示中预测标记的显著性。为简洁起见，我们仅展示文档显著性的公式化，查询显著性的定义类似。具体来说，第 $i$ 个文档标记的显著性 $u_{i}^{d}$ 定义为：

$$
\begin{equation*}
u_{i}^{d}=\lambda_{i}^{d} \cdot f\left(\boldsymbol{W}^{d} \boldsymbol{d}_{i}+b^{d}\right) \tag{3}
\end{equation*}
$$

其中 $\boldsymbol{W}^{d}$ 和 $b^{d}$ 是可学习参数。$f$ 是非线性激活函数，我们使用 ReLU 以确保显著性始终为非负数。$\lambda^{d}=\left\{\lambda_{i}^{d}\right\}$ 是用于控制 $\boldsymbol{u}^{d}$ 整体稀疏性的门控变量，我们将在接下来详细说明。

---

为了使文档显著性有意义，我们将显著性稀疏性作为归纳偏置进行约束。ALIGnER 将稀疏显著性与其他部分联合优化。由于显著性为零的标记不会参与相似性计算，我们的模型将被鼓励识别更重要的标记，以保持良好的检索性能。需要注意的是，在训练过程中，我们没有关于哪些标记是重要的显式标注。相反，$\boldsymbol{u}^{d}$（以及类似的 $\boldsymbol{u}^{q}$）在稀疏性约束 $\left\|\boldsymbol{\lambda}^{d}\right\|_{0}=\left\lceil\alpha^{d} \cdot m\right\rceil$ 下直接优化以最小化训练损失，其中 $\alpha^{d}$ 是常数稀疏率，$m$ 是文档长度。

---

当然，一个关键问题是如何在给定稀疏性控制的情况下优化单元素显著性组件。我们利用一种称为**熵正则化线性规划**的新技术来实现端到端优化。具体来说，令 $k=\left\lceil\alpha^{d} \cdot m\right\rceil$ 表示所需的稀疏性，$s_{i}=f\left(\boldsymbol{W}^{d} \boldsymbol{d}_{i}+b^{d}\right)$ 表示应用稀疏门控 $\lambda_{i}^{d}$ 之前的标记得分，$s, \boldsymbol{\lambda}^{d} \in \mathbb{R}^{m}$ 分别是向量 $\left\{s_{i}\right\}$ 和 $\left\{\lambda_{i}^{d}\right\}$。$\lambda^{d}$ 通过解决以下优化问题计算：

$$
\begin{equation*}
\max _{\boldsymbol{\lambda}} s^{\top} \boldsymbol{\lambda}+\varepsilon H(\boldsymbol{\lambda}) \quad \text { s.t. } \quad \mathbf{1}^{\top} \boldsymbol{\lambda}=k, \quad \lambda_{i} \in[0,1], \forall i=1, \ldots, m . \tag{4}
\end{equation*}
$$

其中 $H(\cdot)$ 是逐元素熵函数，$\varepsilon>0$ 是一个小常数。该优化可以看作是一个松弛的 top-$k$ 操作。如果没有熵项 $\varepsilon H(\cdot)$，它就成为一个线性规划问题，其解 $\boldsymbol{\lambda}^{d}$ 是一个二进制掩码，指示 $s$ 中的前 $k$ 个值，即当且仅当 $s_{i}$ 是 $s$ 中的前 $k$ 个值时，$\lambda_{i}^{d}=1$。通过添加小熵项 $\varepsilon H(\cdot)$ 并将 $\lambda_{i}$ 从精确的二进制松弛到 $[0,1]$，这种 top-$k$ 优化被平滑化。给定较小的 $\varepsilon$，这仍然会产生稀疏解 $\boldsymbol{\lambda}^{d}$，并且可以通过简单的向量操作求解。具体来说，令 $a \in \mathbb{R}$ 和 $b_{i} \in \mathbb{R}$（$i=1, \cdots, m$）为初始化为零的辅助变量。我们使用以下方程迭代更新这些变量：

$$
\begin{equation*}
a^{\prime}=\varepsilon \ln (k)-\varepsilon \ln \left\{\sum_{i} \exp \left(\frac{s_{i}+b_{i}}{\varepsilon}\right)\right\}, \quad b_{i}^{\prime}=\min \left(-s_{i}-a^{\prime}, 0\right) \tag{5}
\end{equation*}
$$

在实践中，只需运行几次迭代就足够了，最终解由 $\lambda_{i}=\exp \left(\frac{s_{i}+b_{i}+a}{\varepsilon}\right)$ 给出。这些向量操作是可微的，因此 $\boldsymbol{\lambda}$ 可以与模型的其他部分一起端到端训练。该迭代算法的完整推导见附录 A.1。

---

**多向量检索的剪枝**  
通过学习到的单元素显著性，我们可以自然地剪枝多向量检索中的标记。剪枝文档标记可以减少搜索索引中的向量数量，剪枝查询标记可以减少搜索次数。在我们的实验中，我们分别使用两个剪枝比率 $\beta^{q}$ 和 $\beta^{d}$ 来控制它们。对于每个文档，我们使用公式 (3) 获得标记显著性，并仅存储索引中前 $\beta^{d}$ 百分比的标记。类似地，我们选择前 $\beta^{q}$ 百分比的查询标记来执行最大内积搜索。需要注意的是，我们通过调整这两个比率来控制检索效率，这些比率可以小于训练时用作约束的稀疏率 $\alpha^{q}$ 和 $\alpha^{d}$。在精炼阶段，我们仍然使用包含所有标记向量的完整模型进行评分。

## 实验

### 4.1 实验设置

ALIGnER 使用从 T5 版本 1.1（Raffel 等，2020）初始化的共享 Transformer 编码器。我们将标记嵌入投影到 128 维并应用 L2 归一化。遵循 GTR（Ni 等，2021），我们在 MS MARCO 上对 ALIGNER 进行微调，并使用 RocketQA（Qu 等，2021）发布的难负样本。模型以 256 的批量大小训练 25k 步，查询序列长度为 64，文档序列长度为 256。我们使用 top-1 成对对齐训练 ALIGNER。

---

对于检索，我们预先计算语料库中所有文档的标记编码，并使用 ScaNN（Guo 等，2020）进行索引和执行最大内积搜索（MIPS）。我们为每个查询标记检索 4,000 个最近邻，并在精炼阶段返回前 1,000 个。我们在 BEIR 基准测试（Thakur 等，2021）上评估 ALIGNER，并与表 [1] 中展示的最先进的检索模型进行比较。需要注意的是，AliGneR 不依赖于对比模型预训练（Izacard 等，2022；Ni 等，2021）、基于模型的难负样本挖掘（Santhanam 等，2021）或蒸馏（Santhanam 等，2021）。我们有意选择这种简单的配置，专注于研究成对对齐和单元素显著性的影响。

---

对于 ALIGNER 的少样本对齐适应（$\S 3.1$），我们将测试数据分为多个折，每折包含 8 个示例。然后，我们找到使每折上的 nDCG@10 最大化的最佳对齐策略，其中 top-$k$ 的 $k \in\{1,2,4,6,8\}$，top-$p$ 的 $p \in\{0.5\%, 1\%, 1.5\%, 2\%\}$。基于每折的最佳对齐策略，我们使用最佳策略测量剩余测试示例的检索性能。我们报告这些测试分数的平均值（$\pm$ 标准差），其中测试分数的数量等于折的数量。少样本适应的平均值表示使用少量示例选择最佳对齐策略的预期性能。

### 4.2 检索准确性

表 2 展示了 ALIGNER 在 MS MARCO 和 BEIR 基准测试中的文档检索性能。在本实验中，我们没有使用单元素显著性剪枝任何查询或文档标记，而是将其效果展示在 $\S 4.3$ 中。ALIGNER xxl 在 MS MARCO 上优于所有基线，展示了多向量检索模型如何从大规模预训练语言模型中受益。ALIGNER ${ }_{\mathrm{xx1}}$ 在 13 个 BEIR 数据集中的 9 个上也优于 GTR $_{\mathrm{xxl}}$，并将仅检索器的最先进水平（ColBERT ${ }_{\mathrm{v} 2}$）平均提高了 1.2 个 nDCG@10 分数。图 3 显示，我们的多向量检索器模型比单向量双编码器 GTR 扩展性更好。

---

**对齐适应**。在表 2 的最右侧列中，我们展示了在 BEIR 基准测试中通过 ALIGNER 调整成对对齐的效果。仅使用 8 个示例找到合适的对齐稀疏性后，其预期性能平均达到 $52.6 \mathrm{nDCG} @ 10$。对齐适应的 ALIGNER 也从扩展中受益，并且始终优于未适应的对应版本，如图 3 所示。表 3 进一步解释了这些增益，其中展示了不同对齐策略下各任务的性能。尽管 ALIGNER 使用 top-1 对齐进行训练，但在推理时 top-1 并不总是最佳策略。具体来说，对于 ArguAna，我们观察到通过将对齐数量与文档长度成比例调整为 $p=1.5 \%$，性能提高了 16 分。其他任务如 Touché-2020 也倾向于其他对齐策略，这表明不同任务可能需要不同的稀疏性。总体而言，保持足够低的稀疏性是更可取的，并支持我们的假设，即成对对齐应该是稀疏的。

---

我们进一步检查了当 ALIGNER 使用其他成对对齐策略进行训练时，这一观察结果是否仍然成立。图 4 展示了使用四种替代策略训练的 ALIGneR 变体。我们使用训练时的对齐策略（默认）和每个数据集选择的最佳对齐策略（oracle）评估它们的性能。尽管这些模型在默认对齐策略下表现不同，但在 oracle 对齐适应后，它们的表现相似。

---

图 5 展示了少样本对齐适应的有效性——基于少量示例动态选择任务特定的对齐策略。当默认对齐（top-k=1）不是最优时，我们仅使用 8 个示例就能确定一个好的对齐策略，从而显著提高了模型在论点检索任务上的性能。使用 16 个示例进一步提高了平均分数并减少了方差。然而，当默认对齐已经是最优时（top-$k=1$ 对于 QA 任务是最优的），由于我们少样本方法的方差，少样本对齐适应会损害性能。尽管如此，ALIGNER 在 11 个数据集中的 6 个上优于 Promptagator（Dai 等，2022b），这是另一个少样本检索基线。

### 4.3 检索效率

接下来的实验展示了 ALIGNER 的单元素显著性如何影响检索效率。我们根据经验性能，使用显著性稀疏比率 $\alpha^{q}=50 \%$ 和 $\alpha^{d}=40 \%$ 训练 ALIGNER $_{\text {base }}$。门控变量通过 $\varepsilon=0.002$ 进行优化。在检索时，我们使用比率 $\beta^{q}$ 和 $\beta^{d}$（$\S 3.2$）剪枝查询和文档标记。

---

图 6 展示了 ALIGNER 在不同剪枝比率下在 MS MARCO 上的性能。当剪枝比率与训练时相同（$\beta^{q}=50 \%$ 和 $\beta_{d}=40 \%$）时，模型接近于未剪枝的完整 ALIGNER 模型（MRR@1038.1 vs. 38.8），但大大节省了计算成本。我们可以通过调整 $\beta^{d}$ 和 $\beta_{q}$ 进一步剪枝标记。模型在 $\beta^{d}=10 \%$ 时实现了 37.3 MRR @ 10，即仅使用原始索引大小的 $10 \%$ 仍能保持准确性。将查询剪枝比率 $\beta^{q}$ 降低到 $30 \%$ 不会对性能造成太大影响，尽管将 $\beta^{q}$ 降低到 $10 \%$ 会导致性能下降。图 6 还比较了 ALIGNER 的熵正则化线性规划（公式 4）与其他方法。仅使用 ReLU 门控且无稀疏约束（图 6 中的 'ReLU'），模型在 $\beta^{d}=40 \%$ 时仍能保持良好的性能，但在 $\beta^{d}$ 较小时性能显著下降。去除公式 4 中的熵正则化会导致简单地选择具有最高预测显著性的硬 top-$k$ 标记（图 6 中的 'Hard'），硬 top-$k$ 解在所有 $\beta^{d}$ 下性能都较差。

---

ALIGNER 的显著性估计也推广到其他检索数据集。如图 7 所示，使用 $\beta_{d}=10 \%$ 和 $\beta^{q}=50 \%$ 进行剪枝对大多数 BEIR 数据集的性能影响最小。我们甚至观察到 Touché-2020 的性能有所提高，因为模型在剪枝后只能检索到显著标记。此外，我们展示了对齐适应可以与剪枝结合，从而形成一个高效且有效的检索模型。

### 4.4 可解释性

表 4 展示了 ALIGNER 学习到的成对对齐和单元素显著性的示例。模型将查询标记与上下文相似的标记对齐，但不一定是完全相同的标记。显著性特征也在表 4 中突出显示。重要的名词短语和动词通常被赋予更高的显著性，这与人类直觉一致。我们在附录 A.3 中展示了不同任务的对齐示例。总体而言，我们观察到问答任务通常需要每个查询标记较少的对齐，而其他需要广泛理解文档的任务则倾向于更多的对齐。

## 5 相关工作

最近的信息检索研究通常通过对比预训练（Ni 等，2021；Izacard 等，2022；Oguz 等，2022）、基于模型的难负样本挖掘（Xiong 等，2020；Lu 等，2021；Qu 等，2021）和知识蒸馏（Santhanam 等，2021；Zhang 等，2022；Reddi 等，2021）来提高检索准确性。检索效率则通过量化（Santhanam 等，2021）或低维向量（Hofstätter 等，2022）来改进。这些改进与本文工作是正交的。

---

术语重要性和显著性在信息检索中有着悠久的历史：从词频（$tf$）和逆文档频率（idf），到最近的基于 BERT 的重要性度量，如 DeepCT（Dai & Callan，2020）、SPARTA（Zhao 等，2021）和 Splade（Formal 等，2021b a）。这些工作主要集中在稀疏词汇检索上，并为稀疏词袋表示学习术语权重。多向量密集检索中的术语重要性较少被探索。我们的工作可能与 Hofstätter 等（2022）最近的一项工作最为相关，该工作通过使用 ReLU 门控和 L1 范数正则化从词嵌入中预测显著性分数来剪枝 ColBERT。

---

最近，Promptagator（Dai 等，2022b）指出了使用少量标注示例来适应新检索任务的重要性。Promptagator 通过使用大语言模型（Sanh 等，2022；Brown 等，2020；Wei 等，2022）进行查询生成（Ma 等，2021；Lee 等，2021b；Dai 等，2022a）实现少样本任务适应，这具有较高的推理成本。Aligner 更加通用，可以通过少样本对齐适应快速适应新任务。

## 6 结论

在本文中，我们介绍了 ALIGNER，一种用于多向量文档检索的新颖稀疏对齐方法。我们首先用标记级别的稀疏对齐公式化不同的检索模型，并提出了 Aligner 来解决现有模型的局限性。具体来说，Aligner 使用成对对齐和单元素显著性，使我们能够分别适应不同任务和剪枝不重要的标记。因此，我们在零样本和少样本文档检索任务上都取得了强劲的性能，同时大幅提高了多向量检索的运行时间和存储复杂度。凭借其可解释的对齐和在大语言模型下的更好性能，我们预计我们的多向量检索模型在未来可以作为一个强大的独立检索器。

## A 附录

## A.1 迭代更新的推导

我们展示了第 3.2 节中用于解决优化问题 (4) 的公式 (5) 的推导。最大化问题 (4) 可以写成等价的最小化问题：

$$
\begin{align*}
& \max _{\boldsymbol{\lambda}} s^{\top} \boldsymbol{\lambda}+\varepsilon H(\boldsymbol{\lambda}) \\
& \Longleftrightarrow \quad \min _{\boldsymbol{\lambda}}-s^{\top} \boldsymbol{\lambda}-\varepsilon H(\boldsymbol{\lambda}) \\
& \Longleftrightarrow \quad \min _{\boldsymbol{\lambda}}-s^{\top} \boldsymbol{\lambda}-\varepsilon H(\boldsymbol{\lambda})-\varepsilon \mathbf{1}^{\top} \boldsymbol{\lambda}  \tag{6}\\
& \text { s.t. } \mathbf{1}^{\top} \boldsymbol{\lambda}=k, \quad \lambda_{i} \in[0,1], \quad i=1, \ldots, m .
\end{align*}
$$

注意，项 $\varepsilon 1^{\top} \boldsymbol{\lambda}$ 将是一个常数 $\varepsilon \times k$，但我们将其包含在最小化目标中，以便稍后简化我们的推导。

---

现在，令 $a \in \mathbb{R}$ 和 $\boldsymbol{b} \in \mathbb{R}^{m}$ 为对应于线性约束 $\mathbf{1}^{\top} \boldsymbol{\lambda}=k$ 和 $\lambda_{i} \leq 1 \forall i$ 的拉格朗日变量。最小化问题等价于其拉格朗日表达式：

$$
\begin{equation*}
\min _{\lambda \in \mathbb{R}^{m}} \max _{a \in \mathbb{R}, \boldsymbol{b} \leq \mathbf{0}}-\boldsymbol{s}^{\top} \boldsymbol{\lambda}-\boldsymbol{\varepsilon} H(\boldsymbol{\lambda})-\boldsymbol{\varepsilon} \mathbf{1}^{\top} \boldsymbol{\lambda}+a\left(k-\mathbf{1}^{\top} \boldsymbol{\lambda}\right)+\boldsymbol{b}^{\top}(\mathbf{1}-\boldsymbol{\lambda}) \tag{7}
\end{equation*}
$$

目标函数 (6) 是强凸的，且 $\boldsymbol{\lambda}$ 的解空间是一个凸集。因此，强对偶性成立，我们可以转而解决在 (7) 中交换 min 和 max 算子的对偶问题：

$$
\begin{equation*}
\max _{a \in \mathbb{R}, \boldsymbol{b} \leq \mathbf{0}} \min _{\lambda \in \mathbb{R}^{m}}-\boldsymbol{s}^{\top} \boldsymbol{\lambda}-\boldsymbol{\varepsilon} H(\boldsymbol{\lambda})-\boldsymbol{\varepsilon} \mathbf{1}^{\top} \boldsymbol{\lambda}+a\left(k-\mathbf{1}^{\top} \boldsymbol{\lambda}\right)+\boldsymbol{b}^{\top}(\mathbf{1}-\boldsymbol{\lambda}) \tag{8}
\end{equation*}
$$

---

最优解 $(a, \boldsymbol{b}, \boldsymbol{\lambda})$ 必须满足 Karush-Kuhn-Tucker (KKT) 条件（Kuhn & Tucker，2014），即：
$$
\begin{gathered}
\frac{\partial\left(-s^{\top} \boldsymbol{\lambda}-\varepsilon H(\boldsymbol{\lambda})+\varepsilon \mathbf{1}^{\top} \boldsymbol{\lambda}+a\left(k-\mathbf{1}^{\top} \boldsymbol{\lambda}\right)+\boldsymbol{b}^{\top}(\mathbf{1}-\boldsymbol{\lambda})\right)}{\partial \boldsymbol{\lambda}}=0 \\
\Longleftrightarrow \quad \boldsymbol{\lambda}=\exp \left(\frac{\boldsymbol{s}+a+\boldsymbol{b}}{\varepsilon}\right) \quad \Longleftrightarrow \quad \lambda_{i}=\exp \left(\frac{s_{i}+a+\boldsymbol{b}_{i}}{\varepsilon}\right) \forall i=1, \ldots, m
\end{gathered}
$$

将 $\boldsymbol{\lambda}$ 代入上述方程 (8)，对偶问题现在具有简单的形式：

$$
\max _{a \in \mathbb{R}, \boldsymbol{b} \leq \mathbf{0}} k \cdot a+\mathbf{1}^{\top} b-\mathbf{1}^{\top} \exp \left(\frac{s+a+\boldsymbol{b}}{\varepsilon}\right)
$$

---

我们可以使用坐标下降法（Wright，2015）通过交替最大化固定 $a$ 或 $\boldsymbol{b}$ 的函数来解决这个问题。这导致了第 3.2 节中描述的迭代更新（公式 5）：
$$
\begin{aligned}
& a^{\prime}=\varepsilon \ln (k)-\varepsilon \ln \left\{\sum_{i} \exp \left(\frac{s_{i}+b_{i}}{\varepsilon}\right)\right\} \\
& b_{i}^{\prime}=\min \left(-s_{i}-a^{\prime}, 0\right)
\end{aligned}
$$

**讨论**。简而言之，我们通过对偶变量 $a$ 和 $\boldsymbol{b}$ 的坐标下降来解决优化问题 (4) 的对偶问题。也就是说，我们找到在固定 $\boldsymbol{b}$ 的情况下最大化对偶目标的最优 $a$，反之亦然。

---

这个迭代算法也与最优传输（Optimal Transport，OT）的 Sinkhorn 算法密切相关。事实上，Sinkhorn 算法解决了最优传输的熵正则化版本（Cuturi，2013）。然而，我们的工作涉及一个不同的优化实例。虽然 OT 解决了一个运输问题，其解空间由运输矩阵的行和列的边际约束定义，但我们的优化问题受到总预算（$\sum_{i} \lambda_{i}=k$）和上界（$\lambda_{i} \leq 1 \forall i$）的约束。这导致了不同的迭代更新。

## A.2 具有稀疏约束的可微对齐

除了 $\S 3.1$ 中的 Top-$k$ 和 Top-$p$ 对齐外，我们还探索了一种具有稀疏约束的可微成对对齐（DA）。Top-$k$ 和 Top-$p$ 都是对对齐进行硬选择，即 $\tilde{\boldsymbol{A}}_{i, j}$ 要么为 1，要么为 0。我们通过引入软稀疏约束来放宽这一限制。类似于我们对单元素显著性的公式化（$\S 3.2$），我们通过以下优化问题确定对齐 $\tilde{A}$：

$$
\begin{align*}
& \max _{\boldsymbol{A}}\langle\boldsymbol{S}, \boldsymbol{A}\rangle+\boldsymbol{\varepsilon H}(\boldsymbol{A}) \\
& \text { s.t. }  \tag{9}\\
& \quad \sum_{j} \boldsymbol{A}_{i, j}=k, i=1, \ldots, n \\
& \quad \boldsymbol{A}_{i, j} \in[0,1], \quad i=1, \ldots, n, j=1, \ldots, m
\end{align*}
$$

其中 $H(\cdot)$ 是逐元素熵函数，$\varepsilon>0$ 是一个小常数。我们约束 $\tilde{\boldsymbol{A}}$ 的每一行的和等于 $k$。当 $\varepsilon=0$ 时，公式 9 的解与 Top-$k$ 相同。当 $\varepsilon>0$ 时，熵项使优化问题强凹，可以使用附录 A.1 中的相同算法求解。该解是可微的，因此可以在我们的模型中端到端训练。
