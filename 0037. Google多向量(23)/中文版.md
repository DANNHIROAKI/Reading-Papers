# Multi-Vector Retrieval as Sparse Alignment 


#### Abstract

Multi-vector retrieval models improve over single-vector dual encoders on many information retrieval tasks. In this paper, we cast the multi-vector retrieval problem as sparse alignment between query and document tokens. We propose ALIGNER, a novel multi-vector retrieval model that learns sparsified pairwise alignments between query and document tokens (e.g. 'dog' vs. 'puppy') and per-token unary saliences reflecting their relative importance for retrieval. We show that controlling the sparsity of pairwise token alignments often brings significant performance gains. While most factoid questions focusing on a specific part of a document require a smaller number of alignments, others requiring a broader understanding of a document favor a larger number of alignments. Unary saliences, on the other hand, decide whether a token ever needs to be aligned with others for retrieval (e.g. 'kind' from 'what kind of currency is used in new zealand'). With sparsified unary saliences, we are able to prune a large number of query and document token vectors and improve the efficiency of multi-vector retrieval. We learn the sparse unary saliences with entropy-regularized linear programming, which outperforms other methods to achieve sparsity. In a zero-shot setting, ALIGNER scores 51.1 points nDCG@10, achieving a new retriever-only state-of-the-art on 13 tasks in the BEIR benchmark. In addition, adapting pairwise alignments with a few examples $(\leq 8)$ further improves the performance up to 15.7 points nDCG@10 for argument retrieval tasks. The unary saliences of ALIGNER helps us to keep only $20 \%$ of the document token representations with minimal performance loss. We further show that our model often produces interpretable alignments and significantly improves its performance when initialized from larger language models.


## 1 Introduction

Neural information retrieval (IR) has become a promising research direction for improving traditional IR systems. The most-commonly adopted approach called the dual encoder operates by representing every query and document as a single dense vector. Given sufficient annotations, dual encoders directly learn task-driven similarity between vectors, and often surpass traditional IR systems on complex tasks such as question answering (Lee et al. 2019, Karpukhin et al. 2020 , Ni et al. 2021). However, these models can struggle to generalize over out-of-domain datasets (Thakur et al. 2021) and/or entity-centric questions (Sciavolino et al., 2021) due to the limited representational capacity of single vectors. As a remedy, multi-vector retrieval models (Khattab \& Zaharia, 2020, Luan et al. 2021; Gao et al. 2021) instead use multiple vectors, typically the contextualized token vectors, to represent the text. These models largely improve the model expressiveness, and exhibit much stronger performance and robustness compared to their single-vector counterparts.

---

Existing multi-vector retrieval models such as ColBERT (Khattab \& Zaharia 2020) computes querydocument similarity by selecting the highest scoring document token for each query token and aggregating the scores. This sum-of-max method has two major limitations. First, restricting the selection to a single document token can be highly sub-optimal for some retrieval tasks. As we will show in our experiments, the retrieval performance can be improved by more than 16 points nDCG@10 by relaxing this constraint. Second, the method also leads to a large search index and expensive computation. Specifically, the retrieval and storage cost scales linearly with the query and document length, making multi-vector retrieval models an inferior choice for efficiency-demanding applications. We directly tackle these challenges to build faster and more accurate models.

---

The representation learning problem of multi-vector retrieval can be formulated as optimizing tokenlevel alignment. Specifically, we use a sparse alignment matrix to aggregate token-level similarities, where each element indicates the alignment of a pair of tokens. From this point of view, we are able to formulate different retrieval models in a unified manner (Figure 1) and discern the drawbacks of existing models.

---

Based on our formulation, we propose Aligner, a novel multi-vector retrieval model that consists of pairwise alignment and unary salience. Pairwise alignments form the basis of ALIGNER, where pairs of query and document tokens are sparsely aligned based on their contextual representations. It is discovered that changing the sparsity of alignment can significantly impact the performance on retrieval tasks. For instance, factoid questions often favor a small number of alignments since they often focus on a small part of a document. However, other queries for different tasks (e.g., argument retrieval and fact checking) require a larger number of alignments for a broader understanding of a document. Our findings also support the claim of Dai et al. (2022b) that retrieval tasks with different intents should be modeled differently.

---

ALIGNER also learns unary saliences, which decides whether each token ever needs to be aligned with any other token for retrieval. This corresponds to masking an entire row or column of the alignment matrix, rather than individual token alignments. To sparsify entire rows or columns, we introduce an algorithm that produces sparse token salience and is end-to-end differentiable based on a novel formulation of entropy-regularized linear programming. Sparsified unary saliences allow us to prune a large number of document and query token representations, making multi-vector retrieval a more efficient and affordable solution.

---

We evaluate ALIGneR on the BEIR benchmark (Thakur et al. 2021), which covers a diverse set of retrieval tasks in multiple domains ${ }^{1}$ In a zero-shot setting, we show that simply scaling our model achieves the state-of-the-art performance, outperforming prior neural retrievers without contrastive pre-training, model-based hard negative mining, or distillation. By adapting the pairwise alignments with a few examples from the target task - similar to the setup of Dai et al. (2022b) - ALIGNER can be further improved by up to 15.7 points nDCG@ 10 on argument retrieval tasks. Meanwhile, pruning with our unary saliences can reduce $50 \%$ of query tokens for better run-time efficiency and $80 \%$ of document tokens for better storage footprint, with less than 1 point decrease of nDCG@ 10 . The pairwise alignments and unary saliences are also highly interpretable so that they often serve as concise rationales for retrieval.

## 2 Multi-Vector Retrieval as Sparse Alignment

Given a query $Q$ and a collection of $N$ documents $\mathscr{C}=\left\{D^{(1)}, \ldots, D^{(N)}\right\}$, a key problem in retrieval is how to represent these textual inputs in order to facilitate efficient search. To this end, one approach is lexical retrieval using sparse bag-of-words representation of the text; the other approach is dense retrieval, which this work focuses on. Dense retrieval models learn a parameterized function that encodes the query and documents into query representation $\boldsymbol{q}$ and document representations $\left\{\boldsymbol{d}^{(1)}, \ldots, \boldsymbol{d}^{(N)}\right\}$ respectively. Typically, each representation is a single $d$-dimensional vector. For retrieval, the similarity function is often defined as $\operatorname{sim}\left(Q, D^{(i)}\right)=\boldsymbol{q}^{\top} \boldsymbol{d}^{(i)}$, and documents having high similarity scores to the query are retrieved.

### 2.1 Multi-Vector Retrieval

Instead of representing each query and document as a single fixed-length vector, multi-vector retrieval represents them with multiple token vectors, mainly to improve the limited capacity of fixedlength representations. Specifically, a query $Q=\left\{q_{1}, \ldots, q_{n}\right\}$ and a document $D=\left\{d_{1}, \ldots, d_{m}\right\}$ are encoded into a set of vectors $\left\{\boldsymbol{q}_{1}, \ldots, \boldsymbol{q}_{n}\right\}$ and $\left\{\boldsymbol{d}_{1}, \ldots, \boldsymbol{d}_{m}\right\}$. The similarity function between a query and a document is re-defined for multi-vector retrieval. For instance, ColBERT (Khattab \& Zaharia 2020) designs the similarity function as follows:

$$
\operatorname{sim}(Q, D)=\sum_{i=1}^{n} \max _{j=1 \ldots m} \boldsymbol{q}_{i}^{\top} \boldsymbol{d}_{j}
$$

For retrieval, instead of indexing $N$ document vectors, multi-vector retrieval pre-computes $N \times \bar{m}$ document token vectors where $\bar{m}$ is the average length of documents. Then, it retrieves $K$ document token vectors for each query token vector with Maximum Inner-Product Search (MIPS), resulting in $n \times K$ candidate document tokens. The retrieved tokens are used to trace back the original documents (Lee et al. 2021a), often followed by a final refinement stage that scores the similarity $\operatorname{sim}(Q, D)$ with all token representations of each document and the query (Khattab \& Zaharia, 2020). We adopt the same practice of ColBERT in our experiments.

### 2.2 Sparse Alignment Formulation

A key design question for retrieval models is defining the similarity function in a manner that balances model expressiveness and inference cost. To facilitate our discussion, we formalize the similarities used in previous methods into a class of sparse alignment functions. The formulation also leads to a principled extension over existing work, which we will describe in $\S 3$ 

---

We begin by defining a similarity matrix $\boldsymbol{S} \in \mathbb{R}^{n \times m}$ computed from all pairs of query and document tokens, where $\boldsymbol{S}_{i, j}=\boldsymbol{q}_{i}^{\top} \boldsymbol{d}_{j}$. Then, we use an alignment matrix $\boldsymbol{A} \in[0,1]^{n \times m}$ to compute the similarity between $Q$ and $D$ as follows:

$$
\begin{equation*}
\operatorname{sim}(Q, D)=\frac{1}{Z} \sum_{i=1}^{n} \sum_{j=1}^{m} \boldsymbol{S}_{i, j} \boldsymbol{A}_{i, j} \tag{1}
\end{equation*}
$$

where $Z$ is a normalization term defined as $Z=\sum_{i, j} \boldsymbol{A}_{i, j}$. The alignment matrix $\boldsymbol{A}$ can be directly derived from $S$ or computed as a function of $Q$ and $D$.

---

On the top of our formulation, the alignment matrix $\boldsymbol{A}$ is constrained to be sparsely activated: $\|\boldsymbol{A}\|_{0} \leq \sigma$ where $\|\cdot\|_{0}$ is the number of non-zero elements in a matrix. Sparse activation assumes that only a few query-document token matches are critical for retrieval, inspired by traditional retrieval methods. Indeed, most existing dense retrieval models already enforce the sparse alignment with their own heuristics. Figure 1 illustrates how different models can be described under our formulation:

- Dense passage retriever (DPR; Karpukhin et al. 2020) uses a single [CLS] vector to represent each query and document. This is equivalent to setting $A_{1,1}=1$ and 0 otherwise, resulting in $\|\boldsymbol{A}\|_{0}=1$.
- ME-BERT (Luan et al., 2021) uses the first $k$ document token vectors for multi-vector representations of documents but a single vector for query. The similarity function is $\max _{j=1 \ldots k} \boldsymbol{q}_{1}^{\top} \boldsymbol{d}_{j}$, which is equivalent to setting $A_{1, j}=1$ when $\boldsymbol{S}_{1, j}$ is the maximum within $\boldsymbol{S}_{1,1}$ to $\boldsymbol{S}_{1, k}$, and 0 otherwise. The alignment sparsity is $\|\boldsymbol{A}\|_{0}=1$.
- CoIBERT uses the sum-of-max similarity function $\sum_{i=1}^{n} \max _{j=1 \ldots m} \boldsymbol{S}_{i, j}$ that is equivalent to setting an alignment matrix to select the maximum element from each row of $\boldsymbol{S}$, i.e., $\boldsymbol{A}_{i, j}=1$ when $\boldsymbol{S}_{i, j}$ is the maximum within $\boldsymbol{S}_{i,:}\|\boldsymbol{A}\|_{0}=n$ in this case.
- COIL (Gao et al., 2021), similar to ColBERT, also selects the maximum element from each row of $\boldsymbol{S}$, but requires a lexical exact match for a selected pair, i.e., $\boldsymbol{A}_{i, j}=1$ when $\boldsymbol{S}_{i, j}$ is the maximum within $\left\{\boldsymbol{S}_{i, j^{\prime}} \mid q_{i}=d_{j^{\prime}}\right\} .\|\boldsymbol{A}\|_{0} \leq n$ in this case.

---

The choice of similarity and sparsity can have a large impact on model capacity and efficiency. For instance, ColBERT is more expressive and robust than DPR (Thakur et al., 2021), but its retrieval and storage costs are much higher. Our work seeks to further advance expressiveness while retaining a strong efficiency. We describe our method in the next section.

## 3 Aligner

In this section, we present Aligner built upon the sparse alignment formulation. Aligner factorizes the alignment matrix into pairwise alignment and unary salience:

$$
\begin{equation*}
\boldsymbol{A}=\tilde{\boldsymbol{A}} \odot\left(\boldsymbol{u}^{q} \otimes \boldsymbol{u}^{d}\right) \tag{2}
\end{equation*}
$$

where $\odot$ is the Hadamard product and $\otimes$ is the outer product of two vectors. Pairwise alignment $\tilde{A} \in \mathbb{R}^{n \times m}$ determines which pairs of query and document tokens should be aligned, with the sparsity constraints tailored for downstream tasks ( $\S 3.1$ ). Unary salience $\boldsymbol{u}^{q} \in \mathbb{R}^{n}$ and $\boldsymbol{u}^{d} \in \mathbb{R}^{m}$ are sparse token weights deciding whether a token ever needs to be aligned ( $\S 3.2$ ).

---

The factorization is introduced based on two critical hypotheses. First, the optimal sparsity of alignment can be task-dependent. Instead of imposing top-1 constraint as in ColBERT, activating more than one alignments for a query token can enhance retrieval performance for certain tasks. In our analyses for instance, we observe factoid questions that only concern a specific part of a document require a small number of alignments, while some other queries (such as fact checking) require more alignments for a broader understanding of the document. We explore different search spaces of the pairwise alignment matrix $\tilde{\boldsymbol{A}}$ in order to achieve better retrieval performance for each downstream task. Second, alignment is only needed for very few tokens. For example, we analyzed 2000 most retrieved documents in our preliminary study, and found only $12.8 \%$ document tokens are retrieved by at least one query ${ }^{2}$ Intuitively, tokens that are uninformative do not need to be aligned and stored, corresponding to sparse activation over an entire row or column of $\boldsymbol{A}$. Aligner directly learns the row and column sparsity as unary salience, and utilizes them to enhance retrieval efficiency.

### 3.1 Adapting Pairwise Alignment

Queries and documents can have varied distributions. For example, a query can be a single entity, a natural question, or a few sentences, and a document can range from a short paragraph to a long article. The search intent also changes from task to task (Dai et al., 2022b). These changes can lead to different optimal alignment strategies. We explore the following sparse alignment variants that go beyond the top-1 strategy commonly adopted in existing work:

- Top-k. Each query token is aligned with $k$ document tokens with highest similarity scores. Precisely, $\tilde{\boldsymbol{A}}_{i, j}=1$ when the $j$-th token is within top- $k$ of the row $\boldsymbol{S}_{i}$. When $k=1$, it is equivalent to ColBERT.
- Top- $p$. This strategy is similar to top- $k$, but instead of aligning each query token with exactly $k$ tokens, it makes the number of alignments proportional to the document length, i.e., each query token aligns with $\max (\lfloor p \cdot m\rfloor, 1)$ tokens where $m$ is the document length and $p \in[0,1]$ is the alignment ratio.

---

Despite their simplicity, these variants can indeed enhance retrieval accuracy significantly on tasks such as argument retrieval. More importantly, while it is possible to train separate models for different alignment variants, we are interested in fast test-time adaptation using a single shared model as many important retrieval tasks lack sufficient training data (Thakur et al., 2021). Specifically, we first train Aligner using a fixed alignment strategy such as top-1 in a source domain, and adapt the alignment strategy to each target task without changing the model parameters ${ }^{3}$ We use the following few-shot alignment adaptation method. Given a corpus $\left\{D^{(1)}, \ldots, D^{(N)}\right\}$, and a few relevance-annotated query-document pairs from the target task $\left\{\left(Q^{1}, D_{+}^{1}\right), \ldots\left(Q^{K}, D_{+}^{K}\right)\right\}$, we first retrieve candidate documents with the learned token representations, and decide the pairwise alignment strategy based on the ranking performance on the annotated data. This adaptation can be performed efficiently because the alignment only concerns the computation of similarity score (Eq. 1p) in the refinement stage. In practice, for some tasks, we are able to find a well-suited alignment strategy and improve the retrieval performance with as few as 8 annotated examples.

### 3.2 Learning Unary Salience

ALIGnER predicts token saliences from their token representations. For brevity, we only present the formulation for document salience, and query salience is defined similarly. Specifically, the salience of the $i$-th document token $u_{i}^{d}$ is defined as:

$$
\begin{equation*}
u_{i}^{d}=\lambda_{i}^{d} \cdot f\left(\boldsymbol{W}^{d} \boldsymbol{d}_{i}+b^{d}\right) \tag{3}
\end{equation*}
$$

where $\boldsymbol{W}^{d}$ and $b^{d}$ are learnable parameters. $f$ is a non-linear activation function and we use ReLU such that salience is always non-negative. $\lambda^{d}=\left\{\lambda_{i}^{d}\right\}$ are gating variables to control the overall sparsity of $\boldsymbol{u}^{d}$, which we will elaborate next.

---

For the document salience to be meaningful, we enforce salience sparsity as an inductive bias. ALIGnER jointly optimizes sparse salience with other parts of the model. Since tokens with zero salience do not contribute to computing similarity, our model will be encouraged to identify more important tokens in order to retain good retrieval performance. Note that during training we do not have any explicit annotation on which tokens are important. Instead, $\boldsymbol{u}^{d}$ (and similarly $\boldsymbol{u}^{q}$ ) are directly optimized to minimize the training loss, under the sparsity constraint that $\left\|\boldsymbol{\lambda}^{d}\right\|_{0}=\left\lceil\alpha^{d} \cdot m\right\rceil$, where $\alpha^{d}$ is a constant sparsity ratio and $m$ is the document length.

---

Of course, a key question is how we can optimize the unary salience component given the controlled sparsity. We leverage a novel technique called entropy-regularized linear programming to enable end-to-end optimization. Specifically, let $k=\left\lceil\alpha^{d} \cdot m\right\rceil$ denotes the desired sparsity, $s_{i}=f\left(\boldsymbol{W}^{d} \boldsymbol{d}_{i}+\right.$ $b^{d}$ ) denotes the token score before the sparse gate $\lambda_{i}^{d}$ is applied, and $s, \boldsymbol{\lambda}^{d} \in \mathbb{R}^{m}$ be the vectors $\left\{s_{i}\right\}$ and $\left\{\lambda_{i}^{d}\right\}$ respectively. $\lambda^{d}$ is computed by solving the following optimization problem:

$$
\begin{equation*}
\max _{\boldsymbol{\lambda}} s^{\top} \boldsymbol{\lambda}+\varepsilon H(\boldsymbol{\lambda}) \quad \text { s.t. } \quad \mathbf{1}^{\top} \boldsymbol{\lambda}=k, \quad \lambda_{i} \in[0,1], \forall i=1, \ldots, m . \tag{4}
\end{equation*}
$$

where $H(\cdot)$ is the elementwise entropy function ${ }^{4}$ and $\varepsilon>0$ is a small constant. The optimization can be seen as a relaxed top- $k$ operation. Without the entropy term $\varepsilon H(\cdot)$, it becomes an instance of linear programming where the solution $\boldsymbol{\lambda}^{d}$ is a binary mask indicating the top- $k$ values of $s$, i.e., $\lambda_{i}^{d}=1$ if and only if $s_{i}$ is one of top- $k$ values in $s$. This top- $k$ optimization is smoothed by adding the small entropy term $\varepsilon H(\cdot)$ and by relaxing $\lambda_{i}$ from exact binary to $[0,1]$. Given small $\varepsilon$, this still produce a sparse solution $\boldsymbol{\lambda}^{d}$ and can be solved using simple vector operations. Specifically, let $a \in \mathbb{R}$ and $b_{i} \in \mathbb{R}$ for $i=1, \cdots, m$ be auxiliary variables that are initialized to zero. We iteratively update these variables using the following equations:

$$
\begin{equation*}
a^{\prime}=\varepsilon \ln (k)-\varepsilon \ln \left\{\sum_{i} \exp \left(\frac{s_{i}+b_{i}}{\varepsilon}\right)\right\}, \quad b_{i}^{\prime}=\min \left(-s_{i}-a^{\prime}, 0\right) \tag{5}
\end{equation*}
$$

In practice, it is sufficient to run only a few iterations and the final solution is given by $\lambda_{i}=$ $\exp \left(\frac{s_{i}+b_{i}+a}{\varepsilon}\right)$. These vector operations are differentiable so $\boldsymbol{\lambda}$ can be end-to-end trained with other parts of our model. The full derivation of this iterative algorithm is given in Appendix A. 1

---

Pruning Multi-vector Retrieval With the learned unary salience, we can naturally prune tokens for multi-vector retrieval. Pruning document tokens reduces the number of vectors in search index, and pruning query tokens reduces the number of searches. In our experiments, we control them using two pruning ratios $\beta^{q}$ and $\beta^{d}$ respectively. For each document, we obtain the token salience using Eq. (3) and only store the top $\beta^{d}$ percent of tokens in the index. Similarly we select the top $\beta^{q}$ percent query tokens to perform max inner-product search. Note that we vary these two ratios to control retrieval efficiency, and these ratios can be smaller than the sparsity ratio $\alpha^{q}$ and $\alpha^{d}$ which we use as constraints at training time. In the refinement stage, we still use the full model with all token vectors for scoring.

## 4 EXPERIMENTS

### 4.1 EXPERIMENTAL SETUP

ALIGnER uses shared transformer encoder initialized from T5 version 1.1 (Raffel et al., 2020). We project token embeddings to 128 dimension and apply L2 normalization. Following GTR (Ni et al. 2021), we finetune ALIGNER on MS MARCO with hard negatives released by RocketQA Qu et al. 2021). The models are trained with a batch size of 256 for 25 k steps, using query sequence length of 64 and document sequence length of 256 . We train ALIGNER with top-1 pairwise alignment ${ }^{5}$

---

For retrieval, we pre-compute the token encodings of all the documents in the corpus, and use ScaNN (Guo et al. 2020) to index and perform max inner-product search (MIPS). We retrieve 4,000 nearest neighbors for each query token ${ }^{6}$ and return the top- 1,000 after the refinement stage. We evaluate ALIGNER on the BEIR benchmark (Thakur et al., 2021) and compare with state-of-the-art retrieval models shown in Table [1] Note that AliGneR does not rely on contrastive model pretraining (Izacard et al., 2022, Ni et al., 2021), model-based hard negative mining (Santhanam et al., 2021), or distillation (Santhanam et al. 2021). We intentionally decide this simple recipe and focus on studying the impact of pairwise alignment and unary salience.

---

For few-shot alignment adaptation of AlIGNER ( $\S 3.1$ ), we split the test data into multiple folds such that each fold contains 8 examples. Then we find the best alignment strategy that maximizes nDCG@ 10 on each fold with $k \in\{1,2,4,6,8\}$ for top- $k$ and $p \in\{0.5 \%, 1 \%, 1.5 \%, 2 \%\}$ for top- $p$. Based on the best alignment strategy from each fold, we measure the retrieval performance on the remaining test examples with the best strategy. We report the average ( $\pm$ std.) of these test scores where the number of test scores equals the number of folds. The average of few-shot adaptation indicates the expected performance of using few examples to choose the best alignment strategy.

### 4.2 Retrieval Accuracy

Table 2 shows the document retrieval performance of ALIGNER on both MS MARCO and the BEIR benchmark. For this experiment, we do not prune any query or document tokens with unary saliences, but show their effects in $\S 4.3$ instead. ALIGNER xxl outperforms all baselines on MSMARCO, showing how multi-vector retrieval models can benefit from large pretrained language models. ALIGNER ${ }_{\mathrm{xx1}}$ also outperforms GTR $_{\mathrm{xxl}}$ on 9 out of 13 BEIR datasets and advances the retriever-only state-of-the-art (ColBERT ${ }_{\mathrm{v} 2}$ ) by 1.2 points nDCG@10 on average. Figure 3 shows that our multi-vector retriever model scales better than single-vector dual encoder GTR.

---

**Alignment Adaptation**. In the rightmost column of Table 2, we show the effect of adapting pairwise alignment with ALIGNER on the BEIR benchmark. With only 8 examples for finding the proper alignment sparsity, its expected performance reaches $52.6 \mathrm{nDCG} @ 10$ on average. Alignmentadapted ALIGNER also benefits from scaling up, and consistently outperforms its non-adapted counterparts, as shown in Figure 3 The gains are further explained in Table 3, where we show individual task's performance under various alignment strategies. Although ALIGNER is trained with top-1 alignment, top-1 is not always the best strategy at inference time. Specifically, for ArguAna, we observe 16 points improvement by adjusting the number of alignments proportional to the document length with $p=1.5 \%$. Other tasks such as Touché-2020 also prefer other alignment strategies, which shows that different tasks might require different sparsity. In general, keeping the sparsity low enough is preferable and supports our hypothesis that pairwise alignments should be sparse.

---

We further check whether this observation holds when ALIGNER is trained with other pairwise alignment strategies. Figure 4 shows ALIGneR variants trained on four alternative strategies. We evaluate their performance with training-time alignment strategy (default) and the optimal alignment strategy selected per dataset (oracle). While these models perform differently with their default alignments, they perform similarly after oracle alignment adaptation.

---

Figure 5 shows the effectiveness of few-shot alignment adaptation - dynamically selecting taskspecific alignment strategy based on a few examples. When the default alignment (top-k=1) is not optimal, we can identify a good alignment strategy using only 8 examples, which significantly improves model performance on argument retrieval tasks. Using 16 examples further improves the average score and reduces the variance. However, when the default alignment is already optimal (top- $k=1$ is optimal for QA tasks), few-shot alignment adaptation hurts performance due to the variance of our few-shot method. Nevertheless, AligneR outperforms Promptagator (Dai et al., 2022b), another few-shot retrieval baseline, in 6 out of 11 datasets.

### 4.3 RETRIEVAL EfFICIENCY

The next experiment shows how AlIGNER's unary salience impacts retrieval efficiency. We train ALIGNER $_{\text {base }}$ with salience sparsity ratios $\alpha^{q}=50 \%$ and $\alpha^{d}=40 \%$ based on empirical performance. The gating variables are optimized with $\varepsilon=0.002$. At retrieval time, we prune query and document tokens with ratios $\beta^{q}$ and $\beta^{d}$ ( $\S 3.2$ ).

---

Figure 6 shows the AlIGNER performance on MS MARCO with various pruning ratios. When pruned at the same ratio as training ( $\beta^{q}=50 \%$ and $\beta_{d}=40 \%$ ), the model is close to a full ALIGNER model without pruning (MRR@1038.1 vs. 38.8), but greatly saves the computation cost. We can further prune tokens by adjusting $\beta^{d}$ and $\beta_{q}$. The model achieves 37.3 MRR @ 10 with is $\beta^{d}=10 \%$, i.e. it remains accurate with only $10 \%$ of the original index size. Decreasing the query pruning ratio $\beta^{q}$ to $30 \%$ does not sacrifice performance too much, although deceasing $\beta^{q}$ to $10 \%$ leads to worse performance. Figure 6 also compares ALIGNER's entropy-regularized linear program (Eq. 4 ) with alternative methods. With just a ReLU gate and no sparsity constraints ('ReLU' in Figure 6), the model performance retains good when $\beta^{d}=40 \%$, but drops significantly for smaller $\beta^{d}$. Removing the entropy regularization in Eq. 4 leads to simply selecting the hard top- $k$ tokens with the highest predicted salience ('Hard' in Figure 6, The hard top- $k$ solution has worse performance for all $\beta^{d}$.

---

ALIGNER's salience estimation also generalizes to other retrieval datasets. As shown in Figure 7 . pruning with $\beta_{d}=10 \%$ with $\beta^{q}=50 \%$ causes minimal performance decrease for a majority of BEIR datasets. We even observe performance increase for Touché-2020, as the model can only retrieve salient tokens after pruning. Besides, we show that alignment adaptation can be combined with pruning, resulting in an effective yet efficient retrieval model.

### 4.4 Interpretability

Table 4 shows examples of the pairwise alignment and unary salience learned by ALIGNER. The model aligns query tokens to contexually similar tokens, but not necessarily identical tokens. The salience features are also highlighted in Table 4 . Important noun phrases and verbs are usually assigned higher salience, which is consistent with human intuition. We show more examples of alignments for different tasks in the Appendix A.3 In general, we observe question answering tasks usually require fewer alignments for each query token, while other tasks that require a broad understanding of the document favor larger number of alignments.

## 5 Related Work

Recent research on information retrieval often improves the retrieval accuracy with contrastive pretraining (Ni et al., 2021; Izacard et al., 2022; Oguz et al., 2022), model-based hard negative mining (Xiong et al.||2020;; Lu et al.,|2021;|Qu et al.||2021) and knowledge distillation (Santhanam et al., 2021;|Zhang et al., 2022 ; Reddi et al., 2021). Retrieval efficiency is improved via quantization (Santhanam et al., 2021) or lower-dimensional vectors (Hofstätter et al., 2022). These improvements are orthogonal to this work.

---

Term importance and salience have a long history in information retrieval: from term frequency $(t f)$ and inverse document frequency (idf), to recent BERT-based importance measures such as DeepCT (Dai \& Callan, 2020), SPARTA (Zhao et al., 2021) and Splade (Formal et al., 2021b a). These works mostly focus on sparse lexical retrieval and learn term weights for sparse bag-of-words representations. Term importance in multi-vector dense retrieval is less explored. Our work is probably most related to a recent work from Hofstätter et al. (2022), which prunes ColBERT by predicting salience scores from a word's embedding with a ReLU gate and L1-norm regularization.

---

Recently, Promptagator (Dai et al. 2022b) points out the importance of using a few annotated examples to adapt to a new retrieval task. Promptagator achieves few-shot task adaptation via query generation (Ma et al., 2021; Lee et al., 2021b; Dai et al., 2022a) using large language models (Sanh et al., 2022, Brown et al.| 2020; Wei et al., 2022), which has high inference cost. Aligner is more versatile and can be fast adapted to a new task via few-shot alignment adaptation.

## 6 Conclusion

In this paper, we introduce ALIGNER, a novel sparse alignment method for multi-vector document retrieval. We first formulate different retrieval models with token-level sparse alignments and propose Aligner to tackle the limitations of existing models. Specifically, Aligner uses pairwise alignments and unary saliences that allow us to adapt to different tasks and prune unimportant tokens, respectively. As a result, we achieve strong performance on both zero-shot and few-shot document retrieval tasks while drastically improving the run-time and storage complexity of multi-vector retrieval. With its interpretable alignments and better performance with large language models, we envision that our multi-vector retrieval model can serve as a strong standalone retriever in the future.

## A Appendix

## A. 1 Derivation of the Iterative Updates

We present the derivation of Eq 5 for solving optimization problem (4) in Section 3.2 The maximization problem (4) can be written as an equivalent minimization problem:

$$
\begin{align*}
& \max _{\boldsymbol{\lambda}} s^{\top} \boldsymbol{\lambda}+\varepsilon H(\boldsymbol{\lambda}) \\
& \Longleftrightarrow \quad \min _{\boldsymbol{\lambda}}-s^{\top} \boldsymbol{\lambda}-\varepsilon H(\boldsymbol{\lambda}) \\
& \Longleftrightarrow \quad \min _{\boldsymbol{\lambda}}-s^{\top} \boldsymbol{\lambda}-\varepsilon H(\boldsymbol{\lambda})-\varepsilon \mathbf{1}^{\top} \boldsymbol{\lambda}  \tag{6}\\
& \text { s.t. } \mathbf{1}^{\top} \boldsymbol{\lambda}=k, \quad \lambda_{i} \in[0,1], \quad i=1, \ldots, m .
\end{align*}
$$

Note the term $\varepsilon 1^{\top} \boldsymbol{\lambda}$ will be a constant $\varepsilon \times k$, but we include it in the minimization object to make our derivation simpler later.

---

Now, let $a \in \mathbb{R}$ and $\boldsymbol{b} \in \mathbb{R}^{m}$ be the Lagrangian variables corresponding to the linear constraints $\mathbf{1}^{\top} \boldsymbol{\lambda}=k$ and $\lambda_{i} \leq 1 \forall i \square^{7}$ The minimization problem is equivalent to its Lagrangian expression:

$$
\begin{equation*}
\min _{\lambda \in \mathbb{R}^{m}} \max _{a \in \mathbb{R}, \boldsymbol{b} \leq \mathbf{0}}-\boldsymbol{s}^{\top} \boldsymbol{\lambda}-\boldsymbol{\varepsilon} H(\boldsymbol{\lambda})-\boldsymbol{\varepsilon} \mathbf{1}^{\top} \boldsymbol{\lambda}+a\left(k-\mathbf{1}^{\top} \boldsymbol{\lambda}\right)+\boldsymbol{b}^{\top}(\mathbf{1}-\boldsymbol{\lambda}) \tag{7}
\end{equation*}
$$

The objective function (6) is strongly convex and the solution space of $\boldsymbol{\lambda}$ is a convex set. As a result, strong duality holds and we can instead solve the dual problem that exchanges the min and max operators in (7)

$$
\begin{equation*}
\max _{a \in \mathbb{R}, \boldsymbol{b} \leq \mathbf{0}} \min _{\lambda \in \mathbb{R}^{m}}-\boldsymbol{s}^{\top} \boldsymbol{\lambda}-\boldsymbol{\varepsilon} H(\boldsymbol{\lambda})-\boldsymbol{\varepsilon} \mathbf{1}^{\top} \boldsymbol{\lambda}+a\left(k-\mathbf{1}^{\top} \boldsymbol{\lambda}\right)+\boldsymbol{b}^{\top}(\mathbf{1}-\boldsymbol{\lambda}) \tag{8}
\end{equation*}
$$

---

The optimal solution $(a, \boldsymbol{b}, \boldsymbol{\lambda})$ must have the Karush-Kuhn-Tucker (KKT) conditions hold (Kuhn \& Tucker, 2014), namely
$$
\begin{gathered}
\frac{\partial\left(-s^{\top} \boldsymbol{\lambda}-\varepsilon H(\boldsymbol{\lambda})+\varepsilon \mathbf{1}^{\top} \boldsymbol{\lambda}+a\left(k-\mathbf{1}^{\top} \boldsymbol{\lambda}\right)+\boldsymbol{b}^{\top}(\mathbf{1}-\boldsymbol{\lambda})\right)}{\partial \boldsymbol{\lambda}}=0 \\
\Longleftrightarrow \quad \boldsymbol{\lambda}=\exp \left(\frac{\boldsymbol{s}+a+\boldsymbol{b}}{\varepsilon}\right) \quad \Longleftrightarrow \quad \lambda_{i}=\exp \left(\frac{s_{i}+a+\boldsymbol{b}_{i}}{\varepsilon}\right) \forall i=1, \ldots, m
\end{gathered}
$$

Substituting $\boldsymbol{\lambda}$ using the above equation in (8), the dual problem now has a simple form:

$$
\max _{a \in \mathbb{R}, \boldsymbol{b} \leq \mathbf{0}} k \cdot a+\mathbf{1}^{\top} b-\mathbf{1}^{\top} \exp \left(\frac{s+a+\boldsymbol{b}}{\varepsilon}\right)
$$

---

We can solve this problem using coordinate descent (Wright, 2015) by successively maximizing the function with either $a$ or $\boldsymbol{b}$ fixed. This leads to the iterative updates (Eq 5 ) described in Section 3.2
$$
\begin{aligned}
& a^{\prime}=\varepsilon \ln (k)-\varepsilon \ln \left\{\sum_{i} \exp \left(\frac{s_{i}+b_{i}}{\varepsilon}\right)\right\} \\
& b_{i}^{\prime}=\min \left(-s_{i}-a^{\prime}, 0\right)
\end{aligned}
$$

**Discussion.** In short, we solve the dual problem of optimization (4) by performing coordinate decent of the dual variables $a$ and $\boldsymbol{b}$. That is, we find the optimal $a$ that maximizes the dual objective given a fixed $\boldsymbol{b}$, and vice versa.

---

This iterative algorithm is also closely related to the Sinkhorn algorithm of Optimal Transport (OT). In fact, Sinkhorn algorithm solves the entropy-regularized version of Optimal Transport (Cuturi, 2013). However, our work concerns an different optimization instance. While OT solves a transportation problem where the solution space is defined with the marginal constraints over the rows and columns of a transportation matrix, our optimization problem is constrained with a total budget ( $\sum_{i} \lambda_{i}=k$ ) and upper bounds ( $\lambda_{i} \leq 1 \forall i$ ). This leads to different iterative updates.

## A. 2 Differentiable Alignment with Sparsity Constraints

Besides the Top- $k$ and Top- $p$ alignments in $\S 3.1$, we also explore a differentiable pairwise alignment with sparsity contraints (DA). Both Top- $k$ adn Top- $p$ are doing hard selection of alignments, i.e., $\tilde{\boldsymbol{A}}_{i, j}$ is either 1 or 0 . We relax it by introducing soft sparsity constraints. Similar to our formulation for unary salience ( $\S 3.2$, we determine the alignment $\tilde{A}$ by the following optimization problem:

$$
\begin{align*}
& \max _{\boldsymbol{A}}\langle\boldsymbol{S}, \boldsymbol{A}\rangle+\boldsymbol{\varepsilon H}(\boldsymbol{A}) \\
& \text { s.t. }  \tag{9}\\
& \quad \sum_{j} \boldsymbol{A}_{i, j}=k, i=1, \ldots, n \\
& \quad \boldsymbol{A}_{i, j} \in[0,1], \quad i=1, \ldots, n, j=1, \ldots, m
\end{align*}
$$

where $H(\cdot)$ is the elementwise entropy function and $\varepsilon>0$ is a small constant. We constrain the sum of each row of $\tilde{\boldsymbol{A}}$ to equal $k$. When $\varepsilon=0$, the solution of Eq. 9 is the same as Top- $k$. When $\varepsilon>0$, the entropy term makes the optimization problem strongly concave, which can be solved by the same algorithm in Appendix A.1. The solution is differentiable, thus can be trained end-to-end in our model.
