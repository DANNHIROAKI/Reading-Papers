# Autoformer: 基于自相关机制的分解Transformer用于长时序预测

#### 摘要

延长预测时间是实际应用中的关键需求，例如极端天气预警和长期能源消耗规划。本文研究了时间序列的长期预测问题。之前的基于Transformer的模型采用了各种自注意力机制来发现长程依赖关系。然而，长期未来的复杂时间模式使得模型难以找到可靠的依赖关系。此外，为了长序列的效率，Transformer不得不采用稀疏的点式自注意力版本，导致了信息利用的瓶颈。在超越Transformer的基础上，我们设计了Autoformer，这是一种具有自相关机制的新型分解架构。我们打破了序列分解的预处理惯例，并将其革新为深度模型的基本内部模块。这一设计使Autoformer具备了处理复杂时间序列的渐进分解能力。进一步地，受随机过程理论的启发，我们基于序列周期性设计了自相关机制，该机制在子序列级别进行依赖关系发现和表示聚合。自相关机制在效率和准确性上均优于自注意力机制。在长期预测任务中，Autoformer在六个基准测试上实现了最先进的准确性，相对提升了38%，涵盖了能源、交通、经济、天气和疾病等五个实际应用。代码可在以下仓库获取：https://github.com/thuml/Autoformer。

## 1 引言

时间序列预测已广泛应用于能源消耗、交通和经济规划、天气和疾病传播预测等领域。在这些实际应用中，一个迫切的需求是将预测时间延伸到更远的未来，这对于长期规划和预警具有重要意义。因此，本文研究了时间序列的长期预测问题，其特点是预测的时间序列长度较大。近年来，深度预测模型（如[48,23,26,34,29,35,25,41]）取得了显著进展，尤其是基于Transformer的模型。得益于自注意力机制，Transformer在建模序列数据的长程依赖关系方面具有显著优势，这使得更强大的大型模型成为可能[8, 13]。

---

然而，在长期预测任务中，预测任务极具挑战性。首先，直接从长期时间序列中发现时间依赖关系是不可靠的，因为这些依赖关系可能被复杂的时间模式所掩盖。其次，由于序列长度的二次复杂度，标准的Transformer及其自注意力机制在长期预测中的计算成本过高。之前的基于Transformer的预测模型（如[48, 23, 26]）主要致力于将自注意力改进为稀疏版本。虽然性能显著提升，但这些模型仍然采用点式表示聚合。因此，在效率提升的过程中，它们会因稀疏的点式连接而牺牲信息利用，从而导致长时序预测的瓶颈。

---

为了应对复杂的时间模式，我们尝试采用分解的思想，这是时间序列分析中的标准方法[1, 33]。它可以用于处理复杂的时间序列并提取更具可预测性的成分。然而，在预测场景下，它只能作为过去序列的预处理，因为未来是未知的[20]。这种常见的用法限制了分解的能力，并忽略了分解成分之间潜在的未来交互。因此，我们尝试超越分解的预处理用法，提出一种通用架构，使深度预测模型具备渐进分解的内在能力。此外，分解可以解开复杂的时间模式，并突出时间序列的内在特性[20]。受益于此，我们尝试利用序列的周期性来革新自注意力中的点式连接。我们观察到，在周期的相同相位位置上的子序列通常表现出相似的时间过程。因此，我们尝试基于序列周期性推导出的过程相似性构建序列级别的连接。

---

基于上述动机，我们提出了一种全新的Autoformer，用于替代Transformer进行长时间序列预测。Autoformer仍然遵循残差和编码器-解码器结构，但将Transformer革新为一种分解预测架构。通过将我们提出的分解块嵌入为内部操作符，Autoformer可以逐步从预测的隐藏变量中分离出长期趋势信息。这一设计使得我们的模型能够在预测过程中交替进行分解和精炼中间结果。受随机过程理论[9, 30]的启发，Autoformer引入了自相关机制来替代自注意力，该机制基于序列周期性发现子序列的相似性，并从底层周期中聚合相似的子序列。这种序列级别的机制实现了长度为$L$的序列的$\mathcal{O}(L \log L)$复杂度，并通过将点式表示聚合扩展到子序列级别，打破了信息利用的瓶颈。Autoformer在六个基准测试上实现了最先进的准确性。我们的贡献总结如下：

- 为了应对长期未来的复杂时间模式，我们提出了Autoformer作为分解架构，并设计了内部分解块，使深度预测模型具备渐进分解的内在能力。
- 我们提出了一种自相关机制，在序列级别进行依赖关系发现和信息聚合。我们的机制超越了之前的自注意力家族，同时提升了计算效率和信息利用。
- Autoformer在长期预测设置下，在六个基准测试上实现了38%的相对提升，涵盖了五个实际应用领域：能源、交通、经济、天气和疾病。

## 2 相关工作

### 2.1 时间序列预测模型

由于时间序列预测的重要性，各种模型已经得到了充分发展。许多时间序列预测方法始于经典工具[38, 10]。ARIMA [7, 6]通过差分将非平稳过程转化为平稳过程来解决预测问题。滤波方法也被引入用于序列预测[24, 12]。此外，递归神经网络（RNNs）模型被用于建模时间序列的时间依赖性[42, 32, 47, 28]。DeepAR [34]结合了自回归方法和RNNs来建模未来序列的概率分布。LSTNet [25]引入了卷积神经网络（CNNs）与递归跳跃连接，以捕捉短期和长期的时间模式。基于注意力的RNNs [46, 36, 37]引入了时间注意力来探索长程依赖关系以进行预测。此外，许多基于时间卷积网络（TCN）的工作[40, 5, 4, 35]尝试用因果卷积建模时间因果关系。这些深度预测模型主要通过递归连接、时间注意力或因果卷积来关注时间关系建模。

---

最近，基于自注意力机制的Transformer [41, 45]在序列数据中展示了强大的能力，例如自然语言处理[13, 8]、音频处理[19]甚至计算机视觉[16, 27]。然而，将自注意力应用于长期时间序列预测在计算上是不可行的，因为序列长度$L$在内存和时间上具有二次复杂度。LogTrans [26]将局部卷积引入Transformer，并提出了LogSparse注意力，按照指数增长的间隔选择时间步，将复杂度降低到$\mathcal{O}\left(L(\log L)^{2}\right)$。Reformer [23]提出了局部敏感哈希（LSH）注意力，并将复杂度降低到$\mathcal{O}(L \log L)$。Informer [48]通过基于KL散度的ProbSparse注意力扩展了Transformer，并同样实现了$\mathcal{O}(L \log L)$复杂度。需要注意的是，这些方法基于原始的Transformer，并尝试将自注意力机制改进为稀疏版本，但仍然遵循点式依赖和聚合。本文提出的自相关机制基于时间序列的固有周期性，可以提供序列级别的连接。

### 2.2 时间序列分解

作为时间序列分析中的标准方法，时间序列分解[1, 33]将时间序列解构为多个成分，每个成分代表更可预测的潜在模式类别之一。它主要用于探索历史随时间的变化。对于预测任务，分解通常用作历史序列的预处理，然后预测未来序列[20, 2]，例如Prophet [39]的趋势-季节性分解、N-BEATS [29]的基扩展以及DeepGLO [35]的矩阵分解。然而，这种预处理受限于历史序列的简单分解效果，并忽略了长期未来中序列潜在模式之间的层次交互。本文从新的渐进维度采用分解思想。我们的Autoformer将分解作为深度模型的内部模块，可以在整个预测过程中逐步分解隐藏序列，包括过去序列和预测的中间结果。

## 3 Autoformer

时间序列预测问题是在给定过去长度为$I$的序列的情况下，预测未来最可能的长为$O$的序列，表示为输入$I$预测$O$。长期预测设置是预测长期未来，即较大的$O$。如前所述，我们强调了长期序列预测的困难：处理复杂的时间模式以及打破计算效率和信息利用的瓶颈。为了解决这两个挑战，我们将分解作为深度预测模型的内置模块引入，并提出Autoformer作为一种分解架构。此外，我们设计了自相关机制，以发现基于周期的依赖关系并从底层周期中聚合相似的子序列。

### 3.1 分解架构

我们将Transformer [41]革新为深度分解架构（图1），包括内部分解块、自相关机制以及相应的编码器和解码器。

---

**序列分解块** 

为了在长期预测背景下学习复杂的时间模式，我们采用了分解的思想[1, 33]，它可以将序列分离为趋势-周期部分和季节性部分。这两部分分别反映了序列的长期进展和季节性。然而，直接分解未来序列是不可行的，因为未来是未知的。为了解决这一困境，我们提出了一个序列分解块作为Autoformer的内部操作（图1），它可以逐步从预测的中间隐藏变量中提取长期平稳趋势。具体来说，我们采用移动平均来平滑周期性波动并突出长期趋势。对于长度为$L$的输入序列$\mathcal{X} \in \mathbb{R}^{L \times d}$，过程如下：
$$
\begin{align*}
& \mathcal{X}_{\mathrm{t}}=\operatorname{AvgPool}(\operatorname{Padding}(\mathcal{X}))  \tag{1}\\
& \mathcal{X}_{\mathrm{s}}=\mathcal{X}-\mathcal{X}_{\mathrm{t}}
\end{align*}
$$

其中$\mathcal{X}_{\mathrm{s}}, \mathcal{X}_{\mathrm{t}} \in \mathbb{R}^{L \times d}$分别表示季节性和提取的趋势-周期部分。我们采用$\operatorname{Avg} \operatorname{Pool}(\cdot)$进行移动平均，并通过填充操作保持序列长度不变。我们用$\mathcal{X}_{\mathrm{s}}, \mathcal{X}_{\mathrm{t}}=\operatorname{SeriesDecomp}(\mathcal{X})$来总结上述方程，这是模型的一个内部模块。

---

**模型输入** 

编码器部分的输入是过去的$I$个时间步$\mathcal{X}_{\text {en }} \in \mathbb{R}^{I \times d}$。作为一种分解架构（图11），Autoformer解码器的输入包含季节性部分$\mathcal{X}_{\text {des }} \in \mathbb{R}^{\left(\frac{I}{2}+O\right) \times d}$和趋势-周期部分$\mathcal{X}_{\text {det }} \in \mathbb{R}^{\left(\frac{I}{2}+O\right) \times d}$，这两部分都需要进行精炼。每个初始化由两部分组成：从编码器输入$\mathcal{X}_{\text {en }}$的后半部分分解出的长度为$\frac{I}{2}$的组件，以提供最新信息；以及长度为$O$的占位符，用标量填充。其公式化表示如下：
$$
\begin{align*}
\begin{aligned}
\mathcal{X}_{\text {ens }}, \mathcal{X}_{\text {ent }} & =\operatorname{SeriesDecomp}\left(\mathcal{X}_{\text {en } \frac{I}{2}: I}\right) \\
\mathcal{X}_{\text {des }} & =\operatorname{Concat}\left(\mathcal{X}_{\text {ens }}, \mathcal{X}_0\right) \\
\mathcal{X}_{\text {det }} & =\operatorname{Concat}\left(\mathcal{X}_{\text {ent }}, \mathcal{X}_{\text {Mean }}\right)
\end{aligned}
\end{align*}
$$

其中$\mathcal{X}_{\text {ens }}, \mathcal{X}_{\text {ent }} \in \mathbb{R}^{\frac{I}{2} \times d}$分别表示$\mathcal{X}_{\text {en }}$的季节性和趋势-周期部分，$\mathcal{X}_{0}, \mathcal{X}_{\text {Mean }} \in \mathbb{R}^{O \times d}$分别表示用零填充的占位符和$\mathcal{X}_{\text {en }}$的均值。

---

**编码器**  

如图1所示，编码器专注于季节性部分的建模。编码器的输出包含过去的季节性信息，并将作为交叉信息用于帮助解码器优化预测结果。假设我们有$N$个编码器层。第$l$个编码器层的整体方程总结为$\mathcal{X}_{\text {en }}^{l}=\operatorname{Encoder}\left(\mathcal{X}_{\text {en }}^{l-1}\right)$。具体细节如下：
$$
\begin{align*}
& \mathcal{S}_{\mathrm{en}}^{l, 1},_{-}=\operatorname{SeriesDecomp}\left(\text { Auto-Correlation }\left(\mathcal{X}_{\mathrm{en}}^{l-1}\right)+\mathcal{X}_{\mathrm{en}}^{l-1}\right) \\
& \mathcal{S}_{\mathrm{en}}^{l, 2},_{-}=\operatorname{Series} \text { Decomp }\left(\operatorname{FeedForward}\left(\mathcal{S}_{\mathrm{en}}^{l, 1}\right)+\mathcal{S}_{\mathrm{en}}^{l, 1}\right), \tag{3}
\end{align*}
$$

其中“\_”表示被消除的趋势部分。$\mathcal{X}_{\text {en }}^{l}=\mathcal{S}_{\text {en }}^{l, 2}, l \in\{1, \cdots, N\}$表示第$l$个编码器层的输出，$\mathcal{X}_{\text {en }}^{0}$是嵌入的$\mathcal{X}_{\text {en }}$。$\mathcal{S}_{\text {en }}^{l, i}, i \in\{1,2\}$分别表示第$l$层中第$i$个序列分解块后的季节性成分。我们将在下一节中详细描述Auto-Correlation $(\cdot)$，它可以无缝替代自注意力机制。

- **图1**：Autoformer架构。编码器通过序列分解块（蓝色块）消除长期趋势-周期性部分，并专注于季节性模式的建模。解码器逐步从隐藏变量中提取趋势部分。编码器输出的过去季节性信息通过编码器-解码器Auto-Correlation（解码器中心的绿色块）被利用。

---

**解码器**  

解码器包含两部分：用于趋势-周期性成分的累积结构和用于季节性成分的堆叠Auto-Correlation机制（图1）。每个解码器层包含内部Auto-Correlation和编码器-解码器Auto-Correlation，分别用于优化预测和利用过去的季节性信息。需要注意的是，模型在解码过程中从中间隐藏变量中提取潜在趋势，使Autoformer能够逐步优化趋势预测，并消除干扰信息，以便在Auto-Correlation中发现基于周期的依赖关系。假设有$M$个解码器层。在编码器的潜在变量$\mathcal{X}_{\mathrm{en}}^{N}$的基础上，第$l$个解码器层的方程总结为$\mathcal{X}_{\mathrm{de}}^{l}=$ $\operatorname{Decoder}\left(\mathcal{X}_{\mathrm{de}}^{l-1}, \mathcal{X}_{\text {en }}^{N}\right)$。解码器可以形式化如下：
$$
\begin{align*}
\mathcal{S}_{\mathrm{de}}^{l, 1}, \mathcal{T}_{\mathrm{de}}^{l, 1} & =\operatorname{SeriesDecomp}\left(\text { Auto-Correlation }\left(\mathcal{X}_{\mathrm{de}}^{l-1}\right)+\mathcal{X}_{\mathrm{de}}^{l-1}\right) \\
\mathcal{S}_{\mathrm{de}}^{l, 2}, \mathcal{T}_{\mathrm{de}}^{l, 2} & =\operatorname{SeriesDecomp}\left(\text { Auto-Correlation }\left(\mathcal{S}_{\mathrm{de}}^{l, 1}, \mathcal{X}_{\mathrm{en}}^{N}\right)+\mathcal{S}_{\mathrm{de}}^{l, 1}\right)  \tag{4}\\
\mathcal{S}_{\mathrm{de}}^{l, 3}, \mathcal{T}_{\mathrm{de}}^{l, 3} & =\operatorname{SeriesDecomp}\left(\text { FeedForward }\left(\mathcal{S}_{\mathrm{de}}^{l, 2}\right)+\mathcal{S}_{\mathrm{de}}^{l, 2}\right) \\
\mathcal{T}_{\mathrm{de}}^{l} & =\mathcal{T}_{\mathrm{de}}^{l-1}+\mathcal{W}_{l, 1} * \mathcal{T}_{\mathrm{de}}^{l, 1}+\mathcal{W}_{l, 2} * \mathcal{T}_{\mathrm{de}}^{l, 2}+\mathcal{W}_{l, 3} * \mathcal{T}_{\mathrm{de}}^{l, 3},
\end{align*}
$$

其中$\mathcal{X}_{\mathrm{de}}^{l}=\mathcal{S}_{\mathrm{de}}^{l, 3}, l \in\{1, \cdots, M\}$表示第$l$个解码器层的输出。$\mathcal{X}_{\mathrm{de}}^{0}$是从$\mathcal{X}_{\text {des }}$嵌入的，用于深度变换，而$\mathcal{T}_{\text {de }}^{0}=\mathcal{X}_{\text {det }}$用于累积。$\mathcal{S}_{\text {de }}^{l, i}, \mathcal{T}_{\text {de }}^{l, i}, i \in\{1,2,3\}$分别表示第$l$层中第$i$个序列分解块后的季节性成分和趋势-周期性成分。$\mathcal{W}_{l, i}, i \in\{1,2,3\}$表示第$i$个提取趋势$\mathcal{T}_{\text {de }}^{l, i}$的投影器。最终预测是这两个优化分解成分的总和，即$\mathcal{W}_{\mathcal{S}} * \mathcal{X}_{\mathrm{de}}^{M}+\mathcal{T}_{\mathrm{de}}^{M}$，其中$\mathcal{W}_{\mathcal{S}}$用于将深度变换后的季节性成分$\mathcal{X}_{\mathrm{de}}^{M}$投影到目标维度。

---

**图2**：Auto-Correlation（左）和时间延迟聚合（右）。我们利用快速傅里叶变换计算自相关$\mathcal{R}(\tau)$，它反映了时间延迟的相似性。然后，基于选择的延迟$\tau$，将相似的子过程滚动到相同的索引，并通过$\mathcal{R}(\tau)$进行聚合。

### 3.2 Auto-Correlation Mechanism

如图2所示，我们提出了具有序列连接的自相关机制，以扩展信息利用率。自相关机制通过计算序列的自相关性来发现基于周期的依赖关系，并通过时间延迟聚合来聚合相似的子序列。

---

**基于周期的依赖关系** 

观察到周期中相同相位位置自然提供了相似的子过程。受随机过程理论[9, 30]的启发，对于一个实离散时间过程$\left\{\mathcal{X}_{t}\right\}$，我们可以通过以下公式获得自相关$\mathcal{R}_{\mathcal{X} \mathcal{X}}(\tau)$：
$$
\begin{equation*}
\mathcal{R}_{\mathcal{X X}}(\tau)=\lim _{L \rightarrow \infty} \frac{1}{L} \sum_{t=1}^{L} \mathcal{X}_{t} \mathcal{X}_{t-\tau} \tag{5}
\end{equation*}
$$

$\mathcal{R}_{\mathcal{X} \mathcal{X}}(\tau)$反映了$\left\{\mathcal{X}_{t}\right\}$与其$\tau$滞后序列$\left\{\mathcal{X}_{t-\tau}\right\}$之间的时间延迟相似性。如图2所示，我们使用自相关$\mathcal{R}(\tau)$作为估计周期长度$\tau$的未归一化置信度。然后，我们选择最可能的$k$个周期长度$\tau_{1}, \cdots, \tau_{k}$。基于周期的依赖关系由上述估计周期推导而来，并可以通过相应的自相关进行加权。

---

**时间延迟聚合** 

基于周期的依赖关系连接了估计周期之间的子序列。因此，我们提出了时间延迟聚合块（图2），它可以根据选择的时间延迟$\tau_{1}, \cdots, \tau_{k}$滚动序列。该操作可以将位于估计周期相同相位位置的相似子序列对齐，这与自注意力家族中的逐点点积聚合不同。最后，我们通过softmax归一化的置信度聚合子序列。

对于单头情况和时间序列$\mathcal{X}$，长度为$L$，经过投影器后，我们得到查询$\mathcal{Q}$、键$\mathcal{K}$和值$\mathcal{V}$。因此，它可以无缝替代自注意力机制。自相关机制如下：
$$
\begin{align*}
\tau_{1}, \cdots, \tau_{k} & =\underset{\tau \in\{1, \cdots, L\}}{\arg \operatorname{Topk}}\left(\mathcal{R}_{\mathcal{Q}, \mathcal{K}}(\tau)\right) \\
\widehat{\mathcal{R}}_{\mathcal{Q}, \mathcal{K}}\left(\tau_{1}\right), \cdots, \widehat{\mathcal{R}}_{\mathcal{Q}, \mathcal{K}}\left(\tau_{k}\right) & =\operatorname{SoftMax}\left(\mathcal{R}_{\mathcal{Q}, \mathcal{K}}\left(\tau_{1}\right), \cdots, \mathcal{R}_{\mathcal{Q}, \mathcal{K}}\left(\tau_{k}\right)\right)  \tag{6}\\
\text { Auto-Correlation }(\mathcal{Q}, \mathcal{K}, \mathcal{V}) & =\sum_{i=1}^{k} \operatorname{Roll}\left(\mathcal{V}, \tau_{i}\right) \widehat{\mathcal{R}}_{\mathcal{Q}, \mathcal{K}}\left(\tau_{i}\right)
\end{align*}
$$

---

其中$\arg \operatorname{Topk}(\cdot)$是获取Topk自相关的参数，令$k=\lfloor c \times \log L\rfloor$，$c$是一个超参数。$\mathcal{R}_{\mathcal{Q}, \mathcal{K}}$是序列$\mathcal{Q}$和$\mathcal{K}$之间的自相关。$\operatorname{Roll}(\mathcal{X}, \tau)$表示对$\mathcal{X}$进行时间延迟$\tau$的操作，在此操作中，超出第一个位置的元素将在最后一个位置重新引入。对于编码器-解码器自相关（图1），$\mathcal{K}, \mathcal{V}$来自编码器$\mathcal{X}_{\text {en }}^{N}$，并将调整为长度$O$，$\mathcal{Q}$来自解码器的前一个块。对于Autoformer中使用的多头版本，隐藏变量具有$d_{\text {model }}$个通道，$h$个头，第$i$个头的查询、键和值为$\mathcal{Q}_{i}, \mathcal{K}_{i}, \mathcal{V}_{i} \in \mathbb{R}^{L \times \frac{d_{\text {model }}}{h}}, i \in\{1, \cdots, h\}$。过程如下：
$$
\begin{align*}
\operatorname{MultiHead}(\mathcal{Q}, \mathcal{K}, \mathcal{V}) & =\mathcal{W}_{\text {output }} * \operatorname{Concat}\left(\operatorname{head}_{1}, \cdots, \operatorname{head}_{h}\right) \\
\text { where head } & =\text { Auto-Correlation }\left(\mathcal{Q}_{i}, \mathcal{K}_{i}, \mathcal{V}_{i}\right) \tag{7}
\end{align*}
$$

- \- **图3**：自相关与自注意力家族的对比。完全注意力[41]（a）在所有时间点之间采用完全连接。稀疏注意力[23, 48]（b）根据提出的相似性度量选择点。LogSparse注意力[26]（c）按照指数增长的间隔选择点。自相关（d）专注于潜在周期中子序列之间的连接。

---

高效计算 对于基于周期的依赖关系，这些依赖关系指向底层周期相同相位位置的子过程，并且本质上是稀疏的。在这里，我们选择最可能的延迟，以避免选择相反的相位。由于我们聚合了长度为 $L$ 的 $\mathcal{O}(\log L)$ 个序列，因此方程 6 和 7 的复杂度为 $\mathcal{O}(L \log L)$。对于自相关计算（方程 5），给定时间序列 $\left\{\mathcal{X}_{t}\right\}$，$\mathcal{R}_{\mathcal{X} \mathcal{X}}(\tau)$ 可以通过基于 Wiener-Khinchin 定理的快速傅里叶变换（FFT）计算得出 [43]：
$$
\begin{align*}
& \mathcal{S}_{\mathcal{X X}}(f)=\mathcal{F}\left(\mathcal{X}_{t}\right) \mathcal{F}^{*}\left(\mathcal{X}_{t}\right)=\int_{-\infty}^{\infty} \mathcal{X}_{t} e^{-i 2 \pi t f} \mathrm{~d} t \overline{\int_{-\infty}^{\infty} \mathcal{X}_{t} e^{-i 2 \pi t f} \mathrm{~d} t}  \tag{8}\\
& \mathcal{R}_{\mathcal{X X}}(\tau)=\mathcal{F}^{-1}\left(\mathcal{S}_{\mathcal{X X}}(f)\right)=\int_{-\infty}^{\infty} \mathcal{S}_{\mathcal{X} \mathcal{X}}(f) e^{i 2 \pi f \tau} \mathrm{~d} f
\end{align*}
$$
其中 $\tau \in\{1, \cdots, L\}$，$\mathcal{F}$ 表示 FFT，$\mathcal{F}^{-1}$ 是其逆变换。$*$ 表示共轭操作，$\mathcal{S}_{\mathcal{X X}}(f)$ 是频域中的表示。需要注意的是，所有滞后 $\{1, \cdots, L\}$ 的序列自相关可以通过 FFT 一次性计算得出。因此，自相关计算的复杂度为 $\mathcal{O}(L \log L)$。

---

自相关 vs. 自注意力家族 与点对点的自注意力家族不同，自相关呈现的是序列级别的连接（图 3）。具体来说，对于时间依赖关系，我们基于周期性找到子序列之间的依赖关系。相比之下，自注意力家族只计算散点之间的关系。尽管一些自注意力机制 [26, 48] 考虑了局部信息，但它们仅利用这些信息来帮助发现点对点的依赖关系。在信息聚合方面，我们采用时间延迟块来聚合来自底层周期的相似子序列。相比之下，自注意力通过点积来聚合选择的点。得益于固有的稀疏性和子序列级别的表示聚合，自相关可以同时提升计算效率和信息利用率。

## 4 实验

我们在六个真实世界的基准数据集上广泛评估了所提出的 Autoformer 模型，涵盖了五个主流的时间序列预测应用领域：能源、交通、经济、气象和疾病。

**数据集** 以下是六个实验数据集的描述：  

（1）**ETT** [48] 数据集包含从电力变压器收集的数据，包括负载和油温，这些数据在 2016 年 7 月至 2018 年 7 月期间每 15 分钟记录一次。  

（2）**Electricity** 数据集包含 2012 年至 2014 年 321 名客户的每小时用电量。  

（3）**Exchange** [25] 记录了 1990 年至 2016 年八个不同国家的每日汇率。  

（4）**Traffic** 数据集是来自加州交通部的每小时数据集合，描述了旧金山湾区高速公路上不同传感器测量的道路占用率。  

（5）**Weather** 数据集记录了 2020 年全年每 10 分钟的气象数据，包含 21 个气象指标，如气温、湿度等。  

（6）**ILI** 数据集包括美国疾病控制与预防中心在 2002 年至 2021 年期间每周记录的流感样疾病（ILI）患者数据，描述了 ILI 患者占就诊患者总数的比例。  

我们遵循标准协议，按时间顺序将所有数据集划分为训练集、验证集和测试集，ETT 数据集的比例为 6:2:2，其他数据集的比例为 7:1:2。

---

**实现细节** 我们的方法使用 L2 损失进行训练，采用 ADAM [22] 优化器，初始学习率为 $10^{-4}$。批量大小设置为 32。训练过程在 10 个 epoch 内提前停止。所有实验重复三次，使用 PyTorch [31] 实现，并在单个 NVIDIA TITAN RTX 24GB GPU 上进行。Auto-Correlation 的超参数 $c$ 在 1 到 3 之间，以权衡性能和效率。标准偏差和敏感性分析见附录 E 和 B。Autoformer 包含 2 个编码器层和 1 个解码器层。

---

**基线方法** 我们纳入了 10 种基线方法。对于多变量设置，我们选择了三种最新的基于 Transformer 的模型：Informer [48]、Reformer [23]、LogTrans [26]，两种基于 RNN 的模型：LSTNet [25]、LSTM [17]，以及基于 CNN 的 TCN [4] 作为基线。对于单变量设置，我们纳入了更具竞争力的基线方法：N-BEATS [29]、DeepAR [34]、Prophet [39] 和 ARIMA [1]。

## 4.1 主要结果

为了比较不同未来时间跨度下的性能，我们固定输入长度，并在广泛的预测长度（$96, 192, 336, 720$）下评估模型。这一设置精确符合长期预测的定义。以下是多变量和单变量设置下的结果。

---

**多变量结果** 在多变量设置中，Autoformer 在所有基准数据集和所有预测长度设置下均取得了最先进的性能（表 10）。特别是在输入 96-预测 336 的设置下，与之前的最先进结果相比，Autoformer 在 ETT 数据集上实现了 $\mathbf{74\%}$ 的 MSE 降低（从 1.334 降至 0.339），在 Electricity 数据集上实现了 $\mathbf{18\%}$ 的降低（从 0.280 降至 0.231），在 Exchange 数据集上实现了 $\mathbf{61\%}$ 的降低（从 1.357 降至 0.509），在 Traffic 数据集上实现了 $\mathbf{15\%}$ 的降低（从 0.733 降至 0.622），在 Weather 数据集上实现了 $\mathbf{21\%}$ 的降低（从 0.455 降至 0.359）。对于 ILI 数据集的输入 36-预测 60 设置，Autoformer 实现了 $\mathbf{43\%}$ 的 MSE 降低（从 4.882 降至 2.770）。总体而言，Autoformer 在上述设置中平均实现了 $\mathbf{38\%}$ 的 MSE 降低。值得注意的是，即使在无明显周期性的 Exchange 数据集上，Autoformer 仍然提供了显著的改进。详细展示见附录 E。此外，我们还可以发现，随着预测长度 $O$ 的增加，Autoformer 的性能变化相当平稳。这表明 Autoformer 在长期预测中保持了更好的鲁棒性，这对于实际应用（如天气预警和长期能源消耗规划）具有重要意义。

---

**单变量结果** 我们在表 2 中列出了两个典型数据集的单变量结果。在与广泛基线方法的比较中，我们的 Autoformer 在长期预测任务中仍然取得了最先进的性能。特别是在输入 96-预测 336 的设置下，我们的模型在具有明显周期性的 ETT 数据集上实现了 $\mathbf{14\%}$ 的 MSE 降低（从 0.180 降至 0.145）。对于无明显周期性的 Exchange 数据集，Autoformer 以 $\mathbf{17\%}$ 的 MSE 降低（从 0.611 降至 0.508）超越了其他基线方法，并展示了更强的长期预测能力。此外，我们发现 ARIMA [1] 在 Exchange 数据集的输入 96-预测 96 设置下表现最佳，但在长期预测设置中表现不佳。ARIMA 的这种表现可能得益于其对非平稳经济数据的固有能力，但受到现实世界时间序列复杂时间模式的限制。

## 4.2 消融实验

##### 分解架构  
通过我们提出的渐进式分解架构，其他模型可以获得一致的性能提升，尤其是在预测长度 $O$ 增加时（表3）。这验证了我们的方法可以推广到其他模型，并释放其他依赖学习机制的潜力，缓解复杂模式带来的干扰。此外，我们的架构优于预处理方法，尽管后者使用了更大的模型和更多的参数。特别是，预先分解甚至可能带来负面影响，因为它忽略了长期未来中组件之间的相互作用，例如Transformer [41] 的预测-720和Informer [48] 的预测-336。

- **表3**：多变量ETT中分解的消融实验，使用MSE指标。我们的方法将渐进式架构应用于其他模型。Sep使用两个模型分别预测预先分解的季节性和趋势-周期性成分。Promotion是与原始模型相比的MSE减少量。

---

##### 自相关 vs. 自注意力家族  
如表4所示，我们提出的自相关方法在各种输入-$I$-预测-$O$ 设置下均取得了最佳性能，这验证了序列级连接相较于点级自注意力的有效性（图3）。此外，从表4的最后一列可以看出，自相关方法在内存效率上也表现出色，可以用于长序列预测，例如输入-336-预测-1440。

- **表4**：自相关与自注意力在多变量ETT中的对比。我们将Autoformer中的自相关替换为不同的自注意力方法。“-”表示内存不足。

---

## 4.3 模型分析

##### 时间序列分解  
如图4所示，在没有我们的序列分解模块的情况下，预测模型无法捕捉到季节部分的上升趋势和峰值。通过添加序列分解模块，Autoformer能够逐步聚合和精炼序列中的趋势-周期性部分。这种设计也有助于学习季节部分，特别是峰值和谷值。这验证了我们提出的渐进式分解架构的必要性。

- **图4**：最后一层解码器中学到的季节部分 $\mathcal{X}_{\text {de }}^{M}$ 和趋势-周期性部分 $\mathcal{T}_{\text {de }}^{M}$ 的可视化。我们逐步从解码器中添加分解模块，从左到右。此案例来自ETT数据集，输入-96-预测-720设置。为了清晰起见，我们在原始数据上额外添加了线性增长。

---

##### 依赖关系学习  
图5(a)中标记的时间延迟大小表示最可能的周期。我们学习到的周期性可以指导模型通过 $\operatorname{Roll}\left(\mathcal{X}, \tau_{i}\right), i \in\{1, \cdots, 6\}$ 聚合来自同一或相邻周期阶段的子序列。对于最后一个时间步（下降阶段），自相关方法充分利用了所有相似的子序列，没有遗漏或错误，而自注意力方法则存在不足。这验证了Autoformer能够更充分、更精确地发现相关信息。

- **图5**：学习到的依赖关系的可视化。为了清晰起见，我们选择了自相关方法的前6个时间延迟大小 $\tau_{1}, \cdots, \tau_{6}$，并在原始序列中标记它们（红线）。对于自注意力方法，我们标记了与最后一个时间步（红星）相似的前6个点（橙色点）。

---

##### 复杂季节性建模  
如图6所示，Autoformer从深层表示中学习到的滞后可以指示原始序列的真实季节性。例如，在每日记录的Exchange数据集中，学习到的滞后显示了每月、每季度和每年的周期（图6(b)）。在每小时记录的Traffic数据集中（图6(c)），学习到的滞后显示了24小时和168小时的间隔，这与现实场景中的每日和每周周期相匹配。这些结果表明，Autoformer能够从深层表示中捕捉到现实世界序列的复杂季节性，并进一步提供人类可解释的预测。

- **图6**：学习到的滞后的统计。对于测试集中的每个时间序列，我们统计了解码器在输入-96-预测-336任务中学习到的前10个滞后。图(a)-(d)是密度直方图。

---

##### 效率分析  
我们比较了基于自相关和基于自注意力的模型在训练阶段的运行内存和时间（图7）。提出的Autoformer在内存和时间上均表现出 $\mathcal{O}(L \log L)$ 的复杂度，并在长序列效率上表现更优。

- **图7**：效率分析。对于内存，我们将Autoformer中的自相关替换为自注意力家族，并记录输入为96时的内存。对于运行时间，我们运行自相关或自注意力 $10^{3}$ 次，得到每步的执行时间。输出长度呈指数增长。

## 5 结论

本文研究了时间序列的长期预测问题，这是现实应用中的迫切需求。然而，复杂的时间模式阻碍了模型学习可靠的依赖关系。我们提出了Autoformer作为一种分解架构，通过将序列分解模块嵌入为内部操作符，能够从中间预测中逐步聚合长期趋势部分。此外，我们设计了一种高效的自相关机制，在序列级别进行依赖关系发现和信息聚合，这与之前的自注意力家族形成鲜明对比。Autoformer自然实现了$\mathcal{O}(L \log L)$的复杂度，并在广泛的现实数据集中取得了持续的最先进性能。

## A ETT数据集上的完整基准测试

如表5所示，我们在四个ETT数据集[48]上建立了基准测试，其中包括每小时记录的ETTh1和ETTh2，以及每15分钟记录的ETTm1和ETTm2。

Autoformer在各种预测时间范围内显著超越了现有技术水平。对于输入-96-预测-336的长期设置，Autoformer在ETTh1上超越了之前的最佳结果$\mathbf{55\%}(1.128 \rightarrow 0.505)$，在ETTh2上超越了$\mathbf{80\%}(2.544 \rightarrow 0.471)$。对于输入-96-预测-288的长期设置，Autoformer在ETTm1上实现了$\mathbf{40\%}(1.056 \rightarrow 0.634)$的MSE减少，在ETTm2上实现了$\mathbf{66\%}(0.969 \rightarrow 0.342)$的MSE减少。这些结果显示，Autoformer在之前的最新技术基础上平均减少了$\mathbf{60\%}$的MSE。

表5：在四个ETT数据集上的多变量结果，预测长度为$\{24,48,168,288,336,672,720\}$。我们将Autoformer的输入长度固定为96。正文中的实验是在ETTm2数据集上进行的。

## B 超参数敏感性

如表6所示，我们可以验证模型在超参数$c$（正文中的公式6）方面的鲁棒性。为了在性能和效率之间取得平衡，我们将$c$设置为1到3的范围内。还观察到，具有明显周期性的数据集倾向于具有较大的因子$c$，例如ETT和Traffic数据集。对于没有明显周期性的ILI数据集，较大的因子可能会带来噪声。

表6：Autoformer在不同超参数$c$选择下的性能，自相关机制中。我们采用输入-36-预测-48的预测设置用于ILI数据集，输入-96-预测-336的预测设置用于其他数据集。

## C 模型输入选择

### C.1 输入长度选择  
由于预测时间范围通常根据应用需求固定，因此在现实应用中需要调整输入长度。我们的研究表明，输入长度与模型性能之间的关系因数据集而异，因此需要根据数据特征选择模型输入。例如，对于具有明显周期性的ETT数据集，长度为96的输入足以提供足够的信息。但对于没有明显周期性的ILI数据集，模型需要更长的输入来发现更多信息丰富的时间依赖关系。因此，在ILI数据集中，更长的输入会带来更好的性能。

- **表7**：不同输入长度下Autoformer的性能。我们将ILI数据集的预测时间范围固定为48，其他数据集固定为336。ILI数据集的输入长度$I$为$\{24,36,48,60\}$，而ETT和Exchange数据集的输入长度$I$为$\{96,192,336,720\}$。

---

### C.2 过去信息的利用  
对于Autoformer的解码器输入，我们将长度为$\frac{I}{2}$的过去信息附加到占位符中。这种设计旨在为解码器提供最近的过去信息。如表8所示，使用更多过去信息的模型会获得更好的性能，但也会导致更大的内存开销。因此，我们将解码器输入设置为$\frac{I}{2}+O$，以在性能和效率之间取得平衡。

- **表8**：不同解码器输入长度下Autoformer的性能。$O, \frac{I}{2}+O, I+O$分别对应没有过去信息、部分过去信息和全部过去信息的解码器输入。我们在ETT数据集上将预测设置固定为输入-96-预测-336。

---

## D 分解架构的消融实验  
在本节中，我们进一步验证了我们提出的渐进式分解架构的有效性。我们采用更成熟的分解算法作为预处理，用于单独预测设置。如表9所示，尽管后者使用了成熟的分解算法和两倍大的模型，我们提出的渐进式分解架构在长期预测设置中始终优于单独预测方法。

- **表9**：在ETT数据集上输入-96-预测-$O$设置下分解架构的消融实验，其中$O \in\{96,192,336,720\}$。单独预测的骨干网络是标准Transformer [41]。我们采用多种分解算法作为预处理，并使用两个Transformer分别预测季节性和趋势-周期性部分。结果是两部分预测的总和。

## E 主要结果的补充

### E.1 多变量预测展示  
为了评估不同模型的预测效果，我们绘制了ETT数据集测试集中预测结果的最后一个维度进行定性比较（图8、9、10和11）。我们的模型在不同模型中表现最佳。此外，我们观察到Autoformer能够准确预测周期性和长期变化。

- **图8**：ETT数据集在输入-96-预测-96设置下的预测案例。蓝线为真实值，橙线为模型预测。前96个点为输入部分。
- **图9**：ETT数据集在输入-96-预测-192设置下的预测案例。
- **图10**：ETT数据集在输入-96-预测-336设置下的预测案例。
- **图11**：ETT数据集在输入-96-预测-720设置下的预测案例。

---

### E.2 无明显周期性数据上的表现  
Autoformer在六个数据集中表现最佳，即使在无明显周期性的Exchange数据集中也是如此。本节将展示多变量Exchange数据集测试集中的一些案例进行定性评估。我们观察到Exchange数据集中的序列表现出快速波动。由于经济数据的固有特性，序列并未呈现明显的周期性。这种非周期性为预测带来了极大困难。如图12所示，与其他模型相比，Autoformer仍能准确预测长期变化，验证了我们的模型在不同数据特性下的鲁棒性。

- **图12**：Exchange数据集在输入-96-预测-192设置下的预测案例。

---

### E.3 单变量预测展示  
如图13所示，Autoformer提供了最准确的预测。与Informer [48]相比，Autoformer能够精确捕捉未来时间范围的周期。此外，与LogTrans [26]相比，我们的模型在中心区域的预测效果更好。与Reformer [23]相比，我们的预测序列更平滑且更接近真实值。同时，DeepAR [34]的预测波动随着预测长度的增加而减小，并存在过度平滑的问题，而Autoformer则未出现这种情况。

- **图13**：ETT数据集在输入-96-预测-720单变量设置下的预测案例。

---

### E.4 包含标准差的主要结果  
为了获得更稳健的实验结果，我们重复了每个实验三次。由于篇幅限制，正文中的结果未显示标准差。表10展示了标准差。

---

## F COVID-19：案例研究  
我们还将模型应用于COVID-19现实世界数据[15]。该数据集包含从2020年1月22日至2021年5月20日每日记录的各国COVID-19确诊、死亡和康复患者数据。我们选择了欧洲的两个匿名国家进行实验。数据按时间顺序分为训练集、验证集和测试集，比例为7:1:2，并进行了归一化。需要注意的是，由于训练数据有限，该问题具有较大挑战性。

- **表10**：不同预测长度$O$下多变量预测的定量结果及其波动。我们将ILI数据集的输入长度$I$设置为36，其他数据集设置为96。较低的MSE或MAE表示更好的性能。

## F.1 定量结果  
我们仍然遵循长期预测任务，让模型分别预测下一周、半个月和一个月。预测长度分别为输入长度的1倍、2.1倍和4.3倍。如表11所示，在数据有限且输入较短的情况下，Autoformer仍保持了最先进的准确性。

- **表11**：COVID-19数据的定量结果。我们将输入长度$I$设置为7，即一周的数据。预测长度$O$为$\{7,15,30\}$，分别代表一周、半个月和一个月。较低的MSE或MAE表示更好的预测效果。

---

## F.2 案例展示  
如图14所示，与其他模型相比，我们的Autoformer能够准确预测初期的峰值和谷值，并且在长期未来几乎能预测出精确值。对极值和长期趋势的预测对于疫情防控至关重要。

- **图14**：COVID-19数据中第二个国家在输入-7-预测-15设置下的案例展示。

---

## G Autoformer：实现细节

### G.1 模型设计  
我们分别在算法1和算法2中提供了Autoformer和自相关机制的伪代码。同时包含了张量形状和超参数设置。除了上述标准版本外，我们为了效率将自相关机制加速为批归一化风格的块，即加速版本。本文的所有实验结果均来自加速版本。以下是实现细节。

---

##### 加速版本  
需要注意的是，算法2中的gather操作对内存访问不友好。我们借鉴了批归一化[21]的设计来加速自相关机制。我们将整个过程分为训练阶段和推理阶段。由于线性层的特性，深层表示的通道是等价的。因此，我们减少了训练和推理阶段的通道和头维度。特别是在训练阶段，我们对批次内的自相关进行平均以简化学习到的滞后。这种设计加速了自相关机制，并作为归一化操作，为学习到的滞后提供全局判断，因为批次内的序列来自同一时间序列数据集。训练阶段的伪代码见算法3。在推理阶段，我们仍然使用与简化滞后相关的gather操作，这比标准版本更友好于内存访问。推理阶段的伪代码见算法4。

---

##### 复杂度分析  
我们的模型为$\lfloor c \times \log L\rfloor$个延迟长度为$L$的序列提供了序列级聚合。因此，无论是标准版本还是加速版本，复杂度均为$\mathcal{O}(L \log L)$。然而，后者更快，因为它对内存访问更友好。

## G.2 实验细节  
为了在性能和效率上进行公平比较，所有基于Transformer的模型均采用两层编码器和一层解码器，包括Informer [48]、Reformer [23]、LogTrans [26]和标准Transformer [41]。此外，这些模型均采用Informer [48]的嵌入方法和一步生成策略。需要注意的是，我们提出的序列级聚合能够提供足够的序列信息，因此我们没有像其他基线模型那样使用位置嵌入，而是保留了值嵌入和时间戳嵌入。

---

## H 更广泛的影响

##### 现实应用  
我们提出的Autoformer专注于长期时间序列预测问题，这是广泛应用中具有价值和迫切需求的任务。我们的方法在能源、交通、经济、天气和疾病等五个现实应用中均取得了持续的最先进性能。此外，我们还提供了COVID-19数据集的案例研究。因此，从事这些领域工作的人员可能会从我们的研究中受益匪浅。我们相信，更好的时间序列预测可以帮助社会在各个领域做出更好的决策并提前预防风险。

---

##### 学术研究  
在本文中，我们从经典时间序列分析和随机过程理论中汲取了灵感。我们创新性地提出了一种通用的深度分解架构和一种新颖的自相关机制，这是对时间序列预测模型的有益补充。代码已开源，可通过以下仓库获取：https://github.com/thuml/Autoformer。

---

##### 模型鲁棒性  
基于广泛的实验，我们并未发现异常失败案例。即使在无明显周期性的Exchange数据集中，Autoformer也表现出了良好的性能和长期鲁棒性。Autoformer通过内部分解模块逐步获得更纯净的序列成分，并使其更容易发现深藏的周期性。但如果数据是随机的或具有极弱的时间相关性，Autoformer及其他任何模型可能会退化，因为序列的预测性较差[14]。

我们的工作仅关注科学问题，因此不存在潜在的伦理风险。
