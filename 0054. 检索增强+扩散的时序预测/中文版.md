### 基于检索增强的扩散模型用于时间序列预测

#### 摘要

尽管时间序列扩散模型在许多近期研究中受到了广泛关注，但现有模型的性能仍然非常不稳定。限制时间序列扩散模型的因素包括时间序列数据集的不足以及缺乏指导。为了解决这些限制，我们提出了一种基于检索增强的时间序列扩散模型（Retrieval-Augmented Time series Diffusion model, RATD）。RATD的框架由两部分组成：基于嵌入的检索过程和参考引导的扩散模型。在第一部分中，RATD从数据库中检索与历史时间序列最相关的时间序列作为参考。这些参考被用于指导第二部分中的去噪过程。我们的方法允许利用数据库中有意义的样本来辅助采样，从而最大化数据集的利用率。同时，这种参考引导机制也弥补了现有时间序列扩散模型在指导方面的不足。在多个数据集上的实验和可视化结果证明了我们方法的有效性，特别是在复杂的预测任务中。我们的代码可在 https://github.com/stanliu96/RATD 获取。

## 1 引言

时间序列预测在多种应用中起着关键作用，包括天气预报 [15, 11]、金融预测 [7, 5]、地震预测 [19] 和能源规划 [6]。处理时间序列预测任务的一种方法是将其视为条件生成任务 [32, 42]，其中条件生成模型用于学习给定观测到的历史序列 $\boldsymbol{x}^{H}$ 的目标时间序列 $\boldsymbol{x}^{P}$ 的条件分布 $P\left(\boldsymbol{x}^{P} \mid \boldsymbol{x}^{H}\right)$。作为当前最先进的条件生成模型，扩散模型 [12] 已被许多研究用于时间序列预测任务 [28, 36, 30]。

---

尽管现有的时间序列扩散模型在某些时间序列预测任务中表现尚可，但在某些场景下其性能仍然不稳定（如1(c)中的示例所示）。限制时间序列扩散模型性能的因素较为复杂，其中两个因素尤为明显。首先，大多数时间序列缺乏直接的语义或标签对应关系，这通常导致时间序列扩散模型在生成过程中缺乏有意义的指导（例如图像扩散模型中的文本指导或标签指导）。这也限制了时间序列扩散模型的潜力。

---

第二个限制因素源于时间序列数据集的两个不足：规模不足和不平衡。与图像数据集相比，时间序列数据集的规模通常较小。流行的图像数据集（如LAION-400M）包含4亿个样本对，而大多数时间序列数据集通常仅包含数万个数据点。训练一个扩散模型以学习规模不足的数据集的精确分布是具有挑战性的。此外，现实世界中的时间序列数据集表现出显著的不平衡性。例如，在现有的心电图数据集MIMIC-IV中，与诊断的预激综合征（PS）相关的记录仅占总记录的不到$0.025\%$。这种不平衡现象可能导致模型忽略一些极为罕见的复杂样本，导致在训练过程中倾向于生成更常见的预测，从而难以处理复杂的预测任务，如图1所示。

- 图1：(a) 该图展示了CSDI [36]（左）和RATD（右）在预测结果上的差异。由于此类案例在训练集中的比例非常小，CSDI难以做出准确预测，通常预测出更常见的结果。我们的方法通过检索有意义的参考作为指导，能够做出更为准确的预测。(b) 我们的方法框架（底部）与传统时间序列扩散模型框架（顶部）的对比。(c) 我们从电力数据集中随机选择了25个预测任务。与我们的方法相比，CSDI和MG-TSD [9]表现出显著更高的不稳定性。这表明RATD在处理其他两种方法难以应对的复杂任务时表现更优。

---

为了解决这些局限性，我们提出了基于检索增强的时间序列扩散模型（Retrieval-Augmented Time series Diffusion Model, RATD）用于复杂的时间序列预测任务。我们的方法由两部分组成：基于嵌入的检索和参考引导的扩散模型。在获得历史时间序列后，将其输入基于嵌入的检索过程，以检索出k个最近的样本作为参考。这些参考被用作去噪过程中的指导。RATD通过从数据集中找到与历史时间序列最相关的参考，从而为去噪过程提供有意义的指导，旨在最大限度地利用现有的时间序列数据集。RATD专注于最大化利用不足的时间序列数据，并在一定程度上缓解了数据不平衡带来的问题。同时，这种参考引导机制也弥补了现有时间序列扩散模型在指导方面的不足。我们的方法在多个数据集上表现出强大的性能，特别是在更复杂的任务中。

---

总结来说，我们的主要贡献如下：

- 为了处理复杂的时间序列预测，我们首次引入了基于检索增强的时间序列扩散模型（RATD），允许更好地利用数据集，并在去噪过程中提供有意义的指导。
- 设计了额外的参考调制注意力（Reference Modulated Attention, RMA）模块，以在去噪过程中从参考中提供合理的指导。RMA有效地整合信息，而不会引入过多的额外计算成本。
- 我们在五个真实世界的数据集上进行了实验，并使用多种指标对结果进行了全面的展示和分析。实验结果表明，与基线方法相比，我们的方法取得了相当或更好的结果。

## 2 相关工作

### 2.1 用于时间序列预测的扩散模型

最近在利用扩散模型进行时间序列预测方面取得了进展。在TimeGrad [28]中，条件扩散模型首次被用作自回归方法进行预测，去噪过程由隐藏状态引导。CSDI [36]采用非自回归生成策略以实现更快的预测。SSSD [1]用结构化状态空间模型替换了噪声匹配网络进行预测。TimeDiff [30]将未来混合和自回归初始化结合到非自回归框架中进行预测。MG-TSD [9]利用多尺度生成策略依次预测时间序列的主要成分和细节。同时，mr-diff [31]利用扩散模型分别预测时间序列的趋势和季节性成分。这些方法在某些预测任务中显示出良好的结果，但在具有挑战性的预测任务中往往表现不佳。我们提出了一种基于检索增强的框架来解决这一问题。

### 2.2 检索增强生成

检索增强机制是生成模型的经典机制之一。许多研究表明，将显式检索步骤引入神经网络具有显著优势。自然语言处理领域的经典工作利用检索增强机制来提高语言生成的质量 [16, 10, 4]。在图像生成领域，一些检索增强模型专注于利用数据库中的样本来生成更逼真的图像 [2, 44]。类似地，[3]在推理过程中利用训练数据中记忆的相似性信息进行检索以增强结果。MQ-ReTCNN [40]专门设计用于涉及多个实体和变量的复杂时间序列预测任务。ReTime [13]基于序列之间的时间接近性创建关系图，并采用关系检索而非基于内容的检索。尽管上述三种方法成功利用检索机制提升了时间序列预测结果，但我们的方法仍具有显著优势。这一优势源于扩散模型的迭代结构，其中参考可以反复影响生成过程，使得参考在整个条件生成过程中发挥更强的作用。

## 3 预备知识

本节将讨论预测任务以及条件时间序列扩散模型的背景知识。为避免冲突，我们使用符号“$s$”表示时间序列，而“$t$”表示扩散过程中的第$t$步。

---

**生成式时间序列预测**。假设我们有一个观测到的历史时间序列$\boldsymbol{x}^{H}=$ $\left\{s_{1}, s_{2}, \cdots, s_{l} \mid s_{i} \in \mathbb{R}^{d}\right\}$，其中$l$是历史时间长度，$d$是每次观测的特征数量，$s_{i}$是时间步$i$的观测值。$\boldsymbol{x}^{P}$是对应的预测目标$\left\{s_{l+1}, s_{l+2}, \cdots, s_{l+h} \mid s_{l+i} \in \mathbb{R}^{d^{\prime}}\right\}\left(d^{\prime} \leq d\right)$，其中$h$是预测范围。生成式时间序列预测的任务是学习一个密度$p_{\theta}\left(\boldsymbol{x}^{P} \mid \boldsymbol{x}^{H}\right)$，使其尽可能接近$p\left(\boldsymbol{x}^{P} \mid \boldsymbol{x}^{H}\right)$，可以表示为：

$$
\begin{equation*}
\min _{p_{\theta}} D\left(p_{\theta}\left(\boldsymbol{x}^{P} \mid \boldsymbol{x}^{H}\right) \| p\left(\boldsymbol{x}^{P} \mid \boldsymbol{x}^{H}\right)\right), \tag{1}
\end{equation*}
$$

其中$\theta$表示参数，$D$是分布之间的某种适当距离度量。给定观测值$x$，目标时间序列可以通过从$p_{\theta}\left(\boldsymbol{x}^{P} \mid \boldsymbol{x}^{H}\right)$中采样直接获得。因此，我们得到时间序列$\left\{s_{1}, s_{2}, \cdots, s_{n+h}\right\}=\left[\boldsymbol{x}^{H}, \boldsymbol{x}^{P}\right]$。

---

**条件时间序列扩散模型**。在观测到时间序列$\boldsymbol{x}^{H}$的情况下，扩散模型通过逐步向目标时间序列$\boldsymbol{x}_{0}^{P}$（等于前文提到的$\boldsymbol{x}^{P}$）注入噪声来破坏其结构，然后从$\boldsymbol{x}_{T}^{P}$开始学习逆转这一过程以生成样本。为方便表达，在本文中，我们使用$\boldsymbol{x}_{t}$表示扩散过程中的第$t$个时间序列，省略字母“P”。前向过程可以表示为具有马尔可夫结构的高斯过程：

$$
\begin{align*}
q\left(\boldsymbol{x}_{t} \mid \boldsymbol{x}_{t-1}\right) & :=\mathcal{N}\left(\boldsymbol{x}_{t} ; \sqrt{1-\beta_{t}} \boldsymbol{x}_{t-1}, \boldsymbol{x}^{H}, \beta_{t} \boldsymbol{I}\right)  \tag{2}\\
q\left(\boldsymbol{x}_{t} \mid \boldsymbol{x}_{0}\right) & :=\mathcal{N}\left(\boldsymbol{x}_{t} ; \sqrt{\overline{\alpha_{t}}} \boldsymbol{x}_{0}, \boldsymbol{x}^{H},\left(1-\bar{\alpha}_{t}\right) \boldsymbol{I}\right)
\end{align*}
$$

其中$\beta_{1}, \ldots, \beta_{T}$表示固定的方差调度，$\alpha_{t}:=1-\beta_{t}$且$\bar{\alpha}_{t}:=\prod_{s=1}^{t} \alpha_{s}$。这一前向过程逐步向数据注入噪声，直到所有结构丢失，最终近似于$\mathcal{N}(0, \boldsymbol{I})$。反向扩散过程学习一个模型$p_{\theta}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}^{H}\right)$，以近似真实后验：

$$
\begin{equation*}
p_{\theta}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}^{H}\right):=\mathcal{N}\left(\boldsymbol{x}_{t-1} ; \mu_{\theta}\left(\boldsymbol{x}_{t}\right), \Sigma_{\theta}\left(\boldsymbol{x}_{t}\right), \boldsymbol{x}^{H}\right), \tag{3}
\end{equation*}
$$

其中$\mu_{\theta}$和$\Sigma_{\theta}$通常由Transformer计算。Ho等人[12]改进了扩散训练过程，并优化以下目标：

$$
\begin{equation*}
\mathcal{L}\left(\boldsymbol{x}_{0}\right)=\sum_{t=1}^{T} \underset{q\left(\boldsymbol{x}_{t}\left|\boldsymbol{x}_{0}\right| \boldsymbol{x}^{H}\right)}{\mathbb{E}}\left\|\mu_{\theta}\left(\boldsymbol{x}_{t}, t \mid \boldsymbol{x}^{H}\right)-\hat{\mu}\left(\boldsymbol{x}_{t}, \boldsymbol{x}_{0} \mid \boldsymbol{x}^{H}\right)\right\|^{2}, \tag{4}
\end{equation*}
$$

其中$\hat{\mu}\left(\boldsymbol{x}_{t}, \boldsymbol{x}_{0} \mid \boldsymbol{x}^{H}\right)$是后验$q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{0}, \boldsymbol{x}_{t}\right)$的均值，它是一个封闭形式的高斯分布，而$\mu_{\theta}\left(\boldsymbol{x}_{t}, t \mid \boldsymbol{x}^{H}\right)$是由神经网络计算的$p_{\theta}\left(\boldsymbol{x}_{t-1}\left|\boldsymbol{x}_{t}\right| \boldsymbol{x}^{H}\right)$的预测均值。

## 4 方法

我们首先在4.1节中描述所提出方法的整体架构。然后，在4.2节中介绍构建数据集的策略。基于嵌入的检索机制和参考引导的时间序列扩散模型将在4.3节中介绍。

### 4.1 框架概述

图2(a)展示了RATD的整体架构。我们基于DiffWave [17]构建了整个流程，它结合了传统的扩散模型框架和2D Transformer结构。在预测任务中，RATD首先根据输入的历史事件序列从数据库$\mathcal{D}^{R}$中检索运动序列。这些检索到的样本随后被输入到参考调制注意力（Reference-Modulated Attention, RMA）模块中作为参考。在RMA层中，我们将时间步$t$的输入$\left[\boldsymbol{x}^{H}, \boldsymbol{x}^{t}\right]$与辅助信息$\mathcal{I}_{s}$和参考$\boldsymbol{x}^{R}$的特征进行整合。通过这种整合，参考引导了生成过程。我们将在后续小节中详细介绍这些过程。

• 图2：所提出的RATD的概述。历史时间序列$\boldsymbol{x}^{H}$被输入到检索模块中，以获取相应的参考$\boldsymbol{x}^{R}$。之后，$\boldsymbol{x}^{H}$与噪声连接作为模型$\mu_{\theta}$的主要输入。$\boldsymbol{x}^{R}$将被用作去噪过程的指导。

### 4.2 构建时间序列的检索数据库

在检索之前，需要构建一个合适的数据库。我们提出了一种针对具有不同特征的时间序列数据集构建数据库的策略。一些时间序列数据集规模不足，并且难以用单一类别标签进行标注（例如电力时间序列），而另一些数据集包含完整的类别标签，但表现出显著的类别不平衡（例如医疗时间序列）。我们对这两种类型的数据集使用两种不同的数据库定义。对于第一种定义，整个训练集直接定义为数据库$\mathcal{D}^{\mathcal{R}}$：

$$
\begin{equation*}
\mathcal{D}^{\mathcal{R}}:=\left\{\boldsymbol{x}_{i} \mid \forall \boldsymbol{x}_{i} \in \mathcal{D}^{\text {train }}\right\} \tag{5}
\end{equation*}
$$

其中$\boldsymbol{x}_{i}=\left\{s_{i}, \cdots, s_{i+l+h}\right\}$是长度为$l+h$的时间序列，$\mathcal{D}^{\text {train }}$是训练集。在第二种方式中，包含数据集中所有类别样本的子集被定义为数据库$\mathcal{D}^{R^{\prime}}$：

$$
\begin{equation*}
\mathcal{D}^{R^{\prime}}=\left\{\boldsymbol{x}_{i}^{c}, \cdots, \boldsymbol{x}_{q}^{c} \mid \forall c \in \mathcal{C}\right\} \tag{6}
\end{equation*}
$$

其中$x_{i}^{k}$是训练集中第$k$类的第$i$个样本，长度为$l+h$。$\mathcal{C}$是原始数据集的类别集合。为简洁起见，我们将这两种数据库统称为$\mathcal{D}^{R}$。

### 4.3 基于检索增强的时间序列扩散

**基于嵌入的检索机制**。对于时间预测任务，理想的参考$\left\{s_{i}, \cdots, s_{i+h}\right\}$是那些前$n$个点$\left\{s_{i-n}, \cdots, s_{i-1}\right\}$与数据库$\mathcal{D}^{R}$中的历史时间序列$\left\{s_{j}, \cdots, s_{j+n}\right\}$最相关的样本。在我们的方法中，更关注时间序列之间的整体相似性。我们通过它们嵌入之间的距离来量化时间序列之间的参考关系。为了确保嵌入能够有效表示整个时间序列，使用了预训练的编码器$E_{\phi}$。$E_{\phi}$在表示学习任务上进行训练，其参数集$\phi$在我们的检索机制中被冻结。对于$\mathcal{D}^{R}$中的时间序列（长度为$n+h$），它们的前$n$个点被编码，因此$\mathcal{D}^{R}$可以表示为$\mathcal{D}_{\text {emb }}^{R}$：

$$
\begin{equation*}
\mathcal{D}_{\mathrm{emb}}^{R}=\left\{\left\{i, E_{\phi}\left(\boldsymbol{x}_{[0: n]}^{i}\right), \boldsymbol{x}_{[n: n+h]}^{i}\right\} \mid \forall \boldsymbol{x}^{i} \in \mathcal{D}^{R}\right\} \tag{7}
\end{equation*}
$$

其中$[p: q]$表示时间序列中从第$p$个点到第$q$个点形成的子序列。历史时间序列对应的嵌入可以表示为$\boldsymbol{v}^{H}=E_{\phi}\left(\boldsymbol{x}^{H}\right)$。我们计算$\boldsymbol{v}^{H}$与$\mathcal{D}_{\text {emb }}^{R}$中所有嵌入之间的距离，并检索与$k$个最小距离对应的参考。这一过程可以表示为：

$$
\begin{align*}
& \operatorname{index}\left(\boldsymbol{v}^{H}\right) \underset{\boldsymbol{x}^{i} \in \mathcal{D}_{\text {emb }}^{R}}{\arg \min }\left\|\boldsymbol{v}^{H}-E_{\phi}\left(\boldsymbol{x}_{[0: n]}^{i}\right)\right\|^{2}  \tag{8}\\
& \boldsymbol{x}^{R}=\left\{\boldsymbol{x}_{[n: n+h]}^{j} \mid \forall j \in \operatorname{index}\left(\boldsymbol{v}^{H}\right)\right\}
\end{align*}
$$

其中$\operatorname{index}(\cdot)$表示给定$\boldsymbol{v}_{\mathcal{D}}$的检索索引。因此，我们基于查询$\boldsymbol{x}^{H}$获得了$\mathcal{D}^{R}$的一个子集$\boldsymbol{x}^{R}$，即$\zeta_{k}: \boldsymbol{x}^{H}, \mathcal{D}^{R} \rightarrow \boldsymbol{x}^{R}$，其中$\left|\boldsymbol{x}^{R}\right|=k$。

---

**参考引导的时间序列扩散模型**。在本节中，我们将介绍我们的参考引导的时间序列扩散模型。在扩散过程中，前向过程与传统扩散过程相同，如公式(2)所示。根据[34, 12, 35]，反向过程的目标是通过以下表达式推断后验分布$p\left(\boldsymbol{z}^{t a r} \mid \boldsymbol{z}^{c}\right)$：

$$
\begin{equation*}
p\left(\boldsymbol{x} \mid \boldsymbol{x}^{H}\right)=\int p\left(\boldsymbol{x}_{T} \mid \boldsymbol{x}^{H}\right) \prod_{t=1}^{T} p_{\theta}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}^{H}, \boldsymbol{x}^{R}\right) \mathcal{D} \boldsymbol{x}_{1: T} \tag{9}
\end{equation*}
$$

其中$p\left(\boldsymbol{x}_{T} \mid \boldsymbol{x}^{H}\right) \approx \mathcal{N}\left(\boldsymbol{x}_{T} \mid \boldsymbol{x}^{H}, \boldsymbol{I}\right)$，$p_{\theta}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}^{H}, \boldsymbol{x}^{R}\right)$是从$\boldsymbol{x}_{t}$到$\boldsymbol{x}_{t-1}$的反向转移核，具有可学习参数$\theta$。根据扩散模型中的大多数文献，我们采用以下假设：

$$
\begin{equation*}
p_{\theta}\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_{t}, \boldsymbol{x}\right)=\mathcal{N}\left(\boldsymbol{x}_{t-1} ; \mu_{\theta}\left(\boldsymbol{x}_{t}, \boldsymbol{x}^{H}, \boldsymbol{x}^{R}, t\right), \Sigma_{\theta}\left(\boldsymbol{x}_{t}, \boldsymbol{x}^{H}, \boldsymbol{x}^{R}, t\right)\right) \tag{10}
\end{equation*}
$$

其中$\mu_{\theta}$是具有参数$\theta$的深度神经网络。经过与[12]中类似的计算，反向过程中的$\Sigma_{\theta}\left(\boldsymbol{x}_{t}, \boldsymbol{x}^{H}, \boldsymbol{x}^{R}, t\right)$被近似为固定的。换句话说，我们可以通过设计一个合理且稳健的$\mu_{\theta}$来实现参考引导的去噪。

---

##### 去噪网络架构

与DiffWave [17]和CSDI [36]类似，我们的流程构建在Transformer层的基础上，如图3所示。然而，现有框架无法有效利用参考作为指导。考虑到注意力模块可以整合$\boldsymbol{x}^{R}$和$\boldsymbol{x}_{t}$，我们提出了一种称为**参考调制注意力（Reference Modulated Attention, RMA）**的新模块。与普通的注意力模块不同，RMA实现了三种特征的融合：当前时间序列特征、辅助特征和参考特征。具体来说，RMA被设置在每个残差模块的开头（图3）。我们使用1D-CNN从输入$\boldsymbol{x}_{t}$、参考$\boldsymbol{x}^{R}$和辅助信息中提取特征。值得注意的是，我们将所有参考连接在一起进行特征提取。辅助信息由两部分组成，分别表示当前时间序列数据集中变量和时间步之间的相关性（见附录B）。我们通过线性层调整这三种特征的维度，并通过矩阵点积进行融合。与文本-图像扩散模型[29]类似，RMA能够有效利用参考信息来指导去噪过程，同时通过适当的参数设置防止结果过度依赖参考。

• **图3**：$\mu_{\theta}$的结构。(a) $\mu_{\theta}$的主体架构是经过验证有效的时间序列Transformer结构。(b) 提出的RMA结构。我们通过矩阵乘法整合了三种不同的特征。

---

##### 训练过程

为了训练RATD（即优化由RATD引起的证据下界），我们使用了与之前工作相同的目标函数。时间步$t-1$的损失定义如下：

$$
\begin{align*}
L_{t-1}^{(x)} & =\frac{1}{2 \tilde{\beta}_{t}^{2}}\left\|\mu_{\theta}\left(\boldsymbol{x}_{t}, \hat{\boldsymbol{x}}_{0}\right)-\hat{\mu}\left(\boldsymbol{x}_{t}, \hat{\boldsymbol{x}}_{0}\right)\right\|^{2}  \tag{11}\\
& =\gamma_{t}\left\|\boldsymbol{x}_{0}-\hat{\boldsymbol{x}}_{0}\right\|
\end{align*}
$$

其中，$\hat{\boldsymbol{x}}_{0}$是从$\boldsymbol{x}_{t}$预测得到的，$\gamma_{t}=\frac{\bar{\alpha}_{t-1} \beta_{t}^{2}}{2 \tilde{\beta}_{t}^{2}\left(1-\bar{\alpha}_{t}\right)^{2}}$是扩散过程中的超参数。我们在**算法1**中总结了RATD的训练过程，并用青色标注了与传统模型的不同之处。采样过程见附录A。

## 5 实验

### 5.1 实验设置

**数据集**。根据之前的工作 [45, 38, 8, 30]，实验在四个流行的真实世界时间序列数据集上进行：(1) **Electricity**，包含321个客户在两年内的每小时电力消耗数据；(2) **Wind** [20]，包含2020-2021年的风力发电记录；(3) **Exchange** [18]，描述了八个国家（澳大利亚、英国、加拿大、瑞士、中国、日本、新西兰和新加坡）的每日汇率；(4) **Weather**${ }^{\dagger}$，记录了2020年至2021年期间每10分钟一次的21个气象指标。此外，我们还将我们的方法应用于一个大型心电图时间序列数据集：**MIMIC-IV-ECG** [14]。该数据集包含来自Beth Israel Deaconess Medical Center (BIDMC)的超过190,000名患者和450,000次住院的临床心电图数据。

---

**基线方法**。为了全面展示我们方法的有效性，我们将RATD与四种时间序列预测方法进行比较。我们的基线包括：(1) 时间序列扩散模型，包括CSDI [36]、mr-Diff [31]、D${ }^{3}$VAE [20]、TimeDiff [30]；(2) 最近的时间序列预测方法，结合频率信息，包括FiLM [46]、Fedformer [47]和FreTS [41]；(3) 时间序列Transformer模型，包括PatchTST [25]、Autoformer [38]、Pyraformer [22]、Informer [45]和iTransformer [23]；(4) 其他流行方法，包括TimesNet [39]、SciNet [21]、Nlinear [43]、DLinear [43]和NBeats [26]。

---

**评估指标**。为了全面评估我们提出的方法，我们的实验采用了三个指标：(1) 概率预测指标：每个时间序列维度上的连续排名概率得分（Continuous Ranked Probability Score, CRPS）[24]；(2) 距离指标：均方误差（Mean Squared Error, MSE）和平均绝对误差（Mean Average Error, MAE）用于衡量预测值与真实值之间的距离。

---

**实现细节**。历史时间序列的长度为168，预测长度分别为（96, 192, 336），结果取平均值。所有实验均在配备40GB显存的Nvidia RTX A6000 GPU上进行。在实验中，针对MIMIC数据集采用了第二种策略进行$\mathcal{D}^{R}$的操作，而对其他四个数据集则采用了第一种策略。为了降低训练成本，我们通过预处理将训练集中每个样本的参考索引存储在字典中。在训练扩散模型时，我们直接访问该字典，以避免冗余的检索过程。更多细节详见附录B。

### 5.2 主要结果

表1展示了我们在四个每日数据集上的主要实验结果。我们的方法超越了现有的时间序列扩散模型。与其他时间序列预测方法相比，我们的方法在四个数据集中的三个上表现优异，在剩余数据集上也展现了竞争力。值得注意的是，我们在风能数据集上取得了突出的结果。由于缺乏明显的短期周期性（每日或每小时），该数据集中的一些预测任务对其他模型来说极具挑战性。检索增强机制能够有效协助解决这些具有挑战性的预测任务。

---

图4展示了我们从风能数据集实验中随机选取的一个案例研究。我们将我们的预测与iTransformer以及两个流行的开源时间序列扩散模型CSDI和$\mathrm{D}_{3}$ VAE进行了比较。尽管CSDI和$\mathrm{D}_{3}$ VAE在初始短期预测中提供了准确的结果，但由于缺乏指导，它们的长期预测与真实值存在显著偏差。iTransformer捕捉到了粗略的趋势和周期性模式，但我们的方法提供了比其他方法更高质量的预测。此外，通过图中预测结果与参考值的对比可以看出，尽管参考值提供了强有力的指导，但它们并未完全替代整个生成结果。这进一步验证了我们方法的合理性。

---

表2展示了我们的方法在MIMIC-IV-ECG数据集上的测试结果。我们选择了一些强大的开源方法作为基线进行比较。我们的实验分为两部分：在第一部分中，我们对整个测试集进行评估；在第二部分中，我们从测试集中选取罕见病例（占总病例数不到$2\%$）作为子集进行评估。第二部分中的预测任务对深度模型更具挑战性。在第一项实验中，我们的方法取得了接近iTransformer的结果，而在第二项任务中，我们的模型显著优于其他方法，证明了我们的方法在处理具有挑战性任务时的有效性。

• 表2：在MIMIC数据集上与流行时间序列预测方法的性能对比。其中，“MIMIC-IV（全部）”表示模型在完整测试集上的测试结果，“MIMIC（罕见）”表示模型在罕见疾病子集上的测试结果。

### 5.3 模型分析

**检索机制的影响**  

为了研究检索增强机制对生成过程的影响，我们进行了消融实验，结果如表3所示。该研究解决了两个问题：检索增强机制是否有效，以及哪种检索方法最有效。首先，我们将检索增强机制从RATD中移除作为基线。此外，使用随机时间序列指导的模型是另一个基线。通过其他方法检索到的参考值均对预测结果产生了积极影响。这表明合理的参考值在指导生成过程中非常有效。

• 表3：不同检索机制的消融研究。“-”表示未使用参考值，“Random”表示随机选择参考值。其他项表示我们用于检索参考值的模型。

---

我们还比较了两种不同的检索机制：基于相关性的检索和基于嵌入的检索。第一种方法直接在时域中检索参考值（例如使用动态时间规整（DTW）或皮尔逊相关系数）。我们的方法采用了第二种机制：通过时间序列的嵌入检索参考值。从结果来看，基于相关性的方法显著逊色于基于嵌入的方法。前者未能捕捉时间序列的关键特征，因此难以检索到最佳的预测参考值。我们还评估了基于嵌入的方法，并使用不同的编码器进行比较。综合结果表明，不同编码器的方法之间没有显著差异。这表明不同方法均能提取有意义的参考值，从而在结果上产生类似的改进。在我们的实验中，使用了TCN，因为TCN在计算成本和性能之间达到了最佳平衡。

---

**检索数据库的影响**  

我们对两个变量$n$和$k$进行了消融研究，以探讨检索数据库$\mathcal{D}^{R}$在RATD中的影响，其中$n$表示数据库中每类样本的数量，$k$表示参考样本的数量。图5中的结果表明，较大的$\mathcal{D}^{R}$可以在预测准确性方面使模型受益，因为更大的$\mathcal{D}^{R}$带来了更高的多样性，从而提供了更多有利于预测的细节，并增强了生成过程。单纯增加$k$并未显示出显著改进，因为使用更多的参考值可能会在去噪过程中引入更多噪声。在我们的实验中，$n$和$k$的设置分别为256和3。

---

##### 推理效率  
在本实验中，我们评估了所提出的RATD（检索增强时间序列扩散模型）的推理效率，并将其与其他基线时间序列扩散模型（TimeGrad、MG-TSD、SSSD）进行了比较。图6展示了在多变量天气数据集上，随着预测范围$(h)$值变化时的推理时间。尽管我们的方法引入了一个额外的检索模块，但由于采用了非自回归的Transformer框架，RATD的采样效率并不低。在所有$h$值下，RATD甚至略微优于其他基线模型。值得注意的是，TimeGrad的推理速度最慢，这归因于其使用了自回归解码机制。

---

##### 参考调制注意力（RMA）的有效性  
为了验证所提出的RMA的有效性，我们设计了额外的消融实验。在这些实验中，我们以CSDI架构作为基线方法，并添加了额外的融合模块，以比较这些模块（线性层、交叉注意力层和RMA）的性能。结果如表4所示。

• 表4：基于CSDI方法的性能对比（均方误差，MSE）。CSDI表示基本的网络框架，CSDI+Linear表示通过线性层将输入和参考值连接并一起输入网络的方法，CSDI+CrossAttention表示使用交叉注意力融合输入和参考值的特征，最后是CSDI+RMA，它额外引入了RMA模块。

---

通过实验，我们发现与基于基本交叉注意力的方法相比，RMA能够更有效地整合边缘信息矩阵（表示时间和特征维度之间的相关性）。额外的融合在实验中非常有益，能够引导模型捕捉不同变量之间的关系。相比之下，基于线性层的方法在初始阶段将输入和参考值连接在一起，这阻碍了直接从参考值中提取有意义的信息，因此性能相对较为一般。

---

##### 预测$\boldsymbol{x}_{0}$与预测$\epsilon$的对比  
根据第4.3节的公式，我们的网络被设计为预测潜在变量$\boldsymbol{x}_{0}$。由于一些现有模型[28, 36]通过预测额外的噪声项$\epsilon$进行训练，我们进行了对比实验，以确定哪种方法更适合我们的框架。具体来说，我们保持网络结构不变，仅将预测目标修改为$\epsilon$。结果如表5所示。预测$x_{0}$被证明更为有效。这可能是因为参考值与$\boldsymbol{x}_{0}$之间的关系更为直接，使得去噪任务相对更容易。

---

##### RMA的位置  
我们研究了RMA在模型中的最佳位置。“前部”、“中部”和“后部”分别表示我们将RMA放置在两个Transformer层的前部、中部和后部。我们发现，将RMA放置在双向Transformer之前，对模型性能的提升最为显著。这也与网络设计的直觉一致：放置在模型前部的交叉注意力模块往往具有更大的影响。

## 6 讨论

**局限性与未来工作**  

作为一种基于Transformer的扩散模型结构，我们的方法仍然面临Transformer框架带来的一些挑战。我们的模型在处理包含过多变量的时间序列时，会消耗大量的计算资源。此外，我们的方法在训练过程中需要额外的预处理（检索过程），这会导致训练时间的额外成本（大约十小时）。

---

**结论**  

在本文中，我们提出了一种新的时间序列扩散建模框架，以解决现有扩散模型在预测性能上的局限性。RATD从构建的数据库中检索与历史时间序列最相关的样本，并将其作为参考值来指导扩散模型的去噪过程，从而获得更准确的预测。通过在五个真实世界数据集上的实验评估，RATD在解决具有挑战性的时间序列预测任务中表现出高效性。

## B 实现细节

### B.1 训练细节

我们的数据集按照7:1:2的比例（训练集：验证集：测试集）进行划分，采用随机划分策略以确保训练集的多样性。对于MIMIC-IV数据集，我们以125 Hz的频率对ECG信号进行采样，并提取固定长度的窗口作为样本。在训练过程中，我们使用了Adam优化器，初始学习率为$10^{-3}$，betas $=(0.95,0.999)$。在移位扩散模型的训练过程中，批量大小设置为64，并采用早停策略，最大训练轮数为200轮。扩散步数$T$设置为100。

### B.2 辅助信息

我们将时间嵌入和特征嵌入结合作为辅助信息$v_{s}$。我们遵循先前研究[37]，使用128维的时间嵌入：

$$
\begin{equation*}
s_{e m b e d d i n g}\left(s_{\zeta}\right)=\left(\sin \left(s_{\zeta} / \tau^{0 / 64}\right), \ldots, \sin \left(s_{\zeta} / \tau^{63 / 64}\right), \cos \left(s_{\zeta} / \tau^{0 / 64}\right), \ldots, \cos \left(s_{\zeta} / \tau^{63 / 64}\right)\right) \tag{12}
\end{equation*}
$$

其中$\tau=10000$。根据[36]，$s_{l}$表示时间序列中第1个点对应的时间戳。这一设置旨在捕捉数据集中的不规则采样，并将其传递给模型。此外，我们使用可学习的嵌入来处理特征维度。具体来说，特征嵌入表示为16维的可学习向量，用于捕捉维度之间的关系。根据[17]，我们将时间嵌入和特征嵌入结合，统称为辅助信息$\mathcal{I}_{s}$。

---

$\mathcal{I}_{s}$的形状并不固定，而是随数据集变化。以Exchange数据集为例，预测目标$\boldsymbol{x}^{R}$的形状为[批量大小（64），7（变量数量），168（时间维度），12（时间维度）]，而对应的$\mathcal{I}_{s}$的形状为[批量大小（64），总通道数（144（时间：128 + 特征：16）），320（频率维度*潜在通道数），12（时间维度）]。

## B.3 Transformer细节

我们的方法采用了CSDI中的Transformer架构，不同之处在于将通道维度扩展到了128。该网络包含时间和特征层，确保模型在处理时频域潜在特征时的全面性，同时保持相对简单的结构。关于Transformer层，我们使用了PyTorch [27]实现的1层Transformer编码器，包括多头注意力层、全连接层和层归一化。我们采用了“线性注意力Transformer”包以提高计算效率。由于包含大量特征和长序列，我们做出了这一决定。该包实现了一种高效的注意力机制 [33]，我们仅使用了包中的全局注意力功能。

## B.4 评估指标

我们将介绍实验中使用的评估指标。总结如下：

**CRPS**  

CRPS [24] 是一种单变量严格评分规则，用于衡量累积分布函数 $F$ 与观测值 $x$ 的兼容性，公式如下：
$$
\begin{equation*}
C R P S(F, x)=\int_{R}\left(F(y)-\mathbb{1}_{(x \leq y)}\right)^{2} d y \tag{13}
\end{equation*}
$$

其中 $\mathbb{1}_{(x \leq y)}$ 是指示函数，如果 $x \leq y$ 则为1，否则为0。当预测分布 $F$ 与数据分布相同时，CRPS达到最小值。

---

**MAE 和 MSE**  
MAE 和 MSE 的计算公式如下，$\hat{\boldsymbol{x}^{P}}$ 表示预测的时间序列，$\boldsymbol{x}^{P}$ 表示真实的时间序列。MAE 计算预测值与真实值之间的平均绝对差，而 MSE 计算预测值与真实值之间的平均平方差。MAE 或 MSE 越小，表示预测效果越好。
$$
\begin{array}{r}
M A E=\operatorname{mean}\left(\left|\hat{\boldsymbol{x}^{P}}-\boldsymbol{x}^{P}\right|\right) \\
M S E=\sqrt{\operatorname{mean}\left(\left|\hat{\boldsymbol{x}^{P}}-\boldsymbol{x}^{P}\right|\right)} \tag{14}
\end{array}
$$