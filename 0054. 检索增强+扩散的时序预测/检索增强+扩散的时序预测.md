# Retrieval-Augmented Diffusion Models for Time Series Forecasting

# 用于时间序列预测的检索增强扩散模型

Jingwei Liu ${}^{1,2 * }$ Ling Yang ${}^{ \dagger  }$ Hongyan Li ${}^{1,2 \ddagger  }$ Shenda Hong ${}^{3,4,5 \ddagger  }$

刘经纬 ${}^{1,2 * }$ 杨玲 ${}^{ \dagger  }$ 李红岩 ${}^{1,2 \ddagger  }$ 洪申达 ${}^{3,4,5 \ddagger  }$

${}^{1}$ School of Intelligence Science and Technology,Peking University

${}^{1}$ 北京大学智能科学与技术学院

${}^{2}$ National Key Laboratory of General Artificial Intelligence,Peking University

${}^{2}$ 北京大学通用人工智能国家重点实验室

${}^{3}$ Institute of Medical Technology,Peking University Health Science Center

${}^{3}$ 北京大学医学部医学技术研究院

${}^{4}$ National Institute of Health Data Science,Peking University

${}^{4}$ 北京大学国家卫生健康数据科学研究院

${}^{5}$ Institute for Artificial Intelligence,Peking University

${}^{5}$ 北京大学人工智能研究院（Institute for Artificial Intelligence, Peking University）

jingweiliu1996@163.com, yangling0818@163.com

jingweiliu1996@163.com，yangling0818@163.com

\{leehy, hongshenda\}@pku.edu.cn

\{leehy, hongshenda\}@pku.edu.cn

## Abstract

## 摘要

While time series diffusion models have received considerable focus from many recent works, the performance of existing models remains highly unstable. Factors limiting time series diffusion models include insufficient time series datasets and the absence of guidance. To address these limitations, we propose a Retrieval-Augmented Time series Diffusion model (RATD). The framework of RATD consists of two parts: an embedding-based retrieval process and a reference-guided diffusion model. In the first part, RATD retrieves the time series that are most relevant to historical time series from the database as references. The references are utilized to guide the denoising process in the second part. Our approach allows leveraging meaningful samples within the database to aid in sampling, thus maximizing the utilization of datasets. Meanwhile, this reference-guided mechanism also compensates for the deficiencies of existing time series diffusion models in terms of guidance. Experiments and visualizations on multiple datasets demonstrate the effectiveness of our approach, particularly in complicated prediction tasks. Our code is available at https://github.com/stanliu96/RATD

尽管时间序列扩散模型在近期的许多研究中受到了广泛关注，但现有模型的性能仍然极不稳定。限制时间序列扩散模型的因素包括时间序列数据集不足以及缺乏引导。为解决这些限制，我们提出了一种检索增强型时间序列扩散模型（Retrieval-Augmented Time series Diffusion model，RATD）。RATD的框架由两部分组成：基于嵌入的检索过程和参考引导的扩散模型。在第一部分中，RATD从数据库中检索与历史时间序列最相关的时间序列作为参考。在第二部分中，这些参考被用于引导去噪过程。我们的方法允许利用数据库中有意义的样本辅助采样，从而最大限度地利用数据集。同时，这种参考引导机制也弥补了现有时间序列扩散模型在引导方面的不足。在多个数据集上的实验和可视化结果证明了我们方法的有效性，特别是在复杂的预测任务中。我们的代码可在https://github.com/stanliu96/RATD获取

## 1 Introduction

## 1 引言

Time series forecasting plays a critical role in a variety of applications including weather forecasting [15, 11], finance forecasting [7, 5], earthquake prediction [19] and energy planning [6]. One way to approach time series forecasting tasks is to view them as conditional generation tasks [32, 42], where conditional generative models are used to learn the conditional distribution $P\left( {{\mathbf{x}}^{P} \mid  {\mathbf{x}}^{H}}\right)$ of predicting the target time series ${\mathbf{x}}^{P}$ given the observed historical sequence ${\mathbf{x}}^{H}$ . As the current state-of-the-art conditional generative model, diffusion models [12] have been utilized in many works for time series forecasting tasks [28, 36, 30].

时间序列预测在包括天气预报[15, 11]、金融预测[7, 5]、地震预测[19]和能源规划[6]等多种应用中发挥着关键作用。处理时间序列预测任务的一种方法是将其视为条件生成任务[32, 42]，其中使用条件生成模型来学习在给定观测到的历史序列${\mathbf{x}}^{H}$的情况下预测目标时间序列${\mathbf{x}}^{P}$的条件分布$P\left( {{\mathbf{x}}^{P} \mid  {\mathbf{x}}^{H}}\right)$。作为当前最先进的条件生成模型，扩散模型[12]已被许多研究用于时间序列预测任务[28, 36, 30]。

Although the performance of the existing time series diffusion models is reasonably well on some time series forecasting tasks, it remains unstable in certain scenarios (an example is provided in Tc)). The factors limiting the performance of time series diffusion models are complex, two of them are particularly evident. First, most time series lack direct semantic or label correspondences, which often results in time series diffusion models lacking meaningful guidance during the generation process(such as text guidance or label guidance in image diffusion models). This also limits the potential of time series diffusion models.

尽管现有的时间序列扩散模型在一些时间序列预测任务上的表现相当不错，但在某些场景下仍不稳定（在Tc中给出了一个示例）。限制时间序列扩散模型性能的因素很复杂，其中有两个因素尤为明显。首先，大多数时间序列缺乏直接的语义或标签对应关系，这常常导致时间序列扩散模型在生成过程中缺乏有意义的引导（例如图像扩散模型中的文本引导或标签引导）。这也限制了时间序列扩散模型的潜力。

---

<!-- Footnote -->

*Contact: Jingwei Liu, jingweiliu1996@163.com

*联系方式：刘经纬（Jingwei Liu），jingweiliu1996@163.com

${}^{ \dagger  }$ Contributed equally.

${}^{ \dagger  }$ 贡献相同。

${}^{ \ddagger  }$ Corresponding Authors: Hongyan Li,Shenda Hong

${}^{ \ddagger  }$ 通讯作者：李红岩（Hongyan Li）、洪申达（Shenda Hong）

<!-- Footnote -->

---

<!-- Media -->

<!-- figureText: (a) -->

<img src="https://cdn.noedgeai.com/01957f6c-7a55-79ee-9cf5-c168c8a66c82_1.jpg?x=312&y=341&w=1173&h=270&r=0"/>

Figure 1: (a) The figure shows the differences in forecasting results between the CSDI [36] (left) and RATD (right). Due to the very small proportion of such cases in the training set, CSDI struggles to make accurate predictions, often predicting more common results. Our method, by retrieving meaningful references as guidance, makes much more accurate predictions. (b) A comparison between our method's framework(bottom) and the conventional time series diffusion model framework(top). (c) We randomly selected 25 forecasting tasks from the electricity dataset. Compared to our method, CSDI and MG-TSD [9] exhibited significantly higher instability. This indicates that the RATD is better at handling complex tasks that are challenging for the other two methods.

图1：(a) 该图展示了CSDI [36]（左）和RATD（右）在预测结果上的差异。由于训练集中此类情况的比例非常小，CSDI难以做出准确预测，通常会预测出更常见的结果。我们的方法通过检索有意义的参考作为指导，做出了更准确的预测。(b) 我们方法的框架（下）与传统时间序列扩散模型框架（上）的比较。(c) 我们从电力数据集中随机选择了25个预测任务。与我们的方法相比，CSDI和MG - TSD [9]表现出明显更高的不稳定性。这表明RATD更擅长处理对其他两种方法来说具有挑战性的复杂任务。

<!-- Media -->

The second limiting factor arises from two shortcomings of the time series datasets: size insufficient and imbalanced. Compared to image datasets, time series datasets typically have a smaller scale. Popular image datasets (such as LAION-400M) contain 400 million sample pairs, while most time series datasets usually only contain tens of thousands of data points. Training a diffusion model to learn the precise distribution of datasets with insufficient size is challenging. Additionally, real-world time series datasets exhibit significant imbalance. For example, in the existing electrocardiogram dataset MIMIC-IV, records related to diagnosed pre-excitation syndrome (PS) account for less than ${0.025}\%$ of the total records. This imbalance phenomenon may cause models to overlook some extremely rare complex samples, leading to a tendency to generate more common predictions during training, thus making it difficult to handle complex prediction tasks, as illustrated in Figure 1.

第二个限制因素源于时间序列数据集的两个缺点：规模不足和数据不平衡。与图像数据集相比，时间序列数据集的规模通常较小。流行的图像数据集（如LAION - 400M）包含4亿个样本对，而大多数时间序列数据集通常只包含数万个数据点。训练扩散模型来学习规模不足的数据集的精确分布具有挑战性。此外，现实世界中的时间序列数据集存在显著的不平衡性。例如，在现有的心电图数据集MIMIC - IV中，与已诊断的预激综合征（Pre - excitation Syndrome，PS）相关的记录占总记录的比例不到${0.025}\%$。这种不平衡现象可能会导致模型忽略一些极其罕见的复杂样本，从而在训练过程中倾向于生成更常见的预测结果，因此难以处理复杂的预测任务，如图1所示。

To address these limitations, we propose the Retrieval-Augmented Time series Diffusion Model (RATD) for complex time series forecasting tasks. Our approach consists of two parts: the embedding-based retrieval and the reference-guided diffusion model. After obtaining a historical time series, it is input into the embedding-based retrieval process to retrieve the $\mathrm{k}$ nearest samples as references. The references are utilized as guidance in the denoising process. RATD focuses on making maximum utilization of existing time series datasets by finding the most relevant references in the dataset to the historical time series, thereby providing meaningful guidance for the denoising process. RATD focuses on maximizing the utilization of insufficient time series data and to some extent mitigates the issues caused by data imbalance. Meanwhile, this reference-guided mechanism also compensates for the deficiencies of guidance in existing time series diffusion models. Our approach demonstrates strong performance across multiple datasets, particularly on more complex tasks.

为解决这些局限性，我们提出了用于复杂时间序列预测任务的检索增强时间序列扩散模型（Retrieval-Augmented Time series Diffusion Model，RATD）。我们的方法由两部分组成：基于嵌入的检索和参考引导的扩散模型。获取历史时间序列后，将其输入到基于嵌入的检索过程中，以检索出$\mathrm{k}$个最近邻样本作为参考。这些参考样本在去噪过程中用作引导。RATD致力于通过在数据集中找到与历史时间序列最相关的参考样本，最大限度地利用现有的时间序列数据集，从而为去噪过程提供有意义的引导。RATD专注于最大限度地利用有限的时间序列数据，并在一定程度上缓解了数据不平衡带来的问题。同时，这种参考引导机制也弥补了现有时间序列扩散模型中引导方面的不足。我们的方法在多个数据集上表现出强大的性能，尤其是在更复杂的任务上。

To summarize, our main contributions are summarized as follows:

综上所述，我们的主要贡献总结如下：

- To handle complex time series forecasting, we for the first time introduce Retrieval-Augmented Time series Diffusion (RATD), allowing for greater utilization of the dataset and providing meaningful guidance in the denoising process.

- 为处理复杂的时间序列预测问题，我们首次引入了检索增强时间序列扩散（Retrieval-Augmented Time series Diffusion，RATD）方法，该方法能够更充分地利用数据集，并在去噪过程中提供有意义的引导。

- Extra Reference Modulated Attention (RMA) module is designed to provide reasonable guidance from the reference during the denoising process. RMA effectively simply integrates information without introducing excessive additional computational costs.

- 额外参考调制注意力（RMA，Extra Reference Modulated Attention）模块旨在在去噪过程中从参考信息中提供合理的指导。RMA 有效地简单整合信息，而不会引入过多的额外计算成本。

- We conducted experiments on five real-world datasets and provided a comprehensive presentation and analysis of the results using multiple metrics. The experimental results demonstrate that our approach achieves comparable or better results compared to baselines.

- 我们在五个真实世界的数据集上进行了实验，并使用多种指标对结果进行了全面的展示和分析。实验结果表明，与基线方法相比，我们的方法取得了相当或更好的结果。

## 2 Related Work

## 2 相关工作

### 2.1 Diffusion Models for Time Series Forecasting

### 2.1 用于时间序列预测的扩散模型

Recent advancements have been made in the utilization of diffusion models for time series forecasting. In TimeGrad [28], the conditional diffusion model was first employed as an autoregressive approach for prediction, with the denoising process guided by the hidden state. CSDI [36] adopted a non-autoregressive generation strategy to achieve faster predictions. SSSD [1] replaced the noise-matching network with a structured state space model for prediction. TimeDiff [30] incorporated future mix-up and autoregressive initialization into a non-autoregressive framework for forecasting. MG-TSD [9] utilized a multi-scale generation strategy to sequentially predict the main components and details of the time series. Meanwhile, mr-diff [31] utilized diffusion models to separately predict the trend and seasonal components of time series. These methods have shown promising results in some prediction tasks, but they often perform poorly in challenging prediction tasks. We propose a retrieval-augmented framework to address this issue.

在将扩散模型应用于时间序列预测方面，近期取得了一些进展。在TimeGrad [28]中，条件扩散模型首次被用作一种自回归方法进行预测，其去噪过程由隐藏状态引导。CSDI [36]采用了非自回归生成策略以实现更快的预测。SSSD [1]用结构化状态空间模型取代了噪声匹配网络进行预测。TimeDiff [30]将未来混合和自回归初始化纳入非自回归框架进行预测。MG - TSD [9]利用多尺度生成策略依次预测时间序列的主要成分和细节。同时，mr - diff [31]利用扩散模型分别预测时间序列的趋势和季节性成分。这些方法在一些预测任务中显示出了有前景的结果，但在具有挑战性的预测任务中往往表现不佳。我们提出了一个检索增强框架来解决这个问题。

### 2.2 Retrival-Augmented Generation

### 2.2 检索增强生成

The retrieval-augmented mechanism is one of the classic mechanisms for generative models. Numerous works have demonstrated the benefits of incorporating explicit retrieval steps into neural networks. Classic works in the field of natural language processing leverage retrieval augmentation mechanisms to enhance the quality of language generation [16, 10, 4]. In the domain of image generation, some retrieval-augmented models focus on utilizing samples from the database to generate more realistic images [2, 44]. Similarly, [3] employed memorized similarity information from training data for retrieval during inference to enhance results. MQ-ReTCNN [40] is specifically designed for complex time series forecasting tasks involving multiple entities and variables. ReTime [13] creates a relation graph based on the temporal closeness between sequences and employs relational retrieval instead of content-based retrieval. Although the aforementioned three methods successfully utilize retrieval mechanisms to enhance time series forecasting results, our approach still holds significant advantages. This advantage stems from the iterative structure of the diffusion model, where references can repeatedly influence the generation process, allowing references to exert a stronger influence on the entire conditional generation process.

检索增强机制是生成模型的经典机制之一。许多研究已经证明了将显式检索步骤融入神经网络的益处。自然语言处理领域的经典研究利用检索增强机制来提高语言生成的质量 [16, 10, 4]。在图像生成领域，一些检索增强模型专注于利用数据库中的样本生成更逼真的图像 [2, 44]。同样，文献 [3] 在推理过程中利用训练数据中记忆的相似性信息进行检索以提升结果。MQ - ReTCNN（多查询关系时间卷积神经网络）[40] 是专门为涉及多个实体和变量的复杂时间序列预测任务设计的。ReTime [13] 基于序列之间的时间接近度创建关系图，并采用关系检索而非基于内容的检索。尽管上述三种方法成功地利用检索机制提高了时间序列预测结果，但我们的方法仍具有显著优势。这种优势源于扩散模型的迭代结构，在该结构中，参考信息可以反复影响生成过程，从而使参考信息对整个条件生成过程产生更强的影响。

## 3 Preliminary

## 3 预备知识

The forecasting task and the background knowledge about the conditional time series diffusion model will be discussed in this section. To avoid conflicts, we use the symbol "s" to represent the time series, and the "t" denotes the t-th step in the diffusion process.

本节将讨论预测任务以及关于条件时间序列扩散模型的背景知识。为避免冲突，我们使用符号 “s” 表示时间序列，“t” 表示扩散过程中的第 t 步。

Generative Time Series Forecasting. Suppose we have an observed historical time series ${\mathbf{x}}^{H} =$ $\left\{  {{s}_{1},{s}_{2},\cdots ,{s}_{l} \mid  {s}_{i} \in  {\mathbb{R}}^{d}}\right\}$ ,where $l$ is the historical time length, $d$ is the number of features per observation and ${s}_{i}$ is the observation at time step $i$ . The ${\mathbf{x}}^{P}$ is the corresponding prediction target $\left\{  {{s}_{l + 1},{s}_{l + 2},\cdots ,{s}_{l + h} \mid  {s}_{l + i} \in  {\mathbb{R}}^{{d}^{\prime }}}\right\}  \left( {{d}^{\prime } \leq  d}\right)$ ,where $h$ is the prediction horizon. The task of generative time series forecasting is to learn a density ${p}_{\theta }\left( {{\mathbf{x}}^{P} \mid  {\mathbf{x}}^{H}}\right)$ that best approximates $p\left( {{\mathbf{x}}^{P} \mid  {\mathbf{x}}^{H}}\right)$ ,

生成式时间序列预测。假设我们有一个已观测到的历史时间序列${\mathbf{x}}^{H} =$ $\left\{  {{s}_{1},{s}_{2},\cdots ,{s}_{l} \mid  {s}_{i} \in  {\mathbb{R}}^{d}}\right\}$，其中$l$是历史时间长度，$d$是每次观测的特征数量，${s}_{i}$是时间步$i$的观测值。${\mathbf{x}}^{P}$是对应的预测目标$\left\{  {{s}_{l + 1},{s}_{l + 2},\cdots ,{s}_{l + h} \mid  {s}_{l + i} \in  {\mathbb{R}}^{{d}^{\prime }}}\right\}  \left( {{d}^{\prime } \leq  d}\right)$，其中$h$是预测时长。生成式时间序列预测的任务是学习一个密度${p}_{\theta }\left( {{\mathbf{x}}^{P} \mid  {\mathbf{x}}^{H}}\right)$，使其能最佳逼近$p\left( {{\mathbf{x}}^{P} \mid  {\mathbf{x}}^{H}}\right)$，

which can be written as:

其可以写成：

$$
\mathop{\min }\limits_{{p}_{\theta }}D\left( {{p}_{\theta }\left( {{\mathbf{x}}^{P} \mid  {\mathbf{x}}^{H}}\right) \parallel p\left( {{\mathbf{x}}^{P} \mid  {\mathbf{x}}^{H}}\right) }\right) , \tag{1}
$$

where $\theta$ denotes parameters and $D$ is some appropriate measure of distance between distributions. Given observation $x$ the target time series can be obtained directly by sampling from ${p}_{\theta }\left( {{\mathbf{x}}^{P} \mid  {\mathbf{x}}^{H}}\right)$ . Therefore,we obtain the time series $\left\{  {{s}_{1},{s}_{2},\cdots ,{s}_{n + h}}\right\}   = \left\lbrack  {{\mathbf{x}}^{H},{\mathbf{x}}^{P}}\right\rbrack$ .

其中 $\theta$ 表示参数，$D$ 是分布之间某种合适的距离度量。给定观测值 $x$，目标时间序列可以直接通过从 ${p}_{\theta }\left( {{\mathbf{x}}^{P} \mid  {\mathbf{x}}^{H}}\right)$ 中采样得到。因此，我们得到时间序列 $\left\{  {{s}_{1},{s}_{2},\cdots ,{s}_{n + h}}\right\}   = \left\lbrack  {{\mathbf{x}}^{H},{\mathbf{x}}^{P}}\right\rbrack$。

Conditional Time Series Diffusion Models. With observed time series ${\mathbf{x}}^{H}$ ,the diffusion model progressively destructs target time series ${\mathbf{x}}_{0}^{P}$ (equals to the ${\mathbf{x}}^{P}$ mentioned in the previous context) by injecting noise,then learns to reverse this process starting from ${\mathbf{x}}_{T}^{P}$ for sample generation. For the convenience of expression,in this paper,we use ${x}_{t}$ to refer to the t-th time series in the diffusion process, with the letter "P" omitted. The forward process can be formulated as a Gaussian process with a Markovian structure:

条件时间序列扩散模型。对于观测到的时间序列${\mathbf{x}}^{H}$，扩散模型通过注入噪声逐步破坏目标时间序列${\mathbf{x}}_{0}^{P}$（等同于前文提到的${\mathbf{x}}^{P}$），然后从${\mathbf{x}}_{T}^{P}$开始学习逆转这一过程以进行样本生成。为便于表述，在本文中，我们用${x}_{t}$表示扩散过程中的第t个时间序列，省略字母“P”。前向过程可以表述为具有马尔可夫结构的高斯过程：

$$
q\left( {{\mathbf{x}}_{t} \mid  {\mathbf{x}}_{t - 1}}\right)  \mathrel{\text{:=}} \mathcal{N}\left( {{\mathbf{x}}_{t};\sqrt{1 - {\beta }_{t}}{\mathbf{x}}_{t - 1},{\mathbf{x}}^{H},{\beta }_{t}\mathbf{I}}\right) , \tag{2}
$$

$$
q\left( {{\mathbf{x}}_{t} \mid  {\mathbf{x}}_{0}}\right)  \mathrel{\text{:=}} \mathcal{N}\left( {{\mathbf{x}}_{t};\sqrt{{\bar{\alpha }}_{t}}{\mathbf{x}}_{0},{\mathbf{x}}^{H},\left( {1 - {\bar{\alpha }}_{t}}\right) \mathbf{I}}\right) ,
$$

<!-- Media -->

<!-- figureText: Retrieval Mechanism Main Framework -->

<img src="https://cdn.noedgeai.com/01957f6c-7a55-79ee-9cf5-c168c8a66c82_3.jpg?x=308&y=204&w=1177&h=331&r=0"/>

Figure 2: Overview of the proposed RATD. The historical time series ${\mathbf{x}}^{H}$ is inputted into the retrieval module to for the corresponding references ${\mathbf{x}}^{R}$ . After that, ${\mathbf{x}}^{H}$ is concatenated with the noise as the main input for the model ${\mu }_{\theta }.{x}^{R}$ will be utilized as the guidance for the denoising process.

图2：所提出的RATD（基于检索的自注意力去噪，Retrieval-based Attention Denoising）概述。将历史时间序列${\mathbf{x}}^{H}$输入到检索模块以获取相应的参考序列${\mathbf{x}}^{R}$。之后，将${\mathbf{x}}^{H}$与噪声拼接作为模型的主要输入，${\mu }_{\theta }.{x}^{R}$将用作去噪过程的指导。

<!-- Media -->

where ${\beta }_{1},\ldots ,{\beta }_{T}$ denotes fixed variance schedule with ${\alpha }_{t} \mathrel{\text{:=}} 1 - {\beta }_{t}$ and ${\bar{\alpha }}_{t} \mathrel{\text{:=}} \mathop{\prod }\limits_{{s = 1}}^{t}{\alpha }_{s}$ . This forward process progressively injects noise into data until all structures are lost, which is well-approximated by $\mathcal{N}\left( {0,\mathbf{I}}\right)$ . The reverse diffusion process learns a model ${p}_{\theta }\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{t},{\mathbf{x}}^{H}}\right)$ that approximates the true posterior:

其中 ${\beta }_{1},\ldots ,{\beta }_{T}$ 表示具有 ${\alpha }_{t} \mathrel{\text{:=}} 1 - {\beta }_{t}$ 和 ${\bar{\alpha }}_{t} \mathrel{\text{:=}} \mathop{\prod }\limits_{{s = 1}}^{t}{\alpha }_{s}$ 的固定方差调度。这个前向过程逐步向数据中注入噪声，直到所有结构信息都丢失，这可以由 $\mathcal{N}\left( {0,\mathbf{I}}\right)$ 很好地近似。反向扩散过程学习一个模型 ${p}_{\theta }\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{t},{\mathbf{x}}^{H}}\right)$ 来近似真实的后验分布：

$$
{p}_{\theta }\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{t},{\mathbf{x}}^{H}}\right)  \mathrel{\text{:=}} \mathcal{N}\left( {{\mathbf{x}}_{t - 1};{\mu }_{\theta }\left( {\mathbf{x}}_{t}\right) ,{\sum }_{\theta }\left( {\mathbf{x}}_{t}\right) ,{\mathbf{x}}^{H}}\right) , \tag{3}
$$

where ${\mu }_{\theta }$ and ${\sum }_{\theta }$ are often computed by the Transformer. Ho et al. [12] improve the diffusion training process and optimize following objective:

其中 ${\mu }_{\theta }$ 和 ${\sum }_{\theta }$ 通常由Transformer（变换器）计算得出。Ho等人 [12] 改进了扩散训练过程，并优化了以下目标：

$$
\mathcal{L}\left( {\mathbf{x}}_{0}\right)  = \mathop{\sum }\limits_{{t = 1}}^{T}\underset{q\left( {{\mathbf{x}}_{t} \mid  {\mathbf{x}}_{0} \mid  {\mathbf{x}}^{H}}\right) }{\mathbb{E}}{\begin{Vmatrix}{\mu }_{\theta }\left( {\mathbf{x}}_{t},t \mid  {\mathbf{x}}^{H}\right)  - \widehat{\mu }\left( {\mathbf{x}}_{t},{\mathbf{x}}_{0} \mid  {\mathbf{x}}^{H}\right) \end{Vmatrix}}^{2}, \tag{4}
$$

where $\widehat{\mu }\left( {{\mathbf{x}}_{t},{\mathbf{x}}_{0} \mid  {\mathbf{x}}^{H}}\right)$ is the mean of the posterior $q\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{0},{\mathbf{x}}_{t}}\right)$ which is a closed from Gaussian, and ${\mu }_{\theta }\left( {{\mathbf{x}}_{t},t \mid  {\mathbf{x}}^{H}}\right)$ is the predicted mean of ${p}_{\theta }\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{t} \mid  {\mathbf{x}}^{H}}\right)$ computed by a neural network.

其中 $\widehat{\mu }\left( {{\mathbf{x}}_{t},{\mathbf{x}}_{0} \mid  {\mathbf{x}}^{H}}\right)$ 是后验 $q\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{0},{\mathbf{x}}_{t}}\right)$ 的均值，后验 $q\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{0},{\mathbf{x}}_{t}}\right)$ 是一个封闭形式的高斯分布，而 ${\mu }_{\theta }\left( {{\mathbf{x}}_{t},t \mid  {\mathbf{x}}^{H}}\right)$ 是由神经网络计算得到的 ${p}_{\theta }\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{t} \mid  {\mathbf{x}}^{H}}\right)$ 的预测均值。

## 4 Method

## 4 方法

We first describe the overall architecture of the proposed method in 4.1 Then we will introduce the strategy of building datasets in Section 4.2. The embedding-based retrieval mechanisms and reference-guided time series diffusion model are introduced in Section 4.3

我们首先在 4.1 节描述所提出方法的整体架构。然后，我们将在 4.2 节介绍构建数据集的策略。基于嵌入的检索机制和参考引导的时间序列扩散模型将在 4.3 节介绍。

### 4.1 Framework Overview

### 4.1 框架概述

Figure 2(a) shows the overall architecture of RATD. We built the entire process based on DiffWave [17], which combines the traditional diffusion model framework and a 2D transformer structure. In the forecasting task,RATD first retrieves motion sequences from the database base ${\mathcal{D}}^{R}$ based on the input sequence of historical events. These retrieved samples are then fed into the Reference-Modulated Attention (RMA) as references. In the RMA layer, we integrate the features of the input $\left\lbrack  {{\mathbf{x}}^{H},{\mathbf{x}}^{t}}\right\rbrack$ at time step $\mathrm{t}$ with side information ${\mathcal{I}}_{s}$ and the references ${\mathbf{x}}^{R}$ . Through this integration, the references guide the generation process. We will introduce these processes in the following subsections.

图2(a)展示了RATD（基于检索的注意力扩散模型，Retrieval-based Attention Diffusion model）的整体架构。我们基于DiffWave [17]构建了整个流程，该模型结合了传统的扩散模型框架和二维变压器（Transformer）结构。在预测任务中，RATD首先根据历史事件的输入序列从数据库${\mathcal{D}}^{R}$中检索运动序列。然后，将这些检索到的样本作为参考输入到参考调制注意力（RMA，Reference-Modulated Attention）模块中。在RMA层，我们将时间步$\mathrm{t}$的输入$\left\lbrack  {{\mathbf{x}}^{H},{\mathbf{x}}^{t}}\right\rbrack$的特征与辅助信息${\mathcal{I}}_{s}$和参考信息${\mathbf{x}}^{R}$进行整合。通过这种整合，参考信息引导生成过程。我们将在以下小节中介绍这些过程。

### 4.2 Constructing Retrieval Database for Time Series

### 4.2 构建时间序列的检索数据库

Before retrieval, it is necessary to construct a proper database. We propose a strategy for constructing databases from time series datasets with different characteristics. Some time series datasets are size-insufficient and are difficult to annotate with a single category label (e.g., electricity time series), while some datasets contain complete category labels but exhibit a significant degree of class imbalance (e.g., medical time series). We use two different definitions of databases for these two different types of datasets. For the first definition,the entire training set is directly defined as the database ${\mathcal{D}}^{\mathcal{R}}$ :

在检索之前，有必要构建一个合适的数据库。我们提出了一种从具有不同特征的时间序列数据集中构建数据库的策略。一些时间序列数据集规模不足，且难以用单一类别标签进行标注（例如，电力时间序列），而一些数据集包含完整的类别标签，但存在显著的类别不平衡问题（例如，医疗时间序列）。针对这两种不同类型的数据集，我们使用两种不同的数据库定义。对于第一种定义，将整个训练集直接定义为数据库 ${\mathcal{D}}^{\mathcal{R}}$ ：

$$
{\mathcal{D}}^{\mathcal{R}} \mathrel{\text{:=}} \left\{  {{\mathbf{x}}_{i}\mid \forall {\mathbf{x}}_{i} \in  {\mathcal{D}}^{\text{train }}}\right\}   \tag{5}
$$

where ${\mathbf{x}}_{i} = \left\{  {{s}_{i},\cdots ,{s}_{i + l + h}}\right\}$ is the time series with length $l + h$ ,and ${\mathcal{D}}^{\text{train }}$ is the training set. In the second way, the subset containing samples from all categories in the dataset is defined as the database

其中 ${\mathbf{x}}_{i} = \left\{  {{s}_{i},\cdots ,{s}_{i + l + h}}\right\}$ 是长度为 $l + h$ 的时间序列， ${\mathcal{D}}^{\text{train }}$ 是训练集。第二种方式是，将数据集中包含所有类别的样本的子集定义为数据库

$$
{\mathcal{D}}^{{R}^{\prime }}\text{:}
$$

$$
{\mathcal{D}}^{{R}^{\prime }} = \left\{  {{\mathbf{x}}_{i}^{c},\cdots ,{\mathbf{x}}_{q}^{c} \mid  \forall c \in  \mathcal{C}}\right\}   \tag{6}
$$

<!-- Media -->

<!-- figureText: (a) Model Architecture -->

<img src="https://cdn.noedgeai.com/01957f6c-7a55-79ee-9cf5-c168c8a66c82_4.jpg?x=311&y=205&w=1180&h=305&r=0"/>

Figure 3: The structure of ${\mu }_{\theta }$ . (a) The main architecture of ${\mu }_{\theta }$ is the time series transformer structure that proved effective. (b) The structure of the proposed RMA. We integrate three different features through matrix multiplication.

图3：${\mu }_{\theta }$的结构。(a) ${\mu }_{\theta }$的主要架构是已被证明有效的时间序列Transformer结构。(b) 所提出的RMA（Retrieval Memory Attention，检索记忆注意力）的结构。我们通过矩阵乘法整合了三种不同的特征。

<!-- Media -->

where ${x}_{i}^{k}$ is the $i$ -th sample in the $k$ -th class of the training set,with a length of $l + h.\mathcal{C}$ is the category set of the original dataset. For brevity,we represent both databases as ${\mathcal{D}}^{R}$ .

其中${x}_{i}^{k}$是训练集中第$k$类的第$i$个样本，长度为$l + h.\mathcal{C}$，是原始数据集的类别集合。为简洁起见，我们将两个数据库都表示为${\mathcal{D}}^{R}$。

### 4.3 Retrieval-Augmented Time Series Diffusion

### 4.3 检索增强的时间序列扩散

Embedding-Based Retrieval Mechanism For time forecasting tasks, the ideal references $\left\{  {{s}_{i},\cdots ,{s}_{i + h}}\right\}$ would be samples where preceding $n$ points $\left\{  {{s}_{i - n},\cdots ,{s}_{i - 1}}\right\}$ is most relevant to the historical time series $\left\{  {{s}_{j},\cdots ,{s}_{j + n}}\right\}$ in the ${\mathcal{D}}^{R}$ . In our approach,the overall similarity between time series is of greater concern. We quantify the reference between time series using the distance between their embeddings. To ensure that embeddings can effectively represent the entire time series,pre-trained encoders ${E}_{\phi }$ are utilized. ${E}_{\phi }$ is trained on representation learning tasks,and the parameter set $\phi$ is frozen in our retrieval mechanism. For time series (with length $n + h$ ) in ${\mathcal{D}}^{R}$ , their first $n$ points are encoded,thus the ${\mathcal{D}}^{R}$ can be represented as ${\mathcal{D}}_{\text{emb }}^{R}$ :

基于嵌入的检索机制 对于时间预测任务而言，理想的参考样本 $\left\{  {{s}_{i},\cdots ,{s}_{i + h}}\right\}$ 应是其中前 $n$ 个点 $\left\{  {{s}_{i - n},\cdots ,{s}_{i - 1}}\right\}$ 与 ${\mathcal{D}}^{R}$ 中的历史时间序列 $\left\{  {{s}_{j},\cdots ,{s}_{j + n}}\right\}$ 最为相关的样本。在我们的方法中，更关注时间序列之间的整体相似性。我们使用时间序列嵌入之间的距离来量化它们之间的参考关系。为确保嵌入能够有效表示整个时间序列，我们利用了预训练的编码器 ${E}_{\phi }$。${E}_{\phi }$ 是在表征学习任务上进行训练的，并且在我们的检索机制中，参数集 $\phi$ 是固定的。对于 ${\mathcal{D}}^{R}$ 中的时间序列（长度为 $n + h$），对它们的前 $n$ 个点进行编码，因此 ${\mathcal{D}}^{R}$ 可以表示为 ${\mathcal{D}}_{\text{emb }}^{R}$：

$$
{\mathcal{D}}_{\mathrm{{emb}}}^{R} = \left\{  {\left\{  {i,{E}_{\phi }\left( {\mathbf{x}}_{\left\lbrack  0 : n\right\rbrack  }^{i}\right) ,{\mathbf{x}}_{\left\lbrack  n : n + h\right\rbrack  }^{i}}\right\}   \mid  \forall {\mathbf{x}}^{i} \in  {\mathcal{D}}^{R}}\right\}   \tag{7}
$$

where $\left\lbrack  {p : q}\right\rbrack$ refers to the subsequence formed by the $p$ -th point to the $q$ -th point in the time series. The embedding corresponding to the historical time series can be represented as ${\mathbf{v}}^{H} = {E}_{\phi }\left( {\mathbf{x}}^{H}\right)$ . We calculate the distances between ${\mathbf{v}}^{H}$ and all embeddings in ${\mathcal{D}}_{\text{emb }}^{R}$ and retrieve the references corresponding to the $k$ smallest distances. This process can be expressed as:

其中 $\left\lbrack  {p : q}\right\rbrack$ 指的是时间序列中从第 $p$ 个点到第 $q$ 个点所形成的子序列。与历史时间序列对应的嵌入可以表示为 ${\mathbf{v}}^{H} = {E}_{\phi }\left( {\mathbf{x}}^{H}\right)$。我们计算 ${\mathbf{v}}^{H}$ 与 ${\mathcal{D}}_{\text{emb }}^{R}$ 中所有嵌入之间的距离，并检索与 $k$ 个最小距离对应的参考。这个过程可以表示为：

$$
\operatorname{index}\left( {\mathbf{v}}^{H}\right)  = \underset{{\mathbf{x}}^{i} \in  {\mathcal{D}}_{\text{emb }}^{R}}{\overset{k}{\arg \min }}{\begin{Vmatrix}{\mathbf{v}}^{H} - {E}_{\phi }\left( {\mathbf{x}}_{\left\lbrack  0 : n\right\rbrack  }^{i}\right) \end{Vmatrix}}^{2} \tag{8}
$$

$$
{\mathbf{x}}^{R} = \left\{  {{\mathbf{x}}_{\left\lbrack  n : n + h\right\rbrack  }^{j} \mid  \forall j \in  \operatorname{index}\left( {\mathbf{v}}^{H}\right) }\right\}  
$$

where index(-) represents retrieved index given ${\mathbf{v}}_{\mathcal{D}}$ . Thus,we obtain a subset ${\mathbf{x}}^{R}$ of ${\mathcal{D}}^{R}$ based on a query ${\mathbf{x}}^{H}$ ,i.e. ${\zeta }_{k} : {\mathbf{x}}^{H},{\mathcal{D}}^{R} \rightarrow  {\mathbf{x}}^{R}$ ,where $\left| {\mathbf{x}}^{R}\right|  = k$ .

其中index(-)表示给定${\mathbf{v}}_{\mathcal{D}}$时检索到的索引。因此，我们基于查询${\mathbf{x}}^{H}$得到${\mathcal{D}}^{R}$的一个子集${\mathbf{x}}^{R}$，即${\zeta }_{k} : {\mathbf{x}}^{H},{\mathcal{D}}^{R} \rightarrow  {\mathbf{x}}^{R}$，其中$\left| {\mathbf{x}}^{R}\right|  = k$。

Reference-Guided Time Series Diffusion Model In this section, we will introduce our reference-guided time series diffusion model. In the diffusion process, the forward process is identical to the traditional diffusion process, as shown in Equation (2). Following [34, 12, 35] the objective of the reverse process is to infer the posterior distribution $p\left( {{\mathbf{z}}^{\text{tar }} \mid  {\mathbf{z}}^{c}}\right)$ through the subsequent expression:

参考引导的时间序列扩散模型 在本节中，我们将介绍我们的参考引导时间序列扩散模型。在扩散过程中，前向过程与传统扩散过程相同，如方程(2)所示。遵循文献[34, 12, 35]，反向过程的目标是通过以下表达式推断后验分布$p\left( {{\mathbf{z}}^{\text{tar }} \mid  {\mathbf{z}}^{c}}\right)$：

$$
p\left( {\mathbf{x} \mid  {\mathbf{x}}^{H}}\right)  = \int p\left( {{\mathbf{x}}_{T} \mid  {\mathbf{x}}^{H}}\right) \mathop{\prod }\limits_{{t = 1}}^{T}{p}_{\theta }\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{t},{\mathbf{x}}^{H},{\mathbf{x}}^{R}}\right) \mathcal{D}{\mathbf{x}}_{1 : T}, \tag{9}
$$

where $p\left( {{\mathbf{x}}_{T} \mid  {\mathbf{x}}^{H}}\right)  \approx  \mathcal{N}\left( {{\mathbf{x}}_{T} \mid  {\mathbf{x}}^{H},\mathbf{I}}\right) ,{p}_{\theta }\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{t},{\mathbf{x}}^{H},{\mathbf{x}}^{R}}\right)$ is the reverse transition kernel from ${\mathbf{x}}_{t}$ to ${x}_{t - 1}$ with a learnable parameter $\theta$ . Following most of the literature in the diffusion model,we adopt the assumption:

其中 $p\left( {{\mathbf{x}}_{T} \mid  {\mathbf{x}}^{H}}\right)  \approx  \mathcal{N}\left( {{\mathbf{x}}_{T} \mid  {\mathbf{x}}^{H},\mathbf{I}}\right) ,{p}_{\theta }\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{t},{\mathbf{x}}^{H},{\mathbf{x}}^{R}}\right)$ 是从 ${\mathbf{x}}_{t}$ 到 ${x}_{t - 1}$ 的反向转移核，具有可学习参数 $\theta$。遵循扩散模型领域的大多数文献，我们采用如下假设：

$$
{p}_{\theta }\left( {{\mathbf{x}}_{t - 1} \mid  {\mathbf{x}}_{t},\mathbf{x}}\right)  = \mathcal{N}\left( {{\mathbf{x}}_{t - 1};{\mu }_{\theta }\left( {{\mathbf{x}}_{t},{\mathbf{x}}^{H},{\mathbf{x}}^{R},t}\right) ,{\sum }_{\theta }\left( {{\mathbf{x}}_{t},{\mathbf{x}}^{H},{\mathbf{x}}^{R},t}\right) }\right)  \tag{10}
$$

where ${\mu }_{\theta }$ is a deep neural network with parameter $\theta$ . After similar computations as those in [12], $\left. {{\sum }_{\theta }\left( {{\mathbf{x}}_{t},{\mathbf{x}}^{H},{\mathbf{x}}^{R},t}\right) }\right)$ in the backward process is approximated as fixed. In other words,we can achieve reference-guided denoising by designing a rational and robust ${\mu }_{\theta }$ .

其中 ${\mu }_{\theta }$ 是一个具有参数 $\theta$ 的深度神经网络。经过与文献 [12] 中类似的计算，反向过程中的 $\left. {{\sum }_{\theta }\left( {{\mathbf{x}}_{t},{\mathbf{x}}^{H},{\mathbf{x}}^{R},t}\right) }\right)$ 可近似为固定值。换句话说，我们可以通过设计一个合理且鲁棒的 ${\mu }_{\theta }$ 来实现参考引导去噪。

Denoising Network Architecture Similar to DiffWave [17] and CSDI [36], our pipeline is constructed on the foundation of transformer layers, as shown in Figure 3. However, the existing framework cannot effectively utilize the reference as guidance. Considering attention modules to integrate the ${\mathbf{x}}^{R}$ and ${\mathbf{x}}_{t}$ as a reasonable intuition,we propose a novel module called Reference Modulated Attention (RMA). Unlike normal attention modules, we realize the fusion of three features in RMA: the current time series feature, the side feature, and the reference feature. To be specific, RMA was set at the beginning of each residual module Figure 3 . We use 1D-CNN to extract features from the input ${\mathbf{x}}_{t}$ ,references ${\mathbf{x}}^{R}$ ,and side information. Notably,we concatenate all references together for feature extraction. Side information consists of two parts, representing the correlation between variables and time steps in the current time series dataset Appendix B. We adjust the dimensions of these three features with linear layers and fuse them through matrix dot products. Similar to text-image diffusion models [29], RMA can effectively utilize reference information to guide the denoising process, while appropriate parameter settings prevent the results from overly depending on the reference.

去噪网络架构与DiffWave [17]和CSDI [36]类似，我们的流程构建在Transformer层的基础上，如图3所示。然而，现有框架无法有效地利用参考信息作为引导。考虑到使用注意力模块将${\mathbf{x}}^{R}$和${\mathbf{x}}_{t}$进行整合是一种合理的思路，我们提出了一种名为参考调制注意力（Reference Modulated Attention，RMA）的新型模块。与普通的注意力模块不同，我们在RMA中实现了三种特征的融合：当前时间序列特征、辅助特征和参考特征。具体而言，RMA设置在每个残差模块的起始位置（图3）。我们使用一维卷积神经网络（1D - CNN）从输入${\mathbf{x}}_{t}$、参考信息${\mathbf{x}}^{R}$和辅助信息中提取特征。值得注意的是，我们将所有参考信息拼接在一起进行特征提取。辅助信息由两部分组成，分别表示当前时间序列数据集中变量之间以及时间步之间的相关性（附录B）。我们使用线性层调整这三种特征的维度，并通过矩阵点积将它们融合。与文本 - 图像扩散模型[29]类似，RMA可以有效地利用参考信息来引导去噪过程，同时适当的参数设置可以防止结果过度依赖参考信息。

Training Procedure To train RATD (i.e., optimize the evidence lower bound induced by RATD), we use the same objective function as previous work. The loss at time step $t - 1$ are defined as follows respectively:

训练过程 为了训练RATD（即优化由RATD导出的证据下界），我们使用与先前工作相同的目标函数。时间步$t - 1$的损失分别定义如下：

$$
{L}_{t - 1}^{\left( x\right) } = \frac{1}{2{\widetilde{\beta }}_{t}^{2}}{\begin{Vmatrix}{\mu }_{\theta }\left( {\mathbf{x}}_{t},{\widehat{\mathbf{x}}}_{0}\right)  - \widehat{\mu }\left( {\mathbf{x}}_{t},{\widehat{\mathbf{x}}}_{0}\right) \end{Vmatrix}}^{2} \tag{11}
$$

$$
 = {\gamma }_{t}\begin{Vmatrix}{{\mathbf{x}}_{0} - {\widehat{\mathbf{x}}}_{0}}\end{Vmatrix}
$$

where ${\widehat{\mathbf{x}}}_{0}$ are predicted from ${\mathbf{x}}_{t}$ ,and ${\gamma }_{t} = \frac{{\bar{\alpha }}_{t - 1}{\beta }_{t}^{2}}{2{\widetilde{\beta }}_{t}^{2}{\left( 1 - {\bar{\alpha }}_{t}\right) }^{2}}$ are hyperparameters in diffusion process. We summarize the training procedure of RATD in Algorithm 1 and highlight the differences from the conventional models, in cyan. The process of sampling is shown in Appendix A.

其中${\widehat{\mathbf{x}}}_{0}$是从${\mathbf{x}}_{t}$预测得到的，${\gamma }_{t} = \frac{{\bar{\alpha }}_{t - 1}{\beta }_{t}^{2}}{2{\widetilde{\beta }}_{t}^{2}{\left( 1 - {\bar{\alpha }}_{t}\right) }^{2}}$是扩散过程中的超参数。我们在算法1中总结了RATD的训练过程，并以青色突出显示与传统模型的差异。采样过程见附录A。

<!-- Media -->

Algorithm 1 Training Procedure of RATD

算法1 RATD的训练过程

---

Require: Time series dataset ${\mathcal{D}}^{\text{train }}$ ,neural network ${\mu }_{\theta }$ ,diffusion step $T$ ,external database ${\mathcal{D}}^{R}$ ,

			pre-trained encoder ${E}_{\phi }$ ,number of references $k$

	1: Retrieve references with top- $k$ high similarity from ${\mathcal{D}}^{R}$ using $E$ to obtain ${\mathbf{x}}^{R}$ as described in

			Section 4.3

			while ${\phi }_{\theta }$ not converge do

					Sample diffusion time $t \in  \mathcal{U}\left( {0,\ldots ,T}\right)$

					Compute the side feature ${\mathcal{I}}_{s}$

					Perturb ${\mathbf{x}}_{0}$ to obtain ${\mathbf{x}}_{t}$

					Predict ${\widehat{\mathbf{x}}}_{0}$ from ${\mathbf{x}}_{t},{\mathcal{I}}_{s}$ and ${\mathbf{x}}^{R}$ (Equation (10))

					Compute loss $L$ with ${\widehat{\mathbf{x}}}_{0}$ and ${\mathbf{x}}_{0}$ (Equation (11))

					Update $\theta$ by minimizing $L$

			end while

---

<!-- Media -->

## 5 Experiments

## 5 实验

### 5.1 Experimental Setup

### 5.1 实验设置

Datasets Following previous work [45, 38, 8, 30], experiments are performed on four popular real-world time series datasets: (1) Electricity, which includes the hourly electricity consumption data from 321 clients over two years.; (2) Wind [20], which contains wind power records from 2020-2021. (3) Exchange [18], which describes the daily exchange rates of eight countries (Australia, British,Canada,Switzerland,China,Japan,New Zealand,and Singapore); (4) Weather ${}^{ \dagger  }$ ,which documents 21 meteorological indicators at 10-minute intervals spanning from 2020 to 2021.; Besides, we also applied our method to a large ECG time series dataset: MIMIC-IV-ECG [14]. The MIMIC-IV-ECG dataset contains clinical electrocardiogram data from over 190,000 patients and 450,000 hospitalizations at Beth Israel Deaconess Medical Center (BIDMC).

数据集 遵循先前的研究 [45, 38, 8, 30]，实验在四个流行的现实世界时间序列数据集上进行：(1) 电力数据集（Electricity），其中包含 321 个客户两年内每小时的电力消耗数据；(2) 风力数据集（Wind） [20]，包含 2020 - 2021 年的风力发电记录；(3) 汇率数据集（Exchange） [18]，描述了八个国家（澳大利亚、英国、加拿大、瑞士、中国、日本、新西兰和新加坡）的每日汇率；(4) 气象数据集（Weather） ${}^{ \dagger  }$，记录了 2020 年至 2021 年期间每 10 分钟的 21 个气象指标。此外，我们还将我们的方法应用于一个大型心电图时间序列数据集：MIMIC - IV - 心电图数据集（MIMIC - IV - ECG） [14]。MIMIC - IV - ECG 数据集包含贝斯以色列女执事医疗中心（Beth Israel Deaconess Medical Center，BIDMC）超过 190,000 名患者和 450,000 次住院的临床心电图数据。

---

<!-- Footnote -->

*https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014

*https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014

${}^{ \dagger  }$ https://www.bgc-jena.mpg.de/wetter/

${}^{ \dagger  }$ https://www.bgc-jena.mpg.de/wetter/

<!-- Footnote -->

---

<!-- Media -->

Table 1: Performance comparisons on four real-world datasets in terms of MSE, MAE, and CRPS. The best is in bold, while the second best is underlined.

表1：在均方误差（MSE）、平均绝对误差（MAE）和连续 ranked 概率得分（CRPS）方面，四个真实世界数据集的性能比较。最佳结果用粗体表示，次佳结果用下划线表示。

<table><tr><td>Dataset</td><td colspan="3">Exchange</td><td colspan="3">Wind</td><td colspan="3">Electricity</td><td colspan="3">Weather</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td></tr><tr><td>RATD (ours)</td><td>0.013</td><td>0.073</td><td>0.339</td><td>0.784</td><td>0.579</td><td>0.673</td><td>0.151</td><td>0.246</td><td>0.373</td><td>0.281</td><td>0.293</td><td>0.301</td></tr><tr><td>TimeDiff</td><td>0.018</td><td>0.091</td><td>0.589</td><td>0.896</td><td>0.687</td><td>0.917</td><td>0.193</td><td>0.305</td><td>0.490</td><td>0.327</td><td>0.312</td><td>0.410</td></tr><tr><td>CSDI</td><td>0.077</td><td>0.194</td><td>0.397</td><td>1.066</td><td>0.741</td><td>0.941</td><td>0.379</td><td>0.579</td><td>0.480</td><td>0.356</td><td>0.374</td><td>0.354</td></tr><tr><td>mr-Diff</td><td>0.016</td><td>0.082</td><td>0.397</td><td>0.881</td><td>0.675</td><td>0.881</td><td>0.173</td><td>0.258</td><td>0.429</td><td>0.296</td><td>0.324</td><td>0.347</td></tr><tr><td>${\mathrm{D}}_{3}$ VAE</td><td>0.200</td><td>0.301</td><td>0.401</td><td>1.118</td><td>0.779</td><td>0.979</td><td>0.286</td><td>0.372</td><td>0.389</td><td>0.315</td><td>0.380</td><td>0.381</td></tr><tr><td>Fedformer</td><td>0.133</td><td>0.233</td><td>0.631</td><td>1.113</td><td>0.762</td><td>1.235</td><td>0.238</td><td>0.341</td><td>0.561</td><td>0.342</td><td>0.347</td><td>0.319</td></tr><tr><td>FreTS</td><td>0.039</td><td>0.140</td><td>0.440</td><td>1.004</td><td>0.703</td><td>0.943</td><td>0.269</td><td>0.371</td><td>0.634</td><td>0.351</td><td>0.354</td><td>0.391</td></tr><tr><td>FiLM</td><td>0.016</td><td>0.079</td><td>0.349</td><td>0.984</td><td>0.717</td><td>0.798</td><td>0.210</td><td>0.320</td><td>0.671</td><td>0.327</td><td>0.336</td><td>0.556</td></tr><tr><td>iTransformer</td><td>0.016</td><td>0.074</td><td>0.343</td><td>0.932</td><td>0.676</td><td>0.811</td><td>0.192</td><td>0.262</td><td>0.402</td><td>0.358</td><td>0.401</td><td>0.318</td></tr><tr><td>Autoformer</td><td>0.056</td><td>0.167</td><td>0.769</td><td>1.083</td><td>0.756</td><td>1.201</td><td>1.026</td><td>0.313</td><td>0.602</td><td>0.360</td><td>0.354</td><td>0.754</td></tr><tr><td>Pyraformer</td><td>0.032</td><td>0.112</td><td>0.532</td><td>1.061</td><td>0.735</td><td>0.994</td><td>0.273</td><td>0.379</td><td>0.732</td><td>0.394</td><td>0.385</td><td>0.485</td></tr><tr><td>Informer</td><td>0.073</td><td>0.192</td><td>0.631</td><td>1.168</td><td>0.772</td><td>1.065</td><td>0.292</td><td>0.383</td><td>0.749</td><td>0.385</td><td>0.364</td><td>0.821</td></tr><tr><td>PatchTST</td><td>0.047</td><td>0.153</td><td>0.629</td><td>1.001</td><td>$\underline{0.672}$</td><td>1.026</td><td>0.225</td><td>0.394</td><td>0.801</td><td>0.782</td><td>0.670</td><td>0.370</td></tr><tr><td>SCINet</td><td>0.038</td><td>0.137</td><td>0.624</td><td>1.055</td><td>0.732</td><td>0.997</td><td>0.171</td><td>0.280</td><td>0.499</td><td>0.329</td><td>0.344</td><td>0.814</td></tr><tr><td>DLinear</td><td>0.022</td><td>0.102</td><td>0.538</td><td>0.899</td><td>0.686</td><td>0.957</td><td>0.215</td><td>0.336</td><td>0.527</td><td>0.488</td><td>0.444</td><td>0.791</td></tr><tr><td>NLinear</td><td>0.019</td><td>0.091</td><td>0.481</td><td>0.989</td><td>0.706</td><td>0.974</td><td>0.147</td><td>0.239</td><td>0.419</td><td>0.369</td><td>0.328</td><td>0.738</td></tr><tr><td>TimesNet</td><td>0.023</td><td>0.120</td><td>0.520</td><td>0.982</td><td>0.771</td><td>1.001</td><td>0.141</td><td>0.361</td><td>0.403</td><td>0.313</td><td>0.364</td><td>0.491</td></tr><tr><td>NBeats</td><td>$\underline{0.016}$</td><td>0.081</td><td>0.399</td><td>1.069</td><td>0.741</td><td>0.981</td><td>0.269</td><td>0.370</td><td>0.697</td><td>0.744</td><td>0.420</td><td>0.871</td></tr></table>

<table><tbody><tr><td>数据集</td><td colspan="3">交换</td><td colspan="3">风</td><td colspan="3">电力</td><td colspan="3">天气</td></tr><tr><td>指标</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>连续排名概率得分（Continuous Ranked Probability Score，CRPS）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>连续排名概率得分（Continuous Ranked Probability Score，CRPS）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>连续排名概率得分（Continuous Ranked Probability Score，CRPS）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>连续排名概率得分（Continuous Ranked Probability Score，CRPS）</td></tr><tr><td>基于注意力机制的时间序列数据修复方法（Recurrent Attention-based Time-series Data imputation，RATD，我们的方法）</td><td>0.013</td><td>0.073</td><td>0.339</td><td>0.784</td><td>0.579</td><td>0.673</td><td>0.151</td><td>0.246</td><td>0.373</td><td>0.281</td><td>0.293</td><td>0.301</td></tr><tr><td>时间差（Time Difference）</td><td>0.018</td><td>0.091</td><td>0.589</td><td>0.896</td><td>0.687</td><td>0.917</td><td>0.193</td><td>0.305</td><td>0.490</td><td>0.327</td><td>0.312</td><td>0.410</td></tr><tr><td>基于条件随机变量的时间序列插补方法（Conditional Random Fields for Time-series Imputation，CSDI）</td><td>0.077</td><td>0.194</td><td>0.397</td><td>1.066</td><td>0.741</td><td>0.941</td><td>0.379</td><td>0.579</td><td>0.480</td><td>0.356</td><td>0.374</td><td>0.354</td></tr><tr><td>差分先生（mr-Diff）</td><td>0.016</td><td>0.082</td><td>0.397</td><td>0.881</td><td>0.675</td><td>0.881</td><td>0.173</td><td>0.258</td><td>0.429</td><td>0.296</td><td>0.324</td><td>0.347</td></tr><tr><td>${\mathrm{D}}_{3}$ 变分自编码器（VAE）</td><td>0.200</td><td>0.301</td><td>0.401</td><td>1.118</td><td>0.779</td><td>0.979</td><td>0.286</td><td>0.372</td><td>0.389</td><td>0.315</td><td>0.380</td><td>0.381</td></tr><tr><td>联邦变压器（Fedformer）</td><td>0.133</td><td>0.233</td><td>0.631</td><td>1.113</td><td>0.762</td><td>1.235</td><td>0.238</td><td>0.341</td><td>0.561</td><td>0.342</td><td>0.347</td><td>0.319</td></tr><tr><td>频率时间序列（FreTS）</td><td>0.039</td><td>0.140</td><td>0.440</td><td>1.004</td><td>0.703</td><td>0.943</td><td>0.269</td><td>0.371</td><td>0.634</td><td>0.351</td><td>0.354</td><td>0.391</td></tr><tr><td>特征线性调制（FiLM）</td><td>0.016</td><td>0.079</td><td>0.349</td><td>0.984</td><td>0.717</td><td>0.798</td><td>0.210</td><td>0.320</td><td>0.671</td><td>0.327</td><td>0.336</td><td>0.556</td></tr><tr><td>智能变压器（iTransformer）</td><td>0.016</td><td>0.074</td><td>0.343</td><td>0.932</td><td>0.676</td><td>0.811</td><td>0.192</td><td>0.262</td><td>0.402</td><td>0.358</td><td>0.401</td><td>0.318</td></tr><tr><td>自动变换器（Autoformer）</td><td>0.056</td><td>0.167</td><td>0.769</td><td>1.083</td><td>0.756</td><td>1.201</td><td>1.026</td><td>0.313</td><td>0.602</td><td>0.360</td><td>0.354</td><td>0.754</td></tr><tr><td>金字塔变换器（Pyraformer）</td><td>0.032</td><td>0.112</td><td>0.532</td><td>1.061</td><td>0.735</td><td>0.994</td><td>0.273</td><td>0.379</td><td>0.732</td><td>0.394</td><td>0.385</td><td>0.485</td></tr><tr><td>信息器（Informer）</td><td>0.073</td><td>0.192</td><td>0.631</td><td>1.168</td><td>0.772</td><td>1.065</td><td>0.292</td><td>0.383</td><td>0.749</td><td>0.385</td><td>0.364</td><td>0.821</td></tr><tr><td>补丁时间序列变换器（PatchTST）</td><td>0.047</td><td>0.153</td><td>0.629</td><td>1.001</td><td>$\underline{0.672}$</td><td>1.026</td><td>0.225</td><td>0.394</td><td>0.801</td><td>0.782</td><td>0.670</td><td>0.370</td></tr><tr><td>自校正网络（SCINet）</td><td>0.038</td><td>0.137</td><td>0.624</td><td>1.055</td><td>0.732</td><td>0.997</td><td>0.171</td><td>0.280</td><td>0.499</td><td>0.329</td><td>0.344</td><td>0.814</td></tr><tr><td>深度线性模型（DLinear）</td><td>0.022</td><td>0.102</td><td>0.538</td><td>0.899</td><td>0.686</td><td>0.957</td><td>0.215</td><td>0.336</td><td>0.527</td><td>0.488</td><td>0.444</td><td>0.791</td></tr><tr><td>非线性（NLinear）</td><td>0.019</td><td>0.091</td><td>0.481</td><td>0.989</td><td>0.706</td><td>0.974</td><td>0.147</td><td>0.239</td><td>0.419</td><td>0.369</td><td>0.328</td><td>0.738</td></tr><tr><td>时间网络（TimesNet）</td><td>0.023</td><td>0.120</td><td>0.520</td><td>0.982</td><td>0.771</td><td>1.001</td><td>0.141</td><td>0.361</td><td>0.403</td><td>0.313</td><td>0.364</td><td>0.491</td></tr><tr><td>N节拍（NBeats）</td><td>$\underline{0.016}$</td><td>0.081</td><td>0.399</td><td>1.069</td><td>0.741</td><td>0.981</td><td>0.269</td><td>0.370</td><td>0.697</td><td>0.744</td><td>0.420</td><td>0.871</td></tr></tbody></table>

<!-- Media -->

Baseline Methods To comprehensively demonstrate the effectiveness of our method, we compare RATD with four kinds of time series forecasting methods. Our baselines include (1) Time series diffusion models,including CSDI [36], mr-Diff [31], D ${}^{3}$ VAE [20], TimeDiff [30]; (2) Recent time series forecasting methods with frequency information, including FiLM [46], Fedformer [47] and FreTS [41] ; (3) Time series transformers, including PatchTST [25], Autoformer [38], Pyraformer [22], Informer [45] and iTransformer [23]; (4) Other popular methods, including TimesNet [39], SciNet [21], Nlinear [43], DLinear [43] and NBeats [26].

基线方法 为了全面展示我们方法的有效性，我们将RATD与四种时间序列预测方法进行了比较。我们的基线方法包括：（1）时间序列扩散模型，包括CSDI [36]、mr - Diff [31]、D ${}^{3}$ VAE [20]、TimeDiff [30]；（2）近期包含频率信息的时间序列预测方法，包括FiLM [46]、Fedformer [47]和FreTS [41]；（3）时间序列Transformer模型，包括PatchTST [25]、Autoformer [38]、Pyraformer [22]、Informer [45]和iTransformer [23]；（4）其他流行的方法，包括TimesNet [39]、SciNet [21]、Nlinear [43]、DLinear [43]和NBeats [26]。

Evaluation Metric To comprehensively assess our proposed methodology, our experiment employs three metrics: (1) Probabilistic forecasting metrics: Continuous Ranked Probability Score (CRPS) on each time series dimension [24]. (2) Distance metrics: Mean Squared Error (MSE), and Mean Average Error(MAE) are employed to measure the distance between predictions and ground truths.

评估指标 为了全面评估我们提出的方法，我们的实验采用了三个指标：（1）概率预测指标：每个时间序列维度上的连续排序概率得分（CRPS）[24]。（2）距离指标：均方误差（MSE）和平均绝对误差（MAE）用于衡量预测值与真实值之间的距离。

Implementation Details The length of the historical time series was 168, and the prediction lengths were (96, 192, 336), with results averaged. All experiments were conducted on an Nvidia RTX A6000 GPU with 40GB memory. During the experiments,the second strategy of conducting ${\mathcal{D}}^{R}$ was employed for the MIMIC dataset, while the first strategy was utilized for the other four datasets. To reduce the training cost, we preprocessed the retrieval process by storing the reference indices of each sample in the training set in a dictionary. During the training on the diffusion model, we accessed this dictionary directly to avoid redundant retrieval processes. More details are shown in Appendix B.

实现细节 历史时间序列的长度为168，预测长度为（96、192、336），并对结果进行了平均。所有实验均在配备40GB内存的英伟达RTX A6000 GPU（Nvidia RTX A6000 GPU）上进行。在实验过程中，针对MIMIC数据集采用了执行${\mathcal{D}}^{R}$的第二种策略，而对其他四个数据集则采用了第一种策略。为降低训练成本，我们通过将训练集中每个样本的参考索引存储在字典中来预处理检索过程。在对扩散模型进行训练时，我们直接访问该字典以避免冗余的检索过程。更多细节见附录B。

### 5.2 Main Results

### 5.2 主要结果

Table 1 presents the primary results of our experiments on four daily datasets. Our approach surpasses existing time series diffusion models. Compared to other time series forecasting methods, our approach exhibits superior performance on three out of four datasets, with competitive performance on the remaining dataset. Notably, we achieve outstanding results on the wind dataset. Due to the lack of clear short-term periodicity (daily or hourly), some prediction tasks in this dataset are exceedingly challenging for other models. Retrieval-augmented mechanisms can effectively assist in addressing these challenging prediction tasks.

表1展示了我们在四个每日数据集上的实验主要结果。我们的方法超越了现有的时间序列扩散模型。与其他时间序列预测方法相比，我们的方法在四个数据集中的三个上表现更优，在剩余一个数据集上也具有竞争力。值得注意的是，我们在风力数据集上取得了出色的结果。由于缺乏明确的短期周期性（每日或每小时），该数据集中的一些预测任务对其他模型来说极具挑战性。检索增强机制可以有效协助解决这些具有挑战性的预测任务。

Figure 4 presents a case study randomly selected from our experiments on the wind dataset. We compare our prediction with iTransformer and two popular open-source time series diffusion models, CSDI and ${\mathrm{D}}_{3}$ VAE. Although CSDI and ${\mathrm{D}}_{3}$ VAE provide accurate predictions in the initial short-term period, their long-term predictions deviate significantly from the ground truth due to the lack of guidance. ITransformer captures rough trends and periodic patterns, yet our method offers higher-quality predictions than the others. Furthermore, through the comparison between the predicted

图4展示了从我们在风力数据集上的实验中随机选取的一个案例研究。我们将我们的预测结果与iTransformer以及两个流行的开源时间序列扩散模型CSDI和${\mathrm{D}}_{3}$ VAE进行了比较。虽然CSDI和${\mathrm{D}}_{3}$ VAE在初始短期阶段提供了准确的预测，但由于缺乏引导，它们的长期预测与真实值有显著偏差。iTransformer捕捉到了大致的趋势和周期性模式，但我们的方法比其他方法提供了更高质量的预测。此外，通过预测结果之间的比较

<!-- Media -->

<img src="https://cdn.noedgeai.com/01957f6c-7a55-79ee-9cf5-c168c8a66c82_7.jpg?x=329&y=208&w=1153&h=569&r=0"/>

Figure 4: Visualizations on wind by CSDI, ${\mathrm{D}}_{3}$ VAE,iTransformer and the proposed RATD (with reference).

图4：由CSDI（条件随机失活插值）、${\mathrm{D}}_{3}$变分自编码器（VAE）、iTransformer和所提出的RATD（带参考）对风进行的可视化。

<!-- Media -->

results and references in the figure, although references provide strong guidance, they do not explicitly substitute for the entire generated results. This further validates the rationality of our approach.

图中的结果和参考信息，尽管参考信息提供了有力的指导，但它们并不能明确替代整个生成结果。这进一步验证了我们方法的合理性。

Table 2 presents the testing results of our method on the MIMIC-IV-ECG dataset. We selected some powerful open-source methods as baselines for comparison. Our experiments are divided into two parts: in the first part, we evaluate the entire test set, while in the second part, we select rare cases (those accounting for less than $2\%$ of total cases) from the test set as a subset for evaluation. Prediction tasks in the second part are more challenging for deep models. In the first experiment, our method achieved results close to iTransformer, while in the second task, our model significantly outperformed other methods, demonstrating the effectiveness of our approach in addressing challenging tasks.

表2展示了我们的方法在MIMIC - IV - ECG数据集上的测试结果。我们选择了一些强大的开源方法作为基线进行比较。我们的实验分为两部分：第一部分，我们对整个测试集进行评估；第二部分，我们从测试集中选择罕见病例（占总病例数少于$2\%$的病例）作为子集进行评估。第二部分的预测任务对深度模型来说更具挑战性。在第一个实验中，我们的方法取得了与iTransformer相近的结果；而在第二个任务中，我们的模型显著优于其他方法，这证明了我们的方法在处理具有挑战性的任务时的有效性。

### 5.3 Model Analysis

### 5.3 模型分析

Influence of Retrieval Mechanism To investigate the impact of the retrieval augmentation mechanism on the generation process, we conducted an ablation study and presented the results in Table 3 The study addresses two questions: whether the retrieval augmentation mechanism is effective and which retrieval method is most effective. Firstly, we removed our retrieval augmentation mechanism from the RATD as a baseline. Besides, the model with random time series guidance is another baseline. The references retrieved by other methods have all positively impacted the prediction results. This suggests that reasonable references are highly effective in guiding the generation process.

检索机制的影响 为了研究检索增强机制对生成过程的影响，我们进行了一项消融研究，并将结果列于表3中。该研究解决了两个问题：检索增强机制是否有效，以及哪种检索方法最有效。首先，我们将检索增强机制从RATD（检索增强时间序列扩散模型，Retrieval Augmented Time-series Diffusion）中移除，作为一个基线。此外，采用随机时间序列引导的模型是另一个基线。通过其他方法检索到的参考信息均对预测结果产生了积极影响。这表明合理的参考信息在引导生成过程方面非常有效。

We also compared two different retrieval mechanisms: correlation-based retrieval and embedding-based retrieval. The first method directly retrieves the reference in the time domain (e.g., using Dynamic Time Warping (DTW) or Pearson correlation coefficient). Our approach adopts the second mechanism: retrieving references through the embedding of time series. From the results, the correlation-based methods are significantly inferior to the embedding-based methods. The former methods fail to capture the key features of the time series, making it difficult to retrieve the best references for forecasting. We also evaluate the embedding-based methods with various encoders for comparison. The comprehensive results show that methods with different encoders do not significantly differ. This indicates that different methods can all extract meaningful references, thereby producing similar improvements in results. TCN was utilized in our experiment because TCN strikes the best balance between computational cost and performance.

我们还比较了两种不同的检索机制：基于相关性的检索和基于嵌入的检索。第一种方法直接在时域中检索参考（例如，使用动态时间规整（Dynamic Time Warping，DTW）或皮尔逊相关系数）。我们的方法采用了第二种机制：通过时间序列的嵌入来检索参考。从结果来看，基于相关性的方法明显不如基于嵌入的方法。前者无法捕捉时间序列的关键特征，难以检索到用于预测的最佳参考。我们还使用各种编码器对基于嵌入的方法进行评估以作比较。综合结果表明，使用不同编码器的方法没有显著差异。这表明不同的方法都可以提取有意义的参考，从而在结果上产生相似的改进。我们的实验中使用了时间卷积网络（Temporal Convolutional Network，TCN），因为TCN在计算成本和性能之间达到了最佳平衡。

Effect of Retrieval Database We conducted an ablation study on two variables, $n$ and $k$ ,to investigate the influence of the retrieval database ${\mathcal{D}}^{R}$ in RATD,where $n$ represents the number of samples in each category of the database,and $k$ represents the number of reference exemplars. The results in Figure 5q can benefit the model in terms of prediction accuracy because a larger ${\mathcal{D}}^{R}$ brings higher diversity, thereby providing more details beneficial for prediction and enhancing the generation process. Simply increasing $\mathrm{k}$ does not show significant improvement,as utilizing more references may introduce more noise into the denoising process. In our experiment,the settings of $n$ and $k$ are 256 and 3 , respectively.

检索数据库的影响 我们对两个变量 $n$ 和 $k$ 进行了消融研究，以探究检索数据库 ${\mathcal{D}}^{R}$ 在 RATD（检索增强时间序列去噪，Retrieval-Augmented Time Series Denoising）中的影响，其中 $n$ 表示数据库中每个类别的样本数量，$k$ 表示参考样本的数量。图 5q 中的结果表明，较大的 ${\mathcal{D}}^{R}$ 能带来更高的多样性，从而为预测提供更多有益的细节并增强生成过程，因此在预测准确性方面对模型有益。单纯增加 $\mathrm{k}$ 并没有显著的改进，因为使用更多的参考样本可能会在去噪过程中引入更多的噪声。在我们的实验中，$n$ 和 $k$ 的设置分别为 256 和 3。

<!-- Media -->

Table 2: Performance comparisons on MIMIC datasets with popular time series forecasting methods. Here, "MIMIC-IV (All)" refers to the model's testing results on the complete test set, while "MIMIC(Rare)" indicates the model's testing results on a rare disease subset.

表 2：使用流行的时间序列预测方法在 MIMIC（多参数智能监测用于重症监护，Multiparameter Intelligent Monitoring in Intensive Care）数据集上的性能比较。这里，“MIMIC - IV（全部）”指的是模型在完整测试集上的测试结果，而“MIMIC（罕见病）”表示模型在罕见病子集上的测试结果。

<table><tr><td>Method</td><td colspan="3">iTransformer</td><td colspan="3">PatchTST</td><td colspan="3">TimesNet</td><td colspan="3">CSDI</td><td colspan="3">RATD</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td></tr><tr><td>MIMIC-IV (All)</td><td>0.174</td><td>0.263</td><td>0.299</td><td>0.219</td><td>0.301</td><td>0.307</td><td>0.193</td><td>0.311</td><td>0.310</td><td>0.268</td><td>0.331</td><td>0.369</td><td>0.172</td><td>0.270</td><td>0.293</td></tr><tr><td>MIMIC-IV (Rare)</td><td>0.423</td><td>0.315</td><td>0.379</td><td>0.483</td><td>0.379</td><td>0.407</td><td>0.627</td><td>0.359</td><td>0.464</td><td>0.499</td><td>0.359</td><td>$\underline{0.374}$</td><td>0.206</td><td>0.299</td><td>0.301</td></tr></table>

<table><tbody><tr><td>方法</td><td colspan="3">iTransformer（iTransformer）</td><td colspan="3">PatchTST（PatchTST）</td><td colspan="3">TimesNet（TimesNet）</td><td colspan="3">CSDI（CSDI）</td><td colspan="3">RATD（RATD）</td></tr><tr><td>指标</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>连续 ranked 概率得分（CRPS）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>连续 ranked 概率得分（CRPS）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>连续 ranked 概率得分（CRPS）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>连续 ranked 概率得分（CRPS）</td><td>均方误差（MSE）</td><td>平均绝对误差（MAE）</td><td>连续 ranked 概率得分（CRPS）</td></tr><tr><td>重症监护数据库 IV（全部）（MIMIC - IV (All)）</td><td>0.174</td><td>0.263</td><td>0.299</td><td>0.219</td><td>0.301</td><td>0.307</td><td>0.193</td><td>0.311</td><td>0.310</td><td>0.268</td><td>0.331</td><td>0.369</td><td>0.172</td><td>0.270</td><td>0.293</td></tr><tr><td>重症监护数据库 IV（罕见）（MIMIC - IV (Rare)）</td><td>0.423</td><td>0.315</td><td>0.379</td><td>0.483</td><td>0.379</td><td>0.407</td><td>0.627</td><td>0.359</td><td>0.464</td><td>0.499</td><td>0.359</td><td>$\underline{0.374}$</td><td>0.206</td><td>0.299</td><td>0.301</td></tr></tbody></table>

Table 3: Ablation study on different retrieval mechanisms. "-" means no references was utilized and "Random" means references are selected randomly. Others refer to what model we use for retrieval references.

表3：不同检索机制的消融研究。“-”表示未使用参考文献，“随机”表示随机选择参考文献。其他指的是我们用于检索参考文献的模型。

<table><tr><td>Dataset</td><td colspan="3">Exchange</td><td colspan="3">Wind</td><td colspan="3">Electricity</td><td colspan="3">Weather</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td></tr><tr><td>-</td><td>0.077</td><td>0.194</td><td>0.397</td><td>1.066</td><td>0.741</td><td>0.941</td><td>0.379</td><td>0.579</td><td>0.480</td><td>0.356</td><td>0.374</td><td>0.354</td></tr><tr><td>Random</td><td>0.153</td><td>0.203</td><td>0.599</td><td>1.593</td><td>0.903</td><td>0.996</td><td>0.471</td><td>0.639</td><td>0.701</td><td>0.431</td><td>0.473</td><td>0.461</td></tr><tr><td>DTW</td><td>0.075</td><td>0.195</td><td>0.403</td><td>1.073</td><td>0.791</td><td>0.942</td><td>0.357</td><td>0.564</td><td>0.449</td><td>0.361</td><td>0.375</td><td>0.356</td></tr><tr><td>Pearson</td><td>0.091</td><td>0.207</td><td>0.411</td><td>1.099</td><td>0.831</td><td>0.953</td><td>0.361</td><td>0.571</td><td>0.483</td><td>0.370</td><td>0.364</td><td>0.391</td></tr><tr><td>DLinear</td><td>0.022</td><td>0.081</td><td>0.361</td><td>0.941</td><td>0.735</td><td>0.895</td><td>0.159</td><td>0.250</td><td>0.390</td><td>0.297</td><td>0.304</td><td>0.332</td></tr><tr><td>Informer</td><td>0.019</td><td>0.078</td><td>0.371</td><td>0.841</td><td>0.645</td><td>0.861</td><td>0.170</td><td>0.263</td><td>0.411</td><td>0.291</td><td>0.305</td><td>0.330</td></tr><tr><td>TimesNet</td><td>0.013</td><td>0.074</td><td>0.341</td><td>0.781</td><td>0.572</td><td>0.669</td><td>0.167</td><td>0.263</td><td>0.397</td><td>0.286</td><td>0.295</td><td>0.311</td></tr><tr><td>TCN</td><td>0.013</td><td>0.073</td><td>0.339</td><td>$\underline{0.784}$</td><td>0.579</td><td>$\underline{0.673}$</td><td>0.161</td><td>0.256</td><td>0.391</td><td>0.281</td><td>0.293</td><td>0.313</td></tr></table>

<table><tbody><tr><td>数据集</td><td colspan="3">交换</td><td colspan="3">风</td><td colspan="3">电力</td><td colspan="3">天气</td></tr><tr><td>指标</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>复杂区域疼痛综合征（Complex Regional Pain Syndrome，CRPS）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>复杂区域疼痛综合征（Complex Regional Pain Syndrome，CRPS）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>复杂区域疼痛综合征（Complex Regional Pain Syndrome，CRPS）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>复杂区域疼痛综合征（Complex Regional Pain Syndrome，CRPS）</td></tr><tr><td>-</td><td>0.077</td><td>0.194</td><td>0.397</td><td>1.066</td><td>0.741</td><td>0.941</td><td>0.379</td><td>0.579</td><td>0.480</td><td>0.356</td><td>0.374</td><td>0.354</td></tr><tr><td>随机</td><td>0.153</td><td>0.203</td><td>0.599</td><td>1.593</td><td>0.903</td><td>0.996</td><td>0.471</td><td>0.639</td><td>0.701</td><td>0.431</td><td>0.473</td><td>0.461</td></tr><tr><td>动态时间规整（Dynamic Time Warping，DTW）</td><td>0.075</td><td>0.195</td><td>0.403</td><td>1.073</td><td>0.791</td><td>0.942</td><td>0.357</td><td>0.564</td><td>0.449</td><td>0.361</td><td>0.375</td><td>0.356</td></tr><tr><td>皮尔逊（Pearson）</td><td>0.091</td><td>0.207</td><td>0.411</td><td>1.099</td><td>0.831</td><td>0.953</td><td>0.361</td><td>0.571</td><td>0.483</td><td>0.370</td><td>0.364</td><td>0.391</td></tr><tr><td>D线性（DLinear）</td><td>0.022</td><td>0.081</td><td>0.361</td><td>0.941</td><td>0.735</td><td>0.895</td><td>0.159</td><td>0.250</td><td>0.390</td><td>0.297</td><td>0.304</td><td>0.332</td></tr><tr><td>信息者（Informer）</td><td>0.019</td><td>0.078</td><td>0.371</td><td>0.841</td><td>0.645</td><td>0.861</td><td>0.170</td><td>0.263</td><td>0.411</td><td>0.291</td><td>0.305</td><td>0.330</td></tr><tr><td>时间网络（TimesNet）</td><td>0.013</td><td>0.074</td><td>0.341</td><td>0.781</td><td>0.572</td><td>0.669</td><td>0.167</td><td>0.263</td><td>0.397</td><td>0.286</td><td>0.295</td><td>0.311</td></tr><tr><td>时间卷积网络（TCN）</td><td>0.013</td><td>0.073</td><td>0.339</td><td>$\underline{0.784}$</td><td>0.579</td><td>$\underline{0.673}$</td><td>0.161</td><td>0.256</td><td>0.391</td><td>0.281</td><td>0.293</td><td>0.313</td></tr></tbody></table>

<!-- Media -->

Inference Efficiency In this experiment, we evaluate the inference efficiency of the proposed RATD in comparison to other baseline time series diffusion models (TimeGrad, MG-TSD, SSSD). Figure 6 illustrates the inference time on the multivariate weather dataset with varying values of the prediction horizon(h). While our method introduces an additional retrieval module,the sampling efficiency of the RATD is not low due to the non-autoregressive transformer framework. It even slightly outperforms other baselines across all $h$ values. Notably,TimeGrad is observed to be the slowest, attributed to its utilization of auto-regressive decoding.

推理效率 在本实验中，我们将所提出的RATD（基于检索的时间序列扩散模型，Retrieval-based Autoencoder for Time-series Diffusion）的推理效率与其他基线时间序列扩散模型（TimeGrad、MG - TSD、SSSD）进行了评估。图6展示了在多变量气象数据集上，预测时域(h)取不同值时的推理时间。虽然我们的方法引入了一个额外的检索模块，但由于采用了非自回归变压器框架，RATD的采样效率并不低。在所有$h$值下，它甚至略微优于其他基线模型。值得注意的是，TimeGrad被观察到是最慢的，这归因于其使用了自回归解码。

<!-- Media -->

<img src="https://cdn.noedgeai.com/01957f6c-7a55-79ee-9cf5-c168c8a66c82_8.jpg?x=353&y=1377&w=1037&h=320&r=0"/>

Figure 5: The effect of hyper-parameter $n$ and $k$ . Figure 6: Inference time (ms) on the Electricity with different prediction horizon $h$

图5：超参数$n$和$k$的影响。图6：不同预测时域$h$下电力数据集的推理时间（毫秒）

<!-- Media -->

Effectiveness of Reference Modulated Attention To validate the effectiveness of the proposed RMA, we designed additional ablation experiments. In these experiments, we used the CSDI architecture as the baseline method and added extra fusion modules to compare the performance of these modules (linear layer, cross-attention layer, and RMA). The results are shown in the Table 4

参考调制注意力机制的有效性 为了验证所提出的RMA（参考调制注意力机制，Reference Modulated Attention）的有效性，我们设计了额外的消融实验。在这些实验中，我们使用CSDI架构作为基线方法，并添加了额外的融合模块，以比较这些模块（线性层、交叉注意力层和RMA）的性能。结果如表4所示

Through our experiments, we found that compared to the basic cross-attention-based approach, RMA can integrate an edge information matrix (representing correlations between time and feature dimensions) more effectively. The extra fusion is highly beneficial in experiments, guiding the model to capture relationships between different variables. In contrast, linear-based methods concatenate inputs and references initially, which prevents the direct extraction of meaningful information from references, resulting in comparatively modest performance.

通过我们的实验，我们发现与基于基本交叉注意力的方法相比，RMA（递归多注意力，Recursive Multi-Attention）能够更有效地整合边缘信息矩阵（表示时间和特征维度之间的相关性）。这种额外的融合在实验中非常有益，它能引导模型捕捉不同变量之间的关系。相比之下，基于线性的方法最初会将输入和参考信息进行拼接，这阻碍了从参考信息中直接提取有意义的信息，从而导致性能相对一般。

<!-- Media -->

Table 4: Performance comparison(MSE) between CSDI-based methods, CSDI represents the basic network framework, CSDI+Linear denotes the approach where inputs and references are concatenated via a linear layer and fed into the network together, CSDI+CrossAttention signifies the use of cross attention to fuse features from inputs and references, and finally, CSDI+RMA, which incorporates an additional RMA.

表4：基于CSDI（基于上下文的序列去噪自编码器，Contextual Sequence Denoising Autoencoder）的方法之间的性能比较（均方误差，MSE）。CSDI代表基本网络框架，CSDI+线性表示通过线性层将输入和参考信息拼接后一起输入网络的方法，CSDI+交叉注意力表示使用交叉注意力来融合输入和参考信息的特征，最后，CSDI+RMA则是加入了额外的RMA。

<table><tr><td>Dataset</td><td>Exchange</td><td>Electricity</td><td>Wind</td><td>Weather</td><td>Solar</td><td>MIMIC-IV</td></tr><tr><td>CSDI</td><td>0.077</td><td>0.379</td><td>1.066</td><td>0.356</td><td>0.381</td><td>0.268</td></tr><tr><td>CSDI+Linear</td><td>0.075</td><td>0.316</td><td>0.932</td><td>0.349</td><td>0.369</td><td>0.265</td></tr><tr><td>CSDI+Cross Attention</td><td>0.028</td><td>0.173</td><td>0.829</td><td>0.291</td><td>0.340</td><td>0.183</td></tr><tr><td>CSDI+RMA</td><td>0.013</td><td>0.151</td><td>0.784</td><td>0.281</td><td>0.327</td><td>0.172</td></tr></table>

<table><tbody><tr><td>数据集</td><td>交换</td><td>电力</td><td>风能</td><td>天气</td><td>太阳能</td><td>重症监护数据库第四版（MIMIC-IV）</td></tr><tr><td>连续时间序列数据插补（CSDI）</td><td>0.077</td><td>0.379</td><td>1.066</td><td>0.356</td><td>0.381</td><td>0.268</td></tr><tr><td>连续时间序列数据插补+线性模型（CSDI+Linear）</td><td>0.075</td><td>0.316</td><td>0.932</td><td>0.349</td><td>0.369</td><td>0.265</td></tr><tr><td>连续时间序列数据插补+交叉注意力机制（CSDI+Cross Attention）</td><td>0.028</td><td>0.173</td><td>0.829</td><td>0.291</td><td>0.340</td><td>0.183</td></tr><tr><td>连续时间序列数据插补+递归均值调整（CSDI+RMA）</td><td>0.013</td><td>0.151</td><td>0.784</td><td>0.281</td><td>0.327</td><td>0.172</td></tr></tbody></table>

<!-- Media -->

Predicting ${x}_{0}$ vs Predicting $\epsilon$ . Following the formulation in Section 4.3,our network is designed to forecast the latent variable ${\mathbf{x}}_{0}$ . Since some existing models [28,36] have been trained by predicting an additional noise term $\epsilon$ ,we conducted a comparative experiment to determine which approach is more suitable for our framework. Specifically, we maintained the network structure unchanged, only modifying the prediction target to be $\epsilon$ . The results are presented in Table 5 Predicting ${\mathbf{x}}_{0}$ proves to be more effective. This may be because the relationship between the reference and ${\mathbf{x}}_{o}$ is more direct, making the denoising task relatively easier.

预测${x}_{0}$与预测$\epsilon$。根据4.3节中的公式，我们的网络旨在预测潜在变量${\mathbf{x}}_{0}$。由于一些现有模型[28,36]是通过预测额外的噪声项$\epsilon$进行训练的，我们进行了一项对比实验，以确定哪种方法更适合我们的框架。具体来说，我们保持网络结构不变，仅将预测目标修改为$\epsilon$。结果如表5所示，预测${\mathbf{x}}_{0}$被证明更有效。这可能是因为参考与${\mathbf{x}}_{o}$之间的关系更直接，使得去噪任务相对更容易。

<!-- Media -->

Table 5: MSEs of two denoising strategies: Predicting ${\mathbf{x}}_{0}$ vs predicting $\epsilon$ .

表5：两种去噪策略的均方误差（MSE）：预测${\mathbf{x}}_{0}$与预测$\epsilon$。

<table><tr><td>denoising strategy</td><td>Wind</td><td>Weather</td><td>Exchange</td></tr><tr><td>${x}_{0}$</td><td>0.784</td><td>0.281</td><td>0.013</td></tr><tr><td>E</td><td>0.841</td><td>0.331</td><td>0.018</td></tr></table>

<table><tbody><tr><td>去噪策略</td><td>风</td><td>天气</td><td>交换；交流；交易</td></tr><tr><td>${x}_{0}$</td><td>0.784</td><td>0.281</td><td>0.013</td></tr><tr><td>E</td><td>0.841</td><td>0.331</td><td>0.018</td></tr></tbody></table>

<!-- Media -->

RMA position We investigate the best position of RMA in the model. Front, middle, and back means we set the RMA in the front of, in the middle of, and the back of two transformer layers, respectively. We found that placing RMA before the bidirectional transformer resulted in the most significant improvement in model performance. This also aligns with the intuition of network design: cross-attention modules placed at the front of the model tend to have a greater impact.

相对位置注意力模块（RMA）的位置 我们研究了相对位置注意力模块（RMA）在模型中的最佳位置。“前”“中”“后”分别表示我们将相对位置注意力模块（RMA）设置在两个Transformer层之前、中间和之后。我们发现，将相对位置注意力模块（RMA）置于双向Transformer之前能使模型性能得到最显著的提升。这也符合网络设计的直觉：置于模型前端的交叉注意力模块往往会产生更大的影响。

<!-- Media -->

Table 6: Ablation study on different RMA positions. The best is in bold.

表6：不同相对位置注意力模块（RMA）位置的消融研究。最佳结果以粗体显示。

<table><tr><td>Dataset</td><td colspan="3">Exchange</td><td colspan="3">Wind</td><td colspan="3">Electricity</td><td colspan="3">Weather</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td><td>MSE</td><td>MAE</td><td>CRPS</td></tr><tr><td>-</td><td>0.077</td><td>0.194</td><td>0.397</td><td>1.066</td><td>0.741</td><td>0.941</td><td>0.379</td><td>0.579</td><td>0.480</td><td>0.356</td><td>0.374</td><td>0.354</td></tr><tr><td>Back</td><td>0.031</td><td>0.105</td><td>0.373</td><td>0.673</td><td>0.611</td><td>0.842</td><td>0.267</td><td>0.434</td><td>0.426</td><td>0.301</td><td>0.321</td><td>0.322</td></tr><tr><td>Middle</td><td>0.057</td><td>0.141</td><td>0.381</td><td>0.799</td><td>0.631</td><td>0.833</td><td>0.291</td><td>0.481</td><td>0.451</td><td>0.333</td><td>0.331</td><td>0.336</td></tr><tr><td>Front</td><td>0.013</td><td>0.063</td><td>0.331</td><td>0.784</td><td>0.579</td><td>0.673</td><td>0.161</td><td>0.256</td><td>0.391</td><td>0.281</td><td>0.293</td><td>0.313</td></tr></table>

<table><tbody><tr><td>数据集</td><td colspan="3">交换；交易</td><td colspan="3">风</td><td colspan="3">电力</td><td colspan="3">天气</td></tr><tr><td>指标；度量</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>复杂性区域疼痛综合征（Complex Regional Pain Syndrome，CRPS）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>复杂性区域疼痛综合征（Complex Regional Pain Syndrome，CRPS）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>复杂性区域疼痛综合征（Complex Regional Pain Syndrome，CRPS）</td><td>均方误差（Mean Squared Error，MSE）</td><td>平均绝对误差（Mean Absolute Error，MAE）</td><td>复杂性区域疼痛综合征（Complex Regional Pain Syndrome，CRPS）</td></tr><tr><td>-</td><td>0.077</td><td>0.194</td><td>0.397</td><td>1.066</td><td>0.741</td><td>0.941</td><td>0.379</td><td>0.579</td><td>0.480</td><td>0.356</td><td>0.374</td><td>0.354</td></tr><tr><td>后面</td><td>0.031</td><td>0.105</td><td>0.373</td><td>0.673</td><td>0.611</td><td>0.842</td><td>0.267</td><td>0.434</td><td>0.426</td><td>0.301</td><td>0.321</td><td>0.322</td></tr><tr><td>中间</td><td>0.057</td><td>0.141</td><td>0.381</td><td>0.799</td><td>0.631</td><td>0.833</td><td>0.291</td><td>0.481</td><td>0.451</td><td>0.333</td><td>0.331</td><td>0.336</td></tr><tr><td>前面</td><td>0.013</td><td>0.063</td><td>0.331</td><td>0.784</td><td>0.579</td><td>0.673</td><td>0.161</td><td>0.256</td><td>0.391</td><td>0.281</td><td>0.293</td><td>0.313</td></tr></tbody></table>

<!-- Media -->

## 6 Discussion

## 6 讨论

Limitation and Future Work As a transformer-based diffusion model structure, our approach still faces some challenges brought by the transformer framework. Our model consumes a significant amount of computational resources dealing with time series consisting of too many variables. Additionally, our approach requires additional preprocessing (retrieval process) during training, which incurs additional costs on training time (around ten hours).

局限性与未来工作 作为一种基于Transformer的扩散模型结构，我们的方法仍然面临着Transformer框架带来的一些挑战。在处理由过多变量组成的时间序列时，我们的模型会消耗大量的计算资源。此外，我们的方法在训练过程中需要额外的预处理（检索过程），这会增加训练时间成本（约十小时）。

Conclusion In this paper, we propose a new framework for time series diffusion modeling to address the forecasting performance limitations of existing diffusion models. RATD retrieves samples most relevant to the historical time series from the constructed database and utilize them as references to guide the denoising process of the diffusion model, thereby obtaining more accurate predictions. RATD is highly effective in solving challenging time series prediction tasks, as evaluated by experiments on five real-world datasets.

结论 本文中，我们提出了一种新的时间序列扩散建模框架，以解决现有扩散模型在预测性能方面的局限性。RATD（检索辅助时间序列扩散模型，Retrieval-Assisted Time-series Diffusion model）从构建的数据库中检索与历史时间序列最相关的样本，并将其作为参考来指导扩散模型的去噪过程，从而获得更准确的预测结果。通过在五个真实世界数据集上的实验评估，RATD在解决具有挑战性的时间序列预测任务方面非常有效。

## Acknowledgements

## 致谢

This work is supported by the National Natural Science Foundation of China (No.62172018, No.62102008) and Wuhan East Lake High-Tech Development Zone National Comprehensive Experimental Base for Governance of Intelligent Society.

本工作得到了国家自然科学基金（项目编号：62172018、62102008）以及武汉东湖高新区国家智能社会治理综合实验基地的支持。

## References

## 参考文献

[1] Juan Miguel Lopez Alcaraz and Nils Strodthoff. Diffusion-based time series imputation and forecasting with structured state space models. arXiv preprint arXiv:2208.09399, 2022.

[1] 胡安·米格尔·洛佩斯·阿尔卡拉兹（Juan Miguel Lopez Alcaraz）和尼尔斯·斯特罗德托夫（Nils Strodthoff）。基于扩散的结构化状态空间模型时间序列插补与预测。预印本 arXiv:2208.09399，2022年。

[2] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, and Björn Ommer. Retrieval-augmented diffusion models. Advances in Neural Information Processing Systems, 35:15309- 15324, 2022.

[2] 安德烈亚斯·布拉特曼（Andreas Blattmann）、罗宾·龙巴赫（Robin Rombach）、卡恩·奥克泰（Kaan Oktay）、乔纳斯·米勒（Jonas Müller）和比约恩·奥默（Björn Ommer）。检索增强扩散模型。《神经信息处理系统进展》，35:15309 - 15324，2022年。

[3] Giovanni Bonetta, Rossella Cancelliere, Ding Liu, and Paul Vozila. Retrieval-augmented transformer-xl for close-domain dialog generation. arXiv preprint arXiv:2105.09235, 2021.

[3] 乔瓦尼·博内塔（Giovanni Bonetta）、罗塞拉·坎切列雷（Rossella Cancelliere）、刘丁（Ding Liu）和保罗·沃齐拉（Paul Vozila）。用于近域对话生成的检索增强Transformer - XL。预印本 arXiv:2105.09235，2021年。

[4] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pp. 2206-2240. PMLR, 2022.

[4] 塞巴斯蒂安·博若 (Sebastian Borgeaud)、亚瑟·门施 (Arthur Mensch)、乔丹·霍夫曼 (Jordan Hoffmann)、特雷弗·蔡 (Trevor Cai)、伊莱扎·拉瑟福德 (Eliza Rutherford)、凯蒂·米利肯 (Katie Millican)、乔治·B·m·范登德里舍 (George Bm Van Den Driessche)、让 - 巴蒂斯特·莱斯皮奥 (Jean - Baptiste Lespiau)、博格丹·达莫克 (Bogdan Damoc)、艾丹·克拉克 (Aidan Clark) 等。通过从数万亿个标记中检索来改进语言模型。见《国际机器学习会议》，第2206 - 2240页。机器学习研究会议录 (PMLR)，2022年。

[5] Jian Cao, Zhi Li, and Jian Li. Financial time series forecasting model based on ceemdan and lstm. Physica A: Statistical mechanics and its applications, 519:127-139, 2019.

[5] 曹健、李志和李建。基于互补集合经验模态分解 (CEEMDAN) 和长短期记忆网络 (LSTM) 的金融时间序列预测模型。《物理A：统计力学及其应用》，519:127 - 139，2019年。

[6] Jui-Sheng Chou and Duc-Son Tran. Forecasting energy consumption time series using machine learning techniques based on usage patterns of residential householders. Energy, 165:709-726, 2018.

[6] 周瑞生 (Jui - Sheng Chou) 和陈德山 (Duc - Son Tran)。基于居民用户使用模式的机器学习技术预测能源消耗时间序列。《能源》，165:709 - 726，2018年。

[7] Alexiei Dingli and Karl Sant Fournier. Financial time series forecasting-a deep learning approach. International Journal of Machine Learning and Computing, 7(5):118-122, 2017.

[7] 阿列克谢·丁利 (Alexiei Dingli) 和卡尔·桑特·富尼耶 (Karl Sant Fournier)。金融时间序列预测——一种深度学习方法。《国际机器学习与计算杂志》，7(5):118 - 122，2017年。

[8] Wei Fan, Shun Zheng, Xiaohan Yi, Wei Cao, Yanjie Fu, Jiang Bian, and Tie-Yan Liu. Depts: deep expansion learning for periodic time series forecasting. arXiv preprint arXiv:2203.07681 , 2022.

[8] 樊伟（Wei Fan）、郑顺（Shun Zheng）、易晓晗（Xiaohan Yi）、曹伟（Wei Cao）、傅彦杰（Yanjie Fu）、卞江（Jiang Bian）和刘铁岩（Tie-Yan Liu）。《Depts：用于周期性时间序列预测的深度扩展学习》。预印本 arXiv:2203.07681 ，2022年。

[9] Xinyao Fan, Yueying Wu, Chang Xu, Yuhao Huang, Weiqing Liu, and Jiang Bian. Mg-tsd: Multi-granularity time series diffusion models with guided learning process. arXiv preprint arXiv:2403.05751, 2024.

[9] 范馨瑶（Xinyao Fan）、吴悦莹（Yueying Wu）、徐畅（Chang Xu）、黄宇豪（Yuhao Huang）、刘卫青（Weiqing Liu）和卞江（Jiang Bian）。《Mg - tsd：具有引导学习过程的多粒度时间序列扩散模型》。预印本 arXiv:2403.05751，2024年。

[10] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pp. 3929-3938. PMLR, 2020.

[10] 凯文·古（Kelvin Guu）、肯顿·李（Kenton Lee）、佐拉·通（Zora Tung）、帕努蓬·帕苏帕特（Panupong Pasupat）和张明伟（Mingwei Chang）。《检索增强语言模型预训练》。收录于国际机器学习会议论文集，第3929 - 3938页。机器学习研究会议录（PMLR），2020年。

[11] Pradeep Hewage, Ardhendu Behera, Marcello Trovati, Ella Pereira, Morteza Ghahremani, Francesco Palmieri, and Yonghuai Liu. Temporal convolutional neural (tcn) network for an effective weather forecasting using time-series data from the local weather station. Soft Computing, 24:16453-16482, 2020.

[11] 普拉迪普·赫瓦吉（Pradeep Hewage）、阿德亨杜·贝赫拉（Ardhendu Behera）、马塞洛·特罗瓦蒂（Marcello Trovati）、埃拉·佩雷拉（Ella Pereira）、莫尔塔扎·加雷马尼（Morteza Ghahremani）、弗朗切斯科·帕尔米耶里（Francesco Palmieri）和刘永怀（Yonghuai Liu）。利用当地气象站的时间序列数据，使用时间卷积神经网络（TCN）进行有效天气预报。《软计算》，24:16453 - 16482，2020年。

[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840-6851, 2020.

[12] 乔纳森·霍（Jonathan Ho）、阿杰·贾恩（Ajay Jain）和彼得·阿贝贝尔（Pieter Abbeel）。去噪扩散概率模型。《神经信息处理系统进展》，33:6840 - 6851，2020年。

[13] Baoyu Jing, Si Zhang, Yada Zhu, Bin Peng, Kaiyu Guan, Andrew Margenot, and Hanghang Tong. Retrieval based time series forecasting. arXiv preprint arXiv:2209.13525, 2022.

[13] 景宝玉（Baoyu Jing）、张思（Si Zhang）、朱雅达（Yada Zhu）、彭斌（Bin Peng）、管开宇（Kaiyu Guan）、安德鲁·马格诺特（Andrew Margenot）和童航航（Hanghang Tong）。基于检索的时间序列预测。预印本arXiv:2209.13525，2022年。

[14] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom J Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. Mimic-iv, a freely accessible electronic health record dataset. Scientific data, 10(1):1, 2023.

[14] 阿利斯泰尔·E·W·约翰逊（Alistair EW Johnson）、卢卡斯·布尔加雷利（Lucas Bulgarelli）、沈璐（Lu Shen）、阿尔文·盖尔斯（Alvin Gayles）、阿亚德·沙穆特（Ayad Shammout）、史蒂文·洪（Steven Horng）、汤姆·J·波拉德（Tom J Pollard）、郝思成（Sicheng Hao）、本杰明·穆迪（Benjamin Moody）、布莱恩·高（Brian Gow）等。Mimic - iv：一个可免费获取的电子健康记录数据集。《科学数据》，10(1):1，2023年。

[15] Zahra Karevan and Johan AK Suykens. Transductive lstm for time-series prediction: An application to weather forecasting. Neural Networks, 125:1-9, 2020.

[15] 扎赫拉·卡雷万（Zahra Karevan）和约翰·A·K·斯凯肯斯（Johan AK Suykens）。用于时间序列预测的直推式长短期记忆网络（Transductive LSTM）：在天气预报中的应用。《神经网络》，125:1 - 9，2020年。

[16] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172, 2019.

[16] 乌尔瓦希·坎德尔瓦尔（Urvashi Khandelwal）、奥默·利维（Omer Levy）、丹·朱拉夫斯基（Dan Jurafsky）、卢克·泽特尔莫耶（Luke Zettlemoyer）和迈克·刘易斯（Mike Lewis）。通过记忆实现泛化：最近邻语言模型。预印本arXiv:1911.00172，2019年。

[17] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, 2020.

[17] 孔志峰（Zhifeng Kong）、平伟（Wei Ping）、黄家吉（Jiaji Huang）、赵可欣（Kexin Zhao）和布莱恩·卡坦扎罗（Bryan Catanzaro）。Diffwave：一种用于音频合成的通用扩散模型。发表于国际学习表征会议，2020年。

[18] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research & development in information retrieval, pp. 95-104, 2018.

[18] 赖国坤（Guokun Lai）、张伟成（Wei-Cheng Chang）、杨一鸣（Yiming Yang）和刘瀚霄（Hanxiao Liu）。利用深度神经网络对长期和短期时间模式进行建模。见第41届ACM国际信息检索研究与发展会议论文集，第95 - 104页，2018年。

[19] S Sri Lakshmi and RK Tiwari. Model dissection from earthquake time series: A comparative analysis using modern non-linear forecasting and artificial neural network approaches. Computers & Geosciences, 35(2):191-204, 2009.

[19] S·斯里·拉克希米（S Sri Lakshmi）和R·K·蒂瓦里（RK Tiwari）。从地震时间序列进行模型剖析：使用现代非线性预测和人工神经网络方法的比较分析。《计算机与地球科学》，35(2):191 - 204，2009年。

[20] Yan Li, Xinjiang Lu, Yaqing Wang, and Dejing Dou. Generative time series forecasting with diffusion, denoise, and disentanglement. Advances in Neural Information Processing Systems, 35:23009-23022, 2022.

[20] 李岩（Yan Li）、卢新疆（Xinjiang Lu）、王亚清（Yaqing Wang）和窦德景（Dejing Dou）。通过扩散、去噪和解纠缠进行生成式时间序列预测。《神经信息处理系统进展》，35:23009 - 23022，2022年。

[21] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet: Time series modeling and forecasting with sample convolution and interaction. Advances in Neural Information Processing Systems, 35:5816-5828, 2022.

[21] 刘敏浩（Minhao Liu）、曾爱玲（Ailing Zeng）、陈慕溪（Muxi Chen）、徐志坚（Zhijian Xu）、赖秋霞（Qiuxia Lai）、马灵娜（Lingna Ma）和徐强（Qiang Xu）。Scinet：基于样本卷积和交互的时间序列建模与预测。《神经信息处理系统进展》，35:5816 - 5828，2022年。

[22] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International conference on learning representations, 2021.

[22] 刘世展（Shizhan Liu）、余航（Hang Yu）、廖聪（Cong Liao）、李建国（Jianguo Li）、林伟耀（Weyao Lin）、刘亚历克斯·X（Alex X Liu）和沙赫拉姆·达斯特达尔（Schahram Dustdar）。《Pyraformer：用于长序列时间序列建模和预测的低复杂度金字塔注意力机制》。发表于2021年国际学习表征会议。

[23] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625, 2023.

[23] 刘永（Yong Liu）、胡腾格（Tengge Hu）、张浩然（Haoran Zhang）、吴海旭（Haixu Wu）、王诗雨（Shiyu Wang）、马林涛（Lintao Ma）和龙明盛（Mingsheng Long）。《itransformer：倒置变压器对时间序列预测有效》。预印本arXiv:2310.06625，2023年。

[24] James E Matheson and Robert L Winkler. Scoring rules for continuous probability distributions. Management science, 22(10):1087-1096, 1976.

[24] 詹姆斯·E·马西森（James E Matheson）和罗伯特·L·温克勒（Robert L Winkler）。《连续概率分布的评分规则》。《管理科学》，22(10):1087 - 1096，1976年。

[25] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.

[25] 聂玉琪（Yuqi Nie）、阮南（Nam H Nguyen）、潘瓦迪·辛通（Phanwadee Sinthong）和贾扬特·卡拉格纳纳姆（Jayant Kalagnanam）。时间序列价值64字：使用Transformer进行长期预测。预印本arXiv:2211.14730，2022年。

[26] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansion analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437, 2019.

[26] 鲍里斯·N·奥列什金（Boris N Oreshkin）、德米特里·卡尔波夫（Dmitri Carpov）、尼古拉斯·查帕多斯（Nicolas Chapados）和约书亚·本吉奥（Yoshua Bengio）。N - beats：用于可解释时间序列预测的神经基扩展分析。预印本arXiv:1905.10437，2019年。

[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.

[27] 亚当·帕兹克（Adam Paszke）、山姆·格罗斯（Sam Gross）、弗朗西斯科·马萨（Francisco Massa）、亚当·勒雷尔（Adam Lerer）、詹姆斯·布拉德伯里（James Bradbury）、格雷戈里·查南（Gregory Chanan）、特雷弗·基林（Trevor Killeen）、林泽明（Zeming Lin）、娜塔莉亚·吉梅尔申（Natalia Gimelshein）、卢卡·安蒂加（Luca Antiga）等。PyTorch：一种命令式风格的高性能深度学习库。《神经信息处理系统进展》，第32卷，2019年。

[28] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. Autoregressive denois-ing diffusion models for multivariate probabilistic time series forecasting. In International Conference on Machine Learning, pp. 8857-8868. PMLR, 2021.

[28] 卡希夫·拉苏尔（Kashif Rasul）、卡尔文·西沃德（Calvin Seward）、英格玛·舒斯特（Ingmar Schuster）和罗兰·沃尔格拉夫（Roland Vollgraf）。用于多元概率时间序列预测的自回归去噪扩散模型。见《国际机器学习会议论文集》，第8857 - 8868页。机器学习研究会议录（PMLR），2021年。

[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684-10695, 2022.

[29] 罗宾·龙巴赫（Robin Rombach）、安德烈亚斯·布拉特曼（Andreas Blattmann）、多米尼克·洛伦茨（Dominik Lorenz）、帕特里克·埃瑟（Patrick Esser）和比约恩·奥默（Björn Ommer）。基于潜在扩散模型的高分辨率图像合成。见《电气与电子工程师协会/计算机视觉基金会计算机视觉与模式识别会议论文集》，第10684 - 10695页，2022年。

[30] Lifeng Shen and James Kwok. Non-autoregressive conditional diffusion models for time series prediction. arXiv preprint arXiv:2306.05043, 2023.

[30] 沈立峰（Lifeng Shen）和詹姆斯·郭（James Kwok）。用于时间序列预测的非自回归条件扩散模型。预印本arXiv:2306.05043，2023年。

[31] Lifeng Shen, Weiyu Chen, and James Kwok. Multi-resolution diffusion models for time series forecasting. In The Twelfth International Conference on Learning Representations, 2023.

[31] 沈立峰（Lifeng Shen）、陈伟宇（Weiyu Chen）和詹姆斯·郭（James Kwok）。用于时间序列预测的多分辨率扩散模型。见《第十二届学习表征国际会议论文集》，2023年。

[32] Zhipeng Shen, Yuanming Zhang, Jiawei Lu, Jun Xu, and Gang Xiao. Seriesnet: a generative time series forecasting model. In 2018 international joint conference on neural networks (IJCNN), pp. 1-8. IEEE, 2018.

[32] 沈志鹏（Zhipeng Shen）、张远明（Yuanming Zhang）、卢嘉伟（Jiawei Lu）、徐军（Jun Xu）和肖刚（Gang Xiao）。Seriesnet：一种生成式时间序列预测模型。见2018年国际神经网络联合会议（IJCNN），第1 - 8页。电气与电子工程师协会（IEEE），2018年。

[33] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 3531-3539, 2021.

[33] 沈卓然（Zhuoran Shen）、张明远（Mingyuan Zhang）、赵海宇（Haiyu Zhao）、易帅（Shuai Yi）和李洪生（Hongsheng Li）。高效注意力机制：具有线性复杂度的注意力机制。见电气与电子工程师协会/计算机视觉基金会（IEEE/CVF）冬季计算机视觉应用会议论文集，第3531 - 3539页，2021年。

[34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranthan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 2256-2265. PMLR, 2015.

[34] 雅沙·索尔 - 迪克斯坦（Jascha Sohl - Dickstein）、埃里克·韦斯（Eric Weiss）、尼鲁·马赫什瓦兰坦（Niru Maheswaranthan）和苏里亚·甘古利（Surya Ganguli）。基于非平衡热力学的深度无监督学习。见国际机器学习会议，第2256 - 2265页。机器学习研究会议录（PMLR），2015年。

[35] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.

[35] 宋杨（Yang Song）、雅沙·索尔 - 迪克斯坦（Jascha Sohl - Dickstein）、迪德里克·P·金玛（Diederik P Kingma）、阿比舍克·库马尔（Abhishek Kumar）、斯特凡诺·埃尔蒙（Stefano Ermon）和本·普尔（Ben Poole）。基于随机微分方程的基于分数的生成式建模。预印本arXiv:2011.13456，2020年。

[36] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation. Advances in Neural Information Processing Systems, 34:24804-24816, 2021.

[36] 田代悠介（Yusuke Tashiro）、宋佳明（Jiaming Song）、宋洋（Yang Song）和斯特凡诺·埃尔蒙（Stefano Ermon）。Csdi：用于概率时间序列插补的基于条件得分的扩散模型。《神经信息处理系统进展》，34:24804 - 24816，2021年。

[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[37] 阿什ish·瓦斯瓦尼（Ashish Vaswani）、诺姆·沙泽尔（Noam Shazeer）、尼基·帕尔马（Niki Parmar）、雅各布·乌斯库雷特（Jakob Uszkoreit）、利昂·琼斯（Llion Jones）、艾丹·N·戈麦斯（Aidan N Gomez）、卢卡斯·凯泽（Łukasz Kaiser）和伊利亚·波洛苏金（Illia Polosukhin）。注意力就是你所需要的一切。《神经信息处理系统进展》，30，2017年。

[38] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419-22430, 2021.

[38] 吴海旭（Haixu Wu）、徐杰辉（Jiehui Xu）、王建民（Jianmin Wang）和龙明盛（Mingsheng Long）。Autoformer：用于长期序列预测的具有自相关的分解变压器。《神经信息处理系统进展》，34:22419 - 22430，2021年。

[39] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The eleventh international conference on learning representations, 2022.

[39] 吴海旭（Haixu Wu）、胡腾格（Tengge Hu）、刘永（Yong Liu）、周航（Hang Zhou）、王建民（Jianmin Wang）和龙明盛（Mingsheng Long）。Timesnet：用于通用时间序列分析的时间二维变化建模。《第十一届国际学习表征会议》，2022年。

[40] Sitan Yang, Carson Eisenach, and Dhruv Madeka. Mqretnn: Multi-horizon time series forecasting with retrieval augmentation. arXiv preprint arXiv:2207.10517, 2022.

[40] 杨思坦（Sitan Yang）、卡森·艾森纳赫（Carson Eisenach）和德鲁夫·马德卡（Dhruv Madeka）。Mqretnn：基于检索增强的多步时间序列预测。预印本 arXiv:2207.10517，2022年。

[41] Kun Yi, Qi Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian, Longbing Cao, and Zhendong Niu. Frequency-domain mlps are more effective learners in time series forecasting. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

[41] 易坤（Kun Yi）、张琦（Qi Zhang）、范伟（Wei Fan）、王首进（Shoujin Wang）、王鹏阳（Pengyang Wang）、何辉（Hui He）、安宁（Ning An）、连德富（Defu Lian）、曹龙兵（Longbing Cao）和牛振东（Zhendong Niu）。频域多层感知器在时间序列预测中是更有效的学习器。发表于第三十七届神经信息处理系统大会，2023年。

[42] Jinsung Yoon, Daniel Jarrett, and Mihaela Van der Schaar. Time-series generative adversarial networks. Advances in neural information processing systems, 32, 2019.

[42] 尹进成（Jinsung Yoon）、丹尼尔·贾勒特（Daniel Jarrett）和米哈埃拉·范德沙尔（Mihaela Van der Schaar）。时间序列生成对抗网络。《神经信息处理系统进展》，第32卷，2019年。

[43] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp. 11121-11128, 2023.

[43] 曾爱玲（Ailing Zeng）、陈慕溪（Muxi Chen）、张磊（Lei Zhang）和徐强（Qiang Xu）。Transformer在时间序列预测中有效吗？发表于《AAAI人工智能会议论文集》，第37卷，第11121 - 11128页，2023年。

[44] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 364-373, 2023.

[44] 张明远（Mingyuan Zhang）、郭心莹（Xinying Guo）、潘亮（Liang Pan）、蔡忠昂（Zhongang Cai）、洪方舟（Fangzhou Hong）、李慧荣（Huirong Li）、杨磊（Lei Yang）和刘子维（Ziwei Liu）。Remodiffuse：检索增强的运动扩散模型。见《电气与电子工程师协会/计算机视觉基金会国际计算机视觉会议论文集》，第364 - 373页，2023年。

[45] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 11106-11115, 2021.

[45] 周浩毅（Haoyi Zhou）、张上航（Shanghang Zhang）、彭杰奇（Jieqi Peng）、张帅（Shuai Zhang）、李建新（Jianxin Li）、熊辉（Hui Xiong）和张万才（Wancai Zhang）。Informer：超越高效Transformer的长序列时间序列预测方法。见《美国人工智能协会人工智能会议论文集》，第35卷，第11106 - 11115页，2021年。

[46] Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et al. Film: Frequency improved legendre memory model for long-term time series forecasting. Advances in Neural Information Processing Systems, 35:12677-12690, 2022.

[46] 周田（Tian Zhou）、马子清（Ziqing Ma）、文青松（Qingsong Wen）、孙亮（Liang Sun）、姚涛（Tao Yao）、尹沃涛（Wotao Yin）、金榕（Rong Jin）等。Film：用于长期时间序列预测的频率改进勒让德记忆模型。《神经信息处理系统进展》，35:12677 - 12690，2022年。

[47] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International Conference on Machine Learning, pp. 27268-27286. PMLR, 2022.

[47] 周田（Tian Zhou）、马子清（Ziqing Ma）、文青松（Qingsong Wen）、王雪（Xue Wang）、孙亮（Liang Sun）和金荣（Rong Jin）。Fedformer：用于长期序列预测的频率增强分解变压器。见《国际机器学习会议》，第27268 - 27286页。机器学习研究会议录（PMLR），2022年。

## A Sampling Procedure

## 采样过程

Like Algorithm 1, we summarize the sampling procedure of RATD in Algorithm 2 and highlight the differences from conventional diffusion models in .

与算法1类似，我们在算法2中总结了RATD的采样过程，并在[此处原文缺失内容]中突出了与传统扩散模型的差异。

<!-- Media -->

Algorithm 2 Sampling Procedure of RATD

算法2：RATD的采样过程

---

Require: The historical time series ${\mathbf{x}}^{H}$ ,the learned model ${\mu }_{\theta }$ ,external database ${\mathcal{D}}^{R}$ ,pre-trained

		${E}_{\phi }$ ,the number of references $k$

	insure: Prediction ${\mathbf{x}}^{P}$ corresponding to the history ${\mathbf{x}}^{H}$

		Sample initial target time series ${\mathbf{x}}_{T}$

		Embed ${\mathbf{x}}^{H}$ into ${\mathbf{v}}^{H}$

		Retrieval the reference ${\mathbf{x}}^{R}$ with ${\mathbf{v}}^{H}$

		Compute the side feature ${\mathcal{I}}_{s}$

		for $t$ in $T,T - 1,\ldots ,1$ do

				Predict ${\widehat{\mathbf{x}}}_{0}$ from ${\mathbf{x}}_{t},{\mathcal{I}}_{s}$ and ${\mathbf{x}}^{R}$ (Equation (10))

				Sample ${\mathbf{x}}_{t - 1}$ from the posterior $q\left( {{\mathbf{x}}_{t} \mid  {\mathbf{x}}_{0}}\right)$ (Equation (2))

		end for

---

<!-- Media -->

## B Impletion Details

## B 实现细节

### B.1 Training Details

### B.1 训练细节

Our dataset is split in the proportion of 7:1:2 (Train: Validation: Test), utilizing a random splitting strategy to ensure diversity in the training set. We sample the ECG signals at ${125}\mathrm{\;{Hz}}$ for the MIMIC-IV dataset and extract fixed-length windows as samples. For training, we utilized the Adam optimizer with an initial learning rate of ${10}^{-3}$ ,betas $= \left( {{0.95},{0.999}}\right)$ . During the training process of shifted diffusion, the batch size was set to 64, and early stopping was applied for a maximum of 200 epochs. The diffusion steps $T$ were set to 100 .

我们的数据集按照7:1:2的比例（训练集: 验证集: 测试集）进行划分，采用随机划分策略以确保训练集的多样性。对于MIMIC - IV数据集，我们在${125}\mathrm{\;{Hz}}$处对心电图（ECG）信号进行采样，并提取固定长度的窗口作为样本。在训练时，我们使用Adam优化器，初始学习率为${10}^{-3}$，β值为$= \left( {{0.95},{0.999}}\right)$。在偏移扩散的训练过程中，批量大小设置为64，并采用提前停止策略，最多训练200个轮次。扩散步数$T$设置为100。

### B.2 Side Information

### B.2 辅助信息

We combine temporal embedding and feature embedding as side information ${v}_{s}$ . We use 128- dimensions temporal embedding following previous studies [37]:

我们将时间嵌入（temporal embedding）和特征嵌入（feature embedding）作为辅助信息 ${v}_{s}$。我们遵循先前的研究 [37]，使用 128 维的时间嵌入：

$$
{s}_{\text{embedding }}\left( {s}_{\zeta }\right)  = \left( {\sin \left( {{s}_{\zeta }/{\tau }^{0/{64}}}\right) ,\ldots ,\sin \left( {{s}_{\zeta }/{\tau }^{{63}/{64}}}\right) ,\cos \left( {{s}_{\zeta }/{\tau }^{0/{64}}}\right) ,\ldots ,\cos \left( {{s}_{\zeta }/{\tau }^{{63}/{64}}}\right) }\right)  \tag{12}
$$

where $\tau  = {10000}$ . Following [36], ${s}_{l}$ represents the timestamp corresponding to the l -th point in the time series. This setup is designed to capture the irregular sampling in the dataset and convey it to the model. Additionally, we utilize learnable embedding to handle feature dimensions. Specifically, feature embedding is represented as 16-dimensional learnable vectors that capture relationships between dimensions. According to [17], we combine time embedding and feature embedding, collectively referred to as side information ${\mathcal{I}}_{s}$ .

其中 $\tau  = {10000}$。遵循文献 [36]，${s}_{l}$ 表示时间序列中第 l 个点对应的时间戳。此设置旨在捕捉数据集中的不规则采样，并将其传达给模型。此外，我们利用可学习嵌入来处理特征维度。具体而言，特征嵌入表示为 16 维的可学习向量，用于捕捉各维度之间的关系。根据文献 [17]，我们将时间嵌入和特征嵌入相结合，统称为辅助信息 ${\mathcal{I}}_{s}$。

The shape of ${\mathcal{I}}_{s}$ is not fixed and varies with datasets. Taking the Exchange dataset as an example,the shape of forecasting target ${\mathbf{x}}^{R}$ is [Batchsize (64),7(number of variables),168 (time-dimension),12 (time-dimension)] and the corresponding shape of ${\mathcal{I}}_{s}$ is [Batchsize (64),total channel(144( time:128 + feature:16)), 320 (frequency-dimension*latent channel), 12 (time-dimension)].

${\mathcal{I}}_{s}$的形状并不固定，会随数据集而变化。以Exchange数据集为例，预测目标${\mathbf{x}}^{R}$的形状为[批量大小（64），7（变量数量），168（时间维度），12（时间维度）]，对应的${\mathcal{I}}_{s}$的形状为[批量大小（64），总通道数（144（时间：128 + 特征：16）），320（频率维度*潜在通道），12（时间维度）]。

### B.3 Transformers Details

### B.3 变压器详情

Our approach employs the Transformer architecture from CSDI, with the distinction of expanding the channel dimension to 128. The network comprises temporal and feature layers, ensuring the comprehensiveness of the model in handling the time-frequency domain latent while maintaining a relatively simple structure. Regarding the transformer layer, we utilized a 1-layer Transformer encoder implemented in PyTorch [27], comprising multi-head attention layers, fully connected layers,and layer normalization. We adopted the "linear attention transformer" package ${}^{2}$ ,to enhance computational efficiency. The inclusion of numerous features and long sequences prompted this decision. The package implements an efficient attention mechanism [33], and we exclusively utilized the global attention feature within the package.

我们的方法采用了来自CSDI（条件随机微分方程推理）的Transformer架构，不同之处在于将通道维度扩展到了128。该网络由时间层和特征层组成，确保模型在处理时频域潜在信息时具有全面性，同时保持相对简单的结构。关于Transformer层，我们使用了在PyTorch [27]中实现的单层Transformer编码器，它由多头注意力层、全连接层和层归一化组成。我们采用了“线性注意力Transformer”包 ${}^{2}$ 来提高计算效率。由于包含大量特征和长序列，我们做出了这一选择。该包实现了一种高效的注意力机制 [33]，并且我们仅使用了该包中的全局注意力特征。

---

<!-- Footnote -->

thttps://github.com/lucidrains/linear-attention-transformer

https://github.com/lucidrains/linear-attention-transformer

<!-- Footnote -->

---

### B.4 Metrics

### B.4 指标

We will introduce the metrics in our experiments. We summarize them as below:

我们将介绍实验中的指标。总结如下：

CRPS. CRPS [24] is a univariate strictly proper scoring rule which ' measures the compatibility of a cumulative distribution function $F$ with an observation $x$ as:

连续 ranked 概率得分（CRPS）。CRPS [24] 是一种单变量严格适当评分规则，用于衡量累积分布函数 $F$ 与观测值 $x$ 的兼容性，公式如下：

$$
\operatorname{CRPS}\left( {F,x}\right)  = {\int }_{R}{\left( F\left( y\right)  - {\mathbb{1}}_{\left( x \leq  y\right) }\right) }^{2}{dy} \tag{13}
$$

where ${\mathbb{1}}_{\left( x \leq  y\right) }$ is the indicator function,which is 1 if $x \leq  y$ and 0 otherwise. The CRPS attains the minimum value when the predictive distribution $F$ same as the data distribution.

其中 ${\mathbb{1}}_{\left( x \leq  y\right) }$ 是指示函数（indicator function），若 $x \leq  y$ 则其值为 1，否则为 0。当预测分布 $F$ 与数据分布相同时，连续 ranked 概率得分（CRPS）达到最小值。

MAE and MSE. MAE and MSE are calculated in the formula below, $\widehat{{\mathbf{x}}^{P}}$ represents the predicted time series,and ${\mathbf{x}}^{P}$ represents the ground truth time series. MAE calculates the average absolute difference between predictions and true values, while MSE calculates the average squared difference between predictions and true values. A smaller MAE or MSE implies better predictions.

平均绝对误差（MAE）和均方误差（MSE）。MAE 和 MSE 按以下公式计算，$\widehat{{\mathbf{x}}^{P}}$ 表示预测的时间序列，${\mathbf{x}}^{P}$ 表示真实的时间序列。MAE 计算预测值与真实值之间的平均绝对差值，而 MSE 计算预测值与真实值之间的平均平方差值。MAE 或 MSE 越小，表示预测效果越好。

$$
{MAE} = \operatorname{mean}\left( \left| {\widehat{{\mathbf{x}}^{P}} - {\mathbf{x}}^{P}}\right| \right) 
$$

$$
{MSE} = \sqrt{\operatorname{mean}\left( \left| {\widehat{{\mathbf{x}}^{P}} - {\mathbf{x}}^{P}}\right| \right) } \tag{14}
$$

## NeurIPS Paper Checklist

## 神经信息处理系统大会（NeurIPS）论文清单

## 1. Claims

## 1. 声明

Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?

问题：摘要和引言中提出的主要声明是否准确反映了论文的贡献和范围？

Answer: [Yes]

答案：[是]

Justification: The abstract and introduction include the paper's contributions and scope.

理由：摘要和引言应涵盖论文的贡献和范围。

Guidelines:

指南：

- The answer NA means that the abstract and introduction do not include the claims made in the paper.

- 答案为“不适用（NA）”意味着摘要和引言未包含论文中提出的主张。

- The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.

- 摘要和/或引言应明确阐述所提出的主张，包括论文的贡献以及重要的假设和局限性。对这个问题回答“否”或“不适用（NA）”可能不会得到评审专家的认可。

- The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.

- 所提出的主张应与理论和实验结果相符，并反映出这些结果在其他场景中的可推广程度。

- It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.

- 只要明确指出论文并未实现这些目标，将理想目标作为研究动机是可以的。

## 2. Limitations

## 2. 局限性

Question: Does the paper discuss the limitations of the work performed by the authors?

问题：论文是否讨论了作者所开展工作的局限性？

Answer: [Yes]

答案：[是]

Justification: We discuss the limitations of our approach at Section 6

理由：我们在第6节讨论了我们方法的局限性

Guidelines:

指南：

- The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.

- 答案“不适用（NA）”表示论文没有局限性，而答案“否”表示论文存在局限性，但论文中未对这些局限性进行讨论。

- The authors are encouraged to create a separate "Limitations" section in their paper.

- 鼓励作者在论文中单独创建一个“局限性”部分。

- The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.

- 论文应指出所有强假设，并说明结果对于违反这些假设的稳健性（例如，独立性假设、无噪声设置、模型设定良好、仅局部成立的渐近近似）。作者应思考这些假设在实践中可能如何被违反以及会产生什么影响。

- The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.

- 作者应反思所提出主张的适用范围，例如，该方法是否仅在少数数据集上或少数几次运行中进行了测试。一般来说，实证结果通常依赖于隐含假设，这些假设应该明确阐述。

- The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.

- 作者应思考影响该方法性能的因素。例如，当图像分辨率较低或在低光照条件下拍摄图像时，人脸识别算法的性能可能较差。或者，语音转文本系统可能无法可靠地用于为在线讲座提供字幕，因为它无法处理专业术语。

- The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.

- 作者应讨论所提出算法的计算效率，以及它们如何随数据集大小进行扩展。

- If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.

- 如果适用，作者应讨论其方法在解决隐私和公平性问题方面可能存在的局限性。

- While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

- 虽然作者可能担心对局限性的完全坦诚会被审稿人用作拒绝的理由，但更糟糕的结果可能是审稿人发现论文中未承认的局限性。作者应做出最佳判断，并认识到支持透明度的个人行为在制定维护社区诚信的规范方面起着重要作用。审稿人将被特别告知，不会因作者坦诚说明局限性而予以惩罚。

## 3. Theory Assumptions and Proofs

## 3. 理论假设与证明

Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?

问题：对于每个理论结果，论文是否提供了完整的假设集和完整（且正确）的证明？

Answer: [NA]

答案：[不适用（NA）]

Justification: The paper does not include theoretical results

理由：论文未包含理论成果

Guidelines:

指南：

- The answer NA means that the paper does not include theoretical results.

- 答案“不适用（NA）”表示论文未包含理论成果。

- All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.

- 论文中的所有定理、公式和证明都应编号并相互引用。

- All assumptions should be clearly stated or referenced in the statement of any theorems.

- 在任何定理的陈述中，所有假设都应明确说明或引用。

- The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.

- 证明可以出现在主论文中，也可以出现在补充材料中。但如果证明出现在补充材料中，建议作者提供一个简短的证明概要，以帮助读者理解。

- Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.

- 相反，论文核心部分提供的任何非正式证明都应该在附录或补充材料中提供正式证明作为补充。

- Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental Result Reproducibility

- 证明所依赖的定理和引理应进行适当引用。4. 实验结果可重复性

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

问题：论文是否充分披露了重现论文主要实验结果所需的所有信息，且这些信息的披露程度会影响论文的主要主张和/或结论（无论是否提供代码和数据）？

Answer: [Yes]

答案：[是]

Justification: We provide all the information needed in the Experiment part.

理由：我们在实验部分提供了所需的所有信息。

Guidelines:

指南：

- The answer NA means that the paper does not include experiments.

- 答案“NA”表示论文未包含实验。

- If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.

- 如果论文包含实验，对这个问题回答“否”可能不会得到评审人员的认可：无论是否提供代码和数据，使论文具有可重复性都很重要。

- If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.

- 如果研究成果是一个数据集和/或模型，作者应描述为使研究结果具有可重复性或可验证性所采取的步骤。

- Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.

- 根据研究成果的不同，可重复性可以通过多种方式实现。例如，如果研究成果是一种新颖的架构，完整描述该架构可能就足够了；如果研究成果是一个特定的模型和实证评估，可能需要让其他人能够使用相同的数据集复制该模型，或者提供对该模型的访问权限。一般来说，发布代码和数据通常是实现这一目标的一种好方法，但也可以通过详细的复制结果说明、提供对托管模型的访问权限（例如，对于大语言模型的情况）、发布模型检查点或其他适合所进行研究的方式来实现可重复性。

- While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example

- 虽然神经信息处理系统大会（NeurIPS）不要求发布代码，但该会议要求所有投稿提供一些合理的可复现途径，这可能取决于研究成果的性质。例如

(a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.

(a) 如果研究成果主要是一种新算法，论文应明确说明如何复现该算法。

(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

(b) 如果研究成果主要是一种新的模型架构，论文应清晰、全面地描述该架构。

(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

(c) 如果研究成果是一个新模型（例如，一个大语言模型），那么应该有办法访问该模型以复现结果，或者有办法复现该模型（例如，使用开源数据集或构建数据集的说明）。

(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

(d) 我们认识到，在某些情况下可复现性可能比较棘手，在这种情况下，欢迎作者描述他们提供的特定可复现方式。对于闭源模型，可能对模型的访问在某种程度上受到限制（例如，仅限注册用户），但其他研究人员应该有某种途径来复现或验证结果。

## 5. Open access to data and code

## 5. 数据和代码的开放获取

Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

问题：该论文是否如补充材料中所述，提供数据和代码的开放获取权限，并附有足够的说明以如实重现主要实验结果？

## Answer: [No]

## 答案：[否]

Justification: The full code will be uploaded in the future.

理由：完整代码将在未来上传。

Guidelines:

指南：

- The answer NA means that paper does not include experiments requiring code.

- 答案“不适用（NA）”表示论文不包含需要代码的实验。

- Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details.

- 更多详情请参阅神经信息处理系统大会（NeurIPS）代码和数据提交指南（https://nips.cc/ public/guides/CodeSubmissionPolicy）。

- While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).

- 虽然我们鼓励发布代码和数据，但我们理解这可能无法实现，因此“否”是一个可以接受的答案。论文不能仅仅因为不包含代码而被拒绝，除非代码是论文贡献的核心内容（例如，对于一个新的开源基准测试）。

- The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details.

- 说明应包含为重现结果所需运行的确切命令和环境。有关详细信息，请参阅神经信息处理系统大会（NeurIPS）代码和数据提交指南（https://nips.cc/public/guides/CodeSubmissionPolicy）。

- The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

- 作者应提供数据访问和准备的说明，包括如何访问原始数据、预处理数据、中间数据和生成的数据等。

- The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

- 作者应提供脚本，以重现新提出的方法和基线的所有实验结果。如果只能重现部分实验，他们应说明脚本中省略了哪些实验以及原因。

- At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).

- 在提交时，为保持匿名性，作者应发布匿名版本（如适用）。

- Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.

- 建议在补充材料（附在论文后面）中提供尽可能多的信息，但允许包含数据和代码的网址。

## 6. Experimental Setting/Details

## 6. 实验设置/细节

Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?

问题：论文是否详细说明了理解研究结果所需的所有训练和测试细节（例如，数据划分、超参数、参数选择方式、优化器类型等）？

Answer: [Yes]

答案：[是]

Justification: We provide all the details about the dataset in appendix.

理由：我们在附录中提供了有关数据集的所有详细信息。

Guidelines:

指南：

- The answer NA means that the paper does not include experiments.

- 答案为“不适用（NA）”表示论文未包含实验内容。

- The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.

- 实验设置应在论文核心部分详细呈现，以便读者能够理解并解读研究结果。

- The full details can be provided either with the code, in appendix, or as supplemental material.

- 完整细节可以随代码一同提供，也可以放在附录中，或者作为补充材料提供。

## 7. Experiment Statistical Significance

## 7. 实验的统计学显著性

Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?

问题：论文是否适当地、正确地报告了误差线，或者提供了关于实验统计学显著性的其他合适信息？

Answer: [No]

答案：[否]

Justification: We do not report the error bars in our experiemnt.

理由：我们在实验中未报告误差线。

Guidelines:

指南：

- The answer NA means that the paper does not include experiments.

- 答案“NA”表示该论文未包含实验。

- The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

- 如果结果附有误差线、置信区间或统计显著性检验（至少针对支持论文主要论点的实验），作者应回答“是”。

- The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

- 应明确说明误差线所反映的变异因素（例如，训练/测试划分、初始化、某些参数的随机抽取，或在给定实验条件下的整体运行）。

- The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)

- 应解释计算误差线的方法（封闭形式公式、调用库函数、自助法等）。

- The assumptions made should be given (e.g., Normally distributed errors).

- 应给出所做的假设（例如，误差呈正态分布）。

- It should be clear whether the error bar is the standard deviation or the standard error of the mean.

- 应明确误差线是标准差还是均值的标准误差。

- It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a ${96}\% \mathrm{{CI}}$ ,if the hypothesis of Normality of errors is not verified.

- 报告1倍标准差（1-sigma）的误差线是可以的，但应该明确说明。如果误差的正态性假设未得到验证，作者最好报告2倍标准差（2-sigma）的误差线，而不是声称他们有${96}\% \mathrm{{CI}}$。

- For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).

- 对于非对称分布，作者应注意不要在表格或图表中显示对称的误差线，因为这可能会得出超出范围的结果（例如，负错误率）。

- If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

- 如果在表格或图表中报告了误差线，作者应在文本中解释这些误差线是如何计算的，并在文本中引用相应的图表或表格。

## 8. Experiments Compute Resources

## 8. 实验计算资源

Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?

问题：对于每个实验，论文是否提供了重现实验所需的计算机资源（计算工作节点类型、内存、执行时间）的足够信息？

## Answer: [Yes]

## 答案：[是]

Justification: The paper provide sufficient information on the computer resources in paragraph implement details.

理由：论文在实施细节段落中提供了关于计算机资源的足够信息。

Guidelines:

指南：

- The answer NA means that the paper does not include experiments.

- 答案“不适用（NA）”表示论文未包含实验。

- The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

- 论文应指明计算工作节点的类型（CPU 或 GPU）、内部集群或云服务提供商，包括相关的内存和存储情况。

- The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.

- 论文应提供每个单独实验运行所需的计算量，并估算总计算量。

- The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).

- 论文应披露整个研究项目所需的计算量是否超过论文中报告的实验所需的计算量（例如，未纳入论文的初步实验或失败实验）。

## 9. Code Of Ethics

## 9. 道德准则

Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?

问题：论文中进行的研究是否在各个方面都符合神经信息处理系统大会（NeurIPS）道德准则（https://neurips.cc/public/EthicsGuidelines）？

---

Answer: [Yes]

---

Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics Guidelines:

理由：论文中进行的研究符合神经信息处理系统大会（NeurIPS）道德准则指南：

- The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.

- 回答“不适用（NA）”意味着作者未查阅神经信息处理系统大会（NeurIPS）道德准则。

- If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

- 如果作者回答“否”，则应解释需要偏离道德准则的特殊情况。

- The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

- 作者应确保保持匿名性（例如，若因所在司法管辖区的法律法规有特殊考虑）。

## 10. Broader Impacts

## 10. 更广泛的影响

Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?

问题：论文是否讨论了所开展工作的潜在积极社会影响和消极社会影响？

Answer: [NA]

答案：[不适用]

Justification: There is no obvious societal impact of the work performed

理由：所开展的工作没有明显的社会影响

Guidelines:

指导原则：

- The answer NA means that there is no societal impact of the work performed.

- 答案“不适用”表示所开展的工作没有社会影响。

- If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

- 如果作者回答“不适用（NA）”或“否”，则应解释其研究工作为何对社会没有影响，或论文为何未探讨社会影响。

- Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

- 负面社会影响的例子包括潜在的恶意或非预期使用（例如，虚假信息、生成虚假档案、监控）、公平性考量（例如，部署可能做出对特定群体产生不公平影响决策的技术）、隐私考量和安全考量。

- The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.

- 会议预计许多论文将是基础研究，与特定应用无关，更不用说实际部署了。然而，如果存在通向任何负面应用的直接途径，作者应指出这一点。例如，指出生成模型质量的提升可用于生成虚假信息的深度伪造内容是合理的。另一方面，无需指出用于优化神经网络的通用算法可能使人们能够更快地训练生成深度伪造内容的模型。

- The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

- 作者应考虑以下几种可能产生的危害：技术按预期使用且正常运行时可能产生的危害；技术按预期使用但给出错误结果时可能产生的危害；以及（有意或无意）滥用技术所导致的危害。

- If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

- 如果存在负面的社会影响，作者还可以讨论可能的缓解策略（例如，对模型进行受限发布、除攻击手段外还提供防御措施、监测滥用情况的机制、监测系统如何随时间从反馈中学习的机制、提高机器学习（ML）的效率和可及性）。

## 11. Safeguards

## 11. 保障措施

Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?

问题：论文是否描述了针对可能被滥用的高风险数据或模型（例如预训练语言模型、图像生成器或抓取的数据集）的负责任发布所采取的保障措施？

Answer: [NA]

答案：[不适用]

Justification: The paper poses no such risks

理由：该论文不存在此类风险

Guidelines:

指南：

- The answer NA means that the paper poses no such risks.

- 答案“不适用”意味着该论文不存在此类风险。

- Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.

- 对于可能被滥用或具有两用性的高风险已发布模型，应附带必要的保障措施进行发布，以便对模型进行可控使用，例如要求用户遵守使用指南或限制条件才能访问模型，或实施安全过滤。

- Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.

- 从互联网上抓取的数据集可能会带来安全风险。作者应描述他们如何避免发布不安全的图像。

- We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

- 我们认识到提供有效的保障措施具有挑战性，而且许多论文并不要求这样做，但我们鼓励作者考虑这一点，并尽最大诚意努力。

## 12. Licenses for existing assets

## 12. 现有资产的许可

Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?

问题：论文中使用的资产（如代码、数据、模型）的创作者或原始所有者是否得到了恰当的认可，许可和使用条款是否被明确提及并得到了妥善遵守？

Answer: [Yes]

答案：[是]

Justification: All the original work are sited properly.

理由：所有原创作品都得到了恰当的引用。

Guidelines:

指南：

- The answer NA means that the paper does not use existing assets.

- 答案“NA”表示该论文未使用现有资源。

- The authors should cite the original paper that produced the code package or dataset.

- 作者应引用生成代码包或数据集的原始论文。

- The authors should state which version of the asset is used and, if possible, include a URL.

- 作者应说明所使用资源的版本，并在可能的情况下提供网址。

- The name of the license (e.g., CC-BY 4.0) should be included for each asset.

- 每个资源都应注明许可协议的名称（例如，知识共享署名 4.0 国际许可协议（CC - BY 4.0））。

- For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

- 对于从特定来源（例如，网站）抓取的数据，应提供该来源的版权信息和服务条款。

- If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

- 如果资产被发布，应提供包中的许可协议、版权信息和使用条款。对于热门数据集，paperswithcode.com/datasets 网站已为部分数据集整理了许可协议。该网站的许可指南有助于确定数据集的许可协议。

- For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

- 对于重新打包的现有数据集，应同时提供原始许可协议和派生资产的许可协议（如果有变更）。

- If this information is not available online, the authors are encouraged to reach out to the asset's creators.

- 如果无法在网上获取这些信息，建议作者联系资产的创建者。

13. New Assets

13. 新资产

Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?

问题：论文中引入的新资产是否有完善的文档记录，且文档是否随资产一并提供？

Answer: [NA]

答案：[不适用]

Justification: The paper does not release new assets

理由：该论文未发布新的资源

Guidelines:

指南：

- The answer NA means that the paper does not release new assets.

- 答案为“不适用（NA）”意味着该论文未发布新的资源。

- Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.

- 研究人员应通过结构化模板，在提交材料中说明数据集/代码/模型的详细信息。这包括训练细节、许可情况、局限性等。

- The paper should discuss whether and how consent was obtained from people whose asset is used.

- 论文应讨论是否以及如何获得了所使用资源所有者的同意。

- At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

- 提交时，记得对资源进行匿名处理（如适用）。你可以创建一个匿名化的URL，或包含一个匿名化的压缩文件。

## 14. Crowdsourcing and Research with Human Subjects

## 14. 众包与涉及人类受试者的研究

Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?

问题：对于涉及人类受试者的众包实验和研究，论文是否包含了提供给参与者的完整说明文本，以及（如适用）截图，还有关于补偿（如有）的详细信息？

Answer: [NA]

答案：[不适用]

Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines:

理由：该论文不涉及众包，也不涉及人类受试者研究。指南：

- The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

- 答案“不适用”表示论文不涉及众包，也不涉及人类受试者研究。

- Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

- 将此信息包含在补充材料中是可以的，但如果论文的主要贡献涉及人类受试者，那么应在正文尽可能详细地包含相关信息。

- According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

- 根据神经信息处理系统大会（NeurIPS）道德准则，参与数据收集、整理或其他工作的人员应至少获得数据收集所在国家的最低工资。

## 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects

## 15. 机构审查委员会（IRB）对涉及人类受试者研究的批准或同等批准

Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

问题：论文是否描述了研究参与者可能面临的风险，这些风险是否已向受试者披露，以及是否获得了机构审查委员会（IRB）的批准（或根据您所在国家或机构的要求获得了同等的批准/审查）？

Answer: [NA]

答案：[不适用]

Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines:

理由：论文不涉及众包，也不涉及对人类受试者的研究。指南：

- The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

- 答案“不适用”表示论文不涉及众包，也不涉及对人类受试者的研究。

- Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

- 根据开展研究的国家不同，任何涉及人类受试者的研究可能都需要获得机构审查委员会（IRB）的批准（或同等批准）。如果您已获得IRB批准，应在论文中明确说明这一点。

- We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.

- 我们认识到，不同机构和地区的相关程序可能存在显著差异，我们期望作者遵守神经信息处理系统大会（NeurIPS）的道德准则以及所在机构的指导方针。

- For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

- 对于初次投稿，请勿包含任何可能破坏匿名性的信息（如适用），例如进行评审的机构。