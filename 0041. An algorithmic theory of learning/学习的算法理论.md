# An algorithmic theory of learning: Robust concepts and random projection

# 学习的算法理论：鲁棒概念与随机投影

Rosa I. Arriaga - Santosh Vempala

罗莎·I·阿里亚加（Rosa I. Arriaga） - 桑托什·文帕拉（Santosh Vempala）

Received: 11 August 2003 / Revised: 27 July 2004 / Accepted: 10 November 2005 / Published online:

收稿日期：2003年8月11日 / 修回日期：2004年7月27日 / 录用日期：2005年11月10日 / 在线发表日期：

Abstract We study the phenomenon of cognitive learning from an algorithmic standpoint. How does the brain effectively learn concepts from a small number of examples despite the fact that each example contains a huge amount of information? We provide a novel algorithmic analysis via a model of robust concept learning (closely related to "margin classifiers"), and show that a relatively small number of examples are sufficient to learn rich concept classes. The new algorithms have several advantages—they are faster, conceptually simpler, and resistant to low levels of noise. For example, a robust half-space can be learned in linear time using only a constant number of training examples, regardless of the number of attributes. A general (algorithmic) consequence of the model, that "more robust concepts are easier to learn", is supported by a multitude of psychological studies.

摘要 我们从算法的角度研究认知学习现象。尽管每个示例都包含大量信息，但大脑是如何从少量示例中有效学习概念的呢？我们通过一个鲁棒概念学习模型（与“间隔分类器”密切相关）进行了新颖的算法分析，并表明相对较少的示例足以学习丰富的概念类。新算法有几个优点——它们速度更快、概念更简单，并且能抵御低水平的噪声。例如，无论属性数量多少，仅使用固定数量的训练示例就可以在线性时间内学习一个鲁棒半空间。该模型的一个普遍（算法）结论，即“更鲁棒的概念更容易学习”，得到了大量心理学研究的支持。

Keywords Learning $\cdot$ Cognition $\cdot$ Random projection $\cdot$ Robust concepts

关键词 学习 $\cdot$ 认知 $\cdot$ 随机投影 $\cdot$ 鲁棒概念

## 1. Introduction

## 1. 引言

One motivation of computational learning theory is to gather insight into cognitive processes. The exact physical processes underlying learning, indeed any aspect of cognition, are far from being understood. Even from a purely theoretical standpoint, it is mostly a mystery as to how the brain copes with huge amounts of data. How does the brain effectively learn concepts from a relatively small number of examples, when each example consists of a huge amount of information?

计算学习理论的一个动机是深入了解认知过程。学习背后的确切物理过程，实际上是认知的任何方面，都远未被理解。即使从纯粹的理论角度来看，大脑如何处理大量数据在很大程度上仍是个谜。当每个示例都包含大量信息时，大脑如何从相对较少的示例中有效地学习概念呢？

---

<!-- Footnote -->

Editor: Shai Ben-David

编辑：沙伊·本 - 戴维（Shai Ben-David）

A preliminary version of this paper appeared in the Proc. of the Symposium on the Foundations of Computer Science, 1999

本文的初稿发表于1999年计算机科学基础研讨会会议录（Proc. of the Symposium on the Foundations of Computer Science）

R. I. Arriaga

R. I. 阿里亚加（R. I. Arriaga）

Department of Psychology, Southern New Hampshire University

新罕布什尔南方大学（Southern New Hampshire University）心理学系

e-mail: r.arriaga@snhu.edu

电子邮件：r.arriaga@snhu.edu

S.Vempala (§)

桑托什·文帕拉（S.Vempala (§)）

Department of Mathematics, M.I.T.

麻省理工学院（M.I.T.）数学系

e-mail: vempala@math.mit.edu

电子邮件：vempala@math.mit.edu

<!-- Footnote -->

---

There are at least two approaches to explaining this phenomenon. The first, due to Valiant, is attribute-efficient learning (Valiant, 1998; Littlestone, 1987, 1991). In this model, it is assumed that the target concept is simple in a specific manner: it is a function of only a small subset of the set of attributes, called the relevant attributes, while the rest are irrelevant. From this assumption one can typically argue that the VC-dimension of the resulting concept class is a function of only the number of relevant attributes(k),and hence derive a bound on the number of examples required. Unfortunately, although the model is theoretically clean and appealing, it is not known how to learn anything more complex than a disjunction of variables (without membership queries). Further,it is NP-hard to learn a disjunction of $k$ variables as a disjunction of fewer than $k\log n$ variables (where $n$ is the total number of variables).

解释这一现象至少有两种方法。第一种方法由瓦利安特（Valiant）提出，即属性高效学习（瓦利安特，1998年；利特尔斯特恩，1987年，1991年）。在这个模型中，假设目标概念以一种特定的方式呈现出简单性：它仅是属性集合中一个小子集（称为相关属性）的函数，而其余属性则无关紧要。基于这一假设，通常可以推断出所得概念类的VC维仅是相关属性数量(k)的函数，从而得出所需示例数量的界限。遗憾的是，尽管该模型在理论上简洁且有吸引力，但目前尚不清楚如何学习比变量析取更复杂的内容（在没有成员查询的情况下）。此外，将$k$个变量的析取学习为少于$k\log n$个变量的析取是NP难问题（其中$n$是变量的总数）。

In this paper, we study a different approach based on a simple idea which is illustrated in the following example. Imagine a child learning the concept of an "elephant". We point the child to pictures of elephants or to real elephants a few times and say "elephant", and perhaps to a few examples of other animals and say their names (i.e., "not elephant"). From then on, the child will almost surely correctly label only elephants as elephants. On the other hand, imagine a child learning the concept of "African elephant" (as opposed to the Indian elephant) just from examples. It will probably take many more examples, and perhaps even be necessary to explicitly point out the bigger ears of the African elephant.

在本文中，我们研究一种基于简单理念的不同方法，该理念将在以下示例中说明。想象一个孩子正在学习“大象”的概念。我们多次让孩子看大象的图片或真正的大象，并说“大象”，也许还会给他看其他一些动物的例子并说出它们的名字（即“不是大象”）。从那时起，这个孩子几乎肯定只会正确地将大象标记为大象。另一方面，想象一个孩子仅通过实例来学习“非洲象”（与亚洲象相对）的概念。这可能需要更多的实例，甚至可能有必要明确指出非洲象耳朵更大这一特征。

The crucial difference in the two concepts above is not in the number of attributes, or even in the number of relevant attributes of the examples presented, but in the similarity of examples with the same label and in the dissimilarity of examples with different labels. There is a clearer demarcation between elephants and non-elephants than there is between African elephants and Indian elephants. This notion will be formalized later as the robustness of a concept. An alternative perspective of robustness is that it is a measure of how much the attributes of an example can be altered without affecting the concept. The main feature of robust concepts is that the number of examples and the time required to learn a robust concept can be bounded as a function of the robustness (denoted by a parameter $\ell$ ),and do not depend on the total number of attributes. The model and the parameter $\ell$ are defined precisely in Section 2. As we discuss there, the model is very closely related to Large Margin classifiers studied in machine learning, that are in turn the basis for Support Vector Machines (Vapnik, 1995; Cortes & Vapnik, 1995).

上述两个概念的关键区别不在于属性的数量，甚至也不在于所呈现示例的相关属性数量，而在于具有相同标签的示例的相似性以及具有不同标签的示例的差异性。大象与非大象之间的界限比非洲象（African elephants）和亚洲象（Indian elephants）之间的界限更为清晰。这一概念稍后将被形式化为概念的鲁棒性。鲁棒性的另一种观点是，它衡量的是在不影响概念的情况下，示例的属性可以改变的程度。鲁棒概念的主要特征是，学习一个鲁棒概念所需的示例数量和时间可以表示为鲁棒性（用参数 $\ell$ 表示）的函数，并且不依赖于属性的总数。模型和参数 $\ell$ 在第 2 节中进行了精确的定义。正如我们在那里所讨论的，该模型与机器学习中研究的大间隔分类器密切相关，而大间隔分类器又是支持向量机（Support Vector Machines，Vapnik，1995；Cortes & Vapnik，1995）的基础。

In the robust concept model, the main new observation is that we can employ a general procedure to reduce the dimensionality of examples, independent of the concept class. While reducing the dimensionality of examples, we would like to preserve concepts. So, for example, if our original concept class is the set of half-spaces (linear thresholds) in $n$ -dimensional space, we would like to map examples to a $k$ -dimensional space,where $k$ is much smaller than $n$ , and maintain the property that some half-space in the $k$ -dimensional space correctly classifies (most of) the examples. We show that Random Projection, the technique of projecting a set of points to a randomly chosen low-dimensional space, is suitable for this purpose. It has been observed that random projection (approximately) preserves key properties of a set of points, e.g., the distances between pairs of points (Johnson & Lindenstrauss, 1984); this has led to efficient algorithms in several other contexts (Kleinberg, 1997; Linial, et al., 1994; Vempala, 2004). In Section 3, we develop "neuronal" versions of random projection, i.e., we demonstrate that it is easy to implement it using a single layer of perceptrons where the weights of the network are chosen independently and from any one of a class of distributions; this class includes discrete distributions such as the picking 1 or -1 with equal probability. Our theorems Springer here can be viewed as extensions/refinements of the work of Johnson & Lindenstrauss (1984) and Frankl and Maehara (1988).

在鲁棒概念模型中，主要的新发现是，我们可以采用一种通用程序来降低示例的维度，而这与概念类无关。在降低示例维度的同时，我们希望保留概念。例如，如果我们最初的概念类是 $n$ 维空间中的半空间（线性阈值）集合，我们希望将示例映射到 $k$ 维空间，其中 $k$ 远小于 $n$ ，并保持 $k$ 维空间中的某个半空间能正确分类（大部分）示例的特性。我们表明，随机投影（将一组点投影到随机选择的低维空间的技术）适用于此目的。已有研究发现，随机投影（近似地）保留了一组点的关键特性，例如点对之间的距离（约翰逊和林登施特劳斯（Johnson & Lindenstrauss），1984 年）；这在其他几个场景中催生了高效的算法（克莱因伯格（Kleinberg），1997 年；利尼尔等人（Linial, et al.），1994 年；温帕拉（Vempala），2004 年）。在第 3 节中，我们开发了随机投影的“神经元”版本，即我们证明了使用单层感知器很容易实现它，其中网络的权重是独立选择的，并且来自一类分布中的任意一种；这类分布包括离散分布，例如以相等概率选取 1 或 -1。我们在这里的定理可以看作是对约翰逊和林登施特劳斯（1984 年）以及弗兰克尔和前原（Frankl and Maehara）（1988 年）工作的扩展/细化。

Then we address the question of how many examples are needed to efficiently learn a concept with robustness $\ell$ . We begin with the concept class of half-spaces with $n$ attributes. In this case,it is already known that one needs $O\left( {1/{\ell }^{2}}\right)$ examples (Bartlett &Shawe-Taylor, 1998; Vapnik, 1995; Freund & Schapire, 1999). Here we show that a simple algorithm based on random projection gives an alternative proof of such a guarantee.

然后，我们探讨了为了高效且稳健地学习一个概念$\ell$需要多少示例的问题。我们从具有$n$个属性的半空间概念类开始。在这种情况下，已知需要$O\left( {1/{\ell }^{2}}\right)$个示例（巴特利特（Bartlett）和肖韦 - 泰勒（Shawe - Taylor），1998年；瓦普尼克（Vapnik），1995年；弗罗因德（Freund）和沙皮尔（Schapire），1999年）。在这里，我们表明基于随机投影的简单算法为这一保证提供了另一种证明。

Next we consider other rich concept classes, namely intersections of half-spaces and ellipsoids. Using neuronal random projection, we demonstrate that the examples can first be projected down to a space whose dimension is a function of $\ell$ ,and in some cases an additional parameter of the concept class (e.g. the number of half-spaces when the concept class is intersections of half-spaces etc.), but does not depend on the number of attributes of the examples. This then allows us to bound the number of examples required to learn the concepts as a function of $\ell$ ,independent of the original number of attributes,via well-known generalization theorems based on the VC-dimension (Vapnik & Chervonenkis, 1971).

接下来，我们考虑其他丰富的概念类，即半空间和椭球体的交集。利用神经元随机投影，我们证明了可以先将示例投影到一个维度是$\ell$的函数的空间中，并且在某些情况下，该维度还是概念类的一个额外参数（例如，当概念类是半空间的交集时，该参数为半空间的数量等）的函数，但不依赖于示例的属性数量。然后，通过基于VC维（Vapnik & Chervonenkis，1971）的著名泛化定理，我们能够将学习这些概念所需的示例数量界定为$\ell$的函数，而与原始属性数量无关。

The proposed algorithms are fast-their running time is linear in $n$ - since after random projection (which takes time linear in $n$ ),all the work happens in the smaller-dimensional space with a small number of sample points. Indeed, this suggests that the algorithms studied here could be used in SVM's in place of current solutions (Cortes & Vapnik, 1995; Freund & Schapire, 1999) such as quadratic optimization in a dual space called the kernel space.

所提出的算法速度很快——它们的运行时间与$n$呈线性关系——因为在随机投影（其所需时间与$n$呈线性关系）之后，所有的计算都在具有少量样本点的低维空间中进行。实际上，这表明本文研究的算法可以在支持向量机（SVM）中替代当前的解决方案（科尔特斯和瓦普尼克（Cortes & Vapnik），1995年；弗罗因德和沙皮尔（Freund & Schapire），1999年），例如在称为核空间的对偶空间中进行二次优化。

In Section 4.4, we mention the noise tolerance properties of the algorithms, notably that agnostic learning is possible, and (equivalently) that it is possible to find hypotheses that minimize the number of misclassified points, for fairly low robustness.

在4.4节中，我们提到了这些算法的噪声容忍特性，特别是无偏学习是可行的，并且（等价地）对于相当低的鲁棒性，可以找到使误分类点数量最小化的假设。

### 1.1. Related work

### 1.1. 相关工作

The main contribution of this paper is a new perspective on learning via a connection to dimension reduction. This facilitates efficient algorithms which use small sample sizes. It also gives a simple intuitive way to see the $O\left( {1/{\epsilon }^{2}}\right)$ sample complexity bounds of margin classifiers (SVM’s) (Bartlett & Shawe-Taylor, 1998). It is related to previous work (Schapire et al., 1998) which showed that generalization error can be bounded in terms of the observed margin of examples (a more refined notion of margin is used there, but is similar in spirit). As we discuss in Section 5.1, it seems to fit well with attempts to model cognition on a computational basis (Valiant, 1998), and predicts the commonly observed phenomenon that finer distinctions take more examples. From a purely computational viewpoint, these are simple new algorithms for fundamental learning theory problems, that might be practical.

本文的主要贡献是通过与降维建立联系，为学习提供了一个新视角。这有助于开发使用小样本量的高效算法。它还提供了一种简单直观的方法来理解间隔分类器（支持向量机）的$O\left( {1/{\epsilon }^{2}}\right)$样本复杂度界限（巴特利特和肖韦 - 泰勒，1998年）。这与之前的工作（沙皮尔等人，1998年）相关，该工作表明泛化误差可以根据观察到的样本间隔来界定（那里使用了更精细的间隔概念，但本质上类似）。正如我们在5.1节中讨论的，它似乎与基于计算对认知进行建模的尝试相契合（瓦利安特，1998年），并能预测常见的现象，即更精细的区分需要更多的样本。从纯粹的计算角度来看，这些是解决基础学习理论问题的简单新算法，可能具有实用性。

There have been further applications of random projection in learning theory subsequent to this work. Garg, et al. (2002) and Garg and Roth (2003) have pursued similar ideas, developing the related notion of projection profile. Recently, Balcan, et al. (2004) have used random projection to give an efficient new interpretation of kernel functions. Klivans and Servedio (2004) have used polynomial threshold functions in the context of robust concepts to get substantially improved time bounds. Specifically, they give faster algorithms for learning intersections (and other functions) of $t$ half-spaces (with some increase in the sample complexity). Finally, Ben-David, et al. (2002) have used random projection to show an interesting lower bound on learning with half-spaces. They prove that "most" concept classes of even constant VC-dimension cannot be embedded into half-spaces where the dimension of the Euclidean space is small or the margin is large. Thus, algorithms based on first transforming to half-spaces cannot gain much in terms of the margin or the dimension.

在这项工作之后，随机投影在学习理论中得到了进一步的应用。加尔格（Garg）等人（2002年）以及加尔格和罗斯（Garg and Roth，2003年）进行了类似的研究，提出了投影轮廓的相关概念。最近，巴尔坎（Balcan）等人（2004年）利用随机投影对核函数给出了一种高效的新解释。克利万斯和瑟维迪奥（Klivans and Servedio，2004年）在鲁棒概念的背景下使用多项式阈值函数，显著改善了时间界限。具体而言，他们针对$t$个半空间的交集（以及其他函数）学习给出了更快的算法（样本复杂度略有增加）。最后，本 - 戴维（Ben - David）等人（2002年）利用随机投影证明了半空间学习的一个有趣的下界。他们证明，即使是具有常数VC维的“大多数”概念类，也无法嵌入到欧几里得空间维度较小或间隔较大的半空间中。因此，基于先转换为半空间的算法在间隔或维度方面无法获得太大优势。

### 2.The model

### 2. 模型

To describe the model, we adopt the terminology used in the literature. We assume that attributes are real valued; an example is a point in ${\mathbf{R}}^{n}$ ; a concept is a subset of ${\mathbf{R}}^{n}$ . An example that belongs to a concept is labelled positive for the concept, and an example that lies outside the concept is labelled a negative example.

为了描述该模型，我们采用文献中使用的术语。我们假设属性是实值的；一个示例是${\mathbf{R}}^{n}$中的一个点；一个概念是${\mathbf{R}}^{n}$的一个子集。属于某个概念的示例被标记为该概念的正例，而位于该概念之外的示例则被标记为反例。

Given a set of labelled examples drawn from an unknown distribution $\mathcal{D}$ in ${\mathbf{R}}^{n}$ ,and labelled according to an unknown target concept the learning task is to find a hypothesis with low error. A hypothesis is a polynomial-time computable function. The error of a hypothesis $h$ with respect to the target concept is the probability that $h$ disagrees with the target function on a random example drawn from $\mathcal{D}$ . Thus,if $h$ has error $\epsilon$ ,then the probability for a random $x$ that $h\left( x\right)$ disagrees with the target concept is at most $\epsilon$ . So,given an error parameter $\epsilon$ and a confidence parameter $\delta$ ,with probability at least $1 - \delta$ ,the algorithm has to find a concept that has error at most $\epsilon$ on $\mathcal{D}$ (Valiant,1984).

给定一组从${\mathbf{R}}^{n}$中未知分布$\mathcal{D}$抽取的带标签示例，并且这些示例根据一个未知的目标概念进行了标注，学习任务就是要找到一个误差较小的假设。假设是一个多项式时间可计算的函数。假设$h$相对于目标概念的误差是指，从$\mathcal{D}$中随机抽取一个示例时，$h$与目标函数不一致的概率。因此，如果$h$的误差为$\epsilon$，那么对于一个随机的$x$，$h\left( x\right)$与目标概念不一致的概率至多为$\epsilon$。所以，给定一个误差参数$\epsilon$和一个置信参数$\delta$，算法必须以至少$1 - \delta$的概率找到一个在$\mathcal{D}$上误差至多为$\epsilon$的概念（瓦利安特（Valiant），1984年）。

The basic insight of the new model is the idea of robustness (implicit in earlier work). Intuitively, a concept is "robust" if it is immune to attribute noise. That is, modifying the attributes of an example by some bounded amount does not change its label. Another interpretation is that points with different labels are far apart. This is formalized below:

新模型的基本见解是鲁棒性（在早期工作中隐含提及）的概念。直观地说，如果一个概念对属性噪声具有免疫力，那么它就是“鲁棒的”。也就是说，在一定范围内修改示例的属性不会改变其标签。另一种解释是，具有不同标签的点相距较远。这一点将在下面进行形式化表述：

Definition 1. For any real number $\ell  > 0$ ,a concept $C$ in conjunction with a distribution $\mathcal{D}$ in ${\mathbf{R}}^{n}$ ,is said to be $\ell$ -robust,if

定义1. 对于任意实数 $\ell  > 0$，与 ${\mathbf{R}}^{n}$ 中的分布 $\mathcal{D}$ 相关联的概念 $C$，若满足以下条件，则称其为 $\ell$ -鲁棒的：

$$
{\mathrm{P}}_{\mathcal{D}}\left( {x\mid \exists y : \operatorname{label}\left( x\right)  \neq  \operatorname{label}\left( y\right) ,\left| \right| x - y\left| \right|  \leq  \ell }\right)  = 0
$$

The norm $\left| \right| x - y\left| \right|$ is the Euclidean distance between $x$ and $y$ . This can be replaced by other norms,but we use the Euclidean norm in this paper. The probability is over all points $x$ with the property that there is some point $y$ with a different label within a distance $\ell$ . In other words, a concept is $\ell$ -robust if there is zero probability of points being within $\ell$ of the boundary of the concept. The definition could be weakened by requiring only that the above probability should be negligible (e.g. $1/{2}^{n}$ ). When $\mathcal{D}$ is over a discrete subset of ${\mathbf{R}}^{n}$ ,then this has a simple interpretation. A ball of radius $\ell$ around any point $x$ of non-zero probability lies entirely on one side of the concept,i.e.,every point in the ball has the same label as $x$ . To avoid scaling issues, we usually consider only distributions whose support is (a subset of) the unit ball in ${\mathbf{R}}^{n}$ ,i.e.,all examples given to the algorithm will have length at most 1 (alternatively,one could incorporate normalize the distance between examples by their length, but we find our definition more convenient). Given access to examples from a robust concept, and parameters $\epsilon ,\delta$ ,a learning algorithm succeeds and is said to $\left( {\epsilon ,\delta }\right)$ -learn if,with probability at least $1 - \delta$ , it produces a hypothesis that is consistent with at least $1 - \epsilon$ of the example distribution. Note that strictly speaking this is not PAC-learning since robustness restricts the example distribution.

范数 $\left| \right| x - y\left| \right|$ 是 $x$ 和 $y$ 之间的欧几里得距离（Euclidean distance）。此范数可以用其他范数替代，但本文使用欧几里得范数（Euclidean norm）。该概率是针对所有具有如下性质的点 $x$ 而言的：存在某个点 $y$，其标签与 $x$ 不同，且二者距离在 $\ell$ 以内。换句话说，如果一个概念边界 $\ell$ 范围内的点出现的概率为零，那么该概念就是 $\ell$ -鲁棒的（$\ell$ -robust）。可以通过仅要求上述概率可忽略不计（例如 $1/{2}^{n}$）来弱化该定义。当 $\mathcal{D}$ 是 ${\mathbf{R}}^{n}$ 的离散子集时，这有一个简单的解释。以任何非零概率点 $x$ 为中心、半径为 $\ell$ 的球完全位于该概念的一侧，即球内的每个点都与 $x$ 具有相同的标签。为避免尺度问题，我们通常只考虑其支撑集是 ${\mathbf{R}}^{n}$ 中单位球（的子集）的分布，即提供给算法的所有示例的长度至多为 1（或者，可以根据示例的长度对示例之间的距离进行归一化处理，但我们发现我们的定义更方便）。给定对来自一个鲁棒概念的示例的访问权限以及参数 $\epsilon ,\delta$，如果一个学习算法以至少 $1 - \delta$ 的概率产生一个与至少 $1 - \epsilon$ 的示例分布一致的假设，那么该算法就成功了，并且称其为 $\left( {\epsilon ,\delta }\right)$ -学习。请注意，严格来说这不是 PAC 学习（Probably Approximately Correct learning），因为鲁棒性限制了示例分布。

In what follows, we present tools and algorithms for learning robust concepts. It is worth noting that "robustness" refers only to the target concept; it is not required of all concepts in the class.

在接下来的内容中，我们将介绍用于学习鲁棒概念（robust concepts）的工具和算法。值得注意的是，“鲁棒性（robustness）”仅指目标概念；并不要求该类别中的所有概念都具备此特性。

### 2.1. Connection to existing models

### 2.1. 与现有模型的联系

The model is closely related to large margin classifiers used in Support Vector Machines (Bartlett & Shawe-Taylor, 1998). Indeed, for the concept class of half-spaces, the robustness Springer as defined here is exactly the largest possible margin of a correctly classifying half-space (with the normalization that all the examples are from the unit ball). In general, however, there is a subtle but important difference. Whereas in SVM's the margin is measured in the "lifted" space where concepts have been transformed to half-spaces, in our model we measure robustness in the space in which examples are presented to us (and hence the natural relationship with attribute noise). The robustness is also closely related to the parameter $\gamma$ used in the definition of the fat-shattering dimension (Kearns & Schapire, 1994; Bartlett & Shawe-Taylor, 1998), and once again coincides (up to a scaling factor) in the case of half-spaces.

该模型与支持向量机中使用的大间隔分类器密切相关（巴特利特和肖 - 泰勒，1998年）。实际上，对于半空间的概念类，此处定义的鲁棒性（Springer）恰好是正确分类半空间的最大可能间隔（假设所有示例都来自单位球）。然而，一般来说，存在一个细微但重要的差异。在支持向量机中，间隔是在概念已被转换为半空间的“提升”空间中测量的，而在我们的模型中，我们在向我们呈现示例的空间中测量鲁棒性（因此与属性噪声存在自然联系）。鲁棒性还与胖粉碎维度定义中使用的参数$\gamma$密切相关（卡恩斯和沙皮尔，1994年；巴特利特和肖 - 泰勒，1998年），并且在半空间的情况下再次重合（相差一个比例因子）。

### 3.The main tool: "neuron-friendly" random projection

### 3. 主要工具：“神经元友好”随机投影

In this section we develop "neuronal" versions of random projection, including a discrete version, and provide probabilistic guarantees for them, all with transparent proofs. Besides being neuron-friendly, these versions of random projection are easier to implement.

在本节中，我们开发了随机投影的“神经元”版本，包括离散版本，并为它们提供了概率保证，所有证明都清晰易懂。除了对神经元友好之外，这些随机投影版本更易于实现。

To project a given point $u \in  {\mathbf{R}}^{n}$ to a $k$ -dimensional space,we first choose $k$ random vectors ${R}_{1},\ldots ,{R}_{k}$ (we will shortly discuss suitable probability distributions for these vectors). Then we compute a $k$ -dimensional vector ${u}^{\prime }$ whose coordinates are the inner products ${u}_{1}^{\prime } =$ ${R}_{1}^{T} \cdot  u,\ldots ,{u}_{k}^{\prime } = {R}_{k}^{T} \cdot  u$ . If we let $R$ be the $n \times  k$ matrix whose columns are the vectors ${R}_{1},\ldots ,{R}_{k}$ ,then the projection can be succinctly written as ${u}^{\prime } = {R}^{T}u$ . To project a set of points ${u}^{1},\ldots ,{u}^{m}$ in ${\mathbf{R}}^{n}$ to ${\mathbf{R}}^{k}$ ,we choose a random matrix $R$ as above,and compute the vectors ${R}^{T}{u}^{1},\ldots ,{R}^{T}{u}^{m}$ .

为了将给定的点 $u \in  {\mathbf{R}}^{n}$ 投影到 $k$ 维空间，我们首先选择 $k$ 个随机向量 ${R}_{1},\ldots ,{R}_{k}$（我们将很快讨论这些向量合适的概率分布）。然后，我们计算一个 $k$ 维向量 ${u}^{\prime }$，其坐标为内积 ${u}_{1}^{\prime } =$ ${R}_{1}^{T} \cdot  u,\ldots ,{u}_{k}^{\prime } = {R}_{k}^{T} \cdot  u$。如果我们令 $R$ 为列向量是 ${R}_{1},\ldots ,{R}_{k}$ 的 $n \times  k$ 矩阵，那么投影可以简洁地表示为 ${u}^{\prime } = {R}^{T}u$。为了将 ${\mathbf{R}}^{n}$ 中的一组点 ${u}^{1},\ldots ,{u}^{m}$ 投影到 ${\mathbf{R}}^{k}$，我们如上选择一个随机矩阵 $R$，并计算向量 ${R}^{T}{u}^{1},\ldots ,{R}^{T}{u}^{m}$。

Given the matrix $R$ ,the above procedure is a simple computational task. It has been shown that if $R$ is a random orthonormal matrix,i.e.,the columns of $R$ are random unit vectors and they are pairwise orthogonal, then the projection preserves all pairwise distances to within a factor of $\left( {1 + \epsilon }\right)$ for a surprisingly small value of $k$ of about $\log n/{\epsilon }^{2}$ (Johnson & Lindenstrauss, 1984). The main observation of this section is to show that this is a rather robust phenomenon,in that the entries of $R$ can be chosen from any distribution with bounded moments. In particular it suffices to use random matrices with independent entries chosen from a distribution with bounded support. It is then an easy consequence that the task of random projection can be achieved by a simple 1-layer neural network,viz., $k$ perceptrons (which compute linear combinations of their inputs) each with one output and the same $n$ inputs. The weights of the neural network are assumed to be random and independent. This is illustrated in Fig. 1. Let $r \in  {\mathbf{R}}^{n}$ be a random vector whose coordinates are independent and identically distributed. We highlight the following two possibilities for the distribution of the coordinates: (a) the standard normal distribution, with mean 0 and variance 1 , referred to as $N\left( {0,1}\right)$ ,(b) the discrete distribution defined by ${r}_{i} = 1$ with probability $\frac{1}{2}$ and ${r}_{i} =  - 1$ with probability $\frac{1}{2}$ ,which we will refer to as $U\left( {-1,1}\right)$ . Following the conference version of this paper (Arriaga &Vempala,1999),another proof for the case $U\left( {-1,1}\right)$ has also appeared (Achlioptas, 2001). The following well-known lemma will be useful. We provide a proof for convenience.

给定矩阵 $R$，上述过程是一项简单的计算任务。研究表明，如果 $R$ 是一个随机正交矩阵，即 $R$ 的列是随机单位向量且两两正交，那么对于约为 $\log n/{\epsilon }^{2}$ 的 $k$ 的一个小得惊人的值，投影能将所有两两之间的距离保持在 $\left( {1 + \epsilon }\right)$ 这个因子范围内（约翰逊（Johnson）和林登施特劳斯（Lindenstrauss），1984 年）。本节的主要发现是，这是一种相当稳健的现象，因为 $R$ 的元素可以从任何具有有界矩的分布中选取。特别地，使用元素相互独立且从具有有界支撑的分布中选取的随机矩阵就足够了。由此很容易得出，随机投影任务可以通过一个简单的单层神经网络来实现，即 $k$ 个感知机（它们计算其输入的线性组合），每个感知机有一个输出和相同的 $n$ 个输入。假设神经网络的权重是随机且相互独立的。这在图 1 中有所说明。设 $r \in  {\mathbf{R}}^{n}$ 是一个坐标相互独立且同分布的随机向量。我们着重指出坐标分布的以下两种可能性：(a) 标准正态分布，均值为 0，方差为 1，记为 $N\left( {0,1}\right)$；(b) 由 ${r}_{i} = 1$ 以概率 $\frac{1}{2}$ 出现和 ${r}_{i} =  - 1$ 以概率 $\frac{1}{2}$ 出现所定义的离散分布，我们将其记为 $U\left( {-1,1}\right)$。继本文的会议版本（阿里亚加（Arriaga）和温帕拉（Vempala），1999 年）之后，针对 $U\left( {-1,1}\right)$ 这种情况的另一个证明也已出现（阿赫利奥普塔斯（Achlioptas），2001 年）。下面这个著名的引理会很有用。为方便起见，我们给出一个证明。

Lemma 1. Let $X$ be drawn from $N\left( {0,\sigma }\right)$ ,the normal distribution with mean zero and standard deviation $\sigma$ . Then for any $\alpha  < \frac{1}{2{\sigma }^{2}}$ ,

引理1。设$X$从$N\left( {0,\sigma }\right)$（均值为零、标准差为$\sigma$的正态分布）中抽取。那么对于任意$\alpha  < \frac{1}{2{\sigma }^{2}}$，

$$
\mathrm{E}\left( {e}^{\alpha {X}^{2}}\right)  = \frac{1}{\sqrt{1 - {2\alpha }{\sigma }^{2}}}.
$$

<!-- Media -->

Fig. 1 Neuronal Random

图1 神经元随机

<!-- figureText: y2 -->

<img src="https://cdn.noedgeai.com/01957bfc-7058-7d65-9065-344d36443d8e_5.jpg?x=639&y=186&w=615&h=614&r=0"/>

Projection

投影

<!-- Media -->

Proof: We recall the density function of $N\left( {0,\sigma }\right)$ ,the normal distribution with mean 0 and standard deviation $\sigma$ ,to be

证明：我们回顾$N\left( {0,\sigma }\right)$（均值为0、标准差为$\sigma$的正态分布）的概率密度函数为

$$
\frac{1}{\sqrt{2\pi }\sigma }{e}^{-\frac{{x}^{2}}{2{\sigma }^{2}}}.
$$

Using this,

利用这一点，

$$
\mathrm{E}\left( {e}^{\alpha {X}^{2}}\right)  = {\int }_{-\infty }^{\infty }{e}^{\alpha {x}^{2}}\frac{1}{\sqrt{2\pi }\sigma }{e}^{-\frac{{x}^{2}}{2{\sigma }^{2}}}{dx}
$$

$$
 = {\int }_{-\infty }^{\infty }\frac{1}{\sqrt{2\pi }\sigma }{e}^{-\frac{{x}^{2}}{2{\sigma }^{2}}\left( {1 - {2\alpha }{\sigma }^{2}}\right) }{dx}
$$

$$
 = \frac{1}{\sqrt{1 - {2\alpha }{\sigma }^{2}}}{\int }_{-\infty }^{\infty }\frac{\sqrt{1 - {2\alpha }{\sigma }^{2}}}{\sqrt{2\pi }\sigma }{e}^{-\frac{{x}^{2}}{2{\sigma }^{2}}\left( {1 - {2\alpha }{\sigma }^{2}}\right) }{dx}
$$

$$
 = \frac{1}{\sqrt{1 - {2\alpha }{\sigma }^{2}}}\text{.}
$$

Here we have used the observation that the integrand is the normal density with standard deviation $\sigma /\sqrt{1 - {2\alpha }{\sigma }^{2}}$ .

这里我们利用了被积函数是标准差为$\sigma /\sqrt{1 - {2\alpha }{\sigma }^{2}}$的正态密度这一观察结果。

We begin with the case when each entry of the projection matrix is chosen independently from the standard Normal distribution.

我们从投影矩阵的每个元素都独立地从标准正态分布中选取的情况开始。

Lemma 2. Let $R = \left( {r}_{ij}\right)$ be a random $n \times  k$ matrix,such that each entry ${r}_{ij}$ is chosen independently according to $N\left( {0,1}\right)$ . For any vector fixed $u \in  {\mathbf{R}}^{n}$ ,and any $\epsilon  > 0$ ,let ${u}^{\prime } =$ $\frac{1}{\sqrt{k}}\left( {{R}^{T}u}\right)$ . Then, $\mathrm{E}\left( {\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2}\right)  = \parallel u{\parallel }^{2}$ and

引理2。设$R = \left( {r}_{ij}\right)$为一个随机的$n \times  k$矩阵，使得每个元素${r}_{ij}$都根据$N\left( {0,1}\right)$独立选取。对于任意固定向量$u \in  {\mathbf{R}}^{n}$以及任意$\epsilon  > 0$，设${u}^{\prime } =$ $\frac{1}{\sqrt{k}}\left( {{R}^{T}u}\right)$。那么，$\mathrm{E}\left( {\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2}\right)  = \parallel u{\parallel }^{2}$且

$$
\Pr \left\lbrack  {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} > \left( {1 + \epsilon }\right) \parallel u{\parallel }^{2}}\right\rbrack   \leq  {\left( \left( 1 + \epsilon \right) {e}^{-\epsilon }\right) }^{k} \leq  {e}^{-\left( {{\epsilon }^{2} - {\epsilon }^{3}}\right) \frac{k}{4}}
$$

$$
\Pr \left\lbrack  {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} < \left( {1 - \epsilon }\right) \parallel u{\parallel }^{2}}\right\rbrack   \leq  {\left( \left( 1 - \epsilon \right) {e}^{\epsilon }\right) }^{k} \leq  {e}^{-\left( {{\epsilon }^{2} - {\epsilon }^{3}}\right) \frac{k}{4}}.
$$

Springer

施普林格（Springer）

Proof: The expectation follows from a simple calculation. To obtain the bound on the concentration near the mean,let ${X}_{j} = \left( {{R}_{j}^{T} \cdot  u}\right) /\parallel u\parallel$ and observe that

证明：期望可通过简单计算得出。为了得到均值附近的集中度界，设 ${X}_{j} = \left( {{R}_{j}^{T} \cdot  u}\right) /\parallel u\parallel$ 并注意到

$$
X = \mathop{\sum }\limits_{{j = 1}}^{k}{X}_{j}^{2} = \mathop{\sum }\limits_{{j = 1}}^{k}\frac{{\left( {R}_{j}^{T} \cdot  u\right) }^{2}}{\parallel u{\parallel }^{2}}
$$

where ${R}_{j}$ denotes the $j$ th column of $R$ . Each ${X}_{j}$ has the standard normal distribution (since each component of ${R}_{j}$ does). Also note that

其中 ${R}_{j}$ 表示 $R$ 的第 $j$ 列。每个 ${X}_{j}$ 都服从标准正态分布（因为 ${R}_{j}$ 的每个分量都服从）。另请注意

$$
{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} = \frac{\parallel u{\parallel }^{2}}{k}X.
$$

Using Markov's inequality, we can then estimate the desired probability as

利用马尔可夫不等式（Markov's inequality），我们可以将所需概率估计为

$$
\mathrm{P}\left( {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} \geq  \left( {1 + \epsilon }\right) \parallel u{\parallel }^{2}}\right)  = \Pr \left( {X \geq  \left( {1 + \epsilon }\right) k}\right)  = \Pr \left( {{e}^{\alpha X} \geq  {e}^{\left( {1 + \epsilon }\right) {k\alpha }}}\right) 
$$

$$
 \leq  \frac{\mathrm{E}\left( {e}^{\alpha X}\right) }{{e}^{\left( {1 + \epsilon }\right) {k\alpha }}}
$$

$$
 = \frac{{\Pi }_{j = 1}^{k}\mathrm{E}\left( {e}^{\alpha {X}_{j}^{2}}\right) }{{e}^{\left( {1 + \epsilon }\right) {k\alpha }}} = {\left( \frac{\mathrm{E}\left( {e}^{\alpha {X}_{1}^{2}}\right) }{{e}^{\left( {1 + \epsilon }\right) \alpha }}\right) }^{k}.
$$

In the last line above,we have used the independence of the ${X}_{j}$ ’s.

在上述最后一行中，我们使用了 ${X}_{j}$ 的独立性。

Similarly,

类似地，

$$
\mathrm{P}\left( {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} \leq  \left( {1 - \epsilon }\right) \parallel u{\parallel }^{2}}\right)  \leq  {\left( \frac{\mathrm{E}\left( {e}^{-\alpha {X}_{1}^{2}}\right) }{{e}^{-\left( {1 - \epsilon }\right) \alpha }}\right) }^{k}.
$$

$\iota$ From Lemma 1,

$\iota$ 根据引理 1，

$$
\mathrm{E}\left( {e}^{\alpha {X}_{1}^{2}}\right)  = \frac{1}{\sqrt{1 - {2\alpha }}}
$$

for any $\alpha  < \frac{1}{2}$ . Thus we get,

对于任意 $\alpha  < \frac{1}{2}$。因此，我们得到

$$
\Pr \left( {X \geq  \left( {1 + \epsilon }\right) k}\right)  \leq  {\left( \frac{{e}^{-2\left( {1 + \epsilon }\right) \alpha }}{\left( 1 - 2\alpha \right) }\right) }^{\frac{k}{2}}.
$$

The optimal choice of $\alpha$ is $\epsilon /2\left( {1 + \epsilon }\right)$ . With this,

$\alpha$ 的最优选择是 $\epsilon /2\left( {1 + \epsilon }\right)$。由此

$$
\Pr \left( {X \geq  \left( {1 + \epsilon }\right) k}\right)  \leq  {\left( \left( 1 + \epsilon \right) {e}^{-\epsilon }\right) }^{\frac{k}{2}} \leq  {e}^{-\left( {{\epsilon }^{2} - {\epsilon }^{3}}\right) \frac{k}{4}}.
$$

Similarly,

类似地

$$
\Pr \left( {X \leq  \left( {1 - \epsilon }\right) k}\right)  \leq  {\left( \frac{{e}^{2\left( {1 - \epsilon }\right) \alpha }}{\left( 1 + 2\alpha \right) }\right) }^{\frac{k}{2}} \leq  {\left( \left( 1 - \epsilon \right) {e}^{\epsilon }\right) }^{\frac{k}{2}} \leq  {e}^{-\left( {{\epsilon }^{2} - {\epsilon }^{3}}\right) \frac{k}{4}}.
$$

The main theorem of this section shows that this phenomenon is not specific to the Normal distribution. In the statement below,the condition that $\mathrm{E}\left( {r}^{2}\right)  = 1$ is for convenience. Instead Springer one could have an arbitrary finite value ${\sigma }^{2}$ for this expectation,and scale the projection by $\sigma$ .

本节的主要定理表明，这种现象并非正态分布（Normal distribution）所特有。在下面的陈述中，条件 $\mathrm{E}\left( {r}^{2}\right)  = 1$ 是为了方便起见。实际上，人们可以为这个期望设定任意有限值 ${\sigma }^{2}$，并通过 $\sigma$ 对投影进行缩放。

Theorem 1. Let $R$ be a random $n \times  k$ matrix,with each entry $r$ chosen independently from a distribution $\mathcal{D}$ that is symmetric about the origin with $\mathrm{E}\left( {r}^{2}\right)  = 1$ . For any fixed vector $u \in  {\mathbf{R}}^{n}$ ,let ${u}^{\prime } = \frac{1}{\sqrt{k}}{R}^{T}u$ .

定理1. 设 $R$ 为一个随机 $n \times  k$ 矩阵，其每个元素 $r$ 独立地从关于原点对称且满足 $\mathrm{E}\left( {r}^{2}\right)  = 1$ 的分布 $\mathcal{D}$ 中选取。对于任意固定向量 $u \in  {\mathbf{R}}^{n}$ ，设 ${u}^{\prime } = \frac{1}{\sqrt{k}}{R}^{T}u$ 。

1. Suppose $B = \mathrm{E}\left( {r}^{4}\right)  < \infty$ . Then for any $\epsilon  > 0$ ,

1. 假设 $B = \mathrm{E}\left( {r}^{4}\right)  < \infty$ 。那么对于任意 $\epsilon  > 0$ ，

$$
\mathrm{P}\left( \left\lbrack  {{\left| \left| {u}^{\prime }\right| \right| }^{2} \leq  \left( {1 - \epsilon }\right) \left| \right| u{\left| \right| }^{2}}\right) \right)  \leq  {e}^{-\frac{\left( {{\epsilon }^{2} - {\epsilon }^{3}}\right) k}{2\left( {B + 1}\right) }}.
$$

2. Suppose $\exists L > 0$ such that for any integer $m > 0,\mathrm{E}\left( {r}^{2m}\right)  \leq  \frac{\left( {2m}\right) !}{{2}^{m}m!}{L}^{2m}$ . Then for any $\epsilon  > 0$ ,

2. 假设 $\exists L > 0$ 满足对于任意整数 $m > 0,\mathrm{E}\left( {r}^{2m}\right)  \leq  \frac{\left( {2m}\right) !}{{2}^{m}m!}{L}^{2m}$ 。那么对于任意 $\epsilon  > 0$ ，

$$
\mathrm{P}\left( {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} \geq  \left( {1 + \epsilon }\right) {L}^{2}\parallel u{\parallel }^{2}}\right)  \leq  {\left( \left( 1 + \epsilon \right) {e}^{-\epsilon }\right) }^{k/2} \leq  {e}^{-\left( {{\epsilon }^{2} - {\epsilon }^{3}}\right) \frac{k}{4}}.
$$

Proof: Without loss of generality,assume that $\parallel u{\parallel }^{2} = 1$ . Let

证明：不失一般性，假设 $\parallel u{\parallel }^{2} = 1$ 。令

$$
{X}_{i} = {R}_{i}^{T}u\;\text{ for }i = 1,\ldots ,k.
$$

We have

我们有

$$
\mathrm{E}\left( {X}_{i}^{2}\right)  = \mathrm{E}\left( {\left( {R}_{i}^{T}u\right) }^{2}\right)  = \mathrm{E}\left( {\left( \mathop{\sum }\limits_{{j = 1}}^{n}{R}_{ij}{u}_{j}\right) }^{2}\right)  = \mathop{\sum }\limits_{{j = 1}}^{n}\mathrm{E}\left( {R}_{ij}^{2}\right) {u}_{j}^{2} = 1.
$$

Then,if we define $Y$ as follows

然后，如果我们如下定义 $Y$ 

$$
Y \mathrel{\text{:=}} \mathop{\sum }\limits_{{i = 1}}^{k}{X}_{i}^{2} = k{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2},\;\mathrm{E}\left( Y\right)  = \mathop{\sum }\limits_{{i = 1}}^{k}\mathrm{E}\left( {X}_{i}^{2}\right)  = k.
$$

The deviation below the mean is relatively easy to bound, using the independence of the ${X}_{i}$ ’s and Markov’s inequality.

利用 ${X}_{i}$ 的独立性和马尔可夫不等式（Markov’s inequality），均值以下的偏差相对容易界定。

$$
\mathrm{P}\left( {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} < \left( {1 - \epsilon }\right) \parallel u{\parallel }^{2}}\right)  = \mathrm{P}\left( {Y < \left( {1 - \epsilon }\right) k}\right) 
$$

$$
 = \mathrm{P}\left( {{e}^{-{\alpha Y}} > {e}^{-\alpha \left( {1 - \epsilon }\right) k}}\right) 
$$

$$
 \leq  \frac{\mathrm{E}\left( {e}^{-{\alpha Y}}\right) }{{e}^{-\alpha \left( {1 - \epsilon }\right) k}}
$$

$$
 = {\left( \mathrm{E}\left( {e}^{-\alpha {X}_{1}^{2}}\right) {e}^{\alpha \left( {1 - \epsilon }\right) }\right) }^{k}
$$

and,using that ${e}^{-\alpha {X}_{1}^{2}} \leq  1 - \alpha {X}_{1}^{2} + {\alpha }^{2}{X}_{1}^{4}/2$ ,we get

并且，利用 ${e}^{-\alpha {X}_{1}^{2}} \leq  1 - \alpha {X}_{1}^{2} + {\alpha }^{2}{X}_{1}^{4}/2$ ，我们得到

$$
\mathrm{P}\left( {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} < \left( {1 - \epsilon }\right) \parallel u{\parallel }^{2}}\right)  \leq  {\left( \left( 1 - \alpha \mathrm{E}\left( {X}_{1}^{2}\right)  + \frac{{\alpha }^{2}}{2}\mathrm{E}\left( {X}_{1}^{4}\right) \right) {e}^{\alpha \left( {1 - \epsilon }\right) }\right) }^{k}.
$$

Springer We can evaluate the moments easily: $\mathrm{E}\left( {X}_{1}^{2}\right)  = 1$ and,if we observe that the expectation of odd powers of $r$ is zero because of symmetry,we have (using the fact that $B \geq  1$ ),

施普林格（Springer） 我们可以轻松地计算矩：$\mathrm{E}\left( {X}_{1}^{2}\right)  = 1$ ，并且，如果我们注意到由于对称性，$r$ 的奇数次幂的期望为零，那么我们有（利用 $B \geq  1$ 这一事实）

$$
\mathrm{E}\left( {X}_{1}^{4}\right)  = \mathrm{E}\left( {\left( \mathop{\sum }\limits_{{j = 1}}^{n}{R}_{1j}{u}_{j}\right) }^{4}\right) 
$$

$$
 = \mathop{\sum }\limits_{{{j}_{1},{j}_{2},{j}_{3},{j}_{4} = 1}}^{n}\mathrm{E}\left( {{R}_{1{j}_{1}}{R}_{1{j}_{2}}{R}_{1{j}_{3}}{R}_{1{j}_{4}}}\right) {u}_{{j}_{1}}{u}_{{j}_{2}}{u}_{{j}_{3}}{u}_{{j}_{4}}
$$

$$
 = \mathop{\sum }\limits_{{j = 1}}^{n}\mathrm{E}\left( {R}_{1j}^{4}\right) {u}_{j}^{4} + 3\mathop{\sum }\limits_{{{j}_{1} \neq  {j}_{2},{j}_{1},{j}_{2} = 1}}^{n}\mathrm{E}\left( {{R}_{1{j}_{1}}^{2}{R}_{1{j}_{2}}^{2}}\right) {u}_{{j}_{1}}^{2}{u}_{{j}_{2}}^{2}
$$

$$
 \leq  B\mathop{\sum }\limits_{{j = 1}}^{n}{u}_{j}^{4} + 3\mathop{\sum }\limits_{{{j}_{1} \neq  {j}_{2},{j}_{1},{j}_{2} = 1}}^{n}{u}_{{j}_{1}}^{2}{u}_{{j}_{2}}^{2}
$$

$$
 \leq  \left( {B + 2}\right) {\left( \mathop{\sum }\limits_{j}{u}_{j}^{2}\right) }^{2}
$$

$$
 = B + 2\text{.}
$$

Therefore,using the Taylor expansion of ${e}^{x}$ ,(in particular, ${e}^{-x + {x}^{2/2}} \geq  1 - x$ for $x \geq  0$ and small enough).

因此，使用 ${e}^{x}$ 的泰勒展开式（特别地，对于 $x \geq  0$ 且足够小的情况，有 ${e}^{-x + {x}^{2/2}} \geq  1 - x$ ）。

$$
\mathrm{P}\left( {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} < \left( {1 - \epsilon }\right) \parallel u{\parallel }^{2}}\right)  \leq  {\left( \left( 1 - \alpha  + \frac{{\alpha }^{2}}{2}\left( B + 2\right) \right) {e}^{\alpha \left( {1 - \epsilon }\right) }\right) }^{k}
$$

$$
 \leq  {\left( {e}^{-\alpha  + \frac{{\alpha }^{2}\left( {B + 2}\right) }{2} - \frac{1}{2}{\left( \alpha  - \frac{{\alpha }^{2}\left( {B + 2}\right) }{2}\right) }^{2}}{e}^{\alpha \left( {1 - \epsilon }\right) }\right) }^{k}
$$

$$
 \leq  {e}^{-\frac{\left( {{\epsilon }^{2} - {\epsilon }^{3}}\right) k}{2\left( {B + 1}\right) }}.
$$

The last line above is obtained by setting $\alpha  = \epsilon /\left( {B + 1}\right)$ and noting that $B \geq  1$ .

上面最后一行是通过令 $\alpha  = \epsilon /\left( {B + 1}\right)$ 并注意到 $B \geq  1$ 得到的。

Similarly, for the deviation above the mean,

类似地，对于均值以上的偏差

$$
\mathrm{P}\left( {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} > \left( {1 + \epsilon }\right) {L}^{2}\parallel u{\parallel }^{2}}\right)  \leq  {\left( \frac{\mathrm{E}\left( {e}^{\alpha {X}_{1}^{2}}\right) }{{e}^{\alpha {L}^{2}\left( {1 + \epsilon }\right) }}\right) }^{k}.
$$

The main task is bounding $\mathrm{E}\left( {e}^{\alpha {X}_{1}^{2}}\right)$ from above using the assumptions of the theorem. This expectation is hard to evaluate directly since we don't know the distribution explicitly. However we have bounds on all the moments of ${X}_{1}^{2}$ . Therefore,if we define a random variable $Z$ whose moments are all at least the moments of ${X}_{1}^{2}$ ,then $\mathrm{E}\left( {e}^{\alpha Z}\right)$ will be an upper bound on the required expectation. The following claim will be useful.

主要任务是利用定理的假设从上方界定$\mathrm{E}\left( {e}^{\alpha {X}_{1}^{2}}\right)$。由于我们并不明确知道分布情况，所以直接计算这个期望很困难。然而，我们对${X}_{1}^{2}$的所有矩都有界。因此，如果我们定义一个随机变量$Z$，其所有矩至少与${X}_{1}^{2}$的矩相等，那么$\mathrm{E}\left( {e}^{\alpha Z}\right)$将是所需期望的一个上界。以下断言将很有用。

Claim 1. Let $f,g$ be distributions on $\mathbf{R}$ that are symmetric about the origin with the property that for any nonnegative integer $m,\mathrm{E}\left( {Y}^{2m}\right)  \leq  \mathrm{E}\left( {Z}^{2m}\right)$ where $Y,Z$ are drawn from $f,g$ respectively. Let ${Y}_{1},\ldots ,{Y}_{n}$ be i.i.d. from $f,{Z}_{1},\ldots ,{Z}_{n}$ be i.i.d from $g$ . Then for any $u \in  {\mathbf{R}}^{n}$ , the random variables $\widehat{Y} = \mathop{\sum }\limits_{{j = 1}}^{n}{u}_{j}{Y}_{j}$ and $\widehat{Z} = \mathop{\sum }\limits_{{j = 1}}^{n}{u}_{j}{Z}_{j}$ satisfy $\mathrm{E}\left( {\left( \widehat{Y}\right) }^{2m}\right)  \leq  \mathrm{E}\left( {\left( \widehat{Z}\right) }^{2m}\right)$ for every nonnegative integer $m$ .

权利要求1。设$f,g$为$\mathbf{R}$上关于原点对称的分布，其具有如下性质：对于任意非负整数$m,\mathrm{E}\left( {Y}^{2m}\right)  \leq  \mathrm{E}\left( {Z}^{2m}\right)$，其中$Y,Z$分别从$f,g$中抽取。设${Y}_{1},\ldots ,{Y}_{n}$为来自$f,{Z}_{1},\ldots ,{Z}_{n}$的独立同分布随机变量，$g$为来自$g$的独立同分布随机变量。那么对于任意$u \in  {\mathbf{R}}^{n}$，随机变量$\widehat{Y} = \mathop{\sum }\limits_{{j = 1}}^{n}{u}_{j}{Y}_{j}$和$\widehat{Z} = \mathop{\sum }\limits_{{j = 1}}^{n}{u}_{j}{Z}_{j}$满足：对于每个非负整数$m$，有$\mathrm{E}\left( {\left( \widehat{Y}\right) }^{2m}\right)  \leq  \mathrm{E}\left( {\left( \widehat{Z}\right) }^{2m}\right)$。

The claim is easy to prove. Compare the expectations of individual terms of ${\left( \widehat{Y}\right) }^{2m}$ and ${\left( \widehat{Z}\right) }^{2m}$ . Since ${Y}_{i},{Z}_{i}$ are symmetric about the origin,all terms in which they appear with an odd power have an expectation of zero. For any term in which all powers are even, by the assumption,the term from $\mathrm{E}\left( {\left( \widehat{Z}\right) }^{2m}\right)$ dominates.

该命题很容易证明。比较${\left( \widehat{Y}\right) }^{2m}$和${\left( \widehat{Z}\right) }^{2m}$各项的期望。由于${Y}_{i},{Z}_{i}$关于原点对称，所有它们以奇数次幂出现的项的期望都为零。对于所有幂次均为偶数的项，根据假设，$\mathrm{E}\left( {\left( \widehat{Z}\right) }^{2m}\right)$中的项占主导地位。

To apply this to our setting, we know that

将此应用到我们的设定中，我们知道

$$
{X}_{1} = \mathop{\sum }\limits_{{j = 1}}^{n}{u}_{j}{r}_{j}
$$

where each ${r}_{j}$ is drawn from the given distribution $\mathcal{D}$ . Define

其中每个${r}_{j}$都从给定分布$\mathcal{D}$中抽取。定义

$$
{Y}_{1} = \mathop{\sum }\limits_{{j = 1}}^{n}{u}_{j}{r}_{j}^{\prime }
$$

where each ${r}_{j}^{\prime }$ is drawn from $N\left( {0,L}\right)$ . Then for all $j$ ,and any integer $m > 0$ ,

其中每个${r}_{j}^{\prime }$都从$N\left( {0,L}\right)$中抽取。那么对于所有$j$，以及任意整数$m > 0$，

$$
\mathrm{E}\left( {r}_{j}^{2m}\right)  \leq  \frac{\left( {2m}\right) !}{{2}^{m}m!}{L}^{2m} = \mathrm{E}\left( {\left( {r}_{j}^{\prime }\right) }^{2m}\right) 
$$

using the well-known formula for the moments of $N\left( {0,L}\right)$ . So, $\mathrm{E}\left( {X}_{1}^{2m}\right)  \leq  \mathrm{E}\left( {Y}_{1}^{2m}\right)$ . Moreover, the distribution of ${Y}_{1}$ is $N\left( {0,L}\right)$ . Therefore,

使用$N\left( {0,L}\right)$矩的著名公式。因此，$\mathrm{E}\left( {X}_{1}^{2m}\right)  \leq  \mathrm{E}\left( {Y}_{1}^{2m}\right)$。此外，${Y}_{1}$的分布是$N\left( {0,L}\right)$。所以，

$$
\mathrm{E}\left( {e}^{\alpha {X}_{1}^{2}}\right)  \leq  \mathrm{E}\left( {e}^{\alpha {Y}_{1}^{2}}\right)  = \frac{1}{\sqrt{1 - {2\alpha }{L}^{2}}}.
$$

Using this,

利用这一点，

$$
\mathrm{P}\left( {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} > \left( {1 + \epsilon }\right) {L}^{2}\parallel u{\parallel }^{2}}\right)  \leq  {\left( \frac{{e}^{-{2\alpha }{L}^{2}\left( {1 + \epsilon }\right) }}{1 - {2\alpha }{L}^{2}}\right) }^{\frac{k}{2}}.
$$

The optimal choice of $\alpha$ is $\epsilon /2{L}^{2}\left( {1 + \epsilon }\right)$ ,and we get that for any $\epsilon  > 0$ ,

$\alpha$的最优选择是$\epsilon /2{L}^{2}\left( {1 + \epsilon }\right)$，并且我们得到对于任意$\epsilon  > 0$，

$$
\mathrm{P}\left( {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} > \left( {1 + \epsilon }\right) {L}^{2}\parallel u{\parallel }^{2}}\right)  \leq  {\left( \left( 1 + \epsilon \right) {e}^{-\epsilon }\right) }^{\frac{k}{2}} \leq  {e}^{-\left( {{\epsilon }^{2} - {\epsilon }^{3}}\right) \frac{k}{4}}.
$$

The last inequality was obtained by using the inequality $\ln \left( {1 + \epsilon }\right)  \leq  \epsilon  - {\epsilon }^{2}/2 + {\epsilon }^{3}/2$ .

最后一个不等式是通过使用不等式$\ln \left( {1 + \epsilon }\right)  \leq  \epsilon  - {\epsilon }^{2}/2 + {\epsilon }^{3}/2$得到的。

Corollary 1. If every entry of an $n \times  k$ matrix $R$ is chosen according to $U\left( {-1,1}\right)$ ,then for any fixed vector $u \in  {\mathbf{R}}^{n}$ and any $\epsilon  > 0$ ,the vector ${u}^{\prime } = \frac{1}{\sqrt{k}}{R}^{T}u$ satisfies

推论1. 如果一个$n \times  k$矩阵$R$的每个元素都是根据$U\left( {-1,1}\right)$选取的，那么对于任何固定向量$u \in  {\mathbf{R}}^{n}$和任何$\epsilon  > 0$，向量${u}^{\prime } = \frac{1}{\sqrt{k}}{R}^{T}u$满足

$$
\mathrm{P}\left( {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} \geq  \left( {1 + \epsilon }\right) \parallel u{\parallel }^{2}}\right)  \leq  {e}^{-\left( {{\epsilon }^{2} - {\epsilon }^{3}}\right) \frac{k}{4}}\text{ and }\mathrm{P}\left( {{\begin{Vmatrix}{u}^{\prime }\end{Vmatrix}}^{2} \leq  \left( {1 - \epsilon }\right) \parallel u{\parallel }^{2}}\right)  \leq  {e}^{-\left( {{\epsilon }^{2} - {\epsilon }^{3}}\right) \frac{k}{4}}.
$$

Proof: For $r$ drawn from $U\left( {-1,1}\right) ,\mathrm{E}\left( {r}^{2m}\right)  = 1$ for any integer $m > 0$ . Therefore,we can apply Theorem 1 with $L = B = 1$ to get the conclusion of the corollary. Springer

证明：对于从$U\left( {-1,1}\right) ,\mathrm{E}\left( {r}^{2m}\right)  = 1$中抽取的$r$，其中$m > 0$为任意整数。因此，我们可以将定理1应用于$L = B = 1$来得到该推论的结论。施普林格（Springer）

Let $R$ be an $n \times  k$ matrix whose entries are chosen independently from either $N\left( {0,1}\right)$ or $U\left( {-1,1}\right)$ ,independently. The following theorem summarizes the results of this section. Alternative proofs for the case of $N\left( {0,1}\right)$ appeared in Indyk and Motwani (1998) and DG.

设 $R$ 为一个 $n \times  k$ 矩阵，其元素独立地从 $N\left( {0,1}\right)$ 或 $U\left( {-1,1}\right)$ 中选取。以下定理总结了本节的结果。关于 $N\left( {0,1}\right)$ 情形的其他证明出现在英迪克（Indyk）和莫特瓦尼（Motwani）（1998 年）以及 DG 的文献中。

Theorem 2 (Neuronal RP). Let $u,v \in  {\mathbf{R}}^{n}$ . Let ${u}^{\prime }$ and ${v}^{\prime }$ be the projections of $u$ and $v$ to ${\mathbf{R}}^{k}$ via a random matrix $R$ whose entries are chosen independently from either $N\left( {0,1}\right)$ or $U\left( {-1,1}\right)$ . Then,

定理 2（神经元随机投影（Neuronal RP））。设 $u,v \in  {\mathbf{R}}^{n}$ 。设 ${u}^{\prime }$ 和 ${v}^{\prime }$ 分别是 $u$ 和 $v$ 通过一个随机矩阵 $R$ 投影到 ${\mathbf{R}}^{k}$ 上的投影，该随机矩阵的元素独立地从 $N\left( {0,1}\right)$ 或 $U\left( {-1,1}\right)$ 中选取。那么，

$$
\mathrm{P}\left\lbrack  {\left( {1 - \epsilon }\right) {\left| \left| u - v\right| \right| }^{2} \leq  {\left| \left| {u}^{\prime } - {v}^{\prime }\right| \right| }^{2} \leq  \left( {1 + \epsilon }\right) {\left| \left| u - v\right| \right| }^{2}}\right\rbrack   \geq  1 - 2{e}^{-\left( {{\epsilon }^{2} - {\epsilon }^{3}}\right) \frac{k}{4}}.
$$

Proof: Apply Theorem 1 to the vector $u - v$ .

证明：对向量 $u - v$ 应用定理 1。

We conclude this section with a useful corollary. A similar proof can be found in Ben-David, et al. (2002).

我们用一个有用的推论来结束本节。类似的证明可在本 - 戴维（Ben - David）等人（2002 年）的文献中找到。

Corollary 2. Let $u,v$ be vectors in ${\mathbf{R}}^{n}$ s.t. $\parallel u\parallel ,\parallel v\parallel  \leq  1$ . Let $R$ be a random matrix whose entries are chosen independently from either $N\left( {0,1}\right)$ or $U\left( {-1,1}\right)$ . Define ${u}^{\prime } = \frac{1}{\sqrt{k}}{R}^{T}u$ and ${v}^{\prime } = \frac{1}{\sqrt{k}}{R}^{T}v$ . Then for any $\epsilon  > 0$ ,

推论 2。设 $u,v$ 为 ${\mathbf{R}}^{n}$ 中的向量，满足 $\parallel u\parallel ,\parallel v\parallel  \leq  1$。设 $R$ 为一个随机矩阵，其元素独立地从 $N\left( {0,1}\right)$ 或 $U\left( {-1,1}\right)$ 中选取。定义 ${u}^{\prime } = \frac{1}{\sqrt{k}}{R}^{T}u$ 和 ${v}^{\prime } = \frac{1}{\sqrt{k}}{R}^{T}v$。那么对于任意 $\epsilon  > 0$，

$$
\mathrm{P}\left( {u \cdot  v - c \leq  {u}^{\prime } \cdot  {v}^{\prime } \leq  u \cdot  v + c}\right)  \geq  1 - 4{e}^{-\left( {{\epsilon }^{2} - {\epsilon }^{3}}\right) \frac{k}{4}}.
$$

Proof: Applying Theorem 2 to the vectors $u,v$ and $u - v$ ,we have that with probability at least $1 - 4{e}^{-\left( {{c}^{2} - {c}^{3}}\right) \frac{k}{4}}$ ,

证明：将定理2应用于向量 $u,v$ 和 $u - v$ ，我们有至少以概率 $1 - 4{e}^{-\left( {{c}^{2} - {c}^{3}}\right) \frac{k}{4}}$ ，

$$
\left( {1 - c}\right) \parallel u - v{\parallel }^{2} \leq  {\begin{Vmatrix}{u}^{\prime } - {v}^{\prime }\end{Vmatrix}}^{2} \leq  \left( {1 + c}\right) \parallel u - v{\parallel }^{2}
$$

$$
\text{and}\;\left( {1 - c}\right) \parallel u + v{\parallel }^{2} \leq  {\begin{Vmatrix}{u}^{\prime } + {v}^{\prime }\end{Vmatrix}}^{2} \leq  \left( {1 + c}\right) \parallel u + v{\parallel }^{2}\text{.}
$$

Then,

然后，

$$
4{u}^{\prime } \cdot  {v}^{\prime } = {\begin{Vmatrix}{u}^{\prime } + {v}^{\prime }\end{Vmatrix}}^{2} - {\begin{Vmatrix}{u}^{\prime } - {v}^{\prime }\end{Vmatrix}}^{2}
$$

$$
 \geq  \left( {1 - c}\right) \parallel u + v{\parallel }^{2} - \left( {1 + c}\right) \parallel u - v{\parallel }^{2}
$$

$$
 = {4u} \cdot  v - {2c}\left( {\parallel u{\parallel }^{2} + \parallel v{\parallel }^{2}}\right) 
$$

$$
 \geq  {4u} \cdot  v - {4c}\text{.}
$$

Thus ${u}^{\prime } \cdot  {v}^{\prime } \geq  u \cdot  v - c$ . The other inequality is similar.

因此 ${u}^{\prime } \cdot  {v}^{\prime } \geq  u \cdot  v - c$ 。另一个不等式同理。

In what follows, we will apply random projection by picking entries of the projection matrix independently from $N\left( {0,1}\right)$ or $U\left( {-1,1}\right)$ . We remark that one could use other distributions via Theorem 1.

在接下来的内容中，我们将通过从$N\left( {0,1}\right)$或$U\left( {-1,1}\right)$中独立选取投影矩阵的元素来应用随机投影。我们注意到，根据定理1，也可以使用其他分布。

## 4. Learning efficiently by reducing dimensionality

## 4. 通过降维实现高效学习

In this section, we describe learning algorithms for robust concepts and derive bounds on the number of examples required and the running times. Our bounds will be functions of the robustness parameter $l$ ,and the $\epsilon ,\delta$ learning parameters,but will be independent of the actual number of attributes of the concept class.

在本节中，我们描述用于学习鲁棒概念的算法，并推导所需示例数量和运行时间的边界。我们推导的边界将是鲁棒性参数$l$和$\epsilon ,\delta$学习参数的函数，但与概念类的实际属性数量无关。

We are given labelled examples from an unknown distribution $\mathcal{D}$ . The generic algorithm for learning robust concepts is based on the following two high-level ideas:

我们得到了来自未知分布$\mathcal{D}$的带标签示例。学习鲁棒概念的通用算法基于以下两个高层次的思路：

1. Since the target concept is robust, random projection of the examples to a much lower-dimensional subspace will "preserve" the concept.

1. 由于目标概念具有鲁棒性，将示例随机投影到低得多的子空间会“保留”该概念。

2. In the lower-dimensional space, the number of examples and the time required to learn concepts are relatively small.

2. 在低维空间中，学习概念所需的示例数量和时间相对较少。

Before applying this approach to specific concept classes, we recall some fundamental theorems in learning theory. For the concept class $\mathcal{C}$ under consideration,let $C\left( {m,k}\right)$ denote the maximum number of distinct labellings of $m$ points that can be obtained by using concepts from $\mathcal{C}$ in ${\mathbf{R}}^{k}$ . The following well-known theorem (see Kearns &Vazirani (1994) or Blumer et al. (1989)) gives a bound on the size of the sample so that a hypothesis that is consistent with the sample also has, with high probability, small error with respect to the entire distribution.

在将这种方法应用于特定概念类之前，我们回顾一下学习理论中的一些基本定理。对于所考虑的概念类$\mathcal{C}$，设$C\left( {m,k}\right)$表示在${\mathbf{R}}^{k}$中使用$\mathcal{C}$中的概念可以得到的$m$个点的不同标记的最大数量。下面这个著名的定理（见卡恩斯（Kearns）和瓦齐拉尼（Vazirani）（1994年）或布卢默（Blumer）等人（1989年））给出了样本大小的一个界限，使得与样本一致的假设也很有可能相对于整个分布具有较小的误差。

Theorem 3. Let $\mathcal{C}$ be any concept class in ${\mathbf{R}}^{k}$ . Let $w$ be a concept from $\mathcal{C}$ that is consistent with $m$ labelled examples of some concept in $C$ . Then with probability at least $1 - \delta ,w$ correctly classifies at least $\left( {1 - \epsilon }\right)$ fraction of $\mathcal{D}$ provided

定理3。设$\mathcal{C}$为${\mathbf{R}}^{k}$中的任意概念类。设$w$是$\mathcal{C}$中的一个概念，它与$C$中某个概念的$m$个带标签示例一致。那么，在概率至少为$1 - \delta ,w$的情况下，$w$能正确分类$\mathcal{D}$中至少$\left( {1 - \epsilon }\right)$比例的样本，前提是

$$
m > \frac{4}{\epsilon }\log C\left( {{2m},k}\right)  + \frac{4}{\epsilon }\log \frac{2}{\delta }.
$$

The notion of VC-dimension (Vapnik & Chervonenkis, 1971) is closely connected to the number of distinct labelings as expressed in the following basic theorem.

VC维（Vapnik和Chervonenkis，1971年）的概念与不同标记的数量密切相关，如以下基本定理所述。

Theorem 4 (Blumer et al. 1989). Let $C$ be a concept class of VC-dimension d. Then,the number of distinct labelings of $m$ points by concepts in $C$ is at most

定理4（Blumer等人，1989年）。设$C$是VC维为d的概念类。那么，$C$中的概念对$m$个点的不同标记数量至多为

$$
C\left\lbrack  m\right\rbrack   \leq  \mathop{\sum }\limits_{{i = 0}}^{d}\left( \begin{matrix} m \\  i \end{matrix}\right) 
$$

If the algorithm finds a hypothesis that is nearly consistent with the sample (rather than fully consistent as in the previous theorem), this too generalizes well. The number of samples required increases by a a constant factor. The theorem below is a slight variant of a similar theorem from Blumer et al. (1989). We give a self-contained proof in the appendix for the reader's convenience.

如果算法找到一个与样本几乎一致（而非像前面定理中那样完全一致）的假设，那么这个假设也具有良好的泛化能力。所需样本数量会增加一个常数因子。下面的定理是布卢默等人（Blumer et al.，1989）提出的类似定理的一个轻微变体。为方便读者，我们在附录中给出了一个独立的证明。

Theorem 5. For $\epsilon  \leq  1/4$ ,let $w$ be a concept from $\mathcal{C}$ in ${\mathbf{R}}^{k}$ that correctly classifies at least $a\left( {1 - \epsilon /8}\right)$ fraction of a sample of $m$ points drawn from $\mathcal{D}$ such that

定理5。对于$\epsilon  \leq  1/4$，设$w$是${\mathbf{R}}^{k}$中$\mathcal{C}$的一个概念，它能正确分类从$\mathcal{D}$中抽取的包含$m$个点的样本中至少$a\left( {1 - \epsilon /8}\right)$比例的点，使得

$$
m \geq  \frac{32}{\epsilon }\log C\left( {{2m},k}\right)  + \frac{32}{\epsilon }\log \frac{2}{\delta }.
$$

Springer Then with probability at least $1 - \delta$ ,w correctly classifies at least a $1 - \epsilon$ fraction of $\mathcal{D}$ .

施普林格出版社（Springer） 那么，至少以$1 - \delta$的概率，w能正确分类$\mathcal{D}$中至少$1 - \epsilon$比例的点。

### 4.1. Half-spaces

### 4.1. 半空间

We begin with the problem of learning a half-space in ${\mathbf{R}}^{n}$ (a linear threshold function). This is one of the oldest problems studied in learning theory. The problem can be solved in polynomial-time by using an algorithm for linear programming on a sample of $O\left( n\right)$ examples (note that this is not a strongly polynomial algorithm-its complexity depends only polynomially on the number of bits in the input). Typically, however, it is solved by using simple greedy methods. A commonly-used greedy algorithm is the Perceptron Algorithm (Agmon, 1954; Rosenblatt, 1962), which has the following guarantee: Given a collection of data points in ${\mathbf{R}}^{n}$ ,each labeled as positive or negative,the algorithm will find a vector $w$ such that $w \cdot  x > 0$ for all positive points $x$ and $w \cdot  x < 0$ for all negative points $x$ ,if such a vector exists. ${}^{1}$ The running time of the algorithm depends on a separation parameter (described below). However,in order for the hypothesis to be reliable,we need to use a sample of $\Omega \left( n\right)$ points,since the VC-dimension of half-spaces in ${\mathbf{R}}^{n}$ is $n + 1$ .

我们从在${\mathbf{R}}^{n}$中学习半空间（线性阈值函数）的问题开始。这是学习理论中研究的最古老的问题之一。通过对$O\left( n\right)$个示例的样本使用线性规划算法，可以在多项式时间内解决该问题（注意，这不是一个强多项式算法——其复杂度仅与输入中的比特数呈多项式关系）。然而，通常使用简单的贪心方法来解决。一种常用的贪心算法是感知机算法（阿格蒙（Agmon），1954年；罗森布拉特（Rosenblatt），1962年），它有以下保证：给定${\mathbf{R}}^{n}$中的一组数据点，每个点都被标记为正或负，如果存在这样一个向量，该算法将找到一个向量$w$，使得对于所有正点$x$有$w \cdot  x > 0$，对于所有负点$x$有$w \cdot  x < 0$。${}^{1}$该算法的运行时间取决于一个分离参数（如下所述）。然而，为了使假设可靠，我们需要使用$\Omega \left( n\right)$个点的样本，因为${\mathbf{R}}^{n}$中半空间的VC维是$n + 1$。

Let ${\mathcal{H}}_{n}$ be the class of homogenous half-spaces in ${\mathbf{R}}^{n}$ . Let(h,D)be a concept-distribution pair such that the half-space $h \in  {\mathcal{H}}_{n}$ is $\ell$ -robust with respect to the distribution $\mathcal{D}$ over ${\mathbf{R}}^{n}$ . We restrict $\mathcal{D}$ to be over the unit sphere (i.e.,all the examples are at unit distance from the origin). The latter condition is not really a restriction since examples can be scaled to have unit length without changing their labels. The parameters $k$ and $m$ in the algorithm below will be specified later.

设${\mathcal{H}}_{n}$为${\mathbf{R}}^{n}$中的齐次半空间类。设(h,D)为一个概念 - 分布对，使得半空间$h \in  {\mathcal{H}}_{n}$相对于${\mathbf{R}}^{n}$上的分布$\mathcal{D}$是$\ell$ - 鲁棒的。我们限制$\mathcal{D}$是在单位球面上（即，所有示例与原点的距离为单位距离）。后一个条件实际上并不是限制，因为可以在不改变示例标签的情况下将其缩放为单位长度。下面算法中的参数$k$和$m$将在后面指定。

## Half-space Algorithm:

## 半空间算法：

1. Choose an $n \times  k$ random matrix $R$ by picking each entry independently from $N\left( {0,1}\right)$ or $U\left( {-1,1}\right)$ .

1. 通过从$N\left( {0,1}\right)$或$U\left( {-1,1}\right)$中独立选取每个元素来选择一个$n \times  k$随机矩阵$R$。

2. Obtain $m$ examples from $\mathcal{D}$ and project them to ${\mathbf{R}}^{k}$ using $R$ .

2. 从$\mathcal{D}$中获取$m$个示例，并使用$R$将它们投影到${\mathbf{R}}^{k}$上。

3. Run the following Perceptron Algorithm in ${\mathbf{R}}^{k}$ : Let $w = 0$ . Perform the following operation until all examples are correctly classified:

3. 在${\mathbf{R}}^{k}$中运行以下感知机算法：令$w = 0$。执行以下操作，直到所有示例都被正确分类：

Pick an arbitrary misclassified example $x$ and let $w \leftarrow  w + \operatorname{label}\left( x\right) x$ .

选择一个任意的分类错误的示例$x$，并令$w \leftarrow  w + \operatorname{label}\left( x\right) x$。

4. Output $R$ and $w$ .

4. 输出$R$和$w$。

A future example $x$ is labelled positive if $w \cdot  \left( {{R}^{T}x}\right)  \geq  0$ and negative otherwise. This is of course the same as checking if $\left( {w{R}^{T}}\right)  \cdot  x > 0$ ,i.e.,a half-space in the original $n$ -dimensional space.

如果$w \cdot  \left( {{R}^{T}x}\right)  \geq  0$，则未来的示例$x$被标记为正例，否则标记为反例。当然，这与检查$\left( {w{R}^{T}}\right)  \cdot  x > 0$是否成立是一样的，即在原始的$n$维空间中的一个半空间。

We can assume that $h$ ,the normal vector to the concept half-space,is of unit length. The idea behind the algorithm is that when $k$ is large enough,in the $k$ -dimensional subspace obtained by projection,the half-space through the origin defined by ${R}^{T}h$ ,i.e., $\left( {{R}^{T}h}\right)  \cdot  y \geq  0$ , classifies most of the projected distribution correctly. We will show that in fact this half-space remains robust with respect to a projected sample of sufficiently large size. To find a consistent half-space, we use the classical perceptron algorithm. It is well-known (see Minsky & Papert (1969)) that the convergence of this algorithm depends on the margin, i.e., in our terminology, the robustness of the target half-space.

我们可以假设 $h$（概念半空间的法向量）为单位长度。该算法背后的思想是，当 $k$ 足够大时，在通过投影得到的 $k$ 维子空间中，由 ${R}^{T}h$ 定义的过原点的半空间，即 $\left( {{R}^{T}h}\right)  \cdot  y \geq  0$，能正确分类大部分投影分布。我们将证明，实际上，对于足够大的投影样本，这个半空间仍然是稳健的。为了找到一个一致的半空间，我们使用经典的感知机算法。众所周知（见明斯基（Minsky）和帕佩特（Papert）（1969 年）），该算法的收敛性取决于间隔，即按照我们的术语，取决于目标半空间的稳健性。

Theorem 6. (Minsky & Papert, 1969) Suppose the data set $S$ can be correctly classified by some unit vector $w$ . Then,the Perceptron Algorithm converges in at most $1/{\sigma }^{2}$ iterations, where

定理 6.（明斯基（Minsky）和帕佩特（Papert），1969 年）假设数据集 $S$ 可以由某个单位向量 $w$ 正确分类。那么，感知机算法最多在 $1/{\sigma }^{2}$ 次迭代中收敛，其中

$$
\sigma  = \mathop{\min }\limits_{{x \in  S}}\frac{\left| w \cdot  x\right| }{\parallel x\parallel }
$$

---

<!-- Footnote -->

${}^{1}$ A zero threshold can be achieved by adding an extra dimension to the space.

${}^{1}$ 通过向空间添加一个额外维度，可以实现零阈值。

<!-- Footnote -->

---

For an $\ell$ -robust half-space,we have $\sigma  \geq  \ell$ . The theorem says that the perceptron algorithm will find a consistent half-space in at most $1/{\ell }^{2}$ iterations. We can now state and prove the main result of this section.

对于一个 $\ell$ -鲁棒半空间，我们有 $\sigma  \geq  \ell$ 。该定理表明，感知机算法最多在 $1/{\ell }^{2}$ 次迭代内会找到一个一致的半空间。现在我们可以陈述并证明本节的主要结果。

Theorem 7. An $\ell$ -robust half-space in ${\mathbf{R}}^{n}$ can be $\left( {\epsilon ,\delta }\right)$ -learned by projecting a set of $m$ examples to ${\mathbf{R}}^{k}$ where

定理 7. ${\mathbf{R}}^{n}$ 中的一个 $\ell$ -鲁棒半空间可以通过将一组 $m$ 个示例投影到 ${\mathbf{R}}^{k}$ 来进行 $\left( {\epsilon ,\delta }\right)$ -学习，其中

$$
k = \frac{100}{{\ell }^{2}}\ln \frac{100}{\epsilon \ell \delta },\;m = \frac{8k}{\epsilon }\log \frac{48}{\epsilon } + \frac{4}{\epsilon }\log \frac{4}{\delta } = O\left( {\frac{1}{{\ell }^{2}} \cdot  \frac{1}{\epsilon } \cdot  \ln \frac{1}{\epsilon }\ln \frac{1}{\epsilon \ell \delta }}\right) 
$$

in time $n \cdot  \operatorname{poly}\left( {\frac{1}{\ell },\frac{1}{\epsilon },\log \frac{1}{\delta }}\right)$ time.

在 $n \cdot  \operatorname{poly}\left( {\frac{1}{\ell },\frac{1}{\epsilon },\log \frac{1}{\delta }}\right)$ 时间内。

Proof: For an example point $x$ ,we let ${x}^{\prime }$ denote its projection. We let ${h}^{\prime }$ denote the projection of $h$ ,the normal to the target half-space. We would like the following events to occur (by the choice of the projection matrix $R$ ):

证明：对于一个示例点 $x$，我们用 ${x}^{\prime }$ 表示其投影。我们用 ${h}^{\prime }$ 表示 $h$（目标半空间的法线）的投影。我们希望以下事件发生（通过选择投影矩阵 $R$）：

1. For each example $x$ ,its projection ${x}^{\prime }$ has length at most $1 + \frac{\ell }{2}$ . Similarly, $\left| \right| {h}^{\prime }\left| \right|  \leq  1 +$ $\frac{\ell }{2}$ .

1. 对于每个示例 $x$，其投影 ${x}^{\prime }$ 的长度至多为 $1 + \frac{\ell }{2}$。类似地，$\left| \right| {h}^{\prime }\left| \right|  \leq  1 +$ $\frac{\ell }{2}$。

2. For each example $x$ ,if $h \cdot  x \geq  \ell$ ,then ${h}^{\prime } \cdot  x \geq  \frac{\ell }{2}$ ; if $h \cdot  x \leq   - \ell$ ,then ${h}^{\prime } \cdot  {x}^{\prime } \leq   - \frac{\ell }{2}$ .

2. 对于每个示例 $x$，若 $h \cdot  x \geq  \ell$，则 ${h}^{\prime } \cdot  x \geq  \frac{\ell }{2}$；若 $h \cdot  x \leq   - \ell$，则 ${h}^{\prime } \cdot  {x}^{\prime } \leq   - \frac{\ell }{2}$。

We now bound the probability that one of these events does not occur. For any single example $x$ ,applying Corollary 2 with $\epsilon  = \ell /2$ and our choice of $k$ ,the probability that $\begin{Vmatrix}{x}^{\prime }\end{Vmatrix} > 1 + \frac{\ell }{2}$ is at most

我们现在来界定这些事件中某一个不发生的概率。对于任意单个示例 $x$，将推论 2 应用于 $\epsilon  = \ell /2$ 以及我们所选择的 $k$，$\begin{Vmatrix}{x}^{\prime }\end{Vmatrix} > 1 + \frac{\ell }{2}$ 的概率至多为

$$
{e}^{-\left( {\frac{{\ell }^{2}}{4} - \frac{{\ell }^{3}}{8}}\right) \frac{k}{4}} \leq  {e}^{-\frac{{\ell }^{2}k}{32}} \leq  {\left( \frac{\epsilon \ell \delta }{100}\right) }^{\frac{100}{32}} < \frac{\delta }{4\left( {m + 1}\right) }.
$$

Adding this up over all the $m$ examples and the vector $h$ ,we get a failure probability of at most $\delta /4$ .

将所有 $m$ 个示例以及向量 $h$ 的情况相加，我们得到的失败概率至多为 $\delta /4$。

Next,by Corollary 2,with $u = h$ and $v = x$ ,the probability that the second event does not occur for any particular example $x$ is at most $\delta /{4m}$ . Again this contributes a total failure probability of at most $\delta /4$ . Thus,both events occur with probability at least $1 - \delta /2$ .

接下来，根据推论 2，对于 $u = h$ 和 $v = x$，第二个事件对于任意特定示例 $x$ 不发生的概率至多为 $\delta /{4m}$。同样，这总共导致的失败概率至多为 $\delta /4$。因此，两个事件同时发生的概率至少为 $1 - \delta /2$。

These events imply that the half-space in ${\mathbf{R}}^{k}$ defined by ${h}^{\prime }$ correctly classifies all the $m$ examples after projection (with probability at least $1 - \delta /2$ ). Moreover,after scaling the examples to have length at most 1 , the margin is at least

这些事件表明，由${h}^{\prime }$定义的${\mathbf{R}}^{k}$中的半空间在投影后能正确分类所有$m$示例（概率至少为$1 - \delta /2$）。此外，在将示例缩放至长度至多为1后，间隔至少为

$$
\sigma  \geq  \frac{\ell /2}{1 + \frac{\ell }{2}} \geq  \frac{\ell }{3}.
$$

Now,by Theorem 6,the perceptron algorithm will find a consistent half-space in $9/{\ell }^{2}$ iterations.

现在，根据定理6，感知机算法将在$9/{\ell }^{2}$次迭代中找到一个一致的半空间。

Finally,we need to show that $m$ is large enough that hypothesis found generalizes well. We will apply Theorem 3 to half-spaces through the origin in ${\mathbf{R}}^{k}$ . The VC-dimension of the Springer latter concept class is $k$ and so,by Theorem 4,we get the following well-known bound on the number of distinct half-spaces (see e.g. Kearns & Vazirani (1994)):

最后，我们需要证明$m$足够大，使得找到的假设具有良好的泛化能力。我们将对${\mathbf{R}}^{k}$中过原点的半空间应用定理3。后者概念类（施普林格出版社（Springer）相关概念类）的VC维数为$k$，因此，根据定理4，我们得到关于不同半空间数量的以下著名界（例如，参见卡恩斯（Kearns）和瓦齐拉尼（Vazirani）（1994年））：

$$
C\left( {{2m},k}\right)  \leq  \mathop{\sum }\limits_{{i = 0}}^{{k - 1}}\left( \begin{matrix} {2m} \\  i \end{matrix}\right)  \leq  {\left( \frac{2em}{k}\right) }^{k}. \tag{1}
$$

Our choice of $m$ satisfies

我们对$m$的选择满足

$$
m = \frac{8k}{\epsilon }\log \frac{48}{\epsilon } + \frac{4}{\epsilon }\log \frac{4}{\delta } > \frac{4}{\epsilon }\log C\left( {{2m},k}\right)  + \frac{4}{\epsilon }\log \frac{4}{\delta }.
$$

Therefore,applying Theorem 3 with $\delta /2$ in place of $\delta$ ,the half-space found by the algorithm correctly classifies at least $1 - \epsilon$ of the original distribution with probability at least $1 - \delta /2$ . This gives an overall success probability of at least $1 - \delta$ .

因此，用 $\delta /2$ 代替 $\delta$ 应用定理3，该算法找到的半空间以至少 $1 - \delta /2$ 的概率正确分类原始分布中至少 $1 - \epsilon$ 的部分。这给出了至少 $1 - \delta$ 的总体成功概率。

The perceptron algorithm and its variants are known to be resistant to various types of random classification noise (Bylander, 1994; Blum et al., 1996). It is a straightforward consequence that these properties continue to hold for the algorithm described here. In the concluding section, we discuss straightforward bounds for agnostic learning.

感知机算法及其变体已知能抵抗各种类型的随机分类噪声（拜兰德（Bylander），1994年；布卢姆（Blum）等人，1996年）。由此可直接得出，这里描述的算法也继续具备这些特性。在结论部分，我们讨论不可知学习的直接边界。

### 4.2. Intersections of half-spaces

### 4.2. 半空间的交集

The next problem we consider is learning an intersection of $t$ half-spaces in ${\mathbf{R}}^{n}$ ,i.e.,the positive examples all lie in the intersection of $t$ half-spaces and the negative examples lie outside that region. It is not known how to solve the problem for an arbitrary distribution. However efficient algorithms have been developed for reasonably general distributions assuming that the number of half-spaces is relatively small (Blum & Kannan, 1993; Vempala, 2004). Here, we derive efficient learning algorithms for robust concepts in this class.

我们接下来考虑的问题是学习$t$个半空间在${\mathbf{R}}^{n}$中的交集，即正例都位于$t$个半空间的交集内，而反例位于该区域之外。目前尚不清楚如何针对任意分布解决该问题。然而，在假设半空间数量相对较少的情况下，已经为相当通用的分布开发出了高效算法（布卢姆（Blum）和坎南（Kannan），1993年；文帕拉（Vempala），2004年）。在这里，我们为该类别中的鲁棒概念推导高效的学习算法。

We assume that all the half-spaces are homogenous. Let the concept class of intersections of half-spaces be denoted by $\mathcal{H}\left( {t,n}\right)$ . A single concept in this class is specified by a set of $t$ half-spaces $P = \left\{  {{h}_{1},\ldots ,{h}_{t}}\right\}$ ,and the positive examples are precisely those that satisfy ${h}_{i} \cdot  x \geq  0$ for $i = 1\ldots t$ . Let(P,D)be a concept-distribution pair such that $P$ is $\ell$ -robust with respect to the distribution $\mathcal{D}$ . We assume that the support $\mathcal{D}$ is a subset of the unit sphere (and remind the reader that this as well as homogeneity are not really restrictive, as they can be achieved by scaling and adding an extra dimension, respectively; see e.g. (Vempala, 2004)).

我们假设所有的半空间都是均匀的。用 $\mathcal{H}\left( {t,n}\right)$ 表示半空间交集的概念类。该类中的单个概念由一组 $t$ 个半空间 $P = \left\{  {{h}_{1},\ldots ,{h}_{t}}\right\}$ 来指定，并且正例恰好是那些满足对于 $i = 1\ldots t$ 有 ${h}_{i} \cdot  x \geq  0$ 的例子。设 (P,D) 是一个概念 - 分布对，使得 $P$ 相对于分布 $\mathcal{D}$ 是 $\ell$ - 鲁棒的。我们假设分布 $\mathcal{D}$ 的支撑集是单位球面的一个子集（并提醒读者，这以及均匀性实际上并非严格限制，因为它们可以分别通过缩放和增加一个额外维度来实现；例如，参见（文帕拉（Vempala），2004 年））。

Let denote $C\left( {m,t,k}\right)$ denote the maximum number of distinct labellings of $m$ examples in ${R}^{k}$ using concepts from $\mathcal{H}\left( {t,k}\right)$ . Then,

设 $C\left( {m,t,k}\right)$ 表示使用 $\mathcal{H}\left( {t,k}\right)$ 中的概念对 ${R}^{k}$ 中的 $m$ 个示例进行不同标记的最大数量。那么，

$$
C\left( {{2m},t,k}\right)  \leq  {\left( \mathop{\sum }\limits_{{i = 0}}^{{k - 1}}\left( \begin{matrix} {2m} \\  i \end{matrix}\right) \right) }^{t} \leq  {\left( \frac{2em}{k}\right) }^{tk}. \tag{2}
$$

This can be seen as follows: For $t = 1$ ,this is just (1),the number of ways to assign + or -1 to ${2m}$ points using a half-space. If we give each point $t$ labels,one for each of $t$ half-spaces, then the total number of possible labellings is the middle term in (2). We consider two labellings distinct iff the subset of points that are labelled + by all $t$ half-spaces is different. Thus the total number of distinct labellings by $t$ half-spaces can only be smaller than this bound.

可以通过以下方式理解：对于 $t = 1$，这就是 (1)，即使用一个半空间为 ${2m}$ 个点分配 +1 或 -1 的方式的数量。如果我们为每个点赋予 $t$ 个标签，每个半空间对应一个标签，那么可能的标记总数就是 (2) 中的中间项。当且仅当被所有 $t$ 个半空间标记为 + 的点的子集不同时，我们认为两种标记是不同的。因此，$t$ 个半空间进行不同标记的总数只能小于这个界限。

Given $m$ examples,we can always find a consistent hypothesis (if one exists) using a brute-force algorithm that enumerates all the combinatorially distinct half-spaces and pick $t$ of them (with replacement). We apply this to learning a robust intersection of $t$ -half-spaces after projecting a sufficiently large sample to a lower-dimensional subspace. The parameters $k$ and $m$ below will be specified shortly.

给定 $m$ 个示例，我们总能使用一种暴力算法找到一个一致的假设（如果存在的话），该算法会枚举所有组合上不同的半空间，并从中选取 $t$ 个（可重复选取）。我们将此方法应用于在把足够大的样本投影到低维子空间后学习 $t$ 个半空间的鲁棒交集。下面的参数 $k$ 和 $m$ 很快会给出具体说明。

## $t$ -Half-spaces Algorithm:

## $t$ 半空间算法：

1. Choose an $n \times  k$ random matrix $R$ for projection by choosing each entry independently from $N\left( {0,1}\right)$ or $U\left( {-1,1}\right)$ .

1. 通过从 $N\left( {0,1}\right)$ 或 $U\left( {-1,1}\right)$ 中独立选取每个元素，选择一个 $n \times  k$ 随机矩阵 $R$ 用于投影。

2. Obtain $m$ examples from $\mathcal{D}$ and project them to ${\mathbf{R}}^{k}$ using $R$ .

2. 从 $\mathcal{D}$ 中获取 $m$ 个示例，并使用 $R$ 将它们投影到 ${\mathbf{R}}^{k}$ 上。

3. Find a hypothesis $Q = \left\{  {{w}_{1},\ldots ,{w}_{t}}\right\}$ where each ${w}_{i} \in  {\mathbf{R}}^{k}$ such that the intersection of the half-spaces ${w}_{i} \cdot  x \geq  0$ for $i = 1,\ldots ,t$ is consistent with the labels of the projected examples.

3. 找到一个假设$Q = \left\{  {{w}_{1},\ldots ,{w}_{t}}\right\}$，其中每个${w}_{i} \in  {\mathbf{R}}^{k}$满足：对于$i = 1,\ldots ,t$，半空间${w}_{i} \cdot  x \geq  0$的交集与投影示例的标签一致。

4. Output $R$ and $Q$ .

4. 输出$R$和$Q$。

A future example $x$ is projected down as ${R}^{T}x$ and labelled according to $Q$ ,i.e.,it is positive if ${w}_{i} \cdot  \left( {{R}^{T}x}\right)  \geq  0$ for all $i = 1,\ldots ,t$ .

未来的一个示例$x$被投影为${R}^{T}x$，并根据$Q$进行标记，即，如果对于所有的$i = 1,\ldots ,t$都有${w}_{i} \cdot  \left( {{R}^{T}x}\right)  \geq  0$，则该示例为正例。

Theorem 8. An $\ell$ -robust intersection of $t$ half-spaces in ${\mathbf{R}}^{n}$ can be $\left( {\epsilon ,\delta }\right)$ -learned by projecting $m$ examples to ${\mathbf{R}}^{k}$ where

定理8. 在${\mathbf{R}}^{n}$中$t$个半空间的$\ell$ - 鲁棒交集可以通过将$m$个示例投影到${\mathbf{R}}^{k}$来进行$\left( {\epsilon ,\delta }\right)$ - 学习，其中

$$
k = \frac{100}{{\ell }^{2}}\ln \frac{100t}{\epsilon \ell \delta }\;\text{ and }\;m = \frac{8kt}{\epsilon }\log \frac{48t}{\epsilon } + \frac{4}{\epsilon }\log \frac{4}{\delta } = O\left( {\frac{t}{\epsilon {\ell }^{2}}\log \frac{t}{\epsilon }\log \frac{t}{\ell {\epsilon \delta }}}\right) 
$$

in time $O\left( {nmk}\right)  + {\left( \frac{48t}{\epsilon }\log \frac{4t}{\epsilon \delta }\right) }^{kt}$ .

时间复杂度为$O\left( {nmk}\right)  + {\left( \frac{48t}{\epsilon }\log \frac{4t}{\epsilon \delta }\right) }^{kt}$。

Proof: The proof is similar to that of Theorem 7 and we only sketch it.

证明：该证明与定理7的证明类似，我们仅简述其过程。

Let the original set of half-spaces be ${h}_{1} \cdot  x \geq  0,\ldots ,{h}_{t} \cdot  x \geq  0$ ,where each ${h}_{i}$ is a unit vector in ${\mathbf{R}}^{n}$ . We consider the projections of these, ${h}_{i}^{\prime } = \frac{1}{\sqrt{k}}{R}^{T}{h}_{i}$ ,and the following events: For each example $x$ and normal vector ${h}_{i}$ ,if ${h}_{i} \cdot  x \geq  \ell$ ,then ${h}_{i}^{\prime } \cdot  {x}^{\prime } > 0$ ; If ${h}_{i} \cdot  x \leq   - \ell$ ,then ${h}_{i}^{\prime } \cdot  {x}^{\prime } < 0$ .

设原始的半空间集合为 ${h}_{1} \cdot  x \geq  0,\ldots ,{h}_{t} \cdot  x \geq  0$，其中每个 ${h}_{i}$ 是 ${\mathbf{R}}^{n}$ 中的单位向量。我们考虑这些半空间的投影 ${h}_{i}^{\prime } = \frac{1}{\sqrt{k}}{R}^{T}{h}_{i}$，以及以下事件：对于每个示例 $x$ 和法向量 ${h}_{i}$，若 ${h}_{i} \cdot  x \geq  \ell$，则 ${h}_{i}^{\prime } \cdot  {x}^{\prime } > 0$；若 ${h}_{i} \cdot  x \leq   - \ell$，则 ${h}_{i}^{\prime } \cdot  {x}^{\prime } < 0$。

For our choice of $k$ and $m$ ,it follows from Corollary 2 that these events all happen with probability at least $1 - \delta /2$ . Therefore,after projection,with this probability,there is a hypothesis from $\mathcal{H}\left( {t,k}\right)$ that is consistent with all $m$ examples. Using Theorem 3 along with (2), it follows that any hypothesis consistent with a sample of size

对于我们对 $k$ 和 $m$ 的选择，根据推论 2 可知，这些事件发生的概率至少为 $1 - \delta /2$。因此，经过投影后，以这个概率，存在一个来自 $\mathcal{H}\left( {t,k}\right)$ 的假设与所有 $m$ 个示例一致。结合定理 3 和 (2) 可知，任何与大小为

$$
m = \frac{8kt}{\epsilon }\log \frac{2t}{\epsilon } + \frac{4}{\epsilon }\log \frac{4}{\delta }
$$

will correctly classify $\left( {1 - \epsilon }\right)$ of the distribution with probability at least $1 - \delta /2$ . This gives an overall success probability of at least $1 - \delta$ . The running time of the enumerative algorithm is $O\left( {\left( 2em/k\right) }^{kt}\right)$ .

的样本一致的假设将以至少 $1 - \delta /2$ 的概率正确分类该分布的 $\left( {1 - \epsilon }\right)$。这给出了至少 $1 - \delta$ 的总体成功概率。枚举算法的运行时间为 $O\left( {\left( 2em/k\right) }^{kt}\right)$。

If $t,\ell ,\epsilon ,\delta$ are all constant,then the algorithm runs in linear time. If only $\ell ,\epsilon ,\delta$ are constant,then the algorithm has running time $O\left( {{nt}{\log }^{3}t}\right)  + {\left( t\log t\right) }^{O\left( {t\log t}\right) }$ . This is significantly faster than the best-known algorithms for the general case (see Section 1.1 for recent improvements). Both results do not need any further assumptions on the distribution $\mathcal{D}$ besides Springer robustness. Previous algorithms for the problem assumed that $\mathcal{D}$ was either symmetric (Baum, 1990), uniform (Blum & Kannan, 1993) or non-concentrated (Vempala, 1997). Recently, an improved time complexity for learning robust intersections of half-spaces was obtained in Klivans and Servedio (2004) using an algorithm for learning polynomial threshold functions in the projected space in place of the enumerative algorithm used here. The improvement in the time complexity comes along with a substantial increase in the sample complexity.

如果 $t,\ell ,\epsilon ,\delta$ 均为常数，则该算法的运行时间为线性时间。如果仅 $\ell ,\epsilon ,\delta$ 为常数，则该算法的运行时间为 $O\left( {{nt}{\log }^{3}t}\right)  + {\left( t\log t\right) }^{O\left( {t\log t}\right) }$。这比一般情况下已知的最佳算法要快得多（有关近期的改进，请参阅第1.1节）。除了施普林格（Springer）鲁棒性之外，这两个结果都不需要对分布 $\mathcal{D}$ 做任何进一步的假设。之前针对该问题的算法假设 $\mathcal{D}$ 要么是对称的（鲍姆（Baum），1990年），要么是均匀的（布卢姆（Blum）和坎南（Kannan），1993年），要么是非集中的（温帕拉（Vempala），1997年）。最近，克利万斯（Klivans）和瑟维迪奥（Servedio）（2004年）通过使用一种在投影空间中学习多项式阈值函数的算法，而非此处使用的枚举算法，提高了学习半空间鲁棒交集的时间复杂度。时间复杂度的改善伴随着样本复杂度的大幅增加。

### 4.3. Balls

### 4.3. 球

Finally,we briefly discuss the concept class of balls in ${\mathbf{R}}^{n}$ ,illustrating how robustness plays a role in learning nonlinear concepts.

最后，我们简要讨论${\mathbf{R}}^{n}$中球的概念类，说明鲁棒性在学习非线性概念中是如何发挥作用的。

A Ball $B\left( {{x}_{0},r}\right)$ in ${\mathbf{R}}^{n}$ is defined as

${\mathbf{R}}^{n}$中的球$B\left( {{x}_{0},r}\right)$定义为

$$
B\left( {{x}_{0},r}\right)  = \left\{  {x \in  {\mathbf{R}}^{n} : \begin{Vmatrix}{x - {x}_{0}}\end{Vmatrix} \leq  r}\right\}  
$$

where ${x}_{0}$ (the center) is a fixed point in ${\Re }^{n}$ and $r$ (the radius) is a fixed real value. The set of points in $B\left( {{x}_{0},r}\right)$ are labelled positive and those outside are labelled negative.

其中${x}_{0}$（球心）是${\Re }^{n}$中的一个固定点，$r$（半径）是一个固定的实数值。$B\left( {{x}_{0},r}\right)$内的点集标记为正，外部的点集标记为负。

It is well-known that the VC-dimension of balls in ${\mathbf{R}}^{n}$ is $n + 1$ and so the number of examples required to $\left( {\epsilon ,\delta }\right)$ -learn a ball is $O\left( {\frac{n}{\epsilon }\log \frac{1}{\epsilon } + \frac{1}{\epsilon }\log \frac{1}{\delta }}\right)$ . How many examples do we need to learn an $\ell$ -robust ball? The following theorem follows easily from the neuronal projection theorem.

众所周知，${\mathbf{R}}^{n}$ 中球的 VC 维（Vapnik-Chervonenkis 维）是 $n + 1$，因此 $\left( {\epsilon ,\delta }\right)$ -学习一个球所需的样本数量是 $O\left( {\frac{n}{\epsilon }\log \frac{1}{\epsilon } + \frac{1}{\epsilon }\log \frac{1}{\delta }}\right)$。我们需要多少个样本才能学习一个 $\ell$ -鲁棒球呢？以下定理可轻松由神经元投影定理推导得出。

Theorem 9. An $\ell$ -robust ball of radius in ${\mathbf{R}}^{n}$ of radius at most 1 can be $\left( {\epsilon ,\delta }\right)$ -learned by projecting $m$ examples to ${\mathbf{R}}^{k}$ where

定理 9. 在 ${\mathbf{R}}^{n}$ 中半径至多为 1 的 $\ell$ -鲁棒球可以通过将 $m$ 个样本投影到 ${\mathbf{R}}^{k}$ 来进行 $\left( {\epsilon ,\delta }\right)$ -学习，其中

$$
k = \frac{100}{{\ell }^{2}}\ln \frac{100}{\epsilon \ell \delta }\;\text{ and }\;m = \frac{8k}{\epsilon }\log \frac{48}{\epsilon } + \frac{4}{\epsilon }\log \frac{4}{\delta }
$$

and then finding a ball in ${\mathbf{R}}^{k}$ consistent with the projected examples.

然后在 ${\mathbf{R}}^{k}$ 中找到一个与投影后的样本一致的球。

Proof: With probability 1,any positive example $x$ drawn from the distribution $\mathcal{D}$ will satisfy

证明：概率为 1 时，从分布 $\mathcal{D}$ 中抽取的任何正例 $x$ 都将满足

$$
\begin{Vmatrix}{x - {x}_{0}}\end{Vmatrix} \leq  r - l
$$

while any negative example $x$ will satisfy

而任何反例 $x$ 都将满足

$$
\begin{Vmatrix}{x - {x}_{0}}\end{Vmatrix} \geq  r + l
$$

Using Theorem 2 with our choice of $k$ and $\epsilon  = \ell /2$ ,for any one $x$ ,its projection ${x}^{\prime }$ satisfies

将定理 2 应用于我们所选择的 $k$ 和 $\epsilon  = \ell /2$，对于任意一个 $x$，其投影 ${x}^{\prime }$ 满足

$$
\left( {1 - \frac{\ell }{2}}\right) \begin{Vmatrix}{x - {x}_{0}}\end{Vmatrix} \leq  \begin{Vmatrix}{{x}^{\prime } - {x}_{0}^{\prime }}\end{Vmatrix} \leq  \left( {1 + \frac{\ell }{2}}\right) \begin{Vmatrix}{x - {x}_{0}}\end{Vmatrix}
$$

with probability at least $1 - \frac{\delta }{2m}$ . So,with probability $1 - \delta /2$ ,all the projected examples satisfy the above inequality. Further, since the radius of the concept ball is at most 1 ,

概率至少为 $1 - \frac{\delta }{2m}$。因此，概率为 $1 - \delta /2$ 时，所有投影示例都满足上述不等式。此外，由于概念球的半径至多为 1，

$$
\begin{Vmatrix}{x - {x}_{0}}\end{Vmatrix} + \frac{\ell }{2} \leq  \begin{Vmatrix}{{x}^{\prime } - {x}_{0}^{\prime }}\end{Vmatrix} \leq  \begin{Vmatrix}{x - {x}_{0}}\end{Vmatrix} + \frac{\ell }{2}.
$$

Thus,the ball $B\left( {{x}_{0}^{\prime },r}\right)$ in ${\mathbf{R}}^{k}$ is consistent with the projected examples and the theorem follows. Finally,we can use Theorem 3 to verify that $m$ is large enough for this to be an $\left( {\epsilon ,\delta }\right)$ -learning algorithm.

因此，${\mathbf{R}}^{k}$ 中的球 $B\left( {{x}_{0}^{\prime },r}\right)$ 与投影示例一致，定理得证。最后，我们可以使用定理 3 来验证 $m$ 足够大，使得这是一个 $\left( {\epsilon ,\delta }\right)$ -学习算法。

### 4.4. Noise tolerance

### 4.4. 噪声容忍度

Here we note that the algorithms can be adapted to be resistant to malicious classification noise (agnostic learning). In a sample of $s$ examples,let the labels of at most ${\gamma s}$ of them be corrupted arbitrarily. Fix a hypothesis class $H$ and let $f\left( \ell \right)$ be the bound on the number of examples required to learn concepts with robustness $\ell$ . Then to deal with this noise "rate" $\gamma$ , we obtain $f\left( \ell \right) /\left( {1 - \gamma }\right)$ examples,and for every subset of size $f\left( \ell \right)$ of the sample,we run the learning algorithm for the hypothesis class and output a hypothesis that correctly classifies the subset. The total number of runs of the algorithms is at most ${2}^{{2f}\left( \ell \right) }$ . So,for example, half-spaces in ${\mathbf{R}}^{n}$ can be learned in $\operatorname{poly}\left( n\right)$ time for robustness as low as $\sqrt{\frac{\log \log n}{\log n}}$ . Another way to interpret this is that we can find hypothesis that minimize the number of mistakes. This was observed by Avrim Blum.

在此我们注意到，这些算法可以进行调整以抵御恶意分类噪声（不可知学习）。在一个包含 $s$ 个示例的样本中，假设其中至多 ${\gamma s}$ 个示例的标签被任意篡改。固定一个假设类 $H$，并设 $f\left( \ell \right)$ 为以鲁棒性 $\ell$ 学习概念所需的示例数量上限。那么，为了处理这个噪声“比率” $\gamma$，我们获取 $f\left( \ell \right) /\left( {1 - \gamma }\right)$ 个示例，并且对于样本中每个大小为 $f\left( \ell \right)$ 的子集，我们针对该假设类运行学习算法，并输出一个能正确分类该子集的假设。算法的总运行次数至多为 ${2}^{{2f}\left( \ell \right) }$。因此，例如，在 ${\mathbf{R}}^{n}$ 中的半空间可以在 $\operatorname{poly}\left( n\right)$ 时间内以低至 $\sqrt{\frac{\log \log n}{\log n}}$ 的鲁棒性进行学习。另一种解释是，我们可以找到使错误数量最小化的假设。这是由阿夫里姆·布卢姆（Avrim Blum）观察到的。

## 5. Discussion

## 5. 讨论

#### 5.1.A robust model of categorization

#### 5.1 一个稳健的分类模型

The model studied in this paper can be viewed as a rudimentary model of human learning. In this model, at the outer level of processing, there is a layer of neurons that produces a random summary of any stimuli presented. It is the summary that is then processed by learning algorithms. The outer level plays the role of random projection. The main insight of the model is that even a random summary that is independent of any specific concept and independent of the distribution on examples (stimuli), can preserve the essential elements necessary for learning the category. The ease of learning and the extent to which the summary preserves the concepts depends on their robustness-the more robust a concept, the shorter a summary needs to be and the easier it is to learn. In this section, we draw from work in cognitive and neuropsychology to see how the predictions of this model hold up. Our model goes beyond previous ones that made similar predictions in suggesting a simple physiological mechanism.

本文研究的模型可被视为人类学习的初级模型。在该模型中，在处理的外层，有一层神经元会对呈现的任何刺激产生随机摘要。然后学习算法会对这个摘要进行处理。外层起到随机投影的作用。该模型的主要见解是，即使是与任何特定概念无关且与示例（刺激）分布无关的随机摘要，也能保留学习类别所需的基本要素。学习的难易程度以及摘要保留概念的程度取决于它们的稳健性——概念越稳健，所需的摘要就越短，学习也越容易。在本节中，我们借鉴认知心理学和神经心理学的研究成果，来检验该模型的预测是否成立。我们的模型在提出一种简单的生理机制方面超越了之前做出类似预测的模型。

An interesting prediction of our model is that learning concepts that are more robust requires fewer examples. This prediction is supported by many psychological studies (Glass, et al., 1979; Komatsu, 1992; Reed, 1982; Reed & Friedman, 1973; Rosch, 1978; Rosch et al., 1976), in particular those that refer to concept formation as stemming from the family resemblence perspective (for a detailed account of other prominent views, see (Komatsu, 1992; Rakinson & Oakes, 2003)). The family resemblence perspective argues that categories (concept classes) as formed by humans are hierarchical (Reed, 1982), with three clear levels called the Superordinate, Basic and Subordinate. For example, for the Superordinate category of Mammals, some Basic level categories are Elephant, Dog, Human, and the Subordinate categories for Elephant would be African Elephant and Indian Elephant. Similarly, the Superordinate category of Musical Instruments has below it the Basic level categories of Guitar, Piano, Drum, etc. and Guitar, has below it Subordinate categories such as Folk Guitar and Steel Guitar. The Basic level categories are considered the most important, and are the most clearly demarcated from each other. In our terminology they are the most robust, and we Springer expect them to be easier to learn. This is indeed the case as noted by Rosch et al. (1976), "...basic level categories are the most differentiated from one another and they are therefore the first categories we learn and the most important in language."

我们模型的一个有趣预测是，学习更具鲁棒性的概念所需的示例更少。这一预测得到了许多心理学研究的支持（格拉斯（Glass）等人，1979年；小松（Komatsu），1992年；里德（Reed），1982年；里德和弗里德曼（Reed & Friedman），1973年；罗施（Rosch），1978年；罗施等人，1976年），尤其是那些从家族相似性视角探讨概念形成的研究（关于其他重要观点的详细阐述，请参阅（小松，1992年；拉金森和奥克斯（Rakinson & Oakes），2003年））。家族相似性视角认为，人类形成的范畴（概念类别）是有层次的（里德，1982年），有三个明确的层次，分别称为上位层次、基本层次和下位层次。例如，对于“哺乳动物”这一上位范畴，一些基本层次范畴包括“大象”“狗”“人类”，而“大象”的下位范畴则有“非洲象”和“印度象”。同样，“乐器”这一上位范畴下有“吉他”“钢琴”“鼓”等基本层次范畴，而“吉他”之下又有“民谣吉他”和“钢弦吉他”等下位范畴。基本层次范畴被认为是最重要的，并且彼此之间的界限最为清晰。用我们的术语来说，它们是最具鲁棒性的，我们斯普林格出版社（Springer）预计它们更容易学习。正如罗施等人（1976年）所指出的，情况确实如此：“……基本层次范畴彼此之间的区分最为明显，因此它们是我们最先学习的范畴，也是语言中最重要的范畴。”

A related theory of how humans form categories is based on the notion of Prototypes (Glass, et al., 1979; Komatsu, 1992). The essential predictions of this theory can also be derived from robustness. Prototypes represent the most typical members of a category. The theory says that we abstract a prototype for a category by forming some weighted average of (a subset of) the defining features of examples from the category. A subsequent instance is compared to the prototype, and if it has a sufficient degree of similarity to the prototype then it is judged to be a member of the category. This explains the results of studies where it is found that when asked to list examples of a category, subjects consistently list members that are closer to the prototype both earlier and more often (e.g., for the category Bird, the examples Sparrow and Robin are produced more often than Ostrich) Rosch (1978). Further when asked to classify instances, it is found that examples closer to the prototype are classified more quickly. Similar results were found in studies with artificially generated categories (Reed & Friedman, 1973).

关于人类如何形成类别的一个相关理论基于原型（Prototypes）的概念（格拉斯（Glass）等人，1979年；小松（Komatsu），1992年）。该理论的基本预测也可以从稳健性推导得出。原型代表了一个类别中最典型的成员。该理论认为，我们通过对某个类别示例的（部分）定义特征进行加权平均来抽象出该类别的一个原型。随后将一个实例与该原型进行比较，如果它与原型有足够的相似度，那么就会被判定为该类别的成员。这解释了一些研究结果，在这些研究中发现，当要求列出某个类别的示例时，受试者总是更早且更频繁地列出更接近原型的成员（例如，对于“鸟类”这个类别，人们列出麻雀（Sparrow）和知更鸟（Robin）的频率比鸵鸟（Ostrich）更高）（罗施（Rosch），1978年）。此外，当要求对实例进行分类时，发现更接近原型的示例分类速度更快。在人工生成类别的研究中也发现了类似的结果（里德（Reed）和弗里德曼（Friedman），1973年）。

For a prototype $P$ ,we could define a family of nested concepts within the category of $P$ according to the distance from $P$ . Then the members of the innermost concept are very similar to $P$ ,the members of the next concept are a bit more varied,and the variation increases as the maximum distance of the concepts from $P$ grows. In other words the innermost concept is the most robust in terms of demarcating the category from objects that are not members (of the family category), and the robustness decreases as we move outward. The arguments in this paper imply that the inner concepts would be easier to learn and label than the outer ones. This is exactly what was observed in the aforementioned studies.

对于一个原型$P$，我们可以根据与$P$的距离，在$P$这一类别中定义一系列嵌套概念。那么，最内层概念的成员与$P$非常相似，下一个概念的成员则略有不同，并且随着概念与$P$的最大距离增加，差异也会增大。换句话说，就将该类别与非成员（家族类别）对象区分开来而言，最内层概念是最稳定的，并且随着我们向外扩展，稳定性会降低。本文的论证表明，内层概念比外层概念更容易学习和标注。这正是上述研究中所观察到的情况。

In our model as the organism is presented with a given stimulus, a random summary of its characteristics is captured. After another member of the same family is observed another summary of characteristics is abstracted. The characteristics that are shared among stimuli from the same "family" would in time lead to a set of summaries with analogou: characteristics. These analogous characteristics would give rise to a protypical family member (say Robin for the bird family) because it embodies many of the characteristics which have a greater probability of appearing in these "random summaries" since they are more common in the family (small, feathers, flies) and not other characteristics which are likely to be atypical (large, does not fly for Ostrich or has no feathers for Penguin).

在我们的模型中，当生物体接收到给定的刺激时，会捕捉到该刺激特征的随机摘要。在观察到同一“家族”的另一个成员后，会提取出另一组特征摘要。来自同一“家族”的刺激所共有的特征，最终会形成一组具有相似特征的摘要。这些相似的特征会催生一个典型的家族成员（例如鸟类家族中的知更鸟），因为它体现了许多在这些“随机摘要”中更有可能出现的特征，因为这些特征在该家族中更为常见（体型小、有羽毛、会飞），而不是那些可能不典型的特征（体型大、鸵鸟不会飞或企鹅没有羽毛）。

Another question that our model addresses concerns the need to make distinctions between perceptual (red, square, loud) or conceptual categories (dessert vs. salad or good vs. evil). This issue is prominent in current research on categorization (Mandler, 2003). While the examples we have mentioned (birds, elephants and guitars) can be described as perceptual (object-based) categories, it is worth noting that our model also applies to conceptual categories. The idea is that along with physical characteristics, abstract characteristics (that are also ultimately functions of the stimuli, e.g. "soulfulness" might include Steel Guitar and Saxophone) are preserved by the random summaries. The predictions of our model are similar to those of the family resemblance view; the latter has been successfully used to go beyond object-based categores to psychological phenomena such as emotions and personality traits (Komatsu, 1992). At the outer-level mechanism of our model, there is no need for separate learning systems for categorization.

我们的模型所解决的另一个问题涉及到对感知类别（如红色、正方形、响亮）或概念类别（如甜点与沙拉、善与恶）进行区分的必要性。这一问题在当前的分类研究中十分突出（曼德勒（Mandler），2003 年）。虽然我们所提及的例子（鸟类、大象和吉他）可被描述为感知（基于物体）类别，但值得注意的是，我们的模型同样适用于概念类别。其理念在于，除了物理特征外，抽象特征（这些特征最终也是刺激的函数，例如“深情特质”可能包括夏威夷吉他和萨克斯风）也能通过随机摘要得以保留。我们模型的预测结果与家族相似性观点的预测结果相似；后者已成功用于超越基于物体的类别，应用于诸如情绪和人格特质等心理现象（小松（Komatsu），1992 年）。在我们模型的外层机制中，无需为分类设置单独的学习系统。

In the same vein, the current model also speaks to the broad neuropsychological issue of whether it is necessary to propose a multiple-system (i.e., various brain regions) model as opposed to a single-system model (general brain processing) to account for object recognition on the one hand and categorization on the other (Knowlton, 1999). We see our model as a single-system model that provides a general physiological "outer-level" for learning. As such we believe this model partially answers the call to "develop formalized single-system and multiple-system mathematical models of categorization, to collect rich sets of parametric data using both normal and brain-damaged population and to test the ability of the respective models to account quatitatively for the data." (Nosofsky & Zaki, 1969).

同样，当前模型也涉及到一个广泛的神经心理学问题，即是否有必要提出一个多系统（即不同大脑区域）模型，而非单系统模型（一般大脑处理），以解释物体识别和分类问题（诺尔顿，1999 年）。我们将我们的模型视为一个单系统模型，它为学习提供了一个通用的生理“外层”。因此，我们认为该模型部分回应了这样的呼吁：“开发形式化的单系统和多系统分类数学模型，使用正常人群和脑损伤人群收集丰富的参数数据集，并测试各个模型对数据进行定量解释的能力”（诺索夫斯基和扎基，1969 年）。

### 5.2. Open problems

### 5.2. 开放性问题

In the discrete setting,an $\ell$ -robust concept is one where a positive example retains its label even if an $\ell$ -fraction of its attributes are changed. An important open problem in computational learning theory is that of learning DNF formulae from the uniform distribution without membership queries. This concept class can be viewed as intersections of half-spaces with robustness $1/\sqrt{n}$ . If there is an algorithm for learning DNF formulae in time polynomial in $n$ and $1/\ell$ ,this would solve the problem of learning DNF formulae without membership queries.

在离散环境中，$\ell$ -鲁棒概念是指一个正例即使其$\ell$比例的属性发生改变，仍能保持其标签不变。计算学习理论中的一个重要开放性问题是在没有成员查询的情况下，从均匀分布中学习析取范式（DNF）公式。这个概念类可以看作是具有鲁棒性$1/\sqrt{n}$的半空间的交集。如果存在一种算法能够在$n$和$1/\ell$的多项式时间内学习析取范式公式，那么这将解决在没有成员查询的情况下学习析取范式公式的问题。

We have seen how robustness reduces the learning complexity of some important concept classes. We conclude with the following questions: What are concept classes for which robustness does not reduce learning complexity? In particular, what is the complexity of learning robust polynomial threshold functions?

我们已经了解了鲁棒性是如何降低一些重要概念类的学习复杂度的。最后，我们提出以下问题：对于哪些概念类，鲁棒性不会降低学习复杂度？特别是，学习鲁棒多项式阈值函数的复杂度是多少？

## Appendix

## 附录

Proof: (of Theorem 5.) We basically mimic the proof of the fundamental VC theorem. The only difference is that in that theorem, it is assumed that there is a hypothesis consistent with the entire sample. Here we can only assume that there is a hypothesis that correctly classifies $1 - \epsilon /4$ fraction of the sample.

证明：（定理5的证明）我们基本上模仿基本VC定理（Vapnik - Chervonenkis定理）的证明。唯一的区别在于，在那个定理中，假设存在一个与整个样本一致的假设。而在这里，我们只能假设存在一个能正确分类$1 - \epsilon /4$比例样本的假设。

Let us call a hypothesis a bad hypothesis if it has error more than $\epsilon$ on the distribution. Let $A$ be the event that there exists a bad consistent hypothesis,i.e.,a hypothesis that has error less than $\epsilon /8$ on the sample and error greater than $\epsilon$ on the distribution. We would like to show that the probability of event $A$ is at most $\delta$ . To do this,we define $B$ to be the event that for a sequence of ${2m}$ examples,there is a concept that has error less than $\epsilon /8$ on the first $m$ and greater than $\epsilon /2$ on the remaining $m$ .

如果一个假设在分布上的误差大于$\epsilon$，我们称其为不良假设。设$A$为存在一个不良一致假设的事件，即一个在样本上误差小于$\epsilon /8$且在分布上误差大于$\epsilon$的假设。我们想要证明事件$A$的概率至多为$\delta$。为此，我们将$B$定义为这样一个事件：对于一个包含${2m}$个示例的序列，存在一个概念，它在前$m$个示例上的误差小于$\epsilon /8$，而在其余$m$个示例上的误差大于$\epsilon /2$。

Next we observe that $\Pr \left( A\right)  \leq  2 \cdot  \Pr \left( B\right)$ . This is because

接下来我们观察到 $\Pr \left( A\right)  \leq  2 \cdot  \Pr \left( B\right)$ 。这是因为

$$
\Pr \left( B\right)  \geq  \Pr \left( {A\text{ and }B}\right)  = \Pr \left( A\right)  \cdot  \Pr \left( {B/A}\right) 
$$

The probability of $B$ given $A,\Pr \left( {B/A}\right)$ is the probability that a hypothesis that has error $\epsilon$ on the distribution has error at least $\epsilon /2$ on a set of $m$ examples. Using Chebychev’s inequality, this latter probability is at least $1/2$ .

在给定$A,\Pr \left( {B/A}\right)$的条件下，$B$的概率是指一个在分布上误差为$\epsilon$的假设在一组$m$个示例上的误差至少为$\epsilon /2$的概率。利用切比雪夫不等式（Chebychev’s inequality），后一个概率至少为$1/2$。

To complete the proof we will bound the probability of $B$ . Fix any set of ${2m}$ examples and consider a random partition of them into two equal-sized sets ${S}_{1}$ and ${S}_{2}$ . Let $\widehat{h}$ be a hypothesis which disagrees with the target hypothesis on at least ${\epsilon m}/2$ of the ${2m}$ examples. This is a candidate for causing event $B$ .

为了完成证明，我们将对$B$的概率进行界定。固定任意一组${2m}$个示例，并考虑将它们随机划分为两个大小相等的集合${S}_{1}$和${S}_{2}$。设$\widehat{h}$是一个在${2m}$个示例中至少有${\epsilon m}/2$个与目标假设不一致的假设。这是导致事件$B$发生的一个候选情况。

Let ${X}_{i}$ ,for $i = 1,\ldots ,m$ denote the event that $\widehat{h}$ makes an error on the $i$ ’th example in ${S}_{1}$ . Then $E\left( {X}_{i}\right)  = \epsilon /4$ . Define

设 ${X}_{i}$ ，对于 $i = 1,\ldots ,m$ ，表示 $\widehat{h}$ 在 ${S}_{1}$ 中的第 $i$ 个示例上出错这一事件。那么 $E\left( {X}_{i}\right)  = \epsilon /4$ 。定义

$$
X = \mathop{\sum }\limits_{{i = 1}}^{m}{X}_{i}
$$

Springer

施普林格（Springer）

Then $E\left( X\right)  = {\epsilon m}/4$ . By Chernoff’s inequality,

那么 $E\left( X\right)  = {\epsilon m}/4$ 。根据切尔诺夫不等式（Chernoff’s inequality），

$$
\Pr \left( {X \leq  \frac{\epsilon }{4}\left( {1 - c}\right) }\right)  \leq  {e}^{-\frac{{\epsilon m}{c}^{2}}{8}}
$$

That is,

即

$$
\Pr \left( {X \leq  \frac{\epsilon }{8}}\right)  \leq  {e}^{-{\epsilon m}/{32}}.
$$

The total number of distinct hypothesis for the set of ${2m}$ examples is at most $C\left( {{2m},k}\right)$ . In other words,this is the number of distinct ways to partition ${2m}$ points using concepts from $\mathcal{C}$ in ${\mathbf{R}}^{k}$ . Adding up over all the hypotheses,we get that

对于 ${2m}$ 个示例的集合，不同假设的总数至多为 $C\left( {{2m},k}\right)$ 。换句话说，这是在 ${\mathbf{R}}^{k}$ 中使用来自 $\mathcal{C}$ 的概念对 ${2m}$ 个点进行划分的不同方式的数量。对所有假设求和，我们得到

$$
\Pr \left( B\right)  \leq  C\left( {{2m},k}\right) {e}^{-{\epsilon m}/{32}}.
$$

For the value of $m$ considered in the theorem,we have $\Pr \left( B\right)  < \delta /2$ and hence $\Pr \left( A\right)  < \delta$ as required.

对于定理中所考虑的 $m$ 的值，我们有 $\Pr \left( B\right)  < \delta /2$，因此正如所需的那样有 $\Pr \left( A\right)  < \delta$。

## References

## 参考文献

Achlioptas, D. (2001). Database friendly random projections. In Proc. Principles of Database Systems (PODS) (pp. 274-281).

阿赫利奥普塔斯（Achlioptas, D.）（2001 年）。数据库友好的随机投影。见《数据库系统原理会议论文集》（Proc. Principles of Database Systems (PODS)）（第 274 - 281 页）。

Agmon, S. (1954). The relaxation method for linear inequalities. Canadian Journal of Mathematics, 6(3), 382-392.

阿格蒙（Agmon, S.）（1954 年）。线性不等式的松弛法。《加拿大数学杂志》（Canadian Journal of Mathematics），6(3)，382 - 392。

Arriaga, R. I., & Vempala, S. (1999). An algorithmic theory of Learning: Robust concepts and random projection. In Proc. of the 39th IEEE Foundations of Computer Science.

阿里亚加（Arriaga, R. I.）和温帕拉（Vempala, S.）（1999 年）。学习的算法理论：鲁棒概念与随机投影。见《第 39 届 IEEE 计算机科学基础会议论文集》（Proc. of the 39th IEEE Foundations of Computer Science）。

Balcan, N., Blum, A., & Vempala, S. (2004). On kernels, margins and low-dimensional mappings. In Proc. of Algorithmic Learning Theory.

巴尔坎（Balcan, N.）、布卢姆（Blum, A.）和温帕拉（Vempala, S.）（2004 年）。关于核、间隔和低维映射。见《算法学习理论会议论文集》（Proc. of Algorithmic Learning Theory）。

Bartlett, P., & Shawe-Taylor, J. (1998). Generalization performance of support vector machines and other pattern classifiers, In B., Schvlkopf, C., Burges, & A.J. Smola, (eds.), Advances in kernel methods-support vector learning. MIT press.

巴特利特（Bartlett, P.）与肖韦 - 泰勒（Shawe - Taylor, J.）（1998 年）。支持向量机及其他模式分类器的泛化性能。收录于 B. 肖尔科普夫（Schvlkopf）、C. 伯吉斯（Burges）和 A.J. 斯莫拉（Smola）主编的《核方法进展——支持向量学习》。麻省理工学院出版社。

Baum, E. B. (1990). On learning a union of half spaces. journal of Complexity, 6(1), 67-101.

鲍姆（Baum, E. B.）（1990 年）。关于学习半空间的并集。《复杂性杂志》（Journal of Complexity），6(1)，67 - 101。

Ben-David, S., Eiron, N., & Simon, H.(2004). Limitations of learning via embeddings in euclidean half spaces. Journal of Machine Learning Research, 3, 441-461.

本 - 戴维德（Ben - David, S.）、艾龙（Eiron, N.）和西蒙（Simon, H.）（2004 年）。通过嵌入欧几里得半空间进行学习的局限性。《机器学习研究杂志》（Journal of Machine Learning Research），3，441 - 461。

Blum, A., Frieze, A. Kannan, R., & Vempala, S. (1996). A polynomial-time algorithm for learning noisy linear threshold functions. In Proc. of the 37th IEEE Foundations of Computer Science.

布卢姆（Blum, A.）、弗里兹（Frieze, A.）、坎南（Kannan, R.）和文帕拉（Vempala, S.）（1996 年）。一种学习含噪线性阈值函数的多项式时间算法。收录于第 37 届电气与电子工程师协会计算机科学基础研讨会论文集。

Blum,A.,&Kannan,R. (1993). Learning an intersection of $k$ halfspaces over a uniform distribution. In Proc. of the 34th IEEE Symposium on the Foundations of Computer Science.

布卢姆（Blum, A.）和坎南（Kannan, R.）（1993 年）。在均匀分布上学习 $k$ 个半空间的交集。收录于第 34 届电气与电子工程师协会计算机科学基础研讨会论文集。

Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1989). Learnability and the vapnik-chervonenkis dimension. Journal of ACM, 36(4), 929-965.

布卢默（Blumer），A.，埃伦费希特（Ehrenfeucht），A.，豪斯勒（Haussler），D.，& 瓦尔穆特（Warmuth），M. K.（1989）。可学习性与瓦普尼克 - 切尔沃年基斯维数（Vapnik - Chervonenkis dimension）。《美国计算机协会期刊》（Journal of ACM），36(4)，929 - 965。

ylander, T. (1994). Learning linear threshold functions in the presence of classification noise. In Proc 7th Workshop on Computational Learning Theory.

扬兰德（Ylander），T.（1994）。存在分类噪声时学习线性阈值函数。见《第7届计算学习理论研讨会论文集》（Proc 7th Workshop on Computational Learning Theory）。

Cohen, E. (1997). Learning noisy perceptrons by a perceptron in polynomial time. In Proc. of the 38th IEEE Foundations of Computer Science.

科恩（Cohen），E.（1997）。用感知机在多项式时间内学习含噪声的感知机。见《第38届电气与电子工程师协会计算机科学基础研讨会论文集》（Proc. of the 38th IEEE Foundations of Computer Science）。

Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning, 20, 273-297.

科尔特斯（Cortes），C.，& 瓦普尼克（Vapnik），V.（1995）。支持向量网络。《机器学习》（Machine Learning），20，273 - 297。

Dasgupta, S., & Gupta, A. (1999). An elementary proof of the Johnson-Lindenstrauss Lemma. Tech Rep. U.C. Berkeley.

达斯古普塔（Dasgupta），S.，& 古普塔（Gupta），A.（1999）。约翰逊 - 林登施特劳斯引理（Johnson - Lindenstrauss Lemma）的初等证明。加州大学伯克利分校技术报告（Tech Rep. U.C. Berkeley）。

Feller, W. (1957). An introduction to probability theory and its applications. John Wiley and Sons, Inc.

费勒（Feller），W.（1957）。概率论及其应用导论。约翰·威利父子公司（John Wiley and Sons, Inc.）。

Frankl, P., & Maehara, H. (1988). The Johnson-Lindenstrauss Lemma and the Sphericity of some graphs, $J$ Comb. Theory, B 44, 355-362.

弗兰克尔（Frankl, P.）和前原（Maehara, H.）（1988年）。《约翰逊 - 林登斯特劳斯引理与某些图的球形性》，$J$ 《组合理论期刊B辑》（Comb. Theory, B）44卷，第355 - 362页。

Freund, Y., & Schapire, R. E. (1999). "Large margin classification using the perceptron algorithm. Machine learning, 37(3), 277-296.

弗罗因德（Freund, Y.）和沙皮尔（Schapire, R. E.）（1999年）。《使用感知机算法进行大间隔分类》。《机器学习》（Machine learning），37(3)，第277 - 296页。

Garg, A., Har-Peled, S., & Roth, D. (2002). On generalization bounds, projection profile, and margin distribution. ICML, 171-178.

加尔格（Garg, A.）、哈尔 - 佩雷德（Har - Peled, S.）和罗斯（Roth, D.）（2002年）。《关于泛化边界、投影轮廓和间隔分布》。国际机器学习会议（ICML），第171 - 178页。

Garg, A., & Roth, D. (2003). Margin distribution and learning. ICML, 210-217.

加尔格（Garg, A.）和罗斯（Roth, D.）（2003年）。《间隔分布与学习》。国际机器学习会议（ICML），第210 - 217页。

Glass, A.L., Holyoak, K.J., & Santa J.L. (1979). The structure of categories. In Cognition. Addison-Wesley.

格拉斯（Glass, A.L.）、霍利约克（Holyoak, K.J.）和圣塔（Santa J.L.）（1979年）。《类别的结构》。收录于《认知》（Cognition）。艾迪生 - 韦斯利出版社（Addison - Wesley）。

Grötschel, M., Lovász, L., & Schrijver, A. (1988). Geometric algorithms and combinatorial optimization, Springer.

格罗特舍尔（Grötschel, M.）、洛瓦兹（Lovász, L.）和施里弗（Schrijver, A.）（1988年）。《几何算法与组合优化》，施普林格出版社（Springer）。

Hoeffding, W. (1963). Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58, 13-30.

霍夫丁（Hoeffding, W.）（1963 年）。有界随机变量之和的概率不等式。《美国统计协会杂志》（Journal of the American Statistical Association），58 卷，第 13 - 30 页。

Indyk, P., & Motwani, R. (1998). Approximate nearest neighbors: Towards removing the curse of dimensionality. In Procedings of ACM STOC.

因迪克（Indyk, P.）和莫特瓦尼（Motwani, R.）（1998 年）。近似最近邻：消除维度灾难的方向。收录于《ACM 计算理论研讨会论文集》（Procedings of ACM STOC）。

Johnson, W. B., & Lindenstrauss, J.(1984). Extensions of lipshitz mapping into Hilbert space. Contemporary Mathematics, 26, 189-206.

约翰逊（Johnson, W. B.）和林登施特劳斯（Lindenstrauss, J.）（1984 年）。利普希茨映射到希尔伯特空间的扩展。《当代数学》（Contemporary Mathematics），26 卷，第 189 - 206 页。

Kearns, M.J., & Schapire, R.E. (1994). Efficient distribution-free learning of probabilistic concepts. Journal of Computer and System Sciences, 48(3), 464-497.

卡恩斯（Kearns, M. J.）和沙皮尔（Schapire, R. E.）（1994 年）。概率概念的高效无分布学习。《计算机与系统科学杂志》（Journal of Computer and System Sciences），48(3) 卷，第 464 - 497 页。

Kearns, M.J., & Vazirani, U. (1994). Introduction to computational learning theory. MIT Press.

卡恩斯（Kearns, M. J.）和瓦齐拉尼（Vazirani, U.）（1994 年）。《计算学习理论导论》（Introduction to computational learning theory）。麻省理工学院出版社（MIT Press）。

Kleinberg, J. (1997). Two algorithms for nearest-neighbor search in high dimensions. In Procedings 29th ACM Symposium on Theory of Computing.

克莱因伯格（Kleinberg, J.）（1997 年）。高维最近邻搜索的两种算法。收录于《第 29 届 ACM 计算理论研讨会论文集》（Procedings 29th ACM Symposium on Theory of Computing）。

Klivans, A., & Servedio, R. (2004). Learning intersections of halfspaces with a margin, In Procedings 17th Workshop on Computational Learning Theory.

克利万斯（Klivans），A.，& 瑟维迪奥（Servedio），R.（2004）。带间隔的半空间交集学习，载于《第17届计算学习理论研讨会会议记录》。

Knowlton, B. (1999). What can neuropsychology tell us about category learning. Trends in Cognitive Science, 3, 123-124.

诺尔顿（Knowlton），B.（1999）。神经心理学能告诉我们关于类别学习的哪些内容。《认知科学趋势》，3，123 - 124。

Komatsu, L.K. (1992). Recent views on conceptual structure. Psychological Bulletin, 112(3).

小松（Komatsu），L.K.（1992）。关于概念结构的最新观点。《心理学公报》，112(3)。

Linial, N., London, E., & Rabinovich, Y. (1994). The geometry of graphs and some of its algorithmic applications. In Proc. of 35th IEEE Foundations of Computer Science.

利尼亚尔（Linial），N.，伦敦（London），E.，& 拉比诺维奇（Rabinovich），Y.（1994）。图的几何性质及其一些算法应用。载于《第35届电气与电子工程师协会计算机科学基础研讨会会议记录》。

Littlestone, N. (1987). Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2,285-318.

利特尔斯通（Littlestone），N.（1987）。当无关属性众多时的快速学习：一种新的线性阈值算法。《机器学习》，2，285 - 318。

Littlestone, N. (1991). Redundant noisy attributes, attribute errors, and linear threshold learning using winnow. In Proc. 4th workshop on computational learning theory.

利特尔斯通（Littlestone），N.（1991）。冗余噪声属性、属性错误以及使用筛选法的线性阈值学习。载于《第4届计算学习理论研讨会会议记录》。

Mandler, J. M. (2003). Conceptual Categorization, Chapter 5. In D. H., Rakinson, & L. M. Oakes, (eds.), Early category and concept development: Making sense of the blooming, buzzing confusion. Oxford University Press.

曼德勒（Mandler），J. M.（2003）。概念分类，第5章。载于D. H. 拉金森（Rakinson）和L. M. 奥克斯（Oakes）主编，《早期类别与概念发展：理解纷繁复杂的世界》。牛津大学出版社。

Minsky, M., & Papert., S., (1969). Perceptrons: An introduction to computational geometry. The MIT press.

明斯基（Minsky），M.和帕佩特（Papert），S.（1969）。《感知机：计算几何导论》。麻省理工学院出版社。

Nosofsky, R., & Zaki, S., (1969). Math modeling, neuropsychology, and category learning: Response to B. Knowlton (1999). Trends in Cognitive Science, 3, 125-126, 1999. Perceptrons: An introduction to computational geometry. The MIT press.

诺索夫斯基（Nosofsky），R.和扎基（Zaki），S.（1969）。数学建模、神经心理学与类别学习：对B. 诺尔顿（Knowlton）（1999）的回应。《认知科学趋势》，3，125 - 126，1999。《感知机：计算几何导论》。麻省理工学院出版社。

Rakinson, D. H., & Oakes, L. M. (eds.), (2003). Early category and concept development: Making sense of the blooming, buzzing confusion. Oxford University Press.

拉金森（Rakinson），D. H.和奥克斯（Oakes），L. M.（主编）（2003）。《早期类别与概念发展：理解纷繁复杂的世界》。牛津大学出版社。

Reed, S. K. (1982). Categorization, in cognition: Theory and applications. brooks/cole.

里德（Reed），S. K.（1982）。《认知中的分类：理论与应用》。布鲁克斯/科尔出版社。

Reed, S. K., & Friedman, M. P. (1973). Perceptual vs. conceptual categorization. Memory and Cognition, 1.

里德（Reed, S. K.）和弗里德曼（Friedman, M. P.）（1973 年）。知觉分类与概念分类。《记忆与认知》，第 1 期。

Rosch, E. (1978). Principles of categorization. in E., Rosch, & Lloyd, B. B. (eds.), Cognition and categorization Hillsdale.

罗施（Rosch, E.）（1978 年）。分类原则。载于 E. 罗施（Rosch）和 B. B. 劳埃德（Lloyd）编，《认知与分类》，希尔斯代尔（Hillsdale）。

Rosch, E. H., Mervis, C. B., Gray, W. D., Johnson, D. M., & Boyes-Braem, P. (1976). Basic ojects in natural categories. Cognitive Psychology, 8.

罗施（Rosch, E. H.）、梅尔维斯（Mervis, C. B.）、格雷（Gray, W. D.）、约翰逊（Johnson, D. M.）和博伊斯 - 布雷姆（Boyes - Braem, P.）（1976 年）。自然范畴中的基本对象。《认知心理学》，第 8 期。

Rosenblatt, F. (1962). Principles of neurodynamics. Spartan Books.

罗森布拉特（Rosenblatt, F.）（1962 年）。神经动力学原理。斯巴达图书公司（Spartan Books）。

Schapire, R. E., Freund, Y., Bartlett, P. L., & Lee, W. S. (1998). Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of Statistics, 26(5), 1651-1686.

沙皮尔（Schapire, R. E.）、弗罗因德（Freund, Y.）、巴特利特（Bartlett, P. L.）和李（Lee, W. S.）（1998 年）。提升边界：投票方法有效性的新解释。《统计学年鉴》，26(5)，1651 - 1686。

Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27(11), 1134-1142.

瓦利安特（Valiant, L. G.）（1984 年）。可学习性理论。《美国计算机协会通讯》，27(11)，1134 - 1142。

Valiant, L. G. (1998). A neuroidal architecture for cognitive computation. In Proc. of ICALP.

瓦利安特（Valiant, L. G.）（1998年）。用于认知计算的类神经架构。收录于《自动机、语言和程序设计国际学术讨论会论文集》（Proc. of ICALP）。

Vapnik, V. N. (1995). The nature of statistical learning theory. Springer.

瓦普尼克（Vapnik, V. N.）（1995年）。统计学习理论的本质。施普林格出版社（Springer）。

Vapnik, V. N., & Chervonenkis, A. Ya., (1971). On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its applications, 26(2), 264-280.

瓦普尼克（Vapnik, V. N.）和切尔沃年基斯（Chervonenkis, A. Ya.）（1971年）。事件相对频率向其概率的一致收敛性。《概率论及其应用》（Theory of Probability and its applications），26(2)，264 - 280。

Vempala, S. (1997). A random sampling based algorithm for learning the Intersection of Half-spaces. In Proc. of the 38th IEEE Foundations of Computer Science.

文帕拉（Vempala, S.）（1997年）。一种基于随机抽样的学习半空间交集的算法。收录于《第38届电气与电子工程师协会计算机科学基础研讨会论文集》（Proc. of the 38th IEEE Foundations of Computer Science）。

Vempala, S. (2004). The random projection method. 65. DIMACS series, AMS.

文帕拉（Vempala, S.）（2004年）。随机投影方法。65。美国数学学会离散数学与理论计算机科学系列丛书（DIMACS series, AMS）。