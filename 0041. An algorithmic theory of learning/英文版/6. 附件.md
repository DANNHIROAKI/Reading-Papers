## Appendix

Proof: (of Theorem 5.) We basically mimic the proof of the fundamental VC theorem. The only difference is that in that theorem, it is assumed that there is a hypothesis consistent with the entire sample. Here we can only assume that there is a hypothesis that correctly classifies $1-\epsilon / 4$ fraction of the sample.

---

Let us call a hypothesis a bad hypothesis if it has error more than $\epsilon$ on the distribution. Let $A$ be the event that there exists a bad consistent hypothesis, i.e., a hypothesis that has error less than $\epsilon / 8$ on the sample and error greater than $\epsilon$ on the distribution. We would like to show that the probability of event $A$ is at most $\delta$. To do this, we define $B$ to be the event that for a sequence of $2 m$ examples, there is a concept that has error less than $\epsilon / 8$ on the first $m$ and greater than $\epsilon / 2$ on the remaining $m$.

---

Next we observe that $\operatorname{Pr}(A) \leq 2 \cdot \operatorname{Pr}(B)$. This is because

$$
\operatorname{Pr}(B) \geq \operatorname{Pr}(A \text { and } B)=\operatorname{Pr}(A) \cdot \operatorname{Pr}(B / A)
$$

The probability of $B$ given $A, \operatorname{Pr}(B / A)$ is the probability that a hypothesis that has error $\epsilon$ on the distribution has error at least $\epsilon / 2$ on a set of $m$ examples. Using Chebychev's inequality, this latter probability is at least $1 / 2$.

---

To complete the proof we will bound the probability of $B$. Fix any set of $2 m$ examples and consider a random partition of them into two equal-sized sets $S_{1}$ and $S_{2}$. Let $\hat{h}$ be a hypothesis which disagrees with the target hypothesis on at least $\epsilon m / 2$ of the $2 m$ examples. This is a candidate for causing event $B$.

---

Let $X_{i}$, for $i=1, \ldots, m$ denote the event that $\hat{h}$ makes an error on the $i$ 'th example in $S_{1}$. Then $E\left(X_{i}\right)=\epsilon / 4$. Define

$$
X=\sum_{i=1}^{m} X_{i} .
$$

Then $E(X)=\epsilon m / 4$. By Chernoff's inequality,

$$
\operatorname{Pr}\left(X \leq \frac{\epsilon}{4}(1-c)\right) \leq e^{-\frac{\epsilon m c^{2}}{8}}
$$

That is,

$$
\operatorname{Pr}\left(X \leq \frac{\epsilon}{8}\right) \leq e^{-\epsilon m / 32} .
$$

The total number of distinct hypothesis for the set of $2 m$ examples is at most $C(2 m, k)$. In other words, this is the number of distinct ways to partition $2 m$ points using concepts from $\mathcal{C}$ in $\boldsymbol{R}^{k}$. Adding up over all the hypotheses, we get that

$$
\operatorname{Pr}(B) \leq C(2 m, k) e^{-\epsilon m / 32} .
$$

For the value of $m$ considered in the theorem, we have $\operatorname{Pr}(B)<\delta / 2$ and hence $\operatorname{Pr}(A)<\delta$ as required.

