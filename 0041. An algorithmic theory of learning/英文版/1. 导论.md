# An algorithmic theory of learning: Robust concepts and random projection 


#### Abstract

We study the phenomenon of cognitive learning from an algorithmic standpoint. How does the brain effectively learn concepts from a small number of examples despite the fact that each example contains a huge amount of information? We provide a novel algorithmic analysis via a model of robust concept learning (closely related to "margin classifiers"), and show that a relatively small number of examples are sufficient to learn rich concept classes. The new algorithms have several advantages-they are faster, conceptually simpler, and resistant to low levels of noise. For example, a robust half-space can be learned in linear time using only a constant number of training examples, regardless of the number of attributes. A general (algorithmic) consequence of the model, that "more robust concepts are easier to learn", is supported by a multitude of psychological studies.

## 1. Introduction

One motivation of computational learning theory is to gather insight into cognitive processes. The exact physical processes underlying learning, indeed any aspect of cognition, are far from being understood. Even from a purely theoretical standpoint, it is mostly a mystery as to how the brain copes with huge amounts of data. How does the brain effectively learn concepts from a relatively small number of examples, when each example consists of a huge amount of information?

---

There are at least two approaches to explaining this phenomenon. The first, due to Valiant, is attribute-efficient learning (Valiant, 1998; Littlestone, 1987, 1991). In this model, it is assumed that the target concept is simple in a specific manner: it is a function of only a small subset of the set of attributes, called the relevant attributes, while the rest are irrelevant. From this assumption one can typically argue that the VC-dimension of the resulting concept class is a function of only the number of relevant attributes $(k)$, and hence derive a bound on the number of examples required. Unfortunately, although the model is theoretically clean and appealing, it is not known how to learn anything more complex than a disjunction of variables (without membership queries). Further, it is NP-hard to learn a disjunction of $k$ variables as a disjunction of fewer than $k \log n$ variables (where $n$ is the total number of variables).

---

In this paper, we study a different approach based on a simple idea which is illustrated in the following example. Imagine a child learning the concept of an "elephant". We point the child to pictures of elephants or to real elephants a few times and say "elephant", and perhaps to a few examples of other animals and say their names (i.e., "not elephant"). From then on, the child will almost surely correctly label only elephants as elephants. On the other hand, imagine a child learning the concept of "African elephant" (as opposed to the Indian elephant) just from examples. It will probably take many more examples, and perhaps even be necessary to explicitly point out the bigger ears of the African elephant.

---

The crucial difference in the two concepts above is not in the number of attributes, or even in the number of relevant attributes of the examples presented, but in the similarity of examples with the same label and in the dissimilarity of examples with different labels. There is a clearer demarcation between elephants and non-elephants than there is between African elephants and Indian elephants. This notion will be formalized later as the robustness of a concept. An alternative perspective of robustness is that it is a measure of how much the attributes of an example can be altered without affecting the concept. The main feature of robust concepts is that the number of examples and the time required to learn a robust concept can be bounded as a function of the robustness (denoted by a parameter $\ell$ ), and do not depend on the total number of attributes. The model and the parameter $\ell$ are defined precisely in Section 2. As we discuss there, the model is very closely related to Large Margin classifiers studied in machine learning, that are in turn the basis for Support Vector Machines (Vapnik, 1995; Cortes \& Vapnik, 1995).

---

In the robust concept model, the main new observation is that we can employ a general procedure to reduce the dimensionality of examples, independent of the concept class. While reducing the dimensionality of examples, we would like to preserve concepts. So, for example, if our original concept class is the set of half-spaces (linear thresholds) in $n$-dimensional space, we would like to map examples to a $k$-dimensional space, where $k$ is much smaller than $n$, and maintain the property that some half-space in the $k$-dimensional space correctly classifies (most of) the examples. We show that Random Projection, the technique of projecting a set of points to a randomly chosen low-dimensional space, is suitable for this purpose. It has been observed that random projection (approximately) preserves key properties of a set of points, e.g., the distances between pairs of points (Johnson \& Lindenstrauss, 1984); this has led to efficient algorithms in several other contexts (Kleinberg, 1997; Linial, et al., 1994; Vempala, 2004). In Section 3, we develop "neuronal" versions of random projection, i.e., we demonstrate that it is easy to implement it using a single layer of perceptrons where the weights of the network are chosen independently and from any one of a class of distributions; this class includes discrete distributions such as the picking 1 or -1 with equal probability. Our theorems here can be viewed as extensions / refinements of the work of Johnson \& Lindenstrauss (1984) and Frankl and Maehara (1988).

---

Then we address the question of how many examples are needed to efficiently learn a concept with robustness $\ell$. We begin with the concept class of half-spaces with $n$ attributes. In this case, it is already known that one needs $O\left(1 / \ell^{2}\right)$ examples (Bartlett \& Shawe-Taylor, 1998; Vapnik, 1995; Freund \& Schapire, 1999). Here we show that a simple algorithm based on random projection gives an alternative proof of such a guarantee.

---

Next we consider other rich concept classes, namely intersections of half-spaces and ellipsoids. Using neuronal random projection, we demonstrate that the examples can first be projected down to a space whose dimension is a function of $\ell$, and in some cases an additional parameter of the concept class (e.g. the number of half-spaces when the concept class is intersections of half-spaces etc.), but does not depend on the number of attributes of the examples. This then allows us to bound the number of examples required to learn the concepts as a function of $\ell$, independent of the original number of attributes, via well-known generalization theorems based on the VC-dimension (Vapnik \& Chervonenkis, 1971).

---

The proposed algorithms are fast-their running time is linear in $n$ - since after random projection (which takes time linear in $n$ ), all the work happens in the smaller-dimensional space with a small number of sample points. Indeed, this suggests that the algorithms studied here could be used in SVM's in place of current solutions (Cortes \& Vapnik, 1995; Freund \& Schapire, 1999) such as quadratic optimization in a dual space called the kernel space.

---

In Section 4.4, we mention the noise tolerance properties of the algorithms, notably that agnostic learning is possible, and (equivalently) that it is possible to find hypotheses that minimize the number of misclassified points, for fairly low robustness.

### 1.1. Related work

The main contribution of this paper is a new perspective on learning via a connection to dimension reduction. This facilitates efficient algorithms which use small sample sizes. It also gives a simple intuitive way to see the $O\left(1 / \epsilon^{2}\right)$ sample complexity bounds of margin classifiers (SVM's) (Bartlett \& Shawe-Taylor, 1998). It is related to previous work (Schapire et al., 1998) which showed that generalization error can be bounded in terms of the observed margin of examples (a more refined notion of margin is used there, but is similar in spirit). As we discuss in Section 5.1, it seems to fit well with attempts to model cognition on a computational basis (Valiant, 1998), and predicts the commonly observed phenomenon that finer distinctions take more examples. From a purely computational viewpoint, these are simple new algorithms for fundamental learning theory problems, that might be practical.

---

There have been further applications of random projection in learning theory subsequent to this work. Garg, et al. (2002) and Garg and Roth (2003) have pursued similar ideas, developing the related notion of projection profile. Recently, Balcan, et al. (2004) have used random projection to give an efficient new interpretation of kernel functions. Klivans and Servedio (2004) have used polynomial threshold functions in the context of robust concepts to get substantially improved time bounds. Specifically, they give faster algorithms for learning intersections (and other functions) of $t$ half-spaces (with some increase in the sample complexity). Finally, Ben-David, et al. (2002) have used random projection to show an interesting lower bound on learning with half-spaces. They prove that "most" concept classes of even constant VC-dimension cannot be embedded into half-spaces where the dimension of the Euclidean space is small or the margin is large. Thus, algorithms based on first transforming to half-spaces cannot gain much in terms of the margin or the dimension.