## 4. Learning efficiently by reducing dimensionality

In this section, we describe learning algorithms for robust concepts and derive bounds on the number of examples required and the running times. Our bounds will be functions of the robustness parameter $l$, and the $\epsilon, \delta$ learning parameters, but will be independent of the actual number of attributes of the concept class.

---

We are given labelled examples from an unknown distribution $\mathcal{D}$. The generic algorithm for learning robust concepts is based on the following two high-level ideas:

1. Since the target concept is robust, random projection of the examples to a much lower-dimensional subspace will "preserve" the concept.
2. In the lower-dimensional space, the number of examples and the time required to learn concepts are relatively small.

---

Before applying this approach to specific concept classes, we recall some fundamental theorems in learning theory. For the concept class $\mathcal{C}$ under consideration, let $C(m, k)$ denote the maximum number of distinct labellings of $m$ points that can be obtained by using concepts from $\mathcal{C}$ in $\boldsymbol{R}^{k}$. The following well-known theorem (see Kearns \& Vazirani (1994) or Blumer et al. (1989)) gives a bound on the size of the sample so that a hypothesis that is consistent with the sample also has, with high probability, small error with respect to the entire distribution.

---

Theorem 3. Let $\mathcal{C}$ be any concept class in $\boldsymbol{R}^{k}$. Let $w$ be a concept from $\mathcal{C}$ that is consistent with $m$ labelled examples of some concept in $C$. Then with probability at least $1-\delta, w$ correctly classifies at least $(1-\epsilon)$ fraction of $\mathcal{D}$ provided

$$
m>\frac{4}{\epsilon} \log C(2 m, k)+\frac{4}{\epsilon} \log \frac{2}{\delta} .
$$

The notion of VC-dimension (Vapnik \& Chervonenkis, 1971) is closely connected to the number of distinct labelings as expressed in the following basic theorem.

Theorem 4 (Blumer et al. 1989). Let $C$ be a concept class of VC-dimension $d$. Then, the number of distinct labelings of $m$ points by concepts in $C$ is at most

$$
C[m] \leq \sum_{i=0}^{d}\binom{m}{i} .
$$

---

If the algorithm finds a hypothesis that is nearly consistent with the sample (rather than fully consistent as in the previous theorem), this too generalizes well. The number of samples required increases by a a constant factor. The theorem below is a slight variant of a similar theorem from Blumer et al. (1989). We give a self-contained proof in the appendix for the reader's convenience.

Theorem 5. For $\epsilon \leq 1 / 4$, let $w$ be a concept from $\mathcal{C}$ in $\boldsymbol{R}^{k}$ that correctly classifies at least $a(1-\epsilon / 8)$ fraction of a sample of $m$ points drawn from $\mathcal{D}$ such that

$$
m \geq \frac{32}{\epsilon} \log C(2 m, k)+\frac{32}{\epsilon} \log \frac{2}{\delta}
$$

Then with probability at least $1-\delta$, w correctly classifies at least a $1-\epsilon$ fraction of $\mathcal{D}$.

### 4.1. Half-spaces

We begin with the problem of learning a half-space in $\boldsymbol{R}^{n}$ (a linear threshold function). This is one of the oldest problems studied in learning theory. The problem can be solved in polynomial-time by using an algorithm for linear programming on a sample of $O(n)$ examples (note that this is not a strongly polynomial algorithm-its complexity depends only polynomially on the number of bits in the input). Typically, however, it is solved by using simple greedy methods. A commonly-used greedy algorithm is the Perceptron Algorithm (Agmon, 1954; Rosenblatt, 1962), which has the following guarantee: Given a collection of data points in $\boldsymbol{R}^{n}$, each labeled as positive or negative, the algorithm will find a vector $w$ such that $w \cdot x>0$ for all positive points $x$ and $w \cdot x<0$ for all negative points $x$, if such a vector exists. ${ }^{1}$ The running time of the algorithm depends on a separation parameter (described below). However, in order for the hypothesis to be reliable, we need to use a sample of $\Omega(n)$ points, since the VC-dimension of half-spaces in $\boldsymbol{R}^{n}$ is $n+1$.

---

Let $\mathcal{H}_{n}$ be the class of homogenous half-spaces in $\boldsymbol{R}^{n}$. Let $(h, \mathcal{D})$ be a concept-distribution pair such that the half-space $h \in \mathcal{H}_{n}$ is $\ell$-robust with respect to the distribution $\mathcal{D}$ over $\boldsymbol{R}^{n}$. We restrict $\mathcal{D}$ to be over the unit sphere (i.e., all the examples are at unit distance from the origin). The latter condition is not really a restriction since examples can be scaled to have unit length without changing their labels. The parameters $k$ and $m$ in the algorithm below will be specified later.

---

##### Half-space Algorithm:

1. Choose an $n \times k$ random matrix $R$ by picking each entry independently from $N(0,1)$ or $U(-1,1)$.
2. Obtain $m$ examples from $\mathcal{D}$ and project them to $\boldsymbol{R}^{k}$ using $R$.
3. Run the following Perceptron Algorithm in $\boldsymbol{R}^{k}$ : Let $w=0$. Perform the following operation until all examples are correctly classified:
   Pick an arbitrary misclassified example $x$ and let $w \leftarrow w+\operatorname{label}(x) x$.
4. $\quad$ Output $R$ and $w$.

A future example $x$ is labelled positive if $w \cdot\left(R^{T} x\right) \geq 0$ and negative otherwise. This is of course the same as checking if $\left(w R^{T}\right) \cdot x>0$, i.e., a half-space in the original $n$-dimensional space.

---

We can assume that $h$, the normal vector to the concept half-space, is of unit length. The idea behind the algorithm is that when $k$ is large enough, in the $k$-dimensional subspace obtained by projection, the half-space through the origin defined by $R^{T} h$, i.e., $\left(R^{T} h\right) \cdot y \geq 0$, classifies most of the projected distribution correctly. We will show that in fact this halfspace remains robust with respect to a projected sample of sufficiently large size. To find a consistent half-space, we use the classical perceptron algorithm. It is well-known (see Minsky \& Papert (1969)) that the convergence of this algorithm depends on the margin, i.e., in our terminology, the robustness of the target half-space.

---

Theorem 6. (Minsky \& Papert, 1969) Suppose the data set S can be correctly classified by some unit vector $w$. Then, the Perceptron Algorithm converges in at most $1 / \sigma^{2}$ iterations, where

$$
\sigma=\min _{x \in S} \frac{|w \cdot x|}{\|x\|} .
$$

For an $\ell$-robust half-space, we have $\sigma \geq \ell$. The theorem says that the perceptron algorithm will find a consistent half-space in at most $1 / \ell^{2}$ iterations. We can now state and prove the main result of this section.

---

Theorem 7. An $\ell$-robust half-space in $\boldsymbol{R}^{n}$ can be $(\epsilon, \delta)$-learned by projecting a set of $m$ examples to $\boldsymbol{R}^{k}$ where

$$
k=\frac{100}{\ell^{2}} \ln \frac{100}{\epsilon \ell \delta}, \quad m=\frac{8 k}{\epsilon} \log \frac{48}{\epsilon}+\frac{4}{\epsilon} \log \frac{4}{\delta}=O\left(\frac{1}{\ell^{2}} \cdot \frac{1}{\epsilon} \cdot \ln \frac{1}{\epsilon} \ln \frac{1}{\epsilon \ell \delta}\right)
$$

in time $n \cdot \operatorname{poly}\left(\frac{1}{\ell}, \frac{1}{\epsilon}, \log \frac{1}{\delta}\right)$ time.
Proof: For an example point $x$, we let $x^{\prime}$ denote its projection. We let $h^{\prime}$ denote the projection of $h$, the normal to the target half-space. We would like the following events to occur (by the choice of the projection matrix $R$ ):

1. For each example $x$, its projection $x^{\prime}$ has length at most $1+\frac{\ell}{2}$. Similarly, $\left\|h^{\prime}\right\| \leq 1+$ $\frac{\ell}{2}$.
2. For each example $x$, if $h \cdot x \geq \ell$, then $h^{\prime} \cdot x \geq \frac{\ell}{2}$; if $h \cdot x \leq-\ell$, then $h^{\prime} \cdot x^{\prime} \leq-\frac{\ell}{2}$.

We now bound the probability that one of these events does not occur. For any single example $x$, applying Corollary 2 with $\epsilon=\ell / 2$ and our choice of $k$, the probability that $\left\|x^{\prime}\right\|>1+\frac{\ell}{2}$ is at most

$$
e^{-\left(\frac{\ell^{2}}{4}-\frac{\ell^{3}}{8}\right) \frac{k}{4}} \leq e^{-\frac{\ell^{2} k}{32}} \leq\left(\frac{\epsilon \ell \delta}{100}\right)^{\frac{100}{32}}<\frac{\delta}{4(m+1)} .
$$

---

Adding this up over all the $m$ examples and the vector $h$, we get a failure probability of at most $\delta / 4$.

Next, by Corollary 2 , with $u=h$ and $v=x$, the probability that the second event does not occur for any particular example $x$ is at most $\delta / 4 m$. Again this contributes a total failure probability of at most $\delta / 4$. Thus, both events occur with probability at least $1-\delta / 2$.

These events imply that the half-space in $\boldsymbol{R}^{k}$ defined by $h^{\prime}$ correctly classifies all the $m$ examples after projection (with probability at least $1-\delta / 2$ ). Moreover, after scaling the examples to have length at most 1 , the margin is at least

$$
\sigma \geq \frac{\ell / 2}{1+\frac{\ell}{2}} \geq \frac{\ell}{3} .
$$

Now, by Theorem 6, the perceptron algorithm will find a consistent half-space in $9 / \ell^{2}$ iterations.

---

Finally, we need to show that $m$ is large enough that hypothesis found generalizes well. We will apply Theorem 3 to half-spaces through the origin in $\boldsymbol{R}^{k}$. The VC-dimension of the latter concept class is $k$ and so, by Theorem 4, we get the following well-known bound on the number of distinct half-spaces (see e.g. Kearns \& Vazirani (1994)):
$$
\begin{equation*}
C(2 m, k) \leq \sum_{i=0}^{k-1}\binom{2 m}{i} \leq\left(\frac{2 e m}{k}\right)^{k} . \tag{1}
\end{equation*}
$$

Our choice of $m$ satisfies

$$
m=\frac{8 k}{\epsilon} \log \frac{48}{\epsilon}+\frac{4}{\epsilon} \log \frac{4}{\delta}>\frac{4}{\epsilon} \log C(2 m, k)+\frac{4}{\epsilon} \log \frac{4}{\delta} .
$$

Therefore, applying Theorem 3 with $\delta / 2$ in place of $\delta$, the half-space found by the algorithm correctly classifies at least $1-\epsilon$ of the original distribution with probability at least $1-\delta / 2$. This gives an overall success probability of at least $1-\delta$.

The perceptron algorithm and its variants are known to be resistant to various types of random classification noise (Bylander, 1994; Blum et al., 1996). It is a straightforward consequence that these properties continue to hold for the algorithm described here. In the concluding section, we discuss straightforward bounds for agnostic learning.

### 4.2. Intersections of half-spaces

The next problem we consider is learning an intersection of $t$ half-spaces in $\boldsymbol{R}^{n}$, i.e., the positive examples all lie in the intersection of $t$ half-spaces and the negative examples lie outside that region. It is not known how to solve the problem for an arbitrary distribution. However efficient algorithms have been developed for reasonably general distributions assuming that the number of half-spaces is relatively small (Blum \& Kannan, 1993; Vempala, 2004). Here, we derive efficient learning algorithms for robust concepts in this class.

---

We assume that all the half-spaces are homogenous. Let the concept class of intersections of half-spaces be denoted by $\mathcal{H}(t, n)$. A single concept in this class is specified by a set of $t$ half-spaces $P=\left\{h_{1}, \ldots, h_{t}\right\}$, and the positive examples are precisely those that satisfy $h_{i} \cdot x \geq 0$ for $i=1 \ldots t$. Let $(P, \mathcal{D})$ be a concept-distribution pair such that $P$ is $\ell$-robust with respect to the distribution $\mathcal{D}$. We assume that the support $\mathcal{D}$ is a subset of the unit sphere (and remind the reader that this as well as homogeneity are not really restrictive, as they can be achieved by scaling and adding an extra dimension, respectively; see e.g. (Vempala, 2004)).

---

Let denote $C(m, t, k)$ denote the maximum number of distinct labellings of $m$ examples in $R^{k}$ using concepts from $\mathcal{H}(t, k)$. Then,

$$
\begin{equation*}
C(2 m, t, k) \leq\left(\sum_{i=0}^{k-1}\binom{2 m}{i}\right)^{t} \leq\left(\frac{2 e m}{k}\right)^{t k} . \tag{2}
\end{equation*}
$$

This can be seen as follows: For $t=1$, this is just (1), the number of ways to assign + or -1 to $2 m$ points using a half-space. If we give each point $t$ labels, one for each of $t$ halfspaces, then the total number of possible labellings is the middle term in (2). We consider two labellings distinct iff the subset of points that are labelled + by all $t$ half-spaces is different. Thus the total number of distinct labellings by $t$ half-spaces can only be smaller than this bound.

---

Given $m$ examples, we can always find a consistent hypothesis (if one exists) using a brute-force algorithm that enumerates all the combinatorially distinct half-spaces and pick $t$ of them (with replacement). We apply this to learning a robust intersection of $t$-half-spaces after projecting a sufficiently large sample to a lower-dimensional subspace. The parameters $k$ and $m$ below will be specified shortly.

---

##### $t$-Half-spaces Algorithm:

1. Choose an $n \times k$ random matrix $R$ for projection by choosing each entry independently from $N(0,1)$ or $U(-1,1)$.
2. Obtain $m$ examples from $\mathcal{D}$ and project them to $\boldsymbol{R}^{k}$ using $R$.
3. Find a hypothesis $Q=\left\{w_{1}, \ldots, w_{t}\right\}$ where each $w_{i} \in \boldsymbol{R}^{k}$ such that the intersection of the half-spaces $w_{i} \cdot x \geq 0$ for $i=1, \ldots, t$ is consistent with the labels of the projected examples.
4. Output $R$ and $Q$.

A future example $x$ is projected down as $R^{T} x$ and labelled according to $Q$, i.e., it is positive if $w_{i} \cdot\left(R^{T} x\right) \geq 0$ for all $i=1, \ldots, t$.

---

**Theorem 8.** An $\ell$-robust intersection of t half-spaces in $\boldsymbol{R}^{n}$ can be $(\epsilon, \delta)$-learned by projecting $m$ examples to $\boldsymbol{R}^{k}$ where
$$
k=\frac{100}{\ell^{2}} \ln \frac{100 t}{\epsilon \ell \delta} \quad \text { and } \quad m=\frac{8 k t}{\epsilon} \log \frac{48 t}{\epsilon}+\frac{4}{\epsilon} \log \frac{4}{\delta}=O\left(\frac{t}{\epsilon \ell^{2}} \log \frac{t}{\epsilon} \log \frac{t}{\ell \epsilon \delta}\right)
$$

in time $O(n m k)+\left(\frac{48 t}{\epsilon} \log \frac{4 t}{\epsilon \delta}\right)^{k t}$.

**Proof:** The proof is similar to that of Theorem 7 and we only sketch it.

---

Let the original set of half-spaces be $h_{1} \cdot x \geq 0, \ldots, h_{t} \cdot x \geq 0$, where each $h_{i}$ is a unit vector in $\boldsymbol{R}^{n}$. We consider the projections of these, $h_{i}^{\prime}=\frac{1}{\sqrt{k}} R^{T} h_{i}$, and the following events: For each example $x$ and normal vector $h_{i}$, if $h_{i} \cdot x \geq \ell$, then $h_{i}^{\prime} \cdot x^{\prime}>0$; If $h_{i} \cdot x \leq-\ell$, then $h_{i}^{\prime} \cdot x^{\prime}<0$.

For our choice of $k$ and $m$, it follows from Corollary 2 that these events all happen with probability at least $1-\delta / 2$. Therefore, after projection, with this probability, there is a hypothesis from $\mathcal{H}(t, k)$ that is consistent with all $m$ examples. Using Theorem 3 along with (2), it follows that any hypothesis consistent with a sample of size

$$
m=\frac{8 k t}{\epsilon} \log \frac{2 t}{\epsilon}+\frac{4}{\epsilon} \log \frac{4}{\delta}
$$

will correctly classify $(1-\epsilon)$ of the distribution with probability at least $1-\delta / 2$. This gives an overall success probability of at least $1-\delta$. The running time of the enumerative algorithm is $O\left((2 e m / k)^{k t}\right)$.

---

If $t, \ell, \epsilon, \delta$ are all constant, then the algorithm runs in linear time. If only $\ell, \epsilon, \delta$ are constant, then the algorithm has running time $O\left(n t \log ^{3} t\right)+(t \log t)^{O(t \log t)}$. This is significantly faster than the best-known algorithms for the general case (see Section 1.1 for recent improvements). Both results do not need any further assumptions on the distribution $\mathcal{D}$ besides robustness. Previous algorithms for the problem assumed that $\mathcal{D}$ was either symmetric (Baum, 1990), uniform (Blum \& Kannan, 1993) or non-concentrated (Vempala, 1997). Recently, an improved time complexity for learning robust intersections of half-spaces was obtained in Klivans and Servedio (2004) using an algorithm for learning polynomial threshold functions in the projected space in place of the enumerative algorithm used here. The improvement in the time complexity comes along with a substantial increase in the sample complexity.

### 4.3. Balls

Finally, we briefly discuss the concept class of balls in $\boldsymbol{R}^{n}$, illustrating how robustness plays a role in learning nonlinear concepts.

A Ball $B\left(x_{0}, r\right)$ in $\boldsymbol{R}^{n}$ is defined as

$$
B\left(x_{0}, r\right)=\left\{x \in \boldsymbol{R}^{n}:\left\|x-x_{0}\right\| \leq r\right\}
$$

where $x_{0}$ (the center) is a fixed point in $\Re^{n}$ and $r$ (the radius) is a fixed real value. The set of points in $B\left(x_{0}, r\right)$ are labelled positive and those outside are labelled negative.

It is well-known that the VC-dimension of balls in $\boldsymbol{R}^{n}$ is $n+1$ and so the number of examples required to $(\epsilon, \delta)$-learn a ball is $O\left(\frac{n}{\epsilon} \log \frac{1}{\epsilon}+\frac{1}{\epsilon} \log \frac{1}{\delta}\right)$. How many examples do we need to learn an $\ell$-robust ball? The following theorem follows easily from the neuronal projection theorem.

---

Theorem 9. An $\ell$-robust ball of radius in $\boldsymbol{R}^{n}$ of radius at most 1 can be $(\epsilon, \delta)$-learned by projecting $m$ examples to $\boldsymbol{R}^{k}$ where

$$
k=\frac{100}{\ell^{2}} \ln \frac{100}{\epsilon \ell \delta} \quad \text { and } \quad m=\frac{8 k}{\epsilon} \log \frac{48}{\epsilon}+\frac{4}{\epsilon} \log \frac{4}{\delta}
$$

and then finding a ball in $\boldsymbol{R}^{k}$ consistent with the projected examples.
Proof: With probability 1, any positive example $x$ drawn from the distribution $\mathcal{D}$ will satisfy

$$
\left\|x-x_{0}\right\| \leq r-l
$$

while any negative example $x$ will satisfy

$$
\left\|x-x_{0}\right\| \geq r+l .
$$

Using Theorem 2 with our choice of $k$ and $\epsilon=\ell / 2$, for any one $x$, its projection $x^{\prime}$ satisfies

$$
\left(1-\frac{\ell}{2}\right)\left\|x-x_{0}\right\| \leq\left\|x^{\prime}-x_{0}^{\prime}\right\| \leq\left(1+\frac{\ell}{2}\right)\left\|x-x_{0}\right\|
$$

with probability at least $1-\frac{\delta}{2 m}$. So, with probability $1-\delta / 2$, all the projected examples satisfy the above inequality. Further, since the radius of the concept ball is at most 1 ,

$$
\left\|x-x_{0}\right\|+\frac{\ell}{2} \leq\left\|x^{\prime}-x_{0}^{\prime}\right\| \leq\left\|x-x_{0}\right\|+\frac{\ell}{2} .
$$

Thus, the ball $B\left(x_{0}^{\prime}, r\right)$ in $\boldsymbol{R}^{k}$ is consistent with the projected examples and the theorem follows. Finally, we can use Theorem 3 to verify that $m$ is large enough for this to be an $(\epsilon, \delta)$-learning algorithm.

### 4.4. Noise tolerance

Here we note that the algorithms can be adapted to be resistant to malicious classification noise (agnostic learning). In a sample of $s$ examples, let the labels of at most $\gamma s$ of them be corrupted arbitrarily. Fix a hypothesis class $H$ and let $f(\ell)$ be the bound on the number of examples required to learn concepts with robustness $\ell$. Then to deal with this noise "rate" $\gamma$, we obtain $f(\ell) /(1-\gamma)$ examples, and for every subset of size $f(\ell)$ of the sample, we run the learning algorithm for the hypothesis class and output a hypothesis that correctly classifies the subset. The total number of runs of the algorithms is at most $2^{2 f(\ell)}$. So, for example, half-spaces in $\boldsymbol{R}^{n}$ can be learned in poly(n) time for robustness as low as $\sqrt{\frac{\log \log n}{\log n}}$. Another way to interpret this is that we can find hypothesis that minimize the number of mistakes. This was observed by Avrim Blum.