## 3. The main tool: "neuron-friendly" random projection

In this section we develop "neuronal" versions of random projection, including a discrete version, and provide probabilistic guarantees for them, all with transparent proofs. Besides being neuron-friendly, these versions of random projection are easier to implement.

---

To project a given point $u \in \boldsymbol{R}^{n}$ to a $k$-dimensional space, we first choose $k$ random vectors $R_{1}, \ldots, R_{k}$ (we will shortly discuss suitable probability distributions for these vectors). Then we compute a $k$-dimensional vector $u^{\prime}$ whose coordinates are the inner products $u_{1}^{\prime}=$ $R_{1}^{T} \cdot u, \ldots, u_{k}^{\prime}=R_{k}^{T} \cdot u$. If we let $R$ be the $n \times k$ matrix whose columns are the vectors $R_{1}, \ldots, R_{k}$, then the projection can be succinctly written as $u^{\prime}=R^{T} u$. To project a set of points $u^{1}, \ldots, u^{m}$ in $\boldsymbol{R}^{n}$ to $\boldsymbol{R}^{k}$, we choose a random matrix $R$ as above, and compute the vectors $R^{T} u^{1}, \ldots, R^{T} u^{m}$.

---

Given the matrix $R$, the above procedure is a simple computational task. It has been shown that if $R$ is a random orthonormal matrix, i.e., the columns of $R$ are random unit vectors and they are pairwise orthogonal, then the projection preserves all pairwise distances to within a factor of $(1+\epsilon)$ for a surprisingly small value of $k$ of about $\log n / \epsilon^{2}$ (Johnson \& Lindenstrauss, 1984). The main observation of this section is to show that this is a rather robust phenomenon, in that the entries of $R$ can be chosen from any distribution with bounded moments. In particular it suffices to use random matrices with independent entries chosen from a distribution with bounded support. It is then an easy consequence that the task of random projection can be achieved by a simple 1-layer neural network, viz., $k$ perceptrons (which compute linear combinations of their inputs) each with one output and the same $n$ inputs. The weights of the neural network are assumed to be random and independent. This is illustrated in Fig. 1. Let $r \in \boldsymbol{R}^{n}$ be a random vector whose coordinates are independent and identically distributed. We highlight the following two possibilities for the distribution of the coordinates: (a) the standard normal distribution, with mean 0 and variance 1 , referred to as $N(0,1)$, (b) the discrete distribution defined by $r_{i}=1$ with probability $\frac{1}{2}$ and $r_{i}=-1$ with probability $\frac{1}{2}$, which we will refer to as $U(-1,1)$. Following the conference version of this paper (Arriaga \& Vempala, 1999), another proof for the case $U(-1,1)$ has also appeared (Achlioptas, 2001). The following well-known lemma will be useful. We provide a proof for convenience.

---

Lemma 1. Let $X$ be drawn from $N(0, \sigma)$, the normal distribution with mean zero and standard deviation $\sigma$. Then for any $\alpha<\frac{1}{2 \sigma^{2}}$,

$$
\mathrm{E}\left(e^{\alpha X^{2}}\right)=\frac{1}{\sqrt{1-2 \alpha \sigma^{2}}}
$$

**Proof:** We recall the density function of $N(0, \sigma)$, the normal distribution with mean 0 and standard deviation $\sigma$, to be
$$
\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{x^{2}}{2 \sigma^{2}}}
$$

Using this,

$$
\begin{aligned}
\mathrm{E}\left(e^{\alpha X^{2}}\right) & =\int_{-\infty}^{\infty} e^{\alpha x^{2}} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{x^{2}}{2 \sigma^{2}}} d x \\
& =\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{x^{2}}{2 \sigma^{2}}\left(1-2 \alpha \sigma^{2}\right)} d x \\
& =\frac{1}{\sqrt{1-2 \alpha \sigma^{2}}} \int_{-\infty}^{\infty} \frac{\sqrt{1-2 \alpha \sigma^{2}}}{\sqrt{2 \pi} \sigma} e^{-\frac{x^{2}}{2 \sigma^{2}}\left(1-2 \alpha \sigma^{2}\right)} d x \\
& =\frac{1}{\sqrt{1-2 \alpha \sigma^{2}}}
\end{aligned}
$$

Here we have used the observation that the integrand is the normal density with standard deviation $\sigma / \sqrt{1-2 \alpha \sigma^{2}}$.

---

We begin with the case when each entry of the projection matrix is chosen independently from the standard Normal distribution.

**Lemma 2.** Let $R=\left(r_{i j}\right)$ be a random $n \times k$ matrix, such that each entry $r_{i j}$ is chosen independently according to $N(0,1)$. For any vector fixed $u \in \boldsymbol{R}^{n}$, and any $\epsilon>0$, let $u^{\prime}=$ $\frac{1}{\sqrt{k}}\left(R^{T} u\right)$. Then, $\mathrm{E}\left(\left\|u^{\prime}\right\|^{2}\right)=\|u\|^{2}$ and
$$
\begin{gathered}
\operatorname{Pr}\left[\left\|u^{\prime}\right\|^{2}>(1+\epsilon)\|u\|^{2}\right] \leq\left((1+\epsilon) e^{-\epsilon}\right)^{k} \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} \\
\operatorname{Pr}\left[\left\|u^{\prime}\right\|^{2}<(1-\epsilon)\|u\|^{2}\right] \leq\left((1-\epsilon) e^{\epsilon}\right)^{k} \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}}
\end{gathered}
$$

**Proof:** The expectation follows from a simple calculation. To obtain the bound on the concentration near the mean, let $X_{j}=\left(R_{j}^{T} \cdot u\right) /\|u\|$ and observe that
$$
X=\sum_{j=1}^{k} X_{j}^{2}=\sum_{j=1}^{k} \frac{\left(R_{j}^{T} \cdot u\right)^{2}}{\|u\|^{2}}
$$

where $R_{j}$ denotes the $j$ th column of $R$. Each $X_{j}$ has the standard normal distribution (since each component of $R_{j}$ does). Also note that

$$
\left\|u^{\prime}\right\|^{2}=\frac{\|u\|^{2}}{k} X .
$$

Using Markov's inequality, we can then estimate the desired probability as

$$
\begin{aligned}
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2} \geq(1+\epsilon)\|u\|^{2}\right)=\operatorname{Pr}(X \geq(1+\epsilon) k) & =\operatorname{Pr}\left(e^{\alpha X} \geq e^{(1+\epsilon) k \alpha}\right) \\
& \leq \frac{\mathrm{E}\left(e^{\alpha X}\right)}{e^{(1+\epsilon) k \alpha}} \\
& =\frac{\Pi_{j=1}^{k} \mathrm{E}\left(e^{\alpha X_{j}^{2}}\right)}{e^{(1+\epsilon) k \alpha}}=\left(\frac{\mathrm{E}\left(e^{\alpha X_{1}^{2}}\right)}{e^{(1+\epsilon) \alpha}}\right)^{k} .
\end{aligned}
$$

In the last line above, we have used the independence of the $X_{j}$ 's.

---

Similarly,
$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2} \leq(1-\epsilon)\|u\|^{2}\right) \leq\left(\frac{\mathrm{E}\left(e^{-\alpha X_{1}^{2}}\right)}{e^{-(1-\epsilon) \alpha}}\right)^{k} .
$$

â€žFrom Lemma 1,

$$
\mathrm{E}\left(e^{\alpha X_{1}^{2}}\right)=\frac{1}{\sqrt{1-2 \alpha}}
$$

for any $\alpha<\frac{1}{2}$. Thus we get,

$$
\operatorname{Pr}(X \geq(1+\epsilon) k) \leq\left(\frac{e^{-2(1+\epsilon) \alpha}}{(1-2 \alpha)}\right)^{\frac{k}{2}} .
$$

The optimal choice of $\alpha$ is $\epsilon / 2(1+\epsilon)$. With this,

$$
\operatorname{Pr}(X \geq(1+\epsilon) k) \leq\left((1+\epsilon) e^{-\epsilon}\right)^{\frac{k}{2}} \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

Similarly,

$$
\operatorname{Pr}(X \leq(1-\epsilon) k) \leq\left(\frac{e^{2(1-\epsilon) \alpha}}{(1+2 \alpha)}\right)^{\frac{k}{2}} \leq\left((1-\epsilon) e^{\epsilon}\right)^{\frac{k}{2}} \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

---

The main theorem of this section shows that this phenomenon is not specific to the Normal distribution. In the statement below, the condition that $\mathrm{E}\left(r^{2}\right)=1$ is for convenience. Instead one could have an arbitrary finite value $\sigma^{2}$ for this expectation, and scale the projection by $\sigma$.

**Theorem 1.** Let $R$ be a random $n \times k$ matrix, with each entry $r$ chosen independently from a distribution $\mathcal{D}$ that is symmetric about the origin with $\mathrm{E}\left(r^{2}\right)=1$. For any fixed vector $u \in \boldsymbol{R}^{n}$, let $u^{\prime}=\frac{1}{\sqrt{k}} R^{T} u$.

1. Suppose $B=\mathrm{E}\left(r^{4}\right)<\infty$. Then for any $\epsilon>0$,

$$
\mathrm{P}\left(\left[\left\|u^{\prime}\right\|^{2} \leq(1-\epsilon)\|u\|^{2}\right)\right] \leq e^{-\frac{\left(\epsilon^{2}-\epsilon^{3}\right) k}{2(B+1)}} .
$$

2. Suppose $\exists L>0$ such that for any integer $m>0, \mathrm{E}\left(r^{2 m}\right) \leq \frac{(2 m)!}{2^{m} m!} L^{2 m}$. Then for any $\epsilon>0$,

$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2} \geq(1+\epsilon) L^{2}\|u\|^{2}\right) \leq\left((1+\epsilon) e^{-\epsilon}\right)^{k / 2} \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

**Proof:** Without loss of generality, assume that $\|u\|^{2}=1$. Let
$$
X_{i}=R_{i}^{T} u \quad \text { for } i=1, \ldots, k
$$

We have

$$
\mathrm{E}\left(X_{i}^{2}\right)=\mathrm{E}\left(\left(R_{i}^{T} u\right)^{2}\right)=\mathrm{E}\left(\left(\sum_{j=1}^{n} R_{i j} u_{j}\right)^{2}\right)=\sum_{j=1}^{n} \mathrm{E}\left(R_{i j}^{2}\right) u_{j}^{2}=1
$$

Then, if we define $Y$ as follows

$$
Y:=\sum_{i=1}^{k} X_{i}^{2}=k\left\|u^{\prime}\right\|^{2}, \quad \mathrm{E}(Y)=\sum_{i=1}^{k} \mathrm{E}\left(X_{i}^{2}\right)=k
$$

---

The deviation below the mean is relatively easy to bound, using the independence of the $X_{i}$ 's and Markov's inequality.
$$
\begin{aligned}
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2}<(1-\epsilon)\|u\|^{2}\right) & =\mathrm{P}(Y<(1-\epsilon) k) \\
& =\mathrm{P}\left(e^{-\alpha Y}>e^{-\alpha(1-\epsilon) k}\right) \\
& \leq \frac{\mathrm{E}\left(e^{-\alpha Y}\right)}{e^{-\alpha(1-\epsilon) k}} \\
& =\left(\mathrm{E}\left(e^{-\alpha X_{1}^{2}}\right) e^{\alpha(1-\epsilon)}\right)^{k}
\end{aligned}
$$

and, using that $e^{-\alpha X_{1}^{2}} \leq 1-\alpha X_{1}^{2}+\alpha^{2} X_{1}^{4} / 2$, we get

$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2}<(1-\epsilon)\|u\|^{2}\right) \leq\left(\left(1-\alpha \mathrm{E}\left(X_{1}^{2}\right)+\frac{\alpha^{2}}{2} \mathrm{E}\left(X_{1}^{4}\right)\right) e^{\alpha(1-\epsilon)}\right)^{k} .
$$

We can evaluate the moments easily: $\mathrm{E}\left(X_{1}^{2}\right)=1$ and, if we observe that the expectation of odd powers of $r$ is zero because of symmetry, we have (using the fact that $B \geq 1$ ),

$$
\begin{aligned}
\mathrm{E}\left(X_{1}^{4}\right) & =\mathrm{E}\left(\left(\sum_{j=1}^{n} R_{1 j} u_{j}\right)^{4}\right) \\
& =\sum_{j_{1}, j_{2}, j_{3}, j_{4}=1}^{n} \mathrm{E}\left(R_{1 j_{1}} R_{1 j_{2}} R_{1 j_{3}} R_{1 j_{4}}\right) u_{j_{1}} u_{j_{2}} u_{j_{3}} u_{j_{4}} \\
& =\sum_{j=1}^{n} \mathrm{E}\left(R_{1 j}^{4}\right) u_{j}^{4}+3 \sum_{j_{1} \neq j_{2}, j_{1}, j_{2}=1}^{n} \mathrm{E}\left(R_{1 j_{1}}^{2} R_{1 j_{2}}^{2}\right) u_{j_{1}}^{2} u_{j_{2}}^{2} \\
& \leq B \sum_{j=1}^{n} u_{j}^{4}+3 \sum_{j_{1} \neq j_{2}, j_{1}, j_{2}=1}^{n} u_{j_{1}}^{2} u_{j_{2}}^{2} \\
& \leq(B+2)\left(\sum_{j} u_{j}^{2}\right)^{2} \\
& =B+2 .
\end{aligned}
$$

Therefore, using the Taylor expansion of $e^{x}$, (in particular, $e^{-x+x^{2 / 2}} \geq 1-x$ for $x \geq 0$ and small enough).

$$
\begin{aligned}
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2}<(1-\epsilon)\|u\|^{2}\right) & \leq\left(\left(1-\alpha+\frac{\alpha^{2}}{2}(B+2)\right) e^{\alpha(1-\epsilon)}\right)^{k} \\
& \leq\left(e^{-\alpha+\frac{\alpha^{2}(B+2)}{2}-\frac{1}{2}\left(\alpha-\frac{\alpha^{2}(B+2)}{2}\right)^{2}} e^{\alpha(1-\epsilon)}\right)^{k} \\
& \leq e^{-\frac{\left.\left(\epsilon^{2}-\epsilon^{3}\right)\right]}{2(B+1)}} .
\end{aligned}
$$

The last line above is obtained by setting $\alpha=\epsilon /(B+1)$ and noting that $B \geq 1$.

---

Similarly, for the deviation above the mean,
$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2}>(1+\epsilon) L^{2}\|u\|^{2}\right) \leq\left(\frac{\mathrm{E}\left(e^{\alpha X_{1}^{2}}\right)}{e^{\alpha L^{2}(1+\epsilon)}}\right)^{k} .
$$

The main task is bounding $\mathrm{E}\left(e^{\alpha X_{1}^{2}}\right)$ from above using the assumptions of the theorem. This expectation is hard to evaluate directly since we don't know the distribution explicitly. However we have bounds on all the moments of $X_{1}^{2}$. Therefore, if we define a random variable $Z$ whose moments are all at least the moments of $X_{1}^{2}$, then $\mathrm{E}\left(e^{\alpha Z}\right)$ will be an upper bound on the required expectation. The following claim will be useful.

---

Claim 1. Let $f, g$ be distributions on $\boldsymbol{R}$ that are symmetric about the origin with the property that for any nonnegative integer $m, \mathrm{E}\left(Y^{2 m}\right) \leq \mathrm{E}\left(Z^{2 m}\right)$ where $Y, Z$ are drawn from $f, g$ respectively. Let $Y_{1}, \ldots, Y_{n}$ be i.i.d. from $f, Z_{1}, \ldots, Z_{n}$ be i.i.d from $g$. Then for any $u \in \boldsymbol{R}^{n}$, the random variables $\hat{Y}=\sum_{j=1}^{n} u_{j} Y_{j}$ and $\hat{Z}=\sum_{j=1}^{n} u_{j} Z_{j}$ satisfy $\mathrm{E}\left((\hat{Y})^{2 m}\right) \leq \mathrm{E}\left((\hat{Z})^{2 m}\right)$ for every nonnegative integer $m$.

---

The claim is easy to prove. Compare the expectations of individual terms of $(\hat{Y})^{2 m}$ and $(\hat{Z})^{2 m}$. Since $Y_{i}, Z_{i}$ are symmetric about the origin, all terms in which they appear with an odd power have an expectation of zero. For any term in which all powers are even, by the assumption, the term from $E\left((\hat{Z})^{2 m}\right)$ dominates.

---

To apply this to our setting, we know that

$$
X_{1}=\sum_{j=1}^{n} u_{j} r_{j}
$$

where each $r_{j}$ is drawn from the given distribution $\mathcal{D}$. Define

$$
Y_{1}=\sum_{j=1}^{n} u_{j} r_{j}^{\prime}
$$

where each $r_{j}^{\prime}$ is drawn from $N(0, L)$. Then for all $j$, and any integer $m>0$,

$$
\mathrm{E}\left(r_{j}^{2 m}\right) \leq \frac{(2 m)!}{2^{m} m!} L^{2 m}=\mathrm{E}\left(\left(r_{j}^{\prime}\right)^{2 m}\right)
$$

using the well-known formula for the moments of $N(0, L)$. So, $\mathrm{E}\left(X_{1}^{2 m}\right) \leq \mathrm{E}\left(Y_{1}^{2 m}\right)$. Moreover, the distribution of $Y_{1}$ is $N(0, L)$. Therefore,

$$
\mathrm{E}\left(e^{\alpha X_{1}^{2}}\right) \leq \mathrm{E}\left(e^{\alpha Y_{1}^{2}}\right)=\frac{1}{\sqrt{1-2 \alpha L^{2}}} .
$$

Using this,

$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2}>(1+\epsilon) L^{2}\|u\|^{2}\right) \leq\left(\frac{e^{-2 \alpha L^{2}(1+\epsilon)}}{1-2 \alpha L^{2}}\right)^{\frac{k}{2}} .
$$

The optimal choice of $\alpha$ is $\epsilon / 2 L^{2}(1+\epsilon)$, and we get that for any $\epsilon>0$,

$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2}>(1+\epsilon) L^{2}\|u\|^{2}\right) \leq\left((1+\epsilon) e^{-\epsilon}\right)^{\frac{k}{2}} \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

The last inequality was obtained by using the inequality $\ln (1+\epsilon) \leq \epsilon-\epsilon^{2} / 2+\epsilon^{3} / 2$.

---

**Corollary 1.** If every entry of an $n \times k$ matrix $R$ is chosen according to $U(-1,1)$, then for any fixed vector $u \in \boldsymbol{R}^{n}$ and any $\epsilon>0$, the vector $u^{\prime}=\frac{1}{\sqrt{k}} R^{T}$ u satisfies
$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2} \geq(1+\epsilon)\|u\|^{2}\right) \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} \text { and } \mathrm{P}\left(\left\|u^{\prime}\right\|^{2} \leq(1-\epsilon)\|u\|^{2}\right) \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

**Proof:** For $r$ drawn from $U(-1,1), \mathrm{E}\left(r^{2 m}\right)=1$ for any integer $m>0$. Therefore, we can apply Theorem 1 with $L=B=1$ to get the conclusion of the corollary.

---

Let $R$ be an $n \times k$ matrix whose entries are chosen independently from either $N(0,1)$ or $U(-1,1)$, independently. The following theorem summarizes the results of this section. Alternative proofs for the case of $N(0,1)$ appeared in Indyk and Motwani (1998) and DG.

**Theorem 2 (Neuronal RP)**. Let $u, v \in \boldsymbol{R}^{n}$. Let $u^{\prime}$ and $v^{\prime}$ be the projections of $u$ and $v$ to $\boldsymbol{R}^{k}$ via a random matrix $R$ whose entries are chosen independently from either $N(0,1)$ or $U(-1,1)$. Then,
$$
\mathrm{P}\left[(1-\epsilon)\|u-v\|^{2} \leq\left\|u^{\prime}-v^{\prime}\right\|^{2} \leq(1+\epsilon)\|u-v\|^{2}\right] \geq 1-2 e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

**Proof:** Apply Theorem 1 to the vector $u-v$.

---

We conclude this section with a useful corollary. A similar proof can be found in Ben-David, et al. (2002).

**Corollary 2.** Let $u, v$ be vectors in $\boldsymbol{R}^{n}$ s.t. $\|u\|,\|v\| \leq 1$. Let $R$ be a random matrix whose entries are chosen independently from either $N(0,1)$ or $U(-1,1)$. Define $u^{\prime}=\frac{1}{\sqrt{k}} R^{T} u$ and $v^{\prime}=\frac{1}{\sqrt{k}} R^{T} v$. Then for any $\epsilon>0$,
$$
\mathrm{P}\left(u \cdot v-c \leq u^{\prime} \cdot v^{\prime} \leq u \cdot v+c\right) \geq 1-4 e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

**Proof:** Applying Theorem 2 to the vectors $u, v$ and $u-v$, we have that with probability at least $1-4 e^{-\left(c^{2}-c^{3}\right) \frac{k}{4}}$,
$$
\begin{aligned}
& (1-c)\|u-v\|^{2} \leq\left\|u^{\prime}-v^{\prime}\right\|^{2} \leq(1+c)\|u-v\|^{2} \\
& \text { and } \quad(1-c)\|u+v\|^{2} \leq\left\|u^{\prime}+v^{\prime}\right\|^{2} \leq(1+c)\|u+v\|^{2} \text {. }
\end{aligned}
$$

Then,

$$
\begin{aligned}
4 u^{\prime} \cdot v^{\prime} & =\left\|u^{\prime}+v^{\prime}\right\|^{2}-\left\|u^{\prime}-v^{\prime}\right\|^{2} \\
& \geq(1-c)\|u+v\|^{2}-(1+c)\|u-v\|^{2} \\
& =4 u \cdot v-2 c\left(\|u\|^{2}+\|v\|^{2}\right) \\
& \geq 4 u \cdot v-4 c .
\end{aligned}
$$

Thus $u^{\prime} \cdot v^{\prime} \geq u \cdot v-c$. The other inequality is similar.

---

In what follows, we will apply random projection by picking entries of the projection matrix independently from $N(0,1)$ or $U(-1,1)$. We remark that one could use other distributions via Theorem 1.