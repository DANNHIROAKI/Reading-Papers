## 3. The main tool: "neuron-friendly" random projection

在本节中，我们开发随机投影的"神经元"版本，包括一个离散版本，并为它们提供概率保证，所有这些都有透明的证明。除了对神经元友好之外，这些随机投影的版本更容易实现。

---

为了将给定点$u \in \boldsymbol{R}^{n}$投影到$k$维空间，我们首先选择$k$个随机向量$R_{1}, \ldots, R_{k}$（我们稍后将讨论这些向量的合适概率分布）。然后我们计算一个$k$维向量$u^{\prime}$，其坐标是内积$u_{1}^{\prime}= R_{1}^{T} \cdot u, \ldots, u_{k}^{\prime}=R_{k}^{T} \cdot u$。如果我们令$R$为以向量$R_{1}, \ldots, R_{k}$为列的$n \times k$矩阵，那么投影可以简洁地写为$u^{\prime}=R^{T} u$。要将$\boldsymbol{R}^{n}$中的一组点$u^{1}, \ldots, u^{m}$投影到$\boldsymbol{R}^{k}$，我们按上述方法选择一个随机矩阵$R$，并计算向量$R^{T} u^{1}, \ldots, R^{T} u^{m}$。

---

给定矩阵$R$，上述过程是一个简单的计算任务。已经证明，如果$R$是一个随机正交矩阵，即$R$的列是随机单位向量且它们两两正交，那么对于一个出人意料地小的$k$值（约为$\log n / \epsilon^{2}$），该投影能在因子$(1+\epsilon)$的范围内保持所有成对距离(Johnson & Lindenstrauss, 1984)。本节的主要观察是表明这是一个相当稳健的现象，因为$R$的元素可以从任何具有有界矩的分布中选择。特别是，使用具有独立项且项从有界支撑分布中选择的随机矩阵就足够了。随之可以得出，随机投影任务可以通过简单的单层神经网络实现，即$k$个感知器（计算输入的线性组合），每个感知器有一个输出和相同的$n$个输入。假定神经网络的权重是随机且独立的。如图1所示。

设$r \in \boldsymbol{R}^{n}$是一个随机向量，其坐标是独立同分布的。我们强调以下两种坐标分布的可能性：
(a) 标准正态分布，均值为0，方差为1，记为$N(0,1)$
(b) 离散分布，定义为$r_{i}=1$的概率为$\cfrac{1}{2}$且$r_{i}=-1$的概率为$\cfrac{1}{2}$，我们记为$U(-1,1)$

在本文的会议版本(Arriaga & Vempala, 1999)之后，对于$U(-1,1)$情况的另一个证明也已发表(Achlioptas, 2001)。下面这个众所周知的引理将会有用。为方便起见，我们提供了证明。

---

引理1. 设$X$服从$N(0, \sigma)$，即均值为零、标准差为$\sigma$的正态分布。那么对于任意$\alpha<\cfrac{1}{2 \sigma^{2}}$，

$$
\mathrm{E}\left(e^{\alpha X^{2}}\right)=\frac{1}{\sqrt{1-2 \alpha \sigma^{2}}}
$$

证明： 我们回顾均值为0、标准差为$\sigma$的正态分布$N(0, \sigma)$的密度函数为：

$$
\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{x^{2}}{2 \sigma^{2}}}
$$

使用这个，

$$
\begin{aligned}
\mathrm{E}\left(e^{\alpha X^{2}}\right) & =\int_{-\infty}^{\infty} e^{\alpha x^{2}} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{x^{2}}{2 \sigma^{2}}} d x \\
& =\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{x^{2}}{2 \sigma^{2}}\left(1-2 \alpha \sigma^{2}\right)} d x \\
& =\frac{1}{\sqrt{1-2 \alpha \sigma^{2}}} \int_{-\infty}^{\infty} \frac{\sqrt{1-2 \alpha \sigma^{2}}}{\sqrt{2 \pi} \sigma} e^{-\frac{x^{2}}{2 \sigma^{2}}\left(1-2 \alpha \sigma^{2}\right)} d x \\
& =\frac{1}{\sqrt{1-2 \alpha \sigma^{2}}}
\end{aligned}
$$

这里我们使用了积分项是标准差为$\sigma / \sqrt{1-2 \alpha \sigma^{2}}$的正态密度这一观察。

---

我们首先考虑投影矩阵的每个元素都独立地从标准正态分布中选择的情况。

引理2： 设$R=\left(r_{i j}\right)$是一个随机$n \times k$矩阵，其中每个元素$r_{i j}$独立地服从$N(0,1)$。对于任意固定向量$u \in \boldsymbol{R}^{n}$和任意$\epsilon>0$，令$u^{\prime}= \cfrac{1}{\sqrt{k}}\left(R^{T} u\right)$。那么，$\mathrm{E}\left(\left|u^{\prime}\right|^{2}\right)=|u|^{2}$且

$$
\begin{gathered}
\operatorname{Pr}\left[\left\|u^{\prime}\right\|^{2}>(1+\epsilon)\|u\|^{2}\right] \leq\left((1+\epsilon) e^{-\epsilon}\right)^{k} \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} \\
\operatorname{Pr}\left[\left\|u^{\prime}\right\|^{2}<(1-\epsilon)\|u\|^{2}\right] \leq\left((1-\epsilon) e^{\epsilon}\right)^{k} \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}}
\end{gathered}
$$

**证明：** 期望是通过简单计算得出的。为了得到关于均值附近的集中度界限，令$X_{j}=\left(R_{j}^{T} \cdot u\right) /|u|$并观察到
$$
X=\sum_{j=1}^{k} X_{j}^{2}=\sum_{j=1}^{k} \frac{\left(R_{j}^{T} \cdot u\right)^{2}}{\|u\|^{2}}
$$

其中$R_{j}$表示$R$的第$j$列。每个$X_{j}$都服从标准正态分布（因为$R_{j}$的每个分量都是如此）。还要注意

$$
\left\|u^{\prime}\right\|^{2}=\frac{\|u\|^{2}}{k} X .
$$

使用马尔可夫不等式，我们可以估计所需的概率为

$$
\begin{aligned}
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2} \geq(1+\epsilon)\|u\|^{2}\right)=\operatorname{Pr}(X \geq(1+\epsilon) k) & =\operatorname{Pr}\left(e^{\alpha X} \geq e^{(1+\epsilon) k \alpha}\right) \\
& \leq \frac{\mathrm{E}\left(e^{\alpha X}\right)}{e^{(1+\epsilon) k \alpha}} \\
& =\frac{\Pi_{j=1}^{k} \mathrm{E}\left(e^{\alpha X_{j}^{2}}\right)}{e^{(1+\epsilon) k \alpha}}=\left(\frac{\mathrm{E}\left(e^{\alpha X_{1}^{2}}\right)}{e^{(1+\epsilon) \alpha}}\right)^{k} .
\end{aligned}
$$

在上面的最后一行中，我们使用了$X_{j}$的独立性。

---

类似地，
$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2} \leq(1-\epsilon)\|u\|^{2}\right) \leq\left(\frac{\mathrm{E}\left(e^{-\alpha X_{1}^{2}}\right)}{e^{-(1-\epsilon) \alpha}}\right)^{k} .
$$

根据引理1

$$
\mathrm{E}\left(e^{\alpha X_{1}^{2}}\right)=\frac{1}{\sqrt{1-2 \alpha}}
$$

对任意$\alpha<\cfrac{1}{2}$，有

$$
\operatorname{Pr}(X \geq(1+\epsilon) k) \leq\left(\frac{e^{-2(1+\epsilon) \alpha}}{(1-2 \alpha)}\right)^{\frac{k}{2}} .
$$

$\alpha$的最优选择是$\epsilon / 2(1+\epsilon)$。有了这个，

$$
\operatorname{Pr}(X \geq(1+\epsilon) k) \leq\left((1+\epsilon) e^{-\epsilon}\right)^{\frac{k}{2}} \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

类似地，

$$
\operatorname{Pr}(X \leq(1-\epsilon) k) \leq\left(\frac{e^{2(1-\epsilon) \alpha}}{(1+2 \alpha)}\right)^{\frac{k}{2}} \leq\left((1-\epsilon) e^{\epsilon}\right)^{\frac{k}{2}} \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

---

本节的主定理表明这种现象并不是正态分布所特有的。在下面的陈述中，条件$\mathrm{E}\left(r^{2}\right)=1$是为了方便。相反，这个期望可以有任意有限值$\sigma^{2}$，并通过$\sigma$来缩放投影。

**定理1：** 设$R$是一个随机$n \times k$矩阵，其中每个元素$r$独立地从一个关于原点对称且满足$\mathrm{E}\left(r^{2}\right)=1$的分布$\mathcal{D}$中选择。对于任意固定向量$u \in \boldsymbol{R}^{n}$，令$u^{\prime}=\cfrac{1}{\sqrt{k}} R^{T} u$ 

1. 假设$B=\mathrm{E}\left(r^{4}\right)<\infty$。那么对于任意$\epsilon>0$

$$
\mathrm{P}\left(\left[\left\|u^{\prime}\right\|^{2} \leq(1-\epsilon)\|u\|^{2}\right)\right] \leq e^{-\frac{\left(\epsilon^{2}-\epsilon^{3}\right) k}{2(B+1)}} .
$$

2. 假设存在$L>0$使得对于任意整数$m>0$，$\mathrm{E}\left(r^{2 m}\right) \leq \cfrac{(2 m)!}{2^{m} m!} L^{2 m}$。那么对于任意$\epsilon>0$，

$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2} \geq(1+\epsilon) L^{2}\|u\|^{2}\right) \leq\left((1+\epsilon) e^{-\epsilon}\right)^{k / 2} \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

**证明：** 不失一般性，假设$|u|^{2}=1$。令
$$
X_{i}=R_{i}^{T} u \quad \text { for } i=1, \ldots, k
$$

我们有

$$
\mathrm{E}\left(X_{i}^{2}\right)=\mathrm{E}\left(\left(R_{i}^{T} u\right)^{2}\right)=\mathrm{E}\left(\left(\sum_{j=1}^{n} R_{i j} u_{j}\right)^{2}\right)=\sum_{j=1}^{n} \mathrm{E}\left(R_{i j}^{2}\right) u_{j}^{2}=1
$$

然后，如果我们定义$Y$如下 

$$
Y:=\sum_{i=1}^{k} X_{i}^{2}=k\left\|u^{\prime}\right\|^{2}, \quad \mathrm{E}(Y)=\sum_{i=1}^{k} \mathrm{E}\left(X_{i}^{2}\right)=k
$$

---

使用$X_{i}$的独立性和马尔可夫不等式，均值以下的偏差相对容易界定。
$$
\begin{aligned}
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2}<(1-\epsilon)\|u\|^{2}\right) & =\mathrm{P}(Y<(1-\epsilon) k) \\
& =\mathrm{P}\left(e^{-\alpha Y}>e^{-\alpha(1-\epsilon) k}\right) \\
& \leq \frac{\mathrm{E}\left(e^{-\alpha Y}\right)}{e^{-\alpha(1-\epsilon) k}} \\
& =\left(\mathrm{E}\left(e^{-\alpha X_{1}^{2}}\right) e^{\alpha(1-\epsilon)}\right)^{k}
\end{aligned}
$$

而且，使用$e^{-\alpha X_{1}^{2}} \leq 1-\alpha X_{1}^{2}+\alpha^{2} X_{1}^{4} / 2$，我们得到

$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2}<(1-\epsilon)\|u\|^{2}\right) \leq\left(\left(1-\alpha \mathrm{E}\left(X_{1}^{2}\right)+\frac{\alpha^{2}}{2} \mathrm{E}\left(X_{1}^{4}\right)\right) e^{\alpha(1-\epsilon)}\right)^{k} .
$$

我们可以轻易计算这些矩：$\mathrm{E}\left(X_{1}^{2}\right)=1$，而且，如果我们观察到由于对称性$r$的奇数幂的期望为零，我们有（使用$B \geq 1$的事实），

$$
\begin{aligned}
\mathrm{E}\left(X_{1}^{4}\right) & =\mathrm{E}\left(\left(\sum_{j=1}^{n} R_{1 j} u_{j}\right)^{4}\right) \\
& =\sum_{j_{1}, j_{2}, j_{3}, j_{4}=1}^{n} \mathrm{E}\left(R_{1 j_{1}} R_{1 j_{2}} R_{1 j_{3}} R_{1 j_{4}}\right) u_{j_{1}} u_{j_{2}} u_{j_{3}} u_{j_{4}} \\
& =\sum_{j=1}^{n} \mathrm{E}\left(R_{1 j}^{4}\right) u_{j}^{4}+3 \sum_{j_{1} \neq j_{2}, j_{1}, j_{2}=1}^{n} \mathrm{E}\left(R_{1 j_{1}}^{2} R_{1 j_{2}}^{2}\right) u_{j_{1}}^{2} u_{j_{2}}^{2} \\
& \leq B \sum_{j=1}^{n} u_{j}^{4}+3 \sum_{j_{1} \neq j_{2}, j_{1}, j_{2}=1}^{n} u_{j_{1}}^{2} u_{j_{2}}^{2} \\
& \leq(B+2)\left(\sum_{j} u_{j}^{2}\right)^{2} \\
& =B+2 .
\end{aligned}
$$

因此，使用$e^{x}$的泰勒展开（特别是，对于$x \geq 0$且足够小时，$e^{-x+x^{2 / 2}} \geq 1-x$）。

$$
\begin{aligned}
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2}<(1-\epsilon)\|u\|^{2}\right) & \leq\left(\left(1-\alpha+\frac{\alpha^{2}}{2}(B+2)\right) e^{\alpha(1-\epsilon)}\right)^{k} \\
& \leq\left(e^{-\alpha+\frac{\alpha^{2}(B+2)}{2}-\frac{1}{2}\left(\alpha-\frac{\alpha^{2}(B+2)}{2}\right)^{2}} e^{\alpha(1-\epsilon)}\right)^{k} \\
& \leq e^{-\frac{\left.\left(\epsilon^{2}-\epsilon^{3}\right)\right]}{2(B+1)}} .
\end{aligned}
$$

上面的最后一行是通过设定$\alpha=\epsilon /(B+1)$并注意到$B \geq 1$得到的。

---

类似地，对于均值以上的偏差，
$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2}>(1+\epsilon) L^{2}\|u\|^{2}\right) \leq\left(\frac{\mathrm{E}\left(e^{\alpha X_{1}^{2}}\right)}{e^{\alpha L^{2}(1+\epsilon)}}\right)^{k} .
$$

主要任务是使用定理的假设从上面界定$\mathrm{E}\left(e^{\alpha X_{1}^{2}}\right)$。这个期望很难直接计算，因为我们不知道具体的分布。然而，我们有$X_{1}^{2}$所有矩的界限。因此，如果我们定义一个随机变量$Z$，其所有矩至少是$X_{1}^{2}$的矩，那么$\mathrm{E}\left(e^{\alpha Z}\right)$将是所需期望的上界。以下声明将会有用。

---

**声明1.** 设$f, g$是关于原点对称的$\boldsymbol{R}$上的分布，具有如下性质：对于任意非负整数$m$，$\mathrm{E}\left(Y^{2 m}\right) \leq \mathrm{E}\left(Z^{2 m}\right)$，其中$Y, Z$分别从$f, g$中抽取。设$Y_{1}, \ldots, Y_{n}$是从$f$中独立同分布抽取的，$Z_{1}, \ldots, Z_{n}$是从$g$中独立同分布抽取的。那么对于任意$u \in \boldsymbol{R}^{n}$，随机变量$\hat{Y}=\displaystyle{}\sum_{j=1}^{n} u_{j} Y_{j}$和$\hat{Z}=\displaystyle{}\sum_{j=1}^{n} u_{j} Z_{j}$对每个非负整数$m$都满足$\mathrm{E}\left((\hat{Y})^{2 m}\right) \leq \mathrm{E}\left((\hat{Z})^{2 m}\right)$。

这个声明很容易证明。比较$(\hat{Y})^{2 m}$和$(\hat{Z})^{2 m}$的个别项的期望。由于$Y_{i}, Z_{i}$关于原点对称，所有以奇数次幂出现的项的期望都为零。对于所有幂次都是偶数的任何项，根据假设，$E\left((\hat{Z})^{2 m}\right)$的项占优势。

---

对于我们的情况，我们知道

$$
X_{1}=\sum_{j=1}^{n} u_{j} r_{j}
$$

其中每个$r_{j}$从给定分布$\mathcal{D}$中抽取。定义

$$
Y_{1}=\sum_{j=1}^{n} u_{j} r_{j}^{\prime}
$$

其中每个$r_{j}^{\prime}$从$N(0, L)$中抽取。那么对于所有$j$和任意整数$m>0$，

$$
\mathrm{E}\left(r_{j}^{2 m}\right) \leq \frac{(2 m)!}{2^{m} m!} L^{2 m}=\mathrm{E}\left(\left(r_{j}^{\prime}\right)^{2 m}\right)
$$

使用$N(0, L)$矩的著名公式。所以，$\mathrm{E}\left(X_{1}^{2 m}\right) \leq \mathrm{E}\left(Y_{1}^{2 m}\right)$。此外，$Y_{1}$的分布是$N(0, L)$。因此，

$$
\mathrm{E}\left(e^{\alpha X_{1}^{2}}\right) \leq \mathrm{E}\left(e^{\alpha Y_{1}^{2}}\right)=\frac{1}{\sqrt{1-2 \alpha L^{2}}} .
$$

使用这个，

$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2}>(1+\epsilon) L^{2}\|u\|^{2}\right) \leq\left(\frac{e^{-2 \alpha L^{2}(1+\epsilon)}}{1-2 \alpha L^{2}}\right)^{\frac{k}{2}} .
$$

$\alpha$的最优选择是$\epsilon / 2 L^{2}(1+\epsilon)$，我们得到对于任意$\epsilon>0$，

$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2}>(1+\epsilon) L^{2}\|u\|^{2}\right) \leq\left((1+\epsilon) e^{-\epsilon}\right)^{\frac{k}{2}} \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

最后的不等式是通过使用不等式$\ln (1+\epsilon) \leq \epsilon-\epsilon^{2} / 2+\epsilon^{3} / 2$得到的。

---

**推论1：** 如果$n \times k$矩阵$R$的每个元素都按照$U(-1,1)$选择，那么对于任意固定向量$u \in \boldsymbol{R}^{n}$和任意$\epsilon>0$，向量$u^{\prime}=\cfrac{1}{\sqrt{k}} R^{T}$ u满足
$$
\mathrm{P}\left(\left\|u^{\prime}\right\|^{2} \geq(1+\epsilon)\|u\|^{2}\right) \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} \text { and } \mathrm{P}\left(\left\|u^{\prime}\right\|^{2} \leq(1-\epsilon)\|u\|^{2}\right) \leq e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

**证明：** 对于从$U(-1,1)$抽取的$r$，$\mathrm{E}\left(r^{2 m}\right)=1$对任意整数$m>0$成立。因此，我们可以应用定理1，取$L=B=1$得到推论的结论。

---

设$R$是一个$n \times k$矩阵，其元素独立地从$N(0,1)$或$U(-1,1)$中选择。以下定理总结了本节的结果。对于$N(0,1)$情况的另一种证明出现在Indyk和Motwani (1998)和DG中。

定理2（神经元随机投影）。设$u, v \in \boldsymbol{R}^{n}$。设$u^{\prime}$和$v^{\prime}$是$u$和$v$通过随机矩阵$R$投影到$\boldsymbol{R}^{k}$的结果，其中$R$的元素独立地从$N(0,1)$或$U(-1,1)$中选择。那么，

$$
\mathrm{P}\left[(1-\epsilon)\|u-v\|^{2} \leq\left\|u^{\prime}-v^{\prime}\right\|^{2} \leq(1+\epsilon)\|u-v\|^{2}\right] \geq 1-2 e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

证明： 对向量$u-v$应用定理1。

---

我们以一个有用的推论结束本节。类似的证明可以在Ben-David, et al. (2002)中找到。

**推论2**： 设$u, v$是$\boldsymbol{R}^{n}$中的向量，满足$|u|,|v| \leq 1$。设$R$是一个随机矩阵，其元素独立地从$N(0,1)$或$U(-1,1)$中选择。定义$u^{\prime}=\cfrac{1}{\sqrt{k}} R^{T} u$和$v^{\prime}=\cfrac{1}{\sqrt{k}} R^{T} v$。那么对于任意$\epsilon>0$，
$$
\mathrm{P}\left(u \cdot v-c \leq u^{\prime} \cdot v^{\prime} \leq u \cdot v+c\right) \geq 1-4 e^{-\left(\epsilon^{2}-\epsilon^{3}\right) \frac{k}{4}} .
$$

证明： 将定理2应用于向量$u, v$和$u-v$，我们有以概率至少$1-4 e^{-\left(c^{2}-c^{3}\right) \cfrac{k}{4}}$，

$$
\begin{aligned}
& (1-c)\|u-v\|^{2} \leq\left\|u^{\prime}-v^{\prime}\right\|^{2} \leq(1+c)\|u-v\|^{2} \\
& \text { and } \quad(1-c)\|u+v\|^{2} \leq\left\|u^{\prime}+v^{\prime}\right\|^{2} \leq(1+c)\|u+v\|^{2} \text {. }
\end{aligned}
$$

然后，

$$
\begin{aligned}
4 u^{\prime} \cdot v^{\prime} & =\left\|u^{\prime}+v^{\prime}\right\|^{2}-\left\|u^{\prime}-v^{\prime}\right\|^{2} \\
& \geq(1-c)\|u+v\|^{2}-(1+c)\|u-v\|^{2} \\
& =4 u \cdot v-2 c\left(\|u\|^{2}+\|v\|^{2}\right) \\
& \geq 4 u \cdot v-4 c .
\end{aligned}
$$

因此$u^{\prime} \cdot v^{\prime} \geq u \cdot v-c$。另一个不等式类似。

---

在接下来的内容中，我们将通过从$N(0,1)$或$U(-1,1)$中独立选择投影矩阵的元素来应用随机投影。我们注意到，通过定理1，可以使用其他分布。