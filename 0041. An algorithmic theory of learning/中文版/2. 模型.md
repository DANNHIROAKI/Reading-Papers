## 2. The model

为了描述该模型，我们采用文献中使用的术语。我们假设属性是实值的；一个例子是$\boldsymbol{R}^{n}$中的一个点；一个概念是$\boldsymbol{R}^{n}$的子集。属于某个概念的例子被标记为该概念的正例，而位于概念之外的例子被标记为负例。

---

给定从$\boldsymbol{R}^{n}$中的未知分布$\mathcal{D}$抽取的一组标记例子，并根据未知目标概念进行标记，学习任务是找到一个具有低错误率的假设。假设是一个多项式时间可计算的函数。假设$h$相对于目标概念的错误率是$h$在从$\mathcal{D}$抽取的随机例子上与目标函数不一致的概率。因此，如果$h$的错误率为$\epsilon$，那么对于随机的$x$，$h(x)$与目标概念不一致的概率最多为$\epsilon$。所以，给定错误参数$\epsilon$和置信参数$\delta$，算法必须以至少$1-\delta$的概率找到一个在$\mathcal{D}$上错误率最多为$\epsilon$的概念(Valiant, 1984)。

---

这个新模型的基本洞察是鲁棒性的概念（在早期工作中隐含）。直观地说，一个概念是"鲁棒的"，如果它对属性噪声免疫。也就是说，在一定范围内修改例子的属性不会改变其标签。另一种解释是具有不同标签的点之间相距较远。这被形式化如下：

定义1. 对于任意实数$\ell>0$，$\boldsymbol{R}^{n}$中的概念$C$结合分布$\mathcal{D}$，如果满足以下条件则称为$\ell$-鲁棒的：

$$
\mathrm{P}_{\mathcal{D}}(x \mid \exists y: \operatorname{label}(x) \neq \operatorname{label}(y),\|x-y\| \leq \ell)=0
$$

范数$|x-y|$是$x$和$y$之间的欧几里得距离。这可以被其他范数替代，但在本文中我们使用欧几里得范数。这个概率是指所有满足以下性质的点$x$：存在某个标签不同的点$y$在距离$\ell$内。换句话说，一个概念是$\ell$-鲁棒的，如果点在概念边界$\ell$范围内的概率为零。这个定义可以被削弱为仅要求上述概率应该可忽略（例如$1/2^n$）。当$\mathcal{D}$是在$\boldsymbol{R}^{n}$的离散子集上时，这有一个简单的解释。任何具有非零概率的点$x$周围半径为$\ell$的球完全位于概念的一侧，即球中的每个点都与$x$具有相同的标签。为避免缩放问题，我们通常只考虑支撑集是$\boldsymbol{R}^{n}$中单位球（的子集）的分布，即提供给算法的所有例子的长度最多为1（或者，也可以通过例子的长度来标准化例子之间的距离，但我们认为我们的定义更方便）。给定来自鲁棒概念的例子以及参数$\epsilon, \delta$，如果一个学习算法以至少$1-\delta$的概率产生一个与至少$1-\epsilon$的例子分布一致的假设，则该算法成功并被称为$(\epsilon, \delta)$-学习。注意，严格来说这不是PAC学习，因为鲁棒性限制了例子分布。

---

在接下来的内容中，我们将介绍用于学习鲁棒概念的工具和算法。值得注意的是，"鲁棒性"仅指目标概念；它不要求类中的所有概念都具有鲁棒性。

### 2.1. Connection to existing models

该模型与支持向量机中使用的大间隔分类器密切相关(Bartlett & Shawe-Taylor, 1998)。事实上，对于半空间的概念类，这里定义的鲁棒性恰好是一个正确分类半空间可能的最大间隔（在所有例子都来自单位球的标准化条件下）。然而，总的来说，存在一个微妙但重要的区别。在SVM中，间隔是在概念已被转换为半空间的"提升"空间中测量的，而在我们的模型中，我们在例子呈现给我们的空间中测量鲁棒性（因此与属性噪声有着自然的关系）。这种鲁棒性也与fat-shattering维数定义中使用的参数$\gamma$密切相关(Kearns & Schapire, 1994; Bartlett & Shawe-Taylor, 1998)，并且在半空间的情况下（除了一个缩放因子外）再次重合。