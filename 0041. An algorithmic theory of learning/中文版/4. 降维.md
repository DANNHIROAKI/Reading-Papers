## 4. Learning efficiently by reducing dimensionality

在本节中，我们将描述鲁棒概念的学习算法，并推导所需样本数量和运行时间的界限。我们的界限将是鲁棒性参数$l$和学习参数$\epsilon, \delta$的函数，但将独立于概念类的实际属性数量。

---

我们得到了来自未知分布$\mathcal{D}$的标记样本。学习鲁棒概念的通用算法基于以下两个高层次想法：

1. 由于目标概念是鲁棒的，将样本随机投影到维度更低的子空间将"保持"该概念。

2. 在低维空间中，学习概念所需的样本数量和时间相对较少。

---

在将这种方法应用于特定概念类之前，我们回顾一些学习理论中的基本定理。对于所考虑的概念类$\mathcal{C}$，令$C(m, k)$表示使用$\boldsymbol{R}^{k}$中的$\mathcal{C}$中的概念所能获得的$m$个点的不同标记的最大数量。以下这个著名定理（参见Kearns & Vazirani (1994)或Blumer et al. (1989)）给出了样本大小的界限，使得与样本一致的假设也以高概率对整个分布具有小误差。

---

**定理3：** 设$\mathcal{C}$是$\boldsymbol{R}^{k}$中的任意概念类。设$w$是$\mathcal{C}$中的一个概念，它与$C$中某个概念的$m$个标记样本一致。那么，如果
$$
m>\frac{4}{\epsilon} \log C(2m, k)+\frac{4}{\epsilon} \log \frac{2}{\delta} .
$$
则以概率至少$1-\delta$，$w$正确分类$\mathcal{D}$的至少$(1-\epsilon)$部分。

VC维（Vapnik & Chervonenkis, 1971）的概念与不同标记的数量密切相关，如以下基本定理所表述。

**定理4**（Blumer et al. 1989）： 设$C$是VC维为$d$的概念类。那么，$C$中的概念对$m$个点的不同标记数量最多为
$$
C[m] \leq \sum_{i=0}^{d}\binom{m}{i} .
$$

---

如果算法找到一个与样本几乎一致的假设（而不是像前面定理中那样完全一致），这也能很好地泛化。所需样本数量增加一个常数因子。以下定理是Blumer et al. (1989)中类似定理的一个轻微变体。为了读者方便，我们在附录中提供了一个独立完整的证明。

**定理5：** 对于$\epsilon \leq 1/4$，设$w$是$\boldsymbol{R}^{k}$中$\mathcal{C}$的一个概念，它正确分类了从$\mathcal{D}$中抽取的$m$个点样本中至少$(1-\epsilon/8)$的部分，其中
$$
m \geq \frac{32}{\epsilon} \log C(2 m, k)+\frac{32}{\epsilon} \log \frac{2}{\delta}
$$

那么以概率至少$1-\delta$，$w$正确分类$\mathcal{D}$的至少$1-\epsilon$部分。

### 4.1. Half-spaces

我们从学习$\boldsymbol{R}^{n}$中的半空间（线性阈值函数）问题开始。这是学习理论中研究最早的问题之一。通过对$O(n)$个样本使用线性规划算法，这个问题可以在多项式时间内解决（注意这不是强多项式算法——其复杂度仅依赖于输入位数的多项式）。然而，通常是通过使用简单的贪心方法来解决。一个常用的贪心算法是感知器算法(Agmon, 1954; Rosenblatt, 1962)，它具有以下保证：给定$\boldsymbol{R}^{n}$中的一组数据点，每个点被标记为正或负，如果存在这样的向量，算法将找到一个向量$w$，使得对所有正点$x$有$w \cdot x>0$，对所有负点$x$有$w \cdot x<0$。$^{1}$算法的运行时间取决于一个分离参数（如下所述）。然而，为了使假设可靠，我们需要使用$\Omega(n)$个点的样本，因为$\boldsymbol{R}^{n}$中半空间的VC维是$n+1$。

---

设$\mathcal{H}*{n}$是$\boldsymbol{R}^{n}$中齐次半空间的类。设$(h, \mathcal{D})$是一个概念-分布对，其中半空间$h \in \mathcal{H}*{n}$相对于$\boldsymbol{R}^{n}$上的分布$\mathcal{D}$是$\ell$-鲁棒的。我们限制$\mathcal{D}$在单位球面上（即，所有样本到原点的距离为1）。后一个条件实际上不是限制，因为样本可以缩放到单位长度而不改变它们的标签。算法中的参数$k$和$m$将在后面指定。

---

##### Half-space Algorithm:

1. 通过从$N(0,1)$或$U(-1,1)$中独立选择每个元素来选择一个$n \times k$随机矩阵$R$。
2. 从$\mathcal{D}$获取$m$个样本并使用$R$将它们投影到$\boldsymbol{R}^{k}$。
3. 在$\boldsymbol{R}^{k}$中运行以下感知器算法：令$w=0$。执行以下操作直到所有样本都被正确分类： 选择一个任意的错分样本$x$并令$w \leftarrow w+\operatorname{label}(x) x$。
4. 输出$R$和$w$。

对未来的样本$x$，如果$w \cdot\left(R^{T} x\right) \geq 0$则标记为正，否则标记为负。这当然等同于检查$\left(w R^{T}\right) \cdot x>0$，即原始$n$维空间中的一个半空间。

---

我们可以假设$h$，即概念半空间的法向量，是单位长度的。算法背后的想法是，当$k$足够大时，在通过投影获得的$k$维子空间中，由$R^{T} h$定义的通过原点的半空间，即$\left(R^{T} h\right) \cdot y \geq 0$，能正确分类大多数投影分布。我们将证明事实上这个半空间相对于足够大规模的投影样本仍然保持鲁棒性。为了找到一个一致的半空间，我们使用经典的感知器算法。众所周知（见Minsky & Papert (1969)），这个算法的收敛性取决于间隔，即用我们的术语来说，就是目标半空间的鲁棒性。

---

定理6：（Minsky & Papert, 1969）假设数据集S能被某个单位向量$w$正确分类。那么，感知器算法在最多$1 / \sigma^{2}$次迭代内收敛，其中

$$
\sigma=\min _{x \in S} \frac{|w \cdot x|}{\|x\|} .
$$

对于一个$\ell$-鲁棒的半空间，我们有$\sigma \geq \ell$。该定理表明感知器算法将在最多$1 / \ell^{2}$次迭代内找到一个一致的半空间。我们现在可以陈述和证明本节的主要结果。

---

**定理7：**$\boldsymbol{R}^{n}$中的$\ell$-鲁棒半空间可以通过将$m$个样本投影到$\boldsymbol{R}^{k}$来实现$(\epsilon, \delta)$-学习，其中
$$
k=\frac{100}{\ell^{2}} \ln \frac{100}{\epsilon \ell \delta}, \quad m=\frac{8 k}{\epsilon} \log \frac{48}{\epsilon}+\frac{4}{\epsilon} \log \frac{4}{\delta}=O\left(\frac{1}{\ell^{2}} \cdot \frac{1}{\epsilon} \cdot \ln \frac{1}{\epsilon} \ln \frac{1}{\epsilon \ell \delta}\right)
$$

时间复杂度为$n \cdot \operatorname{poly}\left(\cfrac{1}{\ell}, \cfrac{1}{\epsilon}, \log \cfrac{1}{\delta}\right)$。

**证明：**对于样本点$x$，我们用$x^{\prime}$表示其投影。我们用$h^{\prime}$表示目标半空间的法向量$h$的投影。我们希望通过选择投影矩阵$R$发生以下事件：

1. 对于每个样本$x$，其投影$x^{\prime}$的长度最多为$1+\cfrac{\ell}{2}$。类似地，$\left|h^{\prime}\right| \leq 1+\cfrac{\ell}{2}$。
2. 对于每个样本$x$，如果$h \cdot x \geq \ell$，那么$h^{\prime} \cdot x \geq \cfrac{\ell}{2}$；如果$h \cdot x \leq-\ell$，那么$h^{\prime} \cdot x^{\prime} \leq-\cfrac{\ell}{2}$。

我们现在来界定这些事件之一不发生的概率。对于任意单个样本$x$，应用推论2，取$\epsilon=\ell/2$和我们选择的$k$，$\left|x^{\prime}\right|>1+\frac{\ell}{2}$的概率最多为

$$
e^{-\left(\frac{\ell^{2}}{4}-\frac{\ell^{3}}{8}\right) \frac{k}{4}} \leq e^{-\frac{\ell^{2} k}{32}} \leq\left(\frac{\epsilon \ell \delta}{100}\right)^{\frac{100}{32}}<\frac{\delta}{4(m+1)} .
$$

---

将这个概率在所有$m$个样本和向量$h$上相加，我们得到失败概率最多为$\delta/4$。

接下来，根据推论2，令$u=h$且$v=x$，第二个事件对任何特定样本$x$不发生的概率最多为$\delta/4m$。同样，这导致总的失败概率最多为$\delta/4$。因此，两个事件都以至少$1-\delta/2$的概率发生。

这些事件意味着由$h^{\prime}$定义的$\boldsymbol{R}^{k}$中的半空间在投影后正确分类了所有$m$个样本（概率至少为$1-\delta/2$）。此外，在将样本缩放到最大长度为1后，间隔至少为

$$
\sigma \geq \frac{\ell / 2}{1+\frac{\ell}{2}} \geq \frac{\ell}{3} .
$$

现在，根据定理6，感知器算法将在$9/\ell^{2}$次迭代内找到一个一致的半空间。

---

最后，我们需要证明$m$足够大，使得找到的假设能够很好地泛化。我们将对$\boldsymbol{R}^{k}$中通过原点的半空间应用定理3。后者概念类的VC维是$k$，所以，根据定理4，我们得到以下关于不同半空间数量的著名界限（参见如Kearns & Vazirani (1994)）：
$$
\begin{equation*}
C(2 m, k) \leq \sum_{i=0}^{k-1}\binom{2 m}{i} \leq\left(\frac{2 e m}{k}\right)^{k} . \tag{1}
\end{equation*}
$$

我们选择的$m$满足

$$
m=\frac{8 k}{\epsilon} \log \frac{48}{\epsilon}+\frac{4}{\epsilon} \log \frac{4}{\delta}>\frac{4}{\epsilon} \log C(2 m, k)+\frac{4}{\epsilon} \log \frac{4}{\delta} .
$$

因此，应用定理3，用$\delta/2$代替$\delta$，算法找到的半空间以至少$1-\delta/2$的概率正确分类原始分布的至少$1-\epsilon$部分。这给出了至少$1-\delta$的总体成功概率。

感知器算法及其变体已知对各种类型的随机分类噪声具有抗性(Bylander, 1994; Blum et al., 1996)。这些性质继续适用于这里描述的算法是一个直接的结果。在结论部分，我们讨论不可知学习的直接界限。

### 4.2. Intersections of half-spaces

我们下一个考虑的问题是学习$\boldsymbol{R}^{n}$中$t$个半空间的交集，即正例都位于$t$个半空间的交集中，而负例位于该区域之外。对于任意分布，还不知道如何解决这个问题。然而，假设半空间的数量相对较小，已经为相当一般的分布开发了高效算法(Blum & Kannan, 1993; Vempala, 2004)。在这里，我们为这类鲁棒概念推导高效的学习算法。

---

我们假设所有半空间都是齐次的。让半空间交集的概念类用$\mathcal{H}(t, n)$表示。这个类中的单个概念由一组$t$个半空间$P=\left\{h_{1}, \ldots, h_{t}\right\}$指定，正例恰好是那些满足$h_{i} \cdot x \geq 0$（对$i=1 \ldots t$）的例子。设$(P, \mathcal{D})$是一个概念-分布对，其中$P$相对于分布$\mathcal{D}$是$\ell$-鲁棒的。我们假设$\mathcal{D}$的支撑是单位球面的子集（并提醒读者，这和齐次性都不是真正的限制，因为它们可以分别通过缩放和添加额外维度来实现；参见如(Vempala, 2004)）。

---

令$C(m, t, k)$表示使用来自$\mathcal{H}(t, k)$的概念对$R^k$中的$m$个样本进行不同标记的最大数量。那么，

$$
\begin{equation*}
C(2 m, t, k) \leq\left(\sum_{i=0}^{k-1}\binom{2 m}{i}\right)^{t} \leq\left(\frac{2 e m}{k}\right)^{t k} . \tag{2}
\end{equation*}
$$

这可以从以下方面理解：对于$t=1$，这就是(1)，即使用半空间对$2m$个点分配+或-1的方式数量。如果我们给每个点$t$个标签，每个半空间一个，那么可能的标记总数就是(2)中的中间项。如果所有$t$个半空间标记为+的点集不同，我们就认为两个标记是不同的。因此，$t$个半空间的不同标记总数只能小于这个界限。

---

给定$m$个样本，我们总是可以使用穷举算法找到一个一致的假设（如果存在的话），该算法列举所有组合上不同的半空间并选择其中的$t$个（允许重复）。我们将这一点应用于在将足够大的样本投影到较低维子空间后学习$t$个半空间的鲁棒交集。下面的参数$k$和$m$将很快被指定。

---

##### $t$个半空间算法：
1. 通过从$N(0,1)$或$U(-1,1)$中独立选择每个元素来选择一个用于投影的$n \times k$随机矩阵$R$。
2. 从$\mathcal{D}$获取$m$个样本并使用$R$将它们投影到$\boldsymbol{R}^{k}$。
3. 找到一个假设$Q=\left\{w_{1}, \ldots, w_{t}\right\}$，其中每个$w_{i} \in \boldsymbol{R}^{k}$，使得半空间$w_{i} \cdot x \geq 0$（对$i=1, \ldots, t$）的交集与投影样本的标签一致。
4. 输出$R$和$Q$。

对于未来的样本$x$，将其投影为$R^{T} x$并根据$Q$标记，即如果对所有$i=1, \ldots, t$都有$w_{i} \cdot\left(R^{T} x\right) \geq 0$，则标记为正。

---

**定理8：** $\boldsymbol{R}^{n}$中$\ell$-鲁棒的$t$个半空间的交集可以通过将$m$个样本投影到$\boldsymbol{R}^{k}$来实现$(\epsilon, \delta)$-学习，其中
$$
k=\frac{100}{\ell^{2}} \ln \frac{100 t}{\epsilon \ell \delta} \quad \text { and } \quad m=\frac{8 k t}{\epsilon} \log \frac{48 t}{\epsilon}+\frac{4}{\epsilon} \log \frac{4}{\delta}=O\left(\frac{t}{\epsilon \ell^{2}} \log \frac{t}{\epsilon} \log \frac{t}{\ell \epsilon \delta}\right)
$$
时间复杂度为$O(n m k)+\left(\cfrac{48 t}{\epsilon} \log \cfrac{4 t}{\epsilon \delta}\right)^{k t}$。

**证明：** 证明类似于定理7，我们只进行概述。

---

设原始半空间集合为$h_{1} \cdot x \geq 0, \ldots, h_{t} \cdot x \geq 0$，其中每个$h_{i}$是$\boldsymbol{R}^{n}$中的单位向量。我们考虑它们的投影$h_{i}^{\prime}=\cfrac{1}{\sqrt{k}} R^{T} h_{i}$，以及以下事件：对于每个样本$x$和法向量$h_{i}$，如果$h_{i} \cdot x \geq \ell$，则$h_{i}^{\prime} \cdot x^{\prime}>0$；如果$h_{i} \cdot x \leq-\ell$，则$h_{i}^{\prime} \cdot x^{\prime}<0$。

对于我们选择的$k$和$m$，根据推论2，这些事件都以至少$1-\delta/2$的概率发生。因此，投影后，以这个概率，存在一个来自$\mathcal{H}(t, k)$的假设与所有$m$个样本一致。使用定理3和(2)，可以得出，任何与大小为

$$
m=\frac{8 k t}{\epsilon} \log \frac{2 t}{\epsilon}+\frac{4}{\epsilon} \log \frac{4}{\delta}
$$

的样本一致的假设将以至少$1-\delta/2$的概率正确分类分布的$(1-\epsilon)$部分。这给出了至少$1-\delta$的总体成功概率。枚举算法的运行时间是$O\left((2 e m/k)^{k t}\right)$。

---

如果$t, \ell, \epsilon, \delta$都是常数，那么算法的运行时间是线性的。如果只有$\ell, \epsilon, \delta$是常数，那么算法的运行时间是$O\left(n t \log ^{3} t\right)+(t \log t)^{O(t \log t)}$。这比已知的一般情况下最好的算法要快得多（参见1.1节的最新改进）。这两个结果除了鲁棒性外，不需要对分布$\mathcal{D}$做任何进一步的假设。该问题的先前算法假设$\mathcal{D}$要么是对称的(Baum, 1990)，要么是均匀的(Blum & Kannan, 1993)，要么是非集中的(Vempala, 1997)。最近，Klivans和Servedio (2004)使用在投影空间中学习多项式阈值函数的算法代替这里使用的枚举算法，获得了学习半空间鲁棒交集的改进时间复杂度。时间复杂度的改进伴随着样本复杂度的显著增加。

### 4.3. Balls

最后，我们简要讨论$\boldsymbol{R}^{n}$中的球的概念类，说明鲁棒性在学习非线性概念中的作用。

$\boldsymbol{R}^{n}$中的球$B\left(x_{0}, r\right)$定义为

$$
B\left(x_{0}, r\right)=\left\{x \in \boldsymbol{R}^{n}:\left\|x-x_{0}\right\| \leq r\right\}
$$

其中$x_{0}$（中心）是$\Re^{n}$中的一个固定点，$r$（半径）是一个固定的实数值。$B\left(x_{0}, r\right)$中的点被标记为正，而外部的点被标记为负。

众所周知，$\boldsymbol{R}^{n}$中球的VC维是$n+1$，所以$(\epsilon, \delta)$-学习一个球所需的样本数量是$O\left(\cfrac{n}{\epsilon} \log \cfrac{1}{\epsilon}+\cfrac{1}{\epsilon} \log \cfrac{1}{\delta}\right)$。学习一个$\ell$-鲁棒的球需要多少样本？以下定理很容易从神经元投影定理中得出。

---

**定理9：** $\boldsymbol{R}^{n}$中半径最大为1的$\ell$-鲁棒球可以通过将$m$个样本投影到$\boldsymbol{R}^{k}$来实现$(\epsilon, \delta)$-学习，其中

$$
k=\frac{100}{\ell^{2}} \ln \frac{100}{\epsilon \ell \delta} \quad \text { and } \quad m=\frac{8 k}{\epsilon} \log \frac{48}{\epsilon}+\frac{4}{\epsilon} \log \frac{4}{\delta}
$$

然后在$\boldsymbol{R}^{k}$中找到一个与投影样本一致的球。

证明： 以概率1，从分布$\mathcal{D}$中抽取的任何正例$x$将满足

$$
\left\|x-x_{0}\right\| \leq r-l
$$

而任何负例$x$将满足

$$
\left\|x-x_{0}\right\| \geq r+l .
$$

使用定理2，取我们选择的$k$和$\epsilon=\ell/2$，对于任意一个$x$，其投影$x^{\prime}$以至少$1-\cfrac{\delta}{2m}$的概率满足

$$
\left(1-\frac{\ell}{2}\right)\left\|x-x_{0}\right\| \leq\left\|x^{\prime}-x_{0}^{\prime}\right\| \leq\left(1+\frac{\ell}{2}\right)\left\|x-x_{0}\right\|
$$

所以，以概率$1-\delta/2$，所有投影样本满足上述不等式。此外，由于概念球的半径最大为1，

$$
\left\|x-x_{0}\right\|+\frac{\ell}{2} \leq\left\|x^{\prime}-x_{0}^{\prime}\right\| \leq\left\|x-x_{0}\right\|+\frac{\ell}{2} .
$$

因此，$\boldsymbol{R}^{k}$中的球$B\left(x_{0}^{\prime}, r\right)$与投影样本一致，定理得证。最后，我们可以使用定理3来验证$m$足够大，使其成为一个$(\epsilon, \delta)$-学习算法。

### 4.4. Noise tolerance

在这里我们注意到这些算法可以被调整以抵抗恶意分类噪声（不可知学习）。在$s$个样本中，假设最多$\gamma s$个样本的标签被任意破坏。固定一个假设类$H$，令$f(\ell)$是学习鲁棒性为$\ell$的概念所需的样本数量界限。那么要处理这个噪声"率"$\gamma$，我们获取$f(\ell)/(1-\gamma)$个样本，并对样本的每个大小为$f(\ell)$的子集，我们运行假设类的学习算法并输出一个能正确分类该子集的假设。算法的总运行次数最多为$2^{2f(\ell)}$。因此，例如，$\boldsymbol{R}^{n}$中的半空间可以在poly(n)时间内学习，鲁棒性低至$\sqrt{\cfrac{\log \log n}{\log n}}$。另一种解释方式是我们可以找到使错误数最小化的假设。这是由Avrim Blum观察到的。