# An algorithmic theory of learning: Robust concepts and random projection 


#### Abstract

我们从算法的角度研究认知学习现象。尽管每个例子都包含大量信息，大脑是如何从少量例子中有效地学习概念的？我们通过鲁棒概念学习模型（与"边界分类器"密切相关）提供了一种新颖的算法分析，并表明相对较少的例子就足以学习丰富的概念类别。新算法具有几个优点——更快、概念更简单，并且能够抵抗低水平的噪声。例如，无论属性数量多少，都可以仅使用固定数量的训练样本在线性时间内学习一个鲁棒半空间。该模型的一个一般性（算法）结论是"更鲁棒的概念更容易学习"，这得到了大量心理学研究的支持。

## 1. Introduction

计算学习理论的一个动机是获得==对认知过程的洞察==。学习过程（实际上是认知的任何方面）背后的确切物理过程还远未被理解。即便从纯理论的角度来看，大脑如何处理海量数据仍然大部分是个谜。当每个例子都包含大量信息时，大脑是如何从相对较少的例子中有效地学习概念的？

---

至少有两种方法可以解释这种现象。第一种源自Valiant的==属性高效学习==(Valiant, 1998; Littlestone, 1987, 1991)。在这个模型中，假设目标概念以特定方式简单：它只是属性集合中一个小子集的函数，称为相关属性，而其余的都是不相关的。基于这个假设，人们通常可以论证所得到的概念类的VC维仅是相关属性数量$(k)$的函数，从而推导出所需例子数量的界限。不幸的是，尽管这个模型在理论上简洁且吸引人，但目前还不知道如何学习比变量析取更复杂的内容（在没有成员查询的情况下）。此外，将$k$个变量的析取作为少于$k \log n$个变量的析取来学习是NP困难的（其中$n$是变量总数）。

---

在本文中，我们研究了一种基于简单想法的不同方法，可以通过以下例子来说明。想象一个孩子在学习"大象"这个概念。我们向孩子指出几次大象的图片或真实的大象并说"大象"，也许还指出一些其他动物的例子并说出它们的名字（即"不是大象"）。从那时起，孩子几乎肯定会只把大象正确标记为大象。另一方面，想象一个孩子仅从例子中学习"非洲象"（相对于印度象）的概念。这可能需要更多的例子，甚至可能需要明确指出非洲象更大的耳朵。

---

上述两个概念的关键区别不在于属性的数量，甚至不在于所呈现例子的相关属性数量，而在于具有相同标签的例子之间的相似性以及具有不同标签的例子之间的差异性。大象和非大象之间的界限比非洲象和印度象之间的界限更加清晰。这个概念稍后将被形式化为概念的鲁棒性。从另一个角度看，鲁棒性是衡量一个例子的属性在不影响概念的情况下可以改变多少的度量。鲁棒概念的主要特征是，学习一个鲁棒概念所需的例子数量和时间可以用鲁棒性（用参数$\ell$表示）来界定，而不依赖于属性总数。该模型和参数$\ell$在第2节中有精确定义。正如我们在那里讨论的，该模型与机器学习中研究的大间隔分类器非常相关，而大间隔分类器又是支持向量机的基础(Vapnik, 1995; Cortes \& Vapnik, 1995)。

---

在鲁棒概念模型中，主要的新发现是我们可以采用一个独立于概念类的通用程序来降低例子的维度。在降低例子维度的同时，我们希望保持概念。例如，如果我们原始的概念类是$n$维空间中的半空间（线性阈值）集合，我们希望将例子映射到$k$维空间，其中$k$远小于$n$，并维持在$k$维空间中存在某个半空间能正确分类（大部分）例子的性质。我们证明随机投影（将点集投影到随机选择的低维空间的技术）适用于这个目的。已经观察到随机投影（近似地）保持了点集的关键性质，例如点对之间的距离(Johnson \& Lindenstrauss, 1984)；这在其他几个场景中已经产生了高效的算法(Kleinberg, 1997; Linial, et al., 1994; Vempala, 2004)。在第3节中，我们开发了随机投影的"神经元"版本，即我们证明了使用单层感知器很容易实现它，其中网络的权重是独立选择的，可以从一类分布中的任何一个分布中选择；这类分布包括离散分布，比如以相等概率选择1或-1。这里我们的定理可以被看作是Johnson \& Lindenstrauss (1984)和Frankl and Maehara (1988)工作的扩展/改进。

---

然后我们讨论了学习一个鲁棒性为$\ell$的概念需要多少个例子才能有效学习。我们从具有$n$个属性的半空间概念类开始。在这种情况下，已知需要$O\left(1 / \ell^{2}\right)$个例子(Bartlett \& Shawe-Taylor, 1998; Vapnik, 1995; Freund \& Schapire, 1999)。在这里，我们证明了一个基于随机投影的简单算法提供了这种保证的另一种证明。

---

接下来我们考虑其他丰富的概念类，即半空间和椭球的交集。使用神经元随机投影，我们证明例子可以首先被投影到一个维度是$\ell$函数的空间中，在某些情况下还需要概念类的额外参数（例如当概念类是半空间交集时的半空间数量等），但不依赖于例子的属性数量。这样就允许我们通过基于VC维的著名泛化定理(Vapnik & Chervonenkis, 1971)，将学习概念所需的例子数量界定为$\ell$的函数，独立于原始属性数量。

---

所提出的算法很快——它们的运行时间对$n$是线性的——因为在随机投影之后（这需要$n$的线性时间），所有工作都发生在具有少量样本点的较小维度空间中。事实上，这表明这里研究的算法可以在SVM中代替当前的解决方案(Cortes & Vapnik, 1995; Freund & Schapire, 1999)，如在称为核空间的对偶空间中的二次优化。

---

在4.4节中，我们提到了算法的抗噪声特性，特别是在相当低的鲁棒性条件下，不可知学习是可能的，并且（等价地）可以找到使错分点数量最小化的假设。

### 1.1. Related work

本文的主要贡献是通过与维度降低的联系提供了一个学习的新视角。这促进了使用小样本量的高效算法。它还为边界分类器(SVM)的$O\left(1 / \epsilon^{2}\right)$样本复杂度界限提供了一个简单直观的理解方式(Bartlett \& Shawe-Taylor, 1998)。它与之前的工作(Schapire et al., 1998)相关，该工作表明泛化误差可以用例子的观测边界来界定（那里使用了一个更精细的边界概念，但在本质上是相似的）。正如我们在5.1节中讨论的，它似乎很好地契合了从计算基础上对认知进行建模的尝试(Valiant, 1998)，并预测了常见的现象：更细微的区分需要更多的例子。从纯计算的角度来看，这些是针对基础学习理论问题的简单新算法，可能具有实用性。

---

在这项工作之后，随机投影在学习理论中有了进一步的应用。Garg, et al. (2002)和Garg and Roth (2003)追求了类似的想法，发展了相关的投影轮廓概念。最近，Balcan, et al. (2004)使用随机投影为核函数提供了一个高效的新解释。Klivans和Servedio (2004)在鲁棒概念的背景下使用多项式阈值函数获得了显著改进的时间界限。具体来说，他们为学习$t$个半空间的交集（和其他函数）提供了更快的算法（但样本复杂度有所增加）。最后，Ben-David, et al. (2002)使用随机投影证明了一个关于半空间学习的有趣的下界。他们证明了即使是常数VC维的"大多数"概念类也不能嵌入到欧几里得空间维度较小或边界较大的半空间中。因此，基于首先转换为半空间的算法在边界或维度方面不能获得太多收益。