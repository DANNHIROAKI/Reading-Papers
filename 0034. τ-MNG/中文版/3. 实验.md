## 7 EXPERIMENTAL EVALUATION

In this section, we evaluate the performance of our proposed techniques. The experiments verify that our method outperforms five current state-of-the-art methods on well-known real-world datasets.

**Datasets and query workloads.** We conduct the experiments on eight real-world datasets, which are widely used for ANN search $[16,17,32,36,53]$. The datasets cover a wide range of applications, including image (SIFT [3], BIGANN [3], DEEP [32], DEEP1B [57], and GIST [3]), text (GLOVE [24] and CRAWL [5]), and audio (MSONG [4]). Table 1 summarizes some characteristics of the datasets, including the dimensionality, the number of data points (\# base), the local intrinsic dimensionality (LID) [1], and the data type. We can observe that the LIDs of the datasets are low. DEEP1B and BIGANN are used in the experiment against dataset size. Following [17, 32, 53], we also use synthetic datasets RAND and GAUSS. RAND is uniformly sampled from the hypercube $[0,1]^{100}$. GAUSS is generated by randomly choosing 10 cluster centers in the hypercube $[0,10]^{100}$, and each cluster follows the Gaussian distribution on each dimension. The query workloads of the real-world and synthetic datasets are given in the datasets and sampled from the datasets, respectively.

---

**Baseline methods.** We compare our method with five current state-of-the-art methods NSG [17], NSSG [16], HNSW [16], DPG [32], and FANNG [20]. NSG and its extension NSSG are the latest RNG-based methods. HNSW is the latest NSWG-based method. DPG is the latest kNNG-based method. FANNG is also an RNG-based method but is the only existing method considering $\delta(q, \bar{v})<\tau$. The source codes of NSG, NSSG, DPG, and HNSW are obtained online. We implement FANNG, as its source code is not published. The code of $\tau$-MNG is built on top of the code of NSG.

---

**Metrics.** The performance metrics follow the previous works [16, 17, 20, 36, 42, 53]. Specifically, we use the recall at $k$ (recall@k) and the relative distance error ( $r$ derr) to measure search accuracy. For a query $q$, recall@ $k=|k A N N s \cap k N N s| / k$, where $k A N N s$ is the set of the approximate $k$-NNs and $k N N s$ is the set of the exact $k$-NNs; $r d e r r=a v g_{i=0}^{k-1}\left(\delta\left(q, i^{t h} A N N\right) / \delta\left(q, i^{t h} N N\right)-1\right)$, where $i^{t h} A N N$ and $i^{t h} N N$ are the $i$-th approximate NN and exact NN of $q$, respectively. We report the average recall@k and $r$ derr of all queries in the workloads. Following [16, 17, 36, 45, 53, 59], we use queries per second (QPS) and number of distance computations (NDC) to measure search efficiency. QPS is the number of queries finished in a second and NDC is the number of distance computations to evaluate a query. We focus on search performance in the high recall region.

---

**Experimental settings.** The experiments are conducted using C++ on a server with a Quad-Core AMD Opteron CPU and 800G RAM. The codes are compiled by g++ 8.5. Following [17, 32, 53], we evaluate the algorithms with a single thread. The larger the size $b$, the higher the recall, but the higher the query latency. Following recent works [31,32,53], we increase $b$ until the target recall is achieved. We focus on $k=100$ in the experiments.

### 7.1 Comparision with the baseline methods

In this experiment, we compare our method with five baseline methods on six datasets. For a fair comparison, we integrate our optimization QEO and two implementation details PDP and PII into all the baseline methods. The results are shown in Fig. 5.

---

From Fig. 5, we can observe that $\tau$-MNG outperforms all the baseline methods. In particular, on DEEP, SIFT, MSONG, GIST, CRAWL, and GLOVE, when recall@100 is $0.95, \tau$-MNG is $\sim 1.1$ to $\sim 1.6$, $\sim 1.2$ to $\sim 2.1, \sim 1.1$ to $\sim 1.6, \sim 1.1$ to $\sim 1.5, \sim 1.2$ to $\sim 2.0$, and $\sim 1.2$ to $\sim 4.1$ times faster than the baseline methods, respectively; when $r d e r r$ is $0.001, \tau$-MNG is $\sim 1.7$ to $\sim 5.2, \sim 1.2$ to $\sim 2.0, \sim 1.2$ to $\sim 1.8, \sim 1.2$ to $\sim 1.7, \sim 1.3$ to $\sim 3.3$, and $\sim 1.4$ to $\sim 5.1$ times faster than the baseline methods, respectively.

### 7.2 Effect of $\tau$

In this experiment, we study the performance of $\tau$-MNG by varying $\tau$. To clearly examine the effect of $\tau$, QEO, PDP, and PII are not used.

Fig. 6 shows the results on the six real-world datasets. From Fig. 6, we observe that the performance of $\tau$-MNG first improves and then deteriorates with the growth of $\tau$. The reason is that the search cost is dominated by the NDC in the search and the expected NDC is bounded by the product of the expected number of search steps and the expected node degree of the PG. If $\tau$ increases, the node degree of $\tau$-MNG grows and the connectivity of $\tau$-MNG is better. The search involves less detour, which reduces NDC. However, if $\tau$ increases further, the node degree becomes too large and the search has to compute distances for more neighbors at each search step, which results in a large NDC. Since the results of NDC vs $r$ derr are consistent with the results of QPS vs recall, we simply present the results of QPS vs recall in following experiments due to space limitations.

---

We further conduct experiments on synthetic datasets GAUSS and RAND with different point densities and deviations to study the behavior of $\tau$-MNG. Fig. 7 shows the results. In Figs. 7(a)-(c), we fix the standard deviation (SD) of GAUSS to be 5. In Fig. 7(d), we tune the value of SD.

---

From Figs. 7(a)-(c), we first observe that $k$-ANN search on GAUSS is faster than RAND. A reason is that the search on RAND has more steps than GAUSS. For example, on the datasets with 10 K points, to achieve recall@ $100=0.95$, the number of search steps on $0-$ MNG of RAND is $\sim 400$ and that on $0-M N G$ of GAUSS is $\sim 90$. Second, we observe that the effect of $\tau$ is more noticeable on GAUSS than RAND. A reason is that the edges among the $k$-NNs of $q$ have a higher chance to be occluded on RAND than GAUSS. Therefore, the decrease of the number of search steps on GAUSS is faster than RAND as $\tau$ increasing. For example, on the datasets with 10 K points, to achieve recall@100 = 0.95, the number of search steps on $0.2-$ MNG of RAND is $\sim 300$ and that on $8-$ MNG of GAUSS is $\sim 50$. Third, we observe that the value of $\tau$, which changes the performance trend of $\tau$-MNG from an improving trend to a deteriorating one, reduces with the growth of point density. A reason is that on dense datasets, the growth of $\tau$-MNG's node degree by increasing $\tau$ is faster than the decrease of the number of search steps.

---

Fig. 7(d) shows the speedup (spd) of $\tau$-MNG on GAUSS with different standard derivations. spd is defined as the QPS of $\tau$-MNG over the QPS of 0 -MNG, when recall@ $100=0.95$. We observe that the performance of $\tau$-MNG first improves and then deteriorates with the growth of $\tau$ for each SD value, but the value of $\tau$, which changes the trend from an improving trend to a deteriorating one, first increases and then decreases with the growth of SD. A reason is that the edge length in $\tau$-MNG first increases and then decreases with the growth of SD. For example, the average edge lengths of 0 -MNG's are $\sim 36, \sim 59, \sim 82$, and $\sim 12$ for GAUSS with SD $3,5,7$, and 10 , respectively.

### 7.3 Performance of QEO

In this experiment, we vary $p$ to study the performance of the query-aware edge occlusion (QEO). $p^{\prime} \%$ and $z$ are set to be 2 and $m / 2$, respectively. To focus on QEO, the partial distance-based pruning and the prefix inner product index techniques are not used in this experiment. Fig. 8 shows the results from the six datasets.

---

Fig. 8 shows that the search performance first increases and then reduces (Fig. 8(b) and 8(d)-8(f)) or remains stable (Fig. 8(a) and 8(c)) with the reduction of $p$. The reason is that with the reduction of $p$, the search occludes more unpromising neighbors at each search step, which reduces NDC. However, if $p$ reduces further, more promising neighbors are occluded and the search has to detour more, which increases NDC.

### 7.4 Performance of PDP and PII

This experiment evaluates the performance of PDP and PII discussed in Section 6.2.2. For space limitations, we just present the results on SIFT in Fig. 9 and the trends on other datasets are similar.

From Fig. 9, we can observe that PDP improves the search efficiency. The reason is that at each search step, many neighbors of the current node can be pruned by the ( $b-1$ )-th node in the priority queue without fully computing their distances from $q$. In particular, when the recall is 0.95 , the QPS of $\tau$-MNG + QEO + PDP is 1.2 times larger than the QPS of $\tau$-MNG + QEO on SIFT. Fig. 9 further shows that the search throughput is greatly improved by PII. In particular, when the recall is 0.95 , the QPS of $\tau$-MNG + QEO + PDP + PII is 2.1 times larger than the QPS of $\tau$-MNG + QEO on SIFT.

### 7.5 Comparison of index size

We examine the index size in this experiment. Fig. 10 shows the results. From Fig. 10, we can observe that $\tau$-MNG with and without PII are only slightly larger than NSG, NSSG, and HNSW, which are the top three baseline methods as compared in Fig. 5. $\tau$-MNG with and without PII are much smaller than FANNG, especially on CRAWL, DEEP, and GLOVE. Therefore, $\tau$-MNG can be comfortably stored in main memory.

### 7.6 Performance against dataset size

In this experiment, we evaluate the performance of $\tau$-MNG against dataset size on DEEP1B and BIGANN. Following the approach of $[16,17,43,53]$ for supporting large datasets, we randomly separate the data points into parts of 10 millions of points. A $\tau$-MNG is built for each part. We perform $k$-ANN search on each part and return the top- $k$ among the results of all parts as the final results. Fig. 11 shows the performance, where the x -axis is the dataset size and the y -axis is the average running time of a query. We observe that the query time of $\tau$-MNG increases linearly with the growth of dataset size. From Fig. 11, we can also observe that the gap between the plots of recall 0.90 and 0.95 is smaller than the gap between the plots of recall 0.95 and recall 0.98 . It is consistent with the observations in previous experiments that the growth of running time is more than a linear function of the growth of recall.

## 8 CONCLUSION

In this paper, we propose a $\tau$-monotonic graph ( $\tau$-MG) for the approximate nearest neighbor search in multi-dimensional databases. The core of $\tau$-MG is a novel edge occlusion rule. When the distance from $q$ to its nearest neighbor in the database is less than a constant $\tau$, the greedy routing on $\tau$-MG
guarantees to find the exact nearest neighbor of $q$ and the expected time complexity of the search is smaller than all existing methods. The expected length of the greedy routing in $\tau$-MG and the expected node degree of $\tau$-MG have been rigorously analyzed and presented in the supplementary materials. For the efficiency of index construction, we propose a $\tau$-monotonic neighborhood graph ( $\tau$-MNG), which is an approximate variant of $\tau$-MG. We further propose an optimization to reduce the number of distance computations in the search on $\tau$-MNG. Our extensive experiments show that our method is effective and outperforms the state-of-the-art ANN search methods on real-world benchmark datasets.
In the future, we plan to incorporate distributed and external-memory ANN search methods into our proposed techniques.

