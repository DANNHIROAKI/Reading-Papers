# 约翰逊 - 林登施特劳斯变换（Johnson-Lindenstrauss Transforms）简介

卡斯珀·本杰明·弗雷克森（Casper Benjamin Freksen）*

2021年3月2日

## 摘要

约翰逊 - 林登施特劳斯变换（Johnson-Lindenstrauss Transforms）是在保留数据关键特征的同时降低数据维度的强大工具，它们已在从机器学习到差分隐私等众多领域得到应用。本说明解释了它们是什么；概述了自20世纪80年代引入以来它们的用途和发展；如果读者希望更深入地探索这些主题，还提供了许多参考文献。

本文曾是我博士论文 [Fre20] 引言的主要部分，但现在已改编为独立内容，希望能为对该主题感兴趣的读者提供一个（有望是不错的）起点。

## 1 原因、内容与方法

### 1.1 问题

考虑以下场景：我们有一些想要处理的数据，但这些数据量太大，例如处理这些数据需要花费太多时间，或者存储这些数据需要占用太多空间。一种解决方案是对数据进行压缩，保留数据中有价值的部分，丢弃其他部分。当然，什么是有价值的，是由我们想要对数据进行的处理来定义的。为了让这个场景更具体，假设我们的数据由高维欧几里得空间（Euclidean space）${\mathbb{R}}^{d}$ 中的向量组成，我们希望找到一种变换，将这些向量嵌入到一个低维空间 ${\mathbb{R}}^{m}$ 中，其中 $m \ll  d$ ，这样我们就可以在这个低维空间中进行数据处理，并且仍然能得到有意义的结果。在这个更具体的场景中，这个问题被称为降维（dimensionality reduction）。

举个例子，假设我们是一个拥有大量文本语料库的图书馆，每当有人归还一本他们喜欢的小说时，我们希望为他们推荐一些类似的小说供其接下来阅读。或者，我们可能希望能够自动将文本分类，例如分为虚构类/非虚构类，或者儿童文学/青少年文学/成人文学。为了能够利用关于相似性搜索和分类的大量研究成果，我们需要对文本进行合适的表示，这里常用的一种表示方法叫做词袋模型。对于包含 $d$ 个不同单词的语言或词汇表，文本 $t$ 的词袋表示是一个向量 $x \in  {\mathbb{R}}^{d}$，其第 $i$ 个元素是第 $i$ 个单词在 $t$ 中出现的次数。例如，如果词汇表仅包含 ["be","is", "not", "or", "question", "that", "the", "to"]，那么文本 "to be or not to be" 就表示为 ${\left( 2,0,1,1,0,0,0,2\right) }^{\top }$。为了捕捉单词的一些上下文信息，我们可以将文本表示为所谓的 $n$ -元语法（${}^{1}$）的计数，$n$ -元语法是由 $n$ 个连续单词组成的序列，例如 "to be or not to be" 的 2 -元语法是 ["to be", "be or", "or not", "not to", "to be"]，我们将这样的 $n$ -元语法词袋表示为 ${\mathbb{R}}^{\left( {d}^{n}\right) }$ 中的一个向量。为了比较两个文本，我们计算这些文本向量之间的距离，因为大部分单词（或 $n$ -元语法）相同的文本向量之间的距离较小 ${}^{2}$。对于像英语这样更现实的语言，有 $d \approx  {171000}$ 个单词 [SW89]，或者在互联网“英语”语境中有 $d \gtrsim  {4790000}$ 个单词 [WZ05]，向量的维度会迅速变得不可行。虽然我们只需要存储单词（或 $n$ -元语法）的非零计数来表示一个向量，但许多数据处理算法依赖于向量的维度 $d$（或 ${d}^{n}$），例如，使用最近邻搜索来查找类似的小说进行推荐 [AI17]，或者使用神经网络对我们的文本进行分类 [Sch18]。如果我们不首先降低数据的维度，这些算法对于我们图书馆的应用场景将不可行。

---

<!-- Footnote -->

*在奥胡斯大学（Aarhus University）计算机科学系工作期间完成。casper@freksen.dk

<!-- Footnote -->

---

一种看似简单的方法是选择坐标的一个子集，例如当数据包含冗余或无关坐标时。这被称为特征选择 [JS08; HTF17; Jam+13a]，并且可以看作是将 ${}^{3}$ 投影到一个轴对齐子空间上，即一个基是 $\left\{  {{e}_{1},\ldots ,{e}_{d}}\right\}$ 的子集的子空间。

我们可以通过从更丰富的向量集中选择基来改进特征选择。例如，在作为降维方法的主成分分析（PCA）[Pea01: Hot33]中，我们让子空间的基为$m$个${X}^{\top }X$的前特征向量（按特征值降序排列），其中$X \in  {\mathbb{R}}^{n \times  d}$的行是我们的$n$个高维向量${}^{4}$。从这个意义上说，这个子空间使数据的方差最大化：第一个特征向量是使方差最大化的轴，后续的特征向量是在与所有先前特征向量正交的条件下使方差最大化的轴[LRU20b; HTF17; Jam+13b]。

但如果我们随机选择一个基会怎样呢？

### 1.2 约翰逊 - 林登斯特劳斯引理

1984年${}^{5}$，人们发现，以高概率将向量投影到随机基上可以近似保持成对距离。为了证明关于从度量空间到${\ell }_{2}$的函数的利普希茨（Lipschitz）延拓的一个定理，约翰逊（Johnson）和林登施特劳斯（Lindenstrauss）[JL84]证明了以下引理。

---

<!-- Footnote -->

${}^{1}$ 这些有时被称为子串（shingles）。

${}^{2}$ 我们可能希望对向量应用某种归一化方法，例如词频 - 逆文档频率（tf - idf）[LRU20a]，这样罕见词的权重会更高，而文本长度的影响会更小。

${}^{3}$ 在此，我们对投影的定义做了一点变通，即我们将投影向量表示为投影所在子空间（选定）基的线性组合的系数，而非该线性组合的结果。如果 $A \in  {\mathbb{R}}^{m \times  d}$ 是一个以子空间基向量为行的矩阵，我们将向量 $x \in  {\mathbb{R}}^{d}$ 的投影表示为 ${Ax} \in  {\mathbb{R}}^{m}$ 的结果，而非 ${A}^{\top }{Ax} \in  {\mathbb{R}}^{d}$ 的结果。

${}^{4}$ 这里假设我们的向量均值为 0，否则应从 $X$ 的每一行中减去我们向量的均值向量。

${}^{5}$ 更准确地说，是在 1982 年，因为那一年耶鲁大学（Yale University）举办了一场特别的“现代分析与概率论会议（Conference in Modern Analysis and Probability）”，但其会议记录于 1984 年出版。

<!-- Footnote -->

---

引理1.1（约翰逊 - 林登斯特劳斯引理 [JL84]）。对于任意的$d \in  {\mathbb{N}}_{1},\varepsilon  \in  \left( {0,1}\right)$和$X \subset  {\mathbb{R}}^{d}$，存在一个函数$f : X \rightarrow  {\mathbb{R}}^{m}$，其中$m = \Theta \left( {{\varepsilon }^{-2}\log \left| X\right| }\right)$，使得对于任意的$x,y \in  X$，

$$
\left| {\parallel f\left( x\right)  - f\left( y\right) {\parallel }_{2}^{2} - \parallel x - y{\parallel }_{2}^{2}}\right|  \leq  \varepsilon \parallel x - y{\parallel }_{2}^{2}. \tag{1}
$$

证明。该证明的要点是首先定义$f\left( x\right)  \mathrel{\text{:=}} {\left( d/m\right) }^{1/2}{Ax}$，其中$A \in  {\mathbb{R}}^{m \times  d}$是一个随机正交矩阵的前$m$行。然后他们证明了$f$以高概率保持任何向量的范数，或者更正式地说，$f$的分布满足以下引理。

引理1.2（分布型约翰逊 - 林登施特劳斯引理 [JL84]）。对于每一个$d \in  {\mathbb{N}}_{1}$和$\varepsilon ,\delta  \in$∈(0,1)，存在一个关于线性函数$f : {\mathbb{R}}^{d} \rightarrow  {\mathbb{R}}^{m}$的概率分布$\mathcal{F}$，其中$m =$≤$\Theta \left( {{\varepsilon }^{-2}\log \frac{1}{\delta }}\right)$，使得对于每一个$x \in  {\mathbb{R}}^{d}$，

$$
\mathop{\Pr }\limits_{{f \sim  \mathcal{F}}}\left\lbrack  {\left| {\parallel f\left( x\right) {\parallel }_{2}^{2} - \parallel x{\parallel }_{2}^{2}}\right|  \leq  \varepsilon \parallel x{\parallel }_{2}^{2}}\right\rbrack   \geq  1 - \delta . \tag{2}
$$

通过选择 $\delta  = 1/{\left| X\right| }^{2}$，我们可以对所有向量对 $x,y \in  X$ 进行联合界估计，并证明它们的距离（即向量 $x - y$ 的 ${\ell }_{2}$ 范数）以至少 $1 - \left( \begin{matrix} \left| X\right| \\  2 \end{matrix}\right) /{\left| X\right| }^{2} > 1/2$ 的概率同时得到保留。

我们将使用术语约翰逊 - 林登施特劳斯分布（Johnson - Lindenstrauss distribution，JLD）来指代引理 1.2 所对应的分布 $\mathcal{F}$，使用术语约翰逊 - 林登施特劳斯变换（Johnson - Lindenstrauss transform，JLT）来指代引理 1.1 所对应的函数 $f$，例如 JLD 的一个样本。

关于这些引理，有几点需要注意：从约翰逊 - 林登施特劳斯分布（JLD）中采样约翰逊 - 林登施特劳斯变换（JLT）时，它与输入向量本身无关；JLT 仅取决于源维度 $d$、向量数量 $\left| X\right|$ 和失真度 $\varepsilon$。这使我们在无法访问输入数据的情况下也能采样 JLT，例如在数据存在之前计算 JLT，或者在数据太大而无法存储在单台机器上或移动到单台机器的情况下计算 JLT ${}^{6}$。其次，目标维度 $m$ 与源维度 $d$ 无关，这意味着在维度方面可能会有非常显著的节省，这一点很快会变得更加明显。

与主成分分析（PCA）相比，约翰逊 - 林登施特劳斯变换（JLT）提供的保证有所不同：主成分分析会找到一种嵌入方式，使原始向量与嵌入向量之间距离的平均失真达到最优，即${A}_{\mathrm{{PCA}}} = \arg \mathop{\min }\limits_{{A \in  {\mathbb{R}}^{m \times  d}}}\mathop{\sum }\limits_{{x \in  X}}{\begin{Vmatrix}{A}^{\top }Ax - x\end{Vmatrix}}_{2}^{2}\left\lbrack  \text{Jol02}\right\rbrack$，而约翰逊 - 林登施特劳斯变换会限制原始空间内距离与嵌入空间内距离之间的最坏情况失真。至于计算变换，执行主成分分析的一种常见方法是计算协方差矩阵，然后进行特征值分解，这会导致运行时间${}^{8}$为$O\left( {\left| X\right| {d}^{2} + {d}^{\omega }}\right)$ [DDH07]，相比之下，约翰逊 - 林登施特劳斯降维（JLD）中的“快速约翰逊 - 林登施特劳斯变换（FJLT）”和“块稀疏约翰逊 - 林登施特劳斯变换（Block SparseJL）”分别可以达到$\Theta \left( {\left| X\right| d\log d}\right)$和${}^{9} \mid  \Theta \left( {\parallel X{\parallel }_{0}{\varepsilon }^{-1}\log \left| X\right| }\right)$的运行时间，这两种方法将在1.4节中介绍。因此，主成分分析和约翰逊 - 林登施特劳斯降维是适用于不同场景的不同工具（例如，参见[Bre+19]，其中在医学成像领域对这两种技术进行了实证比较；另见[Das00; BM01; FB03; FM03; Tan+05; DB06; Arp+14; Woj+16; Bre+20]）。这并不是说这两种方法相互排斥，因为可以先应用约翰逊 - 林登施特劳斯变换快速去除一些维度，然后再应用主成分分析更精细地减少剩余维度[例如，RST09] [HMT11: Xie+16: Yan+20]。关于主成分分析的更多内容，我们建议感兴趣的读者参考[Jol02]，该文献对该主题进行了出色的深入探讨。

---

<!-- Footnote -->

${}^{6}$ 请注意，采样变换仅以一定（较高）概率满足引理1.1。在我们能够获取数据的情况下，我们可以通过重新采样变换来避免这种情况，直到它针对我们的特定数据集满足引理1.1。

${}^{7}$ 还有其他一些方法比这种方法更有效地计算近似主成分分析（PCA），例如 [RST09 AH14]。

<!-- Footnote -->

---

关于约翰逊 - 林登施特劳斯引理（JLDs）和约翰逊 - 林登施特劳斯变换（JLTs），一个自然会提出的问题是目标维度是否为最优。事实的确如此，因为凯恩（Kane）、梅卡（Meka）和纳尔逊（Nelson）[KMN11]以及杰拉姆（Jayram）和伍德拉夫（Woodruff）[JW13]独立地给出了任何满足引理1.2的约翰逊 - 林登施特劳斯引理的匹配下界$m = \Omega \left( {{\varepsilon }^{-2}\log \frac{1}{\delta }}\right)$，并且拉森（Larsen）和纳尔逊（Nelson）[LN17]通过以下定理表明，引理1.1中的界在常数因子范围内对于几乎整个$\varepsilon$范围都是最优的。

定理1.3（[LN17]）。对于任意整数$n,d \geq  2$和${\lg }^{0.5001}n/\sqrt{\min \{ d,n\} } < \varepsilon  < 1$，存在一个大小为$n$的点集$X \subset  {\mathbb{R}}^{d}$，使得任何满足等式(1)的函数$f : X \rightarrow  {\mathbb{R}}^{m}$都必须满足

$$
m = \Omega \left( {{\varepsilon }^{-2}\log \left( {{\varepsilon }^{2}n}\right) }\right) . \tag{3}
$$

注意，如果$\varepsilon  \leq  \sqrt{\lg n/\min \{ d,n\} }$，那么${\varepsilon }^{-2}\lg n \geq  \min \{ d,n\}$，并且分别通过恒等函数或将其投影到$\operatorname{span}\left( X\right)$上，可以等距地将$X$嵌入到维度$\min \{ d,\left| X\right| \}$中。

阿隆（Alon）和克拉塔尔格（Klartag）[AK17]通过给出$\varepsilon$范围内差距的下界，扩展了[LN17]中的结果。

定理1.4（[AK17]）。存在一个绝对正常数$0 < c < 1$，使得对于任何$n \geq  d > {cd} \geq  m$以及所有$\varepsilon  \geq  2/\sqrt{n}$，存在一个大小为$n$的点集$X \subset  {\mathbb{R}}^{d}$，使得任何满足等式（1）的函数$f : X \rightarrow  {\mathbb{R}}^{m}$必定满足

$$
m = \Omega \left( {{\varepsilon }^{-2}\log \left( {2 + {\varepsilon }^{2}n}\right) }\right) . \tag{4}
$$

然而，通过限制我们对其应用约翰逊 - 林登施特劳斯引理（JLTs）的输入向量集，可以规避这些下界。例如，克拉尔塔格（Klartag）和门德尔松（Mendelson）[KM05]、迪克森（Dirksen）[Dir16]，以及布尔甘（Bourgain）、迪克森（Dirksen）和纳尔逊（Nelson）[BDN15b]为依赖于输入集统计特性 $X$ 的约翰逊 - 林登施特劳斯引理（JLTs）提供了目标维度上界。类似地，使用 $m = \Theta \left( {{\varepsilon }^{-2}t\log \left( {t/\varepsilon }\right) }\right)$，约翰逊 - 林登施特劳斯引理（JLTs）可用于同时近似保留整个子空间的成对距离，其中 $t$ 表示子空间的维度 [Sar06]，当 $t \ll  \left| X\right| ,d$ 时，这是一个很大的改进。

约翰逊 - 林登施特劳斯变换（JLT）的另一个有用性质是，它们能近似保留点积。推论1.5依据引理1.1对这一性质进行了形式化表述，不过有时[Sar06, AV06]会依据引理1.2来表述。与引理1.1相比，推论1.5对$f$和$X$有一些额外要求，但如果约翰逊 - 林登施特劳斯变换（JLT）是从约翰逊 - 林登施特劳斯分布（JLD）中采样得到的，或者如果我们将所有向量的负向量添加到$X$中（这只会略微增加目标维度），那么这些要求就不成问题。

---

<!-- Footnote -->

${}^{8}$ 这里的$\omega  \lesssim  {2.373}$是来自方阵乘法运行时间的指数[Wil12,Le 14]。

${}^{9}$ 这里 $\parallel X{\parallel }_{0}$ 是向量集 $X$ 中非零元素的总数，即 $\parallel X{\parallel }_{0} \mathrel{\text{:=}} \mathop{\sum }\limits_{{x \in  X}}\parallel x{\parallel }_{0}$ ，其中对于向量 $x$ 有 $\parallel x{\parallel }_{0} \mathrel{\text{:=}} \left| \left\{  {i \mid  {x}_{i} \neq  0}\right\}  \right|$ 。

<!-- Footnote -->

---

推论1.5。设 $d,\varepsilon ,X$ 和 $f$ 如引理1.1中所定义，此外，设 $f$ 是线性的。那么对于每个 $x,y \in  X$ ，如果 $- y \in  X$ ，则

$$
\left| {\langle f\left( x\right) ,f\left( y\right) \rangle -\langle x,y\rangle }\right|  \leq  \varepsilon \parallel x{\parallel }_{2}\parallel y{\parallel }_{2}. \tag{5}
$$

证明。如果 $x$ 和 $y$ 中至少有一个是零向量，那么由于 $f$ 是线性的，等式 (5) 显然成立。如果 $x$ 和 $y$ 都是单位向量，那么不失一般性，我们假设 $\parallel x + y{\parallel }_{2} \geq  \parallel x - y{\parallel }_{2}$，然后利用极化恒等式 $4\langle u,v\rangle  = \parallel u + v{\parallel }_{2}^{2} - \parallel u - v{\parallel }_{2}^{2}$ 按如下步骤进行。

$$
4\left| {\langle f\left( x\right) ,f\left( y\right) \rangle -\langle x,y\rangle }\right|  = \left| {\parallel f\left( x\right)  + f\left( y\right) {\parallel }_{2}^{2} - \parallel f\left( x\right)  - f\left( y\right) {\parallel }_{2}^{2} - 4\langle x,y\rangle }\right| 
$$

$$
 \leq  \left| {\left( {1 + \varepsilon }\right) \parallel x + y{\parallel }_{2}^{2} - \left( {1 - \varepsilon }\right) \parallel x - y{\parallel }_{2}^{2} - 4\langle x,y\rangle }\right| 
$$

$$
 = \left| {4\langle x,y\rangle  + \varepsilon \left( {\parallel x + y{\parallel }_{2}^{2} + \parallel x - y{\parallel }_{2}^{2}}\right)  - 4\langle x,y\rangle }\right| 
$$

$$
 = \varepsilon \left( {2\parallel x{\parallel }_{2}^{2} + 2\parallel y{\parallel }_{2}^{2}}\right) 
$$

$$
 = {4\varepsilon }\text{.}
$$

否则，我们可以将其简化为单位向量的情况。

$$
\left| {\langle f\left( x\right) ,f\left( y\right) \rangle -\langle x,y\rangle }\right|  = \left| {\left\langle  {f\left( \frac{x}{\parallel x{\parallel }_{2}}\right) ,f\left( \frac{y}{\parallel y{\parallel }_{2}}\right) }\right\rangle   - \left\langle  {\frac{x}{\parallel x{\parallel }_{2}},\frac{y}{\parallel y{\parallel }_{2}}}\right\rangle  }\right| \parallel x{\parallel }_{2}\parallel y{\parallel }_{2}
$$

$$
 \leq  \varepsilon \parallel x{\parallel }_{2}\parallel y{\parallel }_{2}.
$$

2. 米

在第 1.4 节概述联合低维嵌入（JLDs）的发展之前，让我们回到第 1.1 节的场景和示例，展示通过约翰逊 - 林登施特劳斯引理（JLTs）进行降维已得到应用的广泛领域。此外，为了让我们更熟悉引理 1.1 及其相关概念，我们将选取几个该引理的应用示例。

### 1.3 约翰逊 - 林登斯特劳斯（Johnson - Lindenstrauss）的用途（实用性）

约翰逊 - 林登斯特劳斯变换（JLDs）和约翰逊 - 林登斯特劳斯引理（JLTs）在许多领域和任务中得到了应用，并存在相似之处，下面我们将列举其中一些。请注意，以下类别之间存在一些重叠，例如，文献[FB03]使用约翰逊 - 林登斯特劳斯变换（JLD）为一组弱学习器学习高斯混合聚类，而文献[PW15]以一种能提供差分隐私保证的方式解决凸优化问题。

最近邻搜索多次受益于约翰逊 - 林登斯特劳斯引理，包括文献[Kle97; KOR00]，它们使用约翰逊 - 林登斯特劳斯引理（JL）对空间进行随机划分而非降维，而其他文献[AC09; HIM12]则更直接地利用了约翰逊 - 林登斯特劳斯引理（JL）的降维特性。这些结果的变体包括构建局部敏感哈希方案[Dat+04]以及在无漏检的情况下查找最近邻[SW17]。

聚类在多个子领域产生了成果，如高斯混合模型（mixture of Gaussians）[Das99; FB03; UDS07]、子空间聚类（subspace clustering）[HTB17]、图聚类（graph clustering）[SI09; Guo+20]、自组织映射（self-organising maps）[RK89; Kas98]和k均值聚类（k-means）[Bec+19; Coh+15; Bou+14; Liu+17; SF18]，这些将在1.3.1节中详细解释。

离群点检测方面，针对各种离群点设置已有相关研究，包括近似最近邻（approximate nearest-neighbours）[dVCH10; SZK15]和高斯向量（Gaussian vectors）[NC20]。同时，Zha等人在2020年的研究（Zha+20）将约翰逊 - 林登斯特劳斯引理（JL）用作分布式计算模型中一系列离群点检测算法的预处理器，而[AP12]评估了约翰逊 - 林登斯特劳斯变换（JLTs）在文本文档离群点检测中的应用。

集成学习中，独立的约翰逊 - 林登斯特劳斯变换（Johnson - Lindenstrauss Transform，JLT）可用于为装袋法（bagging）的弱学习器生成训练集[SR09]，并且学习器之间的投票会根据给定的JLT对数据的投影效果进行加权[CS17; Can20]。JLT与多个学习器的结合也被用于从少量数据点中学习高维分布的场景（即$\left| X\right|  \ll  d$）[DK13; ZK19; Niy+20]。

对抗式机器学习中，约翰逊 - 林登斯特劳斯变换（Johnson - Lindenstrauss）既可以用于防御对抗性输入$\left\lbrack  {\mathrm{{Ngu}} + {16};\text{Wee} + {19};\text{Tar} + {19}}\right\rbrack$，也可以帮助设计此类攻击[Li+20]。

其他机器学习领域中，除了上述更具体的机器学习主题外，约翰逊 - 林登斯特劳斯引理（Johnson - Lindenstrauss）已与支持向量机[CJS09; Pau + 14; LL20]、费舍尔线性判别法（Fisher's linear discriminant）[DK10]和神经网络[Sch18]结合使用，而[KY20]则使用JL引理来促进分布式环境中的随机梯度下降。

数值线性代数领域的研究主要集中在低秩近似[Coh + 15; MM20]、典型相关分析（canonical correlation analysis）[Avr + 14]，以及局部[THM17; MM09; Kab14; Sla17]和分布式[HMM16]计算模型中的回归问题。此外，由于这些子领域相互关联，一些论文会同时处理多个数值线性代数问题，例如低秩近似、回归和近似矩阵乘法[Sar06]，并且一系列研究[MM13; CW17; NN13a]使用约翰逊 - 林登斯特劳斯嵌入（JLDs）进行子空间嵌入，进而得到了用于${\ell }_{p}$回归、低秩近似和杠杆得分的算法。

如需进一步阅读，有综述文献[Mah11; HMT11; Woo14]涵盖了约翰逊 - 林登斯特劳斯变换（Johnson - Lindenstrauss Transform，JLD）在数值线性代数中的大部分应用。

凸优化领域中，约翰逊 - 林登斯特劳斯变换已被用于（整数）线性规划[VPL15]，并用于改进一种使用分离预言机在凸集中寻找点的割平面法[TSC15; Jia + 20]。此外，文献[Zha + 13]研究了如何从经约翰逊 - 林登斯特劳斯降维后的解中恢复高维优化解。

差分隐私利用约翰逊 - 林登斯特劳斯变换为方差估计[Blo + 12]、回归[She19; SKD19;

ZLW09]、欧几里得距离估计[Ken + 13: LKR06: GLK13: Tur + 08: Xu + 17]和低秩分解[Upa18]等线性代数问题提供净化后的解，以及用于凸优化[PW15; KJ16]、协同过滤[Yan + 17]和图割查询的解[Blo + 12; Upa13]。此外，文献[Upa15]针对差分隐私分析了各种约翰逊 - 林登斯特劳斯变换，并为此目的引入了一种新颖的变换。

神经科学领域，它被用作计算神经科学中处理数据的工具[GS12][ALG13]，同时也作为对神经过程进行建模的一种方式[GS12; ALG13; All+14; PP14]。有趣的是，有一些证据[MFL08; SA09; Car+13]表明，类似约翰逊 - 林登斯特劳斯（Johnson - Lindenstrauss，JL）的操作在自然界中也会出现，因为果蝇大脑中大量的嗅觉感觉输入（投射神经元）会映射到数量较少的一组神经元（肯扬细胞）上，其中每个肯扬细胞都与投射神经元的一个小的、看似随机的子集相连。这让人想起稀疏的JL构造，相关内容将在1.4.1节中介绍。不过，我在神经科学方面的能力还不足以判断生物构造与随机线性代数之间的这些相似性能延伸到何种程度。

约翰逊 - 林登斯特劳斯（Johnson - Lindenstrauss）已得到应用的其他主题包括图稀疏化[SS11]、图在欧几里得空间中的嵌入[FM88]、集成电路设计[Vem98]、生物特征认证[Arp+14]以及近似最小生成树[HI00]。

有关约翰逊 - 林登斯特劳斯（Johnson - Lindenstrauss）用例的更多示例，请参阅[Ind01; Vem04]。

现在，让我们更深入地探讨聚类和流算法领域，看看约翰逊 - 林登斯特劳斯（Johnson - Lindenstrauss）引理在这些领域如何应用。

#### 1.3.1 聚类

聚类可以定义为对数据集进行划分，使得同一分区内的元素彼此相似，而与其他分区内的元素不同。一个经典的聚类问题是所谓的 $k$ - 均值聚类，其中数据集 $X \subset  {\mathbb{R}}^{d}$ 由欧几里得空间中的点组成。任务是选择 $k$ 个聚类中心 ${c}_{1},\ldots ,{c}_{k}$，使得数据点到其最近聚类中心的距离平方和最小，即

$$
\underset{{c}_{1},\ldots ,{c}_{k}}{\arg \min }\mathop{\sum }\limits_{{x \in  X}}\mathop{\min }\limits_{i}{\begin{Vmatrix}x - {c}_{i}\end{Vmatrix}}_{2}^{2} \tag{6}
$$

这会创建一个沃罗诺伊分割（Voronoi partition），因为每个数据点都会被分配到与其最近的聚类中心相对应的分割中。我们用 ${X}_{i} \subseteq  X$ 表示那些以 ${c}_{i}$ 为其最近中心的点的集合。众所周知，对于中心的最优选择，这些中心是其对应分割的均值，此外，任何中心选择的代价都不会低于数据点到其分配分割的均值的距离平方和，即

$$
\mathop{\sum }\limits_{{x \in  X}}\mathop{\min }\limits_{i}{\begin{Vmatrix}x - {c}_{i}\end{Vmatrix}}_{2}^{2} \geq  \mathop{\sum }\limits_{{i = 1}}^{k}\mathop{\sum }\limits_{{x \in  {X}_{i}}}{\begin{Vmatrix}x - \frac{1}{\left| {X}_{i}\right| }\mathop{\sum }\limits_{{y \in  {X}_{i}}}y\end{Vmatrix}}_{2}^{2}. \tag{7}
$$

研究表明，即使对于$k = 2$，寻找最优中心也是NP难问题[阿洛等人09年；达斯08年]；然而，各种启发式方法已取得成功，例如常用的劳埃德算法[劳埃德82年]。在劳埃德算法中，以某种方式初始化中心后，我们通过将每个数据点分配给其最近的中心，然后将中心更新为分配给它的数据点的均值，来迭代地改进中心的选择。然后可以重复这两个步骤，直到满足某个终止条件，例如当中心收敛时。如果我们用$t$表示迭代次数，那么运行时间变为$O\left( {t\left| X\right| {kd}}\right)$，因为我们每次迭代使用$O\left( {\left| X\right| {kd}}\right)$的时间将每个数据点分配给其最近的中心。我们可以通过使用约翰逊 - 林登施特劳斯变换（JLT）将数据点快速嵌入到低维空间，然后在这个较小的空间中运行劳埃德算法来改善这个运行时间。我们稍后将介绍的快速约翰逊 - 林登施特劳斯变换，对于许多参数集，可以在$O\left( {d\log d}\right)$的时间内嵌入一个向量，从而将总运行时间减少到$O\left( {\left| X\right| d\log d + t\left| X\right| k{\varepsilon }^{-2}\log \left| X\right| }\right)$。然而，要使这一方法有用，我们需要低维空间中点的划分与原始高维空间中（几乎）同样好的划分相对应。

为了证明这一结果，我们将使用以下引理，该引理表明，若将分区的中心选为各分区的均值，则分区的代价可以用分区中数据点之间的成对距离来表示。

引理1.6。设$k,d \in  {\mathbb{N}}_{1}$且对于$\overline{10}i \in  \left\lbrack  k\right\rbrack$有${X}_{i} \subset  {\mathbb{R}}^{d}$。

$$
\mathop{\sum }\limits_{{i = 1}}^{k}\mathop{\sum }\limits_{{x \in  {X}_{i}}}{\begin{Vmatrix}x - \frac{1}{\left| {X}_{i}\right| }\mathop{\sum }\limits_{{y \in  {X}_{i}}}y\end{Vmatrix}}_{2}^{2} = \frac{1}{2}\mathop{\sum }\limits_{{i = 1}}^{k}\frac{1}{\left| {X}_{i}\right| }\mathop{\sum }\limits_{{x,y \in  {X}_{i}}}\parallel x - y{\parallel }_{2}^{2}. \tag{8}
$$

引理1.6的证明涉及各种线性代数运算，具体证明过程可在2.1节中找到。现在我们准备证明以下命题，该命题指出，如果我们在低维空间中找到一个代价在最优代价的$\left( {1 + \gamma }\right)$范围内的分区，那么当将该分区移回到高维空间时，其代价在高维空间最优代价的$\left( {1 + {4\varepsilon }}\right) \left( {1 + \gamma }\right)$范围内。

命题1.7。设$k,d \in  {\mathbb{N}}_{1},X \subset  {\mathbb{R}}^{d},\varepsilon  \leq  1/2,m = \Theta \left( {{\varepsilon }^{-2}\log \left| X\right| }\right)$ ，且$f : X \rightarrow  {\mathbb{R}}^{m}$ 为一个JLT（联合线性变换，Joint Linear Transformation）。设$Y \subset  {\mathbb{R}}^{m}$ 为$X$ 的嵌入。设${\kappa }_{m}^{ * }$ 表示关于等式(6)对$Y$ 进行划分的最优成本。设${Y}_{1},\ldots ,{Y}_{k} \subseteq  Y$ 为$Y$ 的一个成本为${\kappa }_{m}$ 的划分，使得对于某个$\gamma  \in  \mathbb{R}$ 有${\kappa }_{m} \leq  \left( {1 + \gamma }\right) {\kappa }_{m}^{ * }$ 。设${\kappa }_{d}^{ * }$ 为对$X$ 进行最优划分的成本，${\kappa }_{d}$ 为划分${X}_{1},\ldots ,{X}_{k} \subseteq  X$ 的成本，满足${Y}_{i} = \left\{  {f\left( x\right)  \mid  x \in  {X}_{i}}\right\}$ 。那么

$$
{\kappa }_{d} \leq  \left( {1 + {4\varepsilon }}\right) \left( {1 + \gamma }\right) {\kappa }_{d}^{ * }. \tag{9}
$$

证明。根据引理1.6以及$f$是一个约翰逊 - 林登施特劳斯变换（Johnson - Lindenstrauss Transform，JLT）这一事实，我们知道当回到高维空间时，我们的划分成本大致保持不变，即${\kappa }_{d} \leq  {\kappa }_{m}/\left( {1 - \varepsilon }\right)$。此外，由于${X}^{\prime }$嵌入到$Y$时的最优划分成本不能低于划分$Y$的最优成本，我们可以得出${\kappa }_{m}^{ * } \leq  \left( {1 + \varepsilon }\right) {\kappa }_{d}^{ * }$。因为$\varepsilon  \leq  1/2$，我们有$1/\left( {1 - \varepsilon }\right)  = 1 + \varepsilon /\left( {1 - \varepsilon }\right)  \leq  1 + {2\varepsilon }$，并且还有$\left( {1 + \varepsilon }\right) \left( {1 + {2\varepsilon }}\right)  = \left( {1 + {3\varepsilon } + 2{\varepsilon }^{2}}\right)  \leq  1 + {4\varepsilon }$。

---

<!-- Footnote -->

${}^{10}$ 我们使用 $\left\lbrack  k\right\rbrack$ 来表示集合 $\{ 1,\ldots ,k\}$。

<!-- Footnote -->

---

将这些不等式组合起来，我们得到

$$
{\kappa }_{d} \leq  \frac{1}{1 - \varepsilon }{\kappa }_{m}
$$

$$
 \leq  \left( {1 + {2\varepsilon }}\right) \left( {1 + \gamma }\right) {\kappa }_{m}^{ * }
$$

$$
 \leq  \left( {1 + {2\varepsilon }}\right) \left( {1 + \gamma }\right) \left( {1 + \varepsilon }\right) {\kappa }_{d}^{ * }
$$

$$
 \leq  \left( {1 + {4\varepsilon }}\right) \left( {1 + \gamma }\right) {\kappa }_{d}^{ * }\text{.}
$$

通过将常数移到$\Theta$符号内，命题1.7表明，我们可以用$m = \Theta \left( {{\varepsilon }^{-2}\log \left| X\right| }\right)$实现对$k$ -均值的$\left( {1 + \varepsilon }\right)$近似（${}^{11}$）。然而，通过更仔细地分析所需的性质，我们可以在$k \ll  \left| X\right|$的情况下对这一结果进行改进。布蒂西迪斯等人（Boutsidis et al. [Bou+14]）表明，将维度投影到目标维度$m = \Theta \left( {{\varepsilon }^{-2}k}\right)$足以实现稍差的$k$ -均值近似因子$\left( {2 + \varepsilon }\right)$。科恩等人（Cohen et al. [Coh+15]）从两个方面扩展了这一结果，他们表明，投影到$m = \Theta \left( {{\varepsilon }^{-2}k}\right)$可实现$\left( {1 + \varepsilon }\right)$近似比，而一直投影到$m = \Theta \left( {{\varepsilon }^{-2}\log k}\right)$仍足以实现$\left( {9 + \varepsilon }\right)$近似比。最近，贝凯蒂等人（Becchetti et al. [Bec+19]）进一步改进了$\left( {1 + \varepsilon }\right)$的情况，他们表明，当投影到$m = \Theta \left( {{\varepsilon }^{-6}\left( {\log k + \log \log \left| X\right| }\right) \log {\varepsilon }^{-1}}\right)$时，可以实现$k$ -均值的$\left( {1 + \varepsilon }\right)$近似比；马卡里切夫、马卡里切夫和拉赞施泰因（Makarychev, Makarychev, and Razenshteyn [MMR19]）也独立证明了一个更好的界$m = \Theta \left( {{\varepsilon }^{-2}\log k/\varepsilon }\right)$，本质上相对于[Coh+15]给出了一个“两全其美”的结果。

关于$k$均值聚类（$k$ -means clustering）的历史概述，我们建议感兴趣的读者参考[Boc08]。

#### 1.3.2 流式处理

流算法领域的特点在于处理这样一类问题：我们会接收到一个项目序列（或流），并需要对目前已接收的项目进行查询。主要限制通常是我们只能有限地访问该序列，例如，只允许对其进行一次遍历，并且我们的可用空间非常有限，例如，空间复杂度为流长度的多项式对数级。为了弥补这些限制，我们可以对查询给出近似答案。我们在这里要研究的流问题子类是那些只允许对序列进行一次遍历的问题，其中项目是对向量的更新，而查询是关于该向量的某种统计信息，例如向量的${\ell }_{2}$范数。更正式地说，为了引入相关符号，设$d \in  {\mathbb{N}}_{1}$为不同项目的数量，设$T \in  {\mathbb{N}}_{1}$为更新流$\left( {{i}_{j},{v}_{j}}\right)  \in  \left\lbrack  d\right\rbrack   \times  \mathbb{R}$（其中$j \in  \left\lbrack  T\right\rbrack$ ）的长度，并将时间$t$时的向量$x$定义为${x}^{\left( t\right) } \mathrel{\text{:=}} \mathop{\sum }\limits_{{j = 1}}^{t}{v}_{j}{e}_{{i}_{j}}$ 。那么在时间$t$的查询$q$就是${x}^{\left( t\right) }$的一个函数，并且在提及当前时间时，我们将省略${}^{\left( t\right) }$上标。

关于更新，此模型有几种常见的变体。在收银机模型（cash register model）或仅插入模型中，$x$ 仅通过有界整数递增，即对于某个 $M \in  {\mathbb{N}}_{1}$，有 ${v}_{j} \in  \left\lbrack  M\right\rbrack$。在旋转门模型（turnstile model）中，$x$ 只能通过有界整数递增或递减，即对于某个 $M \in  {\mathbb{N}}_{1}$，有 ${v}_{j} \in  \{  - M,\ldots ,M\}$，而严格旋转门模型与旋转门模型类似，但有额外的约束条件，即 $x$ 的元素始终是非负的，即对于所有的 $t \in  \left\lbrack  T\right\rbrack$ 和 $i \in  \left\lbrack  d\right\rbrack$，有 ${x}_{i}^{\left( t\right) } \geq  0$。

---

<!-- Footnote -->

${}^{11}$ 这里的近似比是在高维原始数据上运行的任何 $k$ -均值算法与在低维投影数据上运行的该算法之间的比值。

<!-- Footnote -->

---

如上所述，我们通常会受到空间限制，因此无法显式存储 $x$ 。克服这一限制的关键思路是存储 $x$ 的线性概要，即存储 $y \mathrel{\text{:=}} f\left( x\right)$ ，其中 $f : {\mathbb{R}}^{d} \rightarrow  {\mathbb{R}}^{m}$ 是一个线性函数且 $m \ll  d$ ，然后通过对 $y$ 而非 $x$ 应用某个函数来回答查询。请注意，由于 $f$ 是线性的，我们可以分别对每个更新应用它，并将 $y$ 计算为概要更新的总和。此外，我们可以通过将不同的概要相加来聚合来自不同流的结果，从而使我们能够对流式算法进行分布式计算。

在这种情况下，相关的约翰逊 - 林登斯特劳斯引理（Johnson - Lindenstrauss lemma）是引理1.2，因为使用约翰逊 - 林登斯特劳斯变换（JLD）可以获得线性性质，并且能够在查看任何数据之前对约翰逊 - 林登斯特劳斯变换（JLT）进行采样，但代价是引入了一定的失败概率。

基于JLD（联合拉普拉斯分布，Joint Laplace Distribution），要解决的最自然的流式问题是旋转门模型（turnstile model）中的二阶频率矩估计，即近似$\parallel x{\parallel }_{2}^{2}$，这在数据库查询优化[阿洛等人2002年；WDJ 1991年；德韦等人1992年]和网络数据分析[吉尔等人2001年；CG 2005年]等领域有应用。简单地让$f$为来自JLD的一个样本，并在查询时返回$\parallel f\left( x\right) {\parallel }_{2}^{2}$，使用$O\left( {{\varepsilon }^{-2}\log \frac{1}{\delta } + \left| f\right| }\right)$个存储单元（单词，words）${}^{12}$的空间，可得到一个因子为$\left( {1 \pm  \varepsilon }\right)$的近似结果，失败概率为$\delta$，其中$\left| f\right|$表示存储和应用$f$所需的存储单元数量。然而，流式文献中采用的方法是使用$O\left( {{\varepsilon }^{-2} + \left| f\right| }\right)$个存储单元的空间以恒定误差概率估计$\parallel x{\parallel }_{2}^{2}$，然后对$O\left( {\log \frac{1}{\delta }}\right)$个JLT（约翰逊 - 林登施特劳斯变换，Johnson - Lindenstrauss Transform）${f}_{1},\ldots ,{f}_{O\left( {\log 1/\delta }\right) } : {\mathbb{R}}^{d} \rightarrow  {\mathbb{R}}^{O\left( {\varepsilon }^{-2}\right) }$进行采样，并以${\operatorname{median}}_{k}{\begin{Vmatrix}{f}_{k}\left( x\right) \end{Vmatrix}}_{2}^{2}$响应查询，这将误差概率降低到$\delta$。与使用单个更大的JLT相比，这种方法在分析上更简单，并且（在计数草图，Count Sketch的情况下）嵌入效率更高，但代价是不能嵌入到${\ell }_{2}$中，而这在一些流式处理之外的应用中是需要的。在这种设置下，任务在于构建空间高效的JLT，这里的开创性工作是AMS草图（也称为AGMS草图、拔河草图，Tug - of - War Sketch）[AMS 1999年；阿洛等人2002年]，其JLT可定义为${f}_{i} \mathrel{\text{:=}} {m}^{-1/2}{Ax}$，其中$A \in  \{  - 1,1{\} }^{m \times  d}$是一个随机矩阵。关键思想是$A$的每一行$r$可以由一个只需4 - 独立的哈希函数${\sigma }_{r} : \left\lbrack  d\right\rbrack   \rightarrow  \{  - 1,1\}$支持，这意味着对于任意4个不同的键$\left\{  {{k}_{1},\ldots {k}_{4}}\right\}   \subset  \left\lbrack  d\right\rbrack$和4个（不一定不同）的值${v}_{1},\ldots {v}_{4} \in  \{  - 1,1\}$，这些键哈希到那些值的概率为$\mathop{\Pr }\limits_{{\sigma }_{r}}\left\lbrack  {\mathop{\bigwedge }\limits_{i}{\sigma }_{r}\left( {k}_{i}\right)  = {v}_{i}}\right\rbrack   = {\left| \{  - 1,1\} \right| }^{-4}$。例如${}^{13}$，可以通过将${\sigma }_{r}$实现为具有随机系数的三次多项式模一个足够大的素数来实现[WC 1981年]，因此这样的JLT只需要使用$\mathcal{O}\left( {\varepsilon }^{-2}\right)$个存储单元的空间。用这样的JLT嵌入一个缩放的标准单位向量需要$O\left( {\varepsilon }^{-2}\right)$的时间，导致AMS草图的总体更新时间为$O\left( {{\varepsilon }^{-2}\log \frac{1}{\delta }}\right)$。

AMS草图（AMS Sketch）的一个后续改进是所谓的快速AGMS草图（Fast-AGMS Sketch）[CG05]，也称为计数草图（Count Sketch）[CCF04; TZ12]，它对约翰逊 - 林登施特劳斯变换（JLTs）进行了稀疏化处理，使得它们矩阵表示中的每一列只有一个非零元素。每个JLT可以由一个两两独立的哈希函数$h : \left\lbrack  d\right\rbrack   \rightarrow  \left\lbrack  {O\left( {\varepsilon }^{-2}\right) }\right\rbrack$来选择每个非零元素的位置，以及一个4 - 独立的哈希函数$\sigma  : \left\lbrack  d\right\rbrack   \rightarrow  \{  - 1,1\}$像之前一样来选择随机符号。这将标准单位向量嵌入时间减少到$O\left( 1\right)$，因此计数草图的总体更新时间变为$O\left( {\log \frac{1}{\delta }}\right)$。需要注意的是，计数草图中的约翰逊 - 林登施特劳斯维数约减（JLD）也被称为特征哈希（Feature Hashing），我们将在1.4.1节中再讨论它。

---

<!-- Footnote -->

${}^{12}$ 这里我们假设一个字（word）足够大，能够对我们使用的任何实数进行充分近似，并且能够存储数据流中的一个数字，即如果 $w$ 表示一个字中的位数，那么 $w = \Omega \left( {\log d + \log M}\right)$ 。

${}^{13}$ 例如，关于 $k$ -wise 独立哈希函数的其他族，请参阅 [TZ12]。

<!-- Footnote -->

---

尽管由于使用了非线性中位数而无法嵌入到 ${\ell }_{2}$ 中，但 AMS 草图（AMS Sketch）和计数草图（Count Sketch）与推论 1.5 [CG05，定理 2.1 和定理 3.5] 类似，近似地保留了点积。这使我们能够查询任何特定项的（近似）频率，如下所示

$$
\mathop{\operatorname{median}}\limits_{k}\left\langle  {{f}_{k}\left( x\right) ,{f}_{k}\left( {e}_{i}\right) }\right\rangle   = \left\langle  {x,{e}_{i}}\right\rangle   \pm  \varepsilon \parallel x{\parallel }_{2}{\begin{Vmatrix}{e}_{i}\end{Vmatrix}}_{2} = {x}_{i} \pm  \varepsilon \parallel x{\parallel }_{2}
$$

概率至少为 $1 - \delta$ 。

这可以扩展到在仅插入流中查找频繁项[CCF04]。其思路是使用一个稍大的${}^{14}$计数草图（Count Sketch）实例来维护一个堆，该堆包含到目前为止流中$k$个近似最频繁的项。也就是说，如果我们用${i}_{k}$表示第$k$个最频繁的项（即$\left| \left\{  {j \mid  {x}_{j} \geq  {x}_{{i}_{k}}}\right\}  \right|  = k$），那么对于我们堆中的每个项$j$，以概率$1 - \delta$有${x}_{j} > \left( {1 - \varepsilon }\right) {x}_{{i}_{k}}$。

关于流算法的更多内容，我们建议读者参考[Mut05]和[Nel11]，它们还将流与约翰逊 - 林登斯特劳斯（Johnson - Lindenstrauss）变换联系起来。

### 1.4 约翰逊 - 林登斯特劳斯变换的图景

他们惊讶于星星

—第32幕，贝叶挂毯（Bayeux Tapestry）[Unk70]

如1.2节所述，文献[JL84]中最初的约翰逊 - 林登斯特劳斯分布（Johnson - Lindenstrauss distribution，JLD）是函数$f : {\mathbb{R}}^{d} \rightarrow  {\mathbb{R}}^{m}$上的一个分布，其中${}^{15}f\left( x\right)  = {\left( d/m\right) }^{1/2}{Ax}$，且$A$是一个随机$m \times  d$矩阵，其行构成${\mathbb{R}}^{d}$的某个$m$维子空间的一组标准正交基，即这些行向量是单位向量且两两正交。虽然约翰逊（Johnson）和林登斯特劳斯（Lindenstrauss）[JL84]证明了$m = \Theta \left( {{\varepsilon }^{-2}\log \left| X\right| }\right)$足以证明引理1.1，但他们并未给出大$O$表达式中常数的任何界。文献$\left| {\overline{\mathrm{{FM}}}{88}}\right|$弥补了这一不足，证明了若$m < \sqrt{\left| X\right| }$，则$m = \left\lceil  {9{\left( {\varepsilon }^{2} - 2{\varepsilon }^{3}/3\right) }^{-1}\ln \left| X\right| }\right\rceil   + 1$对于相同的约翰逊 - 林登斯特劳斯分布（JLD）是足够的。文献[FM90]进一步改进了这个界，去掉了$m < \sqrt{\left| X\right| }$的限制并将界降低到$m = \left\lceil  {8{\left( {\varepsilon }^{2} - 2{\varepsilon }^{3}/3\right) }^{-1}\ln \left| X\right| }\right\rceil$。

JL研究的下一个方向致力于简化JLD构造，因为英迪克（Indyk）和莫特瓦尼（Motwani）[HIM12]证明了从适当缩放的高斯分布中独立同分布地对矩阵的每个元素进行采样可得到一个JLD。这种矩阵的行并不构成一个基，因为它们大概率不是正交的；然而，文献中仍然将这种以及其他大多数JLD称为随机投影。此后不久，阿里亚加（Arriaga）和文帕拉（Vempala）[AV06]通过从拉德马赫（Rademacher）${}^{16}$分布中独立同分布地采样构造了一个JLD，阿赫利奥普塔斯（Achlioptas）[Ach03]对拉德马赫构造进行了稀疏化处理，使得元素${a}_{ij}$以$\Pr \left\lbrack  {{a}_{ij} = 0}\right\rbrack   =$ $2/3$和$\Pr \left\lbrack  {{a}_{ij} =  - 1}\right\rbrack   = \Pr \left\lbrack  {{a}_{ij} = 1}\right\rbrack   = 1/6$的概率独立同分布地采样。我们将这种稀疏的独立同分布的拉德马赫构造称为阿赫利奥普塔斯构造。高斯和拉德马赫的结果后来得到了推广[Mat08; IN07; KM05]，表明可以通过从任何均值为0、方差为1且具有次高斯尾部${}^{17}$的分布中独立同分布地对$m \times  d$矩阵的每个元素进行采样来构造一个JLD。值得注意的是，这些进展在流式数据文献中有类似情况，因为前面提到的AMS草图[AMS99; Alo+02]与拉德马赫构造[AV06]相同，尽管误差概率是恒定的。

---

<!-- Footnote -->

${}^{14}$ 分析所需的目标维度应为 $O\left( \frac{{\begin{Vmatrix}{\operatorname{tail}}_{k}\left( x\right) \end{Vmatrix}}_{2}^{2}}{{\left( \varepsilon {x}_{{i}_{k}}\right) }^{2}}\right)$，而非每个联合局部变换（JLT）的目标维度为 $O\left( {\varepsilon }^{-2}\right)$，其中 ${\operatorname{tail}}_{k}\left( x\right)$ 表示将 $x$ 中 $k$ 个最大元素置零后的结果。

${}^{15}$ 在讨论联合局部离散化（JLD）时，我们通常会省略归一化或缩放因子（此 JLD 的 ${\left( d/m\right) }^{1/2}$），因为它们在文本上造成干扰，不太有趣，且与随机性和输入数据无关。

${}^{16}$ 拉德马赫分布（Rademacher distribution）是 $\{  - 1,1\}$ 上的均匀分布。

<!-- Footnote -->

---

对于这些构造的目标维度，文献[HIM12]证明了，如果$m \geq  8{\left( {\varepsilon }^{2} - 2{\varepsilon }^{3}/3\right) }^{-1}\left( {\ln \left| X\right|  + \mathcal{O}\left( {\log m}\right) }\right)$，则高斯构造是一个约翰逊 - 林登施特劳斯引理（JLD）构造，这大致对应于在原始构造基础上增加一个附加项$O\left( {{\varepsilon }^{-2}\log \log \left| X\right| }\right)$。文献[DG02]中的证明去掉了这个附加项$\log \log$，该证明针对的是原始的约翰逊 - 林登施特劳斯引理构造，但可以很容易地${}^{18}$应用到高斯构造上；文献[AV06]中的证明也为稠密拉德马赫构造给出了相同的无双重对数界。阿赫利奥普塔斯（Achlioptas）[Ach03]表明他的构造也能达到$m = \left\lceil  {8{\left( {\varepsilon }^{2} - 2{\varepsilon }^{3}/3\right) }^{-1}\ln \left| X\right| }\right\rceil$。对于高斯构造和稠密拉德马赫构造，常数 8 已经得到了改进，具体来说，罗霍（Rojo）和阮（Nguyen）[RN10; Ngu09]能够用更复杂的表达式${}^{19}$来替代这个界，对于许多参数集，这能带来 10% 到 40% 的改进。然而，在分布设置下，文献[BGK18]表明，任何约翰逊 - 林登施特劳斯引理构造要满足引理 1.2 都需要$m \geq  4{\varepsilon }^{-2}\ln \frac{1}{\delta }\left( {1 - o\left( 1\right) }\right)$，如果我们按照通常的方式通过设置$\delta  = {n}^{-2}$并对所有向量对进行并集界估计来证明引理 1.1，这对应于常数 8。

在文献中，关于目标维度的改进似乎存在一些混淆。主要的问题在于，一些论文（例如Ach03；HIM12；DG02；RN10；BGK18）在提及原始构造的目标维度界时，仅参考了[FM88]。因此，[Ach03, HIM12]错误地声称其构造改进了目标维度的常数。此外，尽管拉德马赫（Rademacher）构造是在[AV99]中独立开发并于两年前发表的，但有时（例如在AC09、Mat08、Sch18中）只有[Ach01]被认为是该构造的相关研究。

到目前为止，本节中提到的所有构造方法都是通过执行相对密集且无结构的矩阵 - 向量乘法来嵌入向量，该计算需要$\Theta \left( {m\parallel x{\parallel }_{0}}\right)  = O\left( {md}\right)$时间${}^{20}$。这引发了两条不同但相互交织的研究方向，旨在减少嵌入时间，即基于稀疏性的联合低维表示（JLDs，Joint Low - Dimensional Representations），它处理嵌入矩阵的密度问题；以及基于快速傅里叶变换的方法，它为矩阵引入了更多结构。

---

<!-- Footnote -->

${}^{17}$ 若存在常数$\alpha ,\beta  > 0$，使得对于所有的$\lambda  > 0,\Pr \left\lbrack  {\left| X\right|  > \lambda }\right\rbrack   \leq  \beta {e}^{-\alpha {\lambda }^{2}}.$，均值为 0 的实随机变量$X$具有次高斯尾部。

${}^{18}$ [DG02]中证明的主要部分是表明独立同分布高斯变量构成的向量的${\ell }_{2}$范数集中在期望值附近。通过高斯构造进行投影的向量的分布与独立同分布高斯变量构成的向量的分布相同。

${}^{19}$ 例如，拉德马赫构造（Rademacher construction）的一个边界是$m \geq  \frac{2\left( {d - 1}\right) {\alpha }^{2}}{d{\varepsilon }^{2}}$ ，其中$\alpha  \mathrel{\text{:=}} \frac{Q + \sqrt{{Q}^{2} + {5.98}}}{2}$ ，$Q \mathrel{\text{:=}} {\Phi }^{-1}\left( {1 - 1/{\left| X\right| }^{2}}\right)$ ，并且${\Phi }^{-1}$ 是标准高斯随机变量的分位数函数。

${}^{20}\parallel x{\parallel }_{0}$ 是向量 $x$ 中非零元素的数量。

<!-- Footnote -->

---

#### 1.4.1 稀疏约翰逊 - 林登斯特劳斯变换（Sparse Johnson-Lindenstrauss Transforms）

以下一系列结果背后的简单事实是，如果$A$每列有$s$个非零元素，那么$f\left( x\right)  = {Ax}$可以在$\Theta \left( {s\parallel x{\parallel }_{0}}\right)$时间内计算得出。这里的第一个结果是上述提到的阿赫利奥普塔斯构造（Achlioptas construction，[Ach03]），其列稀疏度$s$的期望值为$m/3$，这使得嵌入时间是完整拉德马赫构造（Rademacher construction，${}^{21}$）的三分之一。然而，首次实现超常数改进归功于达斯古普塔（Dasgupta）、库马尔（Kumar）和萨尔洛斯（Sarlós）（[DKS10]），他们基于启发式方法（[Wei+09; Shi+09b; LLS07; GD08]）构造了一个具有$s = O\left( {{\varepsilon }^{-1}\log \frac{1}{\delta }{\log }^{2}\frac{m}{\delta }}\right)$的约翰逊 - 林登施特劳斯引理（JLD）。他们的构造（我们将其称为DKS构造）通过独立采样$s$个哈希函数${h}_{1},\ldots ,{h}_{s} : \left\lbrack  d\right\rbrack   \rightarrow  \{  - 1,1\}  \times  \left\lbrack  m\right\rbrack$来实现，使得每个源元素${x}_{i}$将被哈希到$s$个随机符号${\sigma }_{i,1},\ldots ,{\sigma }_{i,s}$和$s$个目标坐标${j}_{i,1},\ldots ,{j}_{i,s}$（可重复）。然后嵌入可以定义为$f\left( x\right)  \mathrel{\text{:=}} \mathop{\sum }\limits_{i}\mathop{\sum }\limits_{k}{e}_{{j}_{i,k}}{\sigma }_{i,k}{x}_{i}$，也就是说，每个源坐标被哈希到$s$个输出坐标，并随机地加到或从这些输出坐标中减去。后来对稀疏性分析进行了优化，结果表明对于DKS构造，$s = O\left( {{\varepsilon }^{-1}\log \frac{1}{\delta }\log \frac{m}{\delta }}\right)$就足够了（$\left\lbrack  {\mathrm{{KN}}{10} : \mathrm{{KN}}{14}}\right\rbrack$），假设$\varepsilon  < {\log }^{-2}\frac{1}{\delta }\left\lbrack  \mathrm{{BOR10}}\right\rbrack$，甚至$s = O\left( {{\varepsilon }^{-1}{\left( \frac{\log \frac{1}{\delta }\log \log \log \frac{1}{\delta }}{\log \log \frac{1}{\delta }}\right) }^{2}}\right)$也足够，而$\left\lbrack  \mathrm{{KN14}}\right\rbrack$表明对于DKS构造，$s = \Omega \left( {{\varepsilon }^{-1}{\log }^{2}\frac{1}{\delta }/{\log }^{2}\frac{1}{\varepsilon }}\right)$是必要的。

凯恩（Kane）和纳尔逊（Nelson）[KN14]提出了两种构造方法，通过确保哈希函数在列内不发生冲突，即对于所有$i,a$和$b$满足${j}_{i,a} \neq  {j}_{i,b}$，从而绕过了DKS下界。第一种构造方法，我们称之为图构造（graph construction），它只是无放回地对$s$个坐标进行采样。第二种构造方法，我们称之为块构造（block construction），它将输出向量划分为$s$个长度为$m/s$的连续块，并从每个块中采样一个输出坐标。请注意，块构造与流式文献[CG05; CCF04]中的计数草图（Count Sketch）相同，不过哈希函数不同，输出的解释也不同。凯恩和纳尔逊[KN14]证明，为了使他们的两种构造方法满足引理1.2，$s = \Theta \left( {{\varepsilon }^{-1}\log \frac{1}{\delta }}\right)$既是必要条件也是充分条件。请注意，虽然计数草图比块构造的下界还要稀疏，但这并不矛盾，因为计数草图计算的是中位数（这是非线性的），它并不嵌入到${\ell }_{2}^{m}$中。就一般的稀疏性下界而言，[DKS10]表明，对于稀疏JLD，平均列稀疏度为${s}_{\text{avg }} = \Omega \left( {\min \left\{  {{\varepsilon }^{-2},{\varepsilon }^{-1}\sqrt{{\log }_{m}d}}\right\}  }\right)$是必要的，而纳尔逊和阮（Nguyên）[NN13b]对此进行了改进，他们证明存在一组点$X \in  {\mathbb{R}}^{d}$，使得该集合的任何JLT为了满足引理1.1，其列稀疏度必须为$s = \Omega \left( {{\varepsilon }^{-1}\log \left| X\right| /\log \frac{1}{\varepsilon }}\right)$。因此，似乎我们已经几乎达到了稀疏约翰逊 - 林登施特劳斯（JL）方法的极限，但为什么理论要阻碍取得好的结果呢？让我们对定义进行调整，以绕过这些下界。

用于证明下界的困难实例[NN13b; KN14]由非常稀疏的向量组成，例如$x = {\left( 1/\sqrt{2},1/\sqrt{2},0,\ldots ,0\right) }^{\mathrm{T}}$，但我们有兴趣应用约翰逊 - 林登施特劳斯引理（JLT）的向量可能并非如此糟糕。因此，通过将输入向量限制为足够“良好”，我们可以得到比悲观下界所表明的结果更优的有意义结果。这种“良好性”的正式表述是对向量的${\ell }_{\infty }/{\ell }_{2}$比率进行界定，引理1.1和1.2需要应用于此。我们将这个范数比率记为$v \in  \left\lbrack  {1/\sqrt{d},1}\right\rbrack$，并重新审视一些稀疏的约翰逊 - 林登施特劳斯分布（JLD）。阿赫利奥普塔斯构造（Achlioptas construction）[Ach03]可以进行推广，使得对于参数$q \in  \left\lbrack  {0,1}\right\rbrack$，每列的非零元素的期望数量为${qm}$而非$\frac{1}{3}m$。艾隆和查泽尔（Ailon and Chazelle）[AC09]表明，如果$v = O\left( {\sqrt{\log \frac{1}{\delta }}/\sqrt{d}}\right)$，那么选择$q = \Theta \left( \frac{{\log }^{2}1/\delta }{d}\right)$并从高斯分布中采样非零元素就足够了。在[Mat08]中，通过证明对于所有的$v \in  \left\lbrack  {1/\sqrt{d},1}\right\rbrack$，选择$q = \Theta \left( {{v}^{2}\log \frac{d}{\varepsilon \delta }}\right)$并从拉德马赫分布中采样非零元素，对于受该$v$约束的向量而言是一种约翰逊 - 林登施特劳斯分布（JLD），从而将这一结果进行了推广。

---

<!-- Footnote -->

${}^{21}$ 在此，我们忽略切换到稀疏矩阵表示法会带来的任何开销。

<!-- Footnote -->

---

请注意，有时（例如在DKS10；BOR10中），关于$q$的这个界${}^{22}$会被错误地解读为一个下界，即声称当$v = 1$时，${qm} = \widetilde{\Omega }\left( {\varepsilon }^{-2}\right)$对于阿赫利奥普塔斯构造（Achlioptas construction）是必要的。然而，马托谢克（Matoušek）$\left\lbrack  \text{Mat08}\right\rbrack$只是大致论证了他的界对于$v \leq  {d}^{-{0.1}}$是紧的，并且如果它在$v = 1$处确实是紧的，那么由$\widetilde{\Omega }$所隐藏的因子将会导致$m \geq  {qm} = \Omega \left( {{\varepsilon }^{-2}\log \frac{1}{\delta }\log \frac{d}{\varepsilon \delta }}\right)  = \omega \left( m\right) .$这一矛盾结果

[DKS10]所基于的启发式方法[Wei+09; Shi+09b; LLS07; GD08]被称为特征哈希（Feature Hashing，又名哈希技巧（hashing trick）或哈希核（hashing kernel）），它是一种稀疏的约翰逊 - 林登斯特劳斯（JL）构造，每列恰好有1个非零元素${}^{23}$。块构造可以看作是$s = \Theta \left( {{\varepsilon }^{-1}\log \frac{1}{\delta }}\right)$个特征哈希实例的串联，而DKS构造可以看作是$s = O\left( {{\varepsilon }^{-1}\log \frac{1}{\delta }\log \frac{m}{\delta }}\right)$个特征哈希实例的和，或者也可以看作是先将$x \in  {\mathbb{R}}^{d}s$中的每个元素复制若干次，然后对扩展后的向量${x}^{\prime } \in  {\mathbb{R}}^{sd}$应用特征哈希：设${f}_{\text{dup }} : {\mathbb{R}}^{d} \rightarrow  {\mathbb{R}}^{sd}$是一个将其输入中的每个元素复制$s$次的函数，即对于$i \in  \left\lbrack  d\right\rbrack  ,j \in  \left\lbrack  s\right\rbrack$有${f}_{\mathrm{{dup}}}{\left( x\right) }_{\left( {i - 1}\right) s + j} = {x}_{\left( {i - 1}\right) s + j}^{\prime } \mathrel{\text{:=}} {x}_{i}$，那么${f}_{\mathrm{{DKS}}} = {f}_{\mathrm{{FH}}} \circ  {f}_{\mathrm{{dup}}}$。

这种重复是[DKS10]中分析的关键，因为${f}_{\text{dup }}$是等距的（直至归一化），并且它确保了${x}^{\prime }$的${\ell }_{\infty }/{\ell }_{2}$比率较小，即从特征哈希（Feature Hashing）数据结构$\left( {f}_{\mathrm{{FH}}}\right)$的角度来看为$v \leq  1/\sqrt{s}$。因此，DKS构造的稀疏性的任何下界（例如[KN14]中给出的下界）都给出了特征哈希成为JLD时$v$值的上界：如果$u$是一个单位向量，使得稀疏性为$\widehat{s}$的DKS实例以概率$\delta$无法将$u$的范数保持在$1 \pm  \varepsilon$范围内，那么特征哈希必定以概率$\delta$无法将${f}_{\text{dup }}\left( u\right)$的范数保持在$1 \pm  \varepsilon$范围内，因此特征哈希能够处理所有向量时的${\ell }_{\infty }/{\ell }_{2}$比率严格小于$1/\sqrt{\widehat{s}}$。

更简洁地表述，该陈述为${s}_{\mathrm{{DKS}}} = \Omega \left( a\right)  \Rightarrow  {v}_{\mathrm{{FH}}} = O\left( {1/\sqrt{a}}\right)$，通过逆否命题为${}^{24}{v}_{\mathrm{{FH}}} = \Omega \left( {1/\sqrt{a}}\right)  \Rightarrow  {s}_{\mathrm{{DKS}}} = O\left( a\right)$，其中${s}_{\mathrm{{DKS}}}$是作为联合线性鉴别器（JLD）的DKS构造的最小列稀疏度，${v}_{\mathrm{{FH}}}$是特征哈希（Feature Hashing）作为联合线性鉴别器（JLD）时的最大${\ell }_{\infty }/{\ell }_{2}$约束，$a$是任意正表达式。此外，如果我们使用一个与DKS构造在复制后所能生成的${x}^{\prime }$相同的困难实例来证明${v}_{\mathrm{{FH}}}$的上界，我们可以将上述两个蕴含关系替换为双向蕴含关系。

[魏（Wei）等人，2009年]声称给出了${v}_{\mathrm{{FH}}}$的一个界，但遗憾的是，其对该界的证明存在错误[达尔加德（Dahlgaard）等人，2010年；魏（Wei）等人，2010年]。达尔加德（Dahlgaard）、克努森（Knudsen）和索鲁普（Thorup）[DKT17]将${v}_{\mathrm{{FH}}}$的下界改进为${v}_{\mathrm{{FH}}} = \Omega \left( \sqrt{\frac{\varepsilon \log \left( {1 + \frac{\delta }{\varepsilon }}\right) }{\log \frac{1}{\delta }\log \frac{m}{\delta }}}\right)$，弗雷克森（Freksen）、卡马（Kamma）和拉森（Larsen）[FKL18]给出了定理1.8中所示的关于${v}_{\mathrm{{FH}}}$的一个复杂但精确的界，其中用于证明上界的困难实例与DKS构造中的一个${x}^{\prime }$相同。

---

<!-- Footnote -->

${}^{22}$ 这似乎是[马特（Mat），2008年]中唯一与$q$的界相关的内容。

${}^{23}$ 即具有 $s = 1$ 的DKS、图或块构造。

${}^{24}$ 逆否命题是 $\left( {P \Rightarrow  Q}\right)  \Rightarrow  \left( {\neg Q \Rightarrow  \neg P}\right)$ ，并且如果没有一些关于 ${s}_{\mathrm{{DKS}}},{v}_{\mathrm{{FH}}}$ 和 $a$ 不会表现得过于不规则的假设，它并不能完全证明刚才所声称的内容。

<!-- Footnote -->

---

定理1.8（[FKL18]）。存在常数 $C \geq  D > 0$ ，使得对于每个 $\varepsilon ,\delta  \in  \left( {0,1}\right)$ 和 $m \in  {\mathbb{N}}_{1}$ ，以下结论成立。如果 $\frac{C\lg \frac{1}{\delta }}{{\varepsilon }^{2}} \leq  m < \frac{2}{{\varepsilon }^{2}\delta }$ ，那么

$$
{v}_{\mathrm{{FH}}}\left( {m,\varepsilon ,\delta }\right)  = \Theta \left( {\sqrt{\varepsilon }\min \left\{  {\frac{\log \frac{\varepsilon m}{\log \frac{1}{\delta }}}{\log \frac{1}{\delta }},\sqrt{\frac{\log \frac{{\varepsilon }^{2}m}{\log \frac{1}{\delta }}}{\log \frac{1}{\delta }}}}\right\}  }\right) .
$$

否则，如果 $m \geq  \frac{2}{{\varepsilon }^{2}\delta }$ 成立，那么 ${v}_{\mathrm{{FH}}}\left( {m,\varepsilon ,\delta }\right)  = 1$ 成立。此外，如果 $m < \frac{D\lg \frac{1}{\delta }}{{\varepsilon }^{2}}$ 成立，那么 ${v}_{\mathrm{{FH}}}\left( {m,\varepsilon ,\delta }\right)  = 0$ 成立。

此外，如果一个 $x \in  \{ 0,1{\} }^{d}$ 满足 ${v}_{\mathrm{{FH}}} < \parallel x{\parallel }_{2}^{-1} < 1$，那么

$$
\mathop{\Pr }\limits_{{f \sim  \mathrm{{FH}}}}\left\lbrack  {\left| {\parallel f\left( x\right) {\parallel }_{2}^{2} - \parallel x{\parallel }_{2}^{2}}\right|  > \varepsilon \parallel x{\parallel }_{2}^{2}}\right\rbrack   > \delta .
$$

该边界给出了特征哈希（Feature Hashing）的目标维度 $m$、失真度 $\varepsilon$、错误概率 $\delta$ 以及 ${\ell }_{\infty }/{\ell }_{2}$ 约束 $v$ 之间的紧密权衡关系，同时展示了如何为特征哈希构造困难实例：如果形状为 $x = {\left( 1,\ldots ,1,0,\ldots ,0\right) }^{\top }$ 的向量中 1 的数量很少，那么它们就是困难实例，这意味着特征哈希无法以概率 $\delta$ 将它们的范数保持在 $1 \pm  \varepsilon$ 范围内。推论 1.9 中使用定理 1.8 给出了 DKS 构造中 $m,\varepsilon ,\delta ,v$ 和列稀疏性 $s$ 之间的紧密权衡关系。

推论1.9。设${v}_{\mathrm{{DKS}}} \in  \left\lbrack  {1/\sqrt{d},1}\right\rbrack$表示所需的最大${\ell }_{\infty }/{\ell }_{2}$比率，${v}_{\mathrm{{FH}}}$表示定理1.8中所定义的特征哈希（Feature Hashing）的${\ell }_{\infty }/{\ell }_{2}$约束，${s}_{\mathrm{{DKS}}} \in  \left\lbrack  m\right\rbrack$表示最小列稀疏度，使得具有该稀疏度的DKS构造是满足$\parallel x{\parallel }_{\infty }/\parallel x{\parallel }_{2} \leq  {v}_{\mathrm{{DKS}}}$的向量子集$x \in  {\mathbb{R}}^{d}$的JLD。那么

$$
{s}_{\mathrm{{DKS}}} = \Theta \left( \frac{{v}_{\mathrm{{DKS}}}^{2}}{{v}_{\mathrm{{FH}}}^{2}}\right) .
$$

该推论的证明推迟到2.2节

贾加迪桑（Jagadeesan）[Jag19]将[FKL18]的结果进行了推广，针对任意具有选定列稀疏性的稀疏拉德马赫构造（例如块构造和图构造），给出了$m,\varepsilon ,\delta$、$v$和$s$权衡的下界${}^{25}$，并为图构造给出了匹配的上界。

#### 1.4.2 结构化约翰逊 - 林登斯特劳斯变换

当我们不再关注稀疏的约翰逊 - 林登斯特劳斯降维（JLD）时，我们对高效 JLD 的定义会稍有改变。在上一节中，当向量是稀疏的时候，JLD 特别快，因为运行时间与$\parallel x{\parallel }_{0}$成比例，而在本节中，我们将针对密集输入向量进行优化，使得嵌入时间为$O\left( {d\log d}\right)$是一个令人满意的结果。

在时间顺序上，对原始JLD构造的首次渐进式改进归功于艾隆（Ailon）和查泽尔（Chazelle）[AC09]，他们引入了所谓的快速约翰逊 - 林登施特劳斯变换（Fast Johnson - Lindenstrauss Transform，FJLT）。如前一节所述，[AC09]表明，只要向量的${\ell }_{\infty }/{\ell }_{2}$比率较低，我们就可以使用非常稀疏（因此速度非常快）的嵌入矩阵，此外，对向量应用随机化的沃尔什 - 哈达玛变换（Walsh - Hadamard transform）很可能会得到较低的${\ell }_{\infty }/{\ell }_{2}$比率。因此，FJLT定义为$f\left( x\right)  \mathrel{\text{:=}} {PHDx}$，其中$P \in  {\mathbb{R}}^{m \times  d}$是一个具有高斯元素的稀疏阿赫利奥普塔斯矩阵（Achlioptas matrix），$q = \Theta \left( \frac{{\log }^{2}1/\delta }{d}\right) ,H \in  \{  - 1,1{\} }^{d \times  d}$是一个沃尔什 - 哈达玛矩阵${}^{26}$，$D \in  \{  - 1,0,1{\} }^{d \times  d}$是一个对角线上具有独立同分布拉德马赫变量（Rademachers）的随机对角矩阵。由于可以使用简单的递归公式计算沃尔什 - 哈达玛变换，因此预期的嵌入时间变为$O\left( {d\log d + m{\log }^{2}\frac{1}{\delta }}\right)$。如前所述，[Mat08]表明，在构造矩阵$P$时，我们可以从拉德马赫分布而非高斯分布中采样。与之前的构造相比，FJLT的嵌入时间改进取决于$m$和$d$之间的关系。如果$m = \Theta \left( {{\varepsilon }^{-2}\log \frac{1}{\delta }}\right)$且$m = O\left( {{\varepsilon }^{-4/3}{d}^{1/3}}\right)$，FJLT的嵌入时间受限于沃尔什 - 哈达玛变换，为$O\left( {d\log d}\right)$，但在$m = \Theta \left( {d}^{1/2}\right)$时，FJLT仅比原始构造略快。

---

<!-- Footnote -->

${}^{25}$ 这里，下界指的是作为 $m,\varepsilon ,\delta$ 和 $s$ 的函数的 $v$ 的下界。

<!-- Footnote -->

---

艾隆（Ailon）和利伯蒂（Liberty）[AL09]将FJLT构造的运行时间改进为$O\left( {d\log m}\right)$，其中$m = O\left( {d}^{1/2 - \gamma }\right)$对于任何固定的$\gamma  > 0$成立。通过应用多次随机沃尔什 - 哈达玛变换（Walsh - Hadamard transformations），即把${HD}$替换为$\mathop{\prod }\limits_{i}H{D}^{\left( i\right) }$（其中${D}^{\left( i\right) }$是固定数量的独立对角拉德马赫矩阵（Rademacher matrices）），以及把$P$替换为${BD}$（其中$D$是另一个具有拉德马赫元素的对角矩阵，$B$是特定部分沃尔什 - 哈达玛矩阵的连续块（基于所谓的二元对偶BCH码（binary dual BCH codes）[例如参见MS77]）），实现了$m$适用范围的扩大。运行时间的减少源于对变换进行了轻微修改，即将输入划分为长度为poly(m)的连续块，并对每个块独立应用随机沃尔什 - 哈达玛变换。我们将这种FJLT的变体称为BCHJL构造。

下一组结果模式源于压缩感知（compressed sensing），并从另一个角度来处理这个问题：它们并非仅在$m \ll  d$时速度快，即使$m$接近$d$，也能实现$O\left( {d\log d}\right)$的嵌入时间，代价是$m$并非最优。在描述这些构造之前，让我们通过简要介绍压缩感知中的一些概念来做个铺垫。

大致来说，压缩感知关注的是通过少量线性测量来恢复稀疏信号，这里的一个关键概念是受限等距性质（Restricted Isometry Property）[CT05; CT06; CRT06; Don06]。

定义1.10（受限等距性质）。设$d,m,k \in  {\mathbb{N}}_{1}$，其中$m,k < d$且$\varepsilon  \in  \left( {0,1}\right)$。若对于所有满足$\parallel x{\parallel }_{0} \leq  k$的$x \in  {\mathbb{R}}^{d}$，线性函数$f : {\mathbb{R}}^{d} \rightarrow  {\mathbb{R}}^{m}$都满足，则称该线性函数具有阶为$k$、水平为$\varepsilon$的受限等距性质（我们将其记为$\left( {k,\varepsilon }\right)$ -RIP）。

$$
\left| {\parallel f\left( x\right) {\parallel }_{2}^{2} - \parallel x{\parallel }_{2}^{2}}\right|  \leq  \varepsilon \parallel x{\parallel }_{2}^{2}. \tag{10}
$$

在压缩感知文献中，已有研究[CT06; RV08]表明，定义为$f\left( x\right)  \mathrel{\text{:=}} {SHx}$的下采样哈达玛变换（Subsampled Hadamard Transform，SHT），对于$m = \Omega \left( {{\varepsilon }^{-2}k{\log }^{4}d}\right)$而言，大概率具有$\left( {k,\varepsilon }\right)$ - 受限等距性质（Restricted Isometry Property，RIP），同时允许向量在$O\left( {d\log d}\right)$时间内完成嵌入。这里$H \in  \{  - 1,1{\} }^{d \times  d}$是沃尔什 - 哈达玛矩阵（Walsh - Hadamard matrix），$S \in  \{ 0,1{\} }^{m \times  d}$对${Hx}$的$m$个元素进行有放回采样，即$S$中的每一行都有一个非零元素，该元素是均匀且独立选取的，也就是说，$S$是一个均匀随机特征选择矩阵。受此变换以及前文提到的FJLT启发，艾隆（Ailon）和利伯蒂（Liberty）[AL13]证明了，定义为$\bar{f}\left( x\right)  \mathrel{\text{:=}} {SHDx}$的下采样随机哈达玛变换（Subsampled Randomised Hadamard Transform，SRHT），若满足$m = \Theta \left( {{\varepsilon }^{-4}\log \left| X\right| {\log }^{4}d}\right)$，则是一个约翰逊 - 林登斯特劳斯引理（Johnson - Lindenstrauss Lemma，JLT）变换。同样，$D$表示一个具有拉德马赫（Rademacher）元素的随机对角矩阵，$S$和$H$的定义与SHT中的相同。一些相关结果包括：多（Do）等人[Do + 09]在[AL13]之前，在$\left| X\right|  \geq  d$的大集合情况下得到了$m = \Theta \left( {{\varepsilon }^{-2}{\log }^{3}\left| X\right| }\right)$的界；特罗（Tro）[Tro11]展示了SRHT构造如何近似保留向量子空间的范数；以及林（Lin）和李（Li）[LL20]对采样矩阵$S$进行修改，在作为支持向量机（Support Vector Machines，SVMs）的预处理器使用时，通过牺牲输入数据的独立性来提高精度。

---

<!-- Footnote -->

${}^{26}$ 沃尔什 - 哈达玛矩阵（Walsh - Hadamard matrices）的一种定义是，对于所有的 $i,j \in  \left\lbrack  d\right\rbrack$，其元素为 ${H}_{ij} = {\left( -1\right) }^{\langle i - 1,j - 1\rangle }$，其中 $\langle a,b\rangle$ 表示对应于数字 $a,b \in  \{ 0,\ldots ,d - 1\}$ 的二进制表示的 $\left( {\lg d}\right)$ 位向量的点积，并且 $d$ 是 2 的幂。为了说明其递归性质，一个大的沃尔什 - 哈达玛矩阵可以描述为较小的沃尔什 - 哈达玛矩阵的克罗内克积（Kronecker product），即如果 $d > 2$ 且 ${H}^{\left( n\right) }$ 指的是一个 $n \times  n$ 沃尔什 - 哈达玛矩阵，那么 ${H}^{\left( d\right) } = {H}^{\left( 2\right) } \otimes  {H}^{\left( d/2\right) } = \left( \begin{matrix} {H}^{\left( d/2\right) } & {H}^{\left( d/2\right) } \\  {H}^{\left( d/2\right) } &  - {H}^{\left( d/2\right) } \end{matrix}\right)$。

<!-- Footnote -->

---

<!-- Media -->

$$
\left( \begin{matrix} {t}_{0} & {t}_{1} & {t}_{2} & \cdots & {t}_{d - 1} \\  {t}_{-1} & {t}_{0} & {t}_{1} & \cdots & {t}_{d - 2} \\  {t}_{-2} & {t}_{-1} & {t}_{0} & \cdots & {t}_{d - 3} \\  \vdots & \vdots & \vdots &  \ddots  & \vdots \\  {t}_{-\left( {m - 1}\right) } & {t}_{-\left( {m - 2}\right) } & {t}_{-\left( {m - 3}\right) } & \cdots & {t}_{d - m} \end{matrix}\right) 
$$

图1：托普利茨矩阵（Toeplitz matrix）的结构。

<!-- Media -->

[AL13]的这一目标维度界后来被克拉默（Krahmer）和沃德（Ward）[KW11]收紧，他们证明了对于子采样随机哈达玛变换（SRHT），$m = \Theta \left( {{\varepsilon }^{-2}\log \left| X\right| {\log }^{4}d}\right)$就足够了。这是一个更一般结果的推论，即如果$\sigma  : {\mathbb{R}}^{d} \rightarrow  {\mathbb{R}}^{d}$对前面提到的$D$矩阵等效地应用随机符号，并且$f : {\mathbb{R}}^{d} \rightarrow  {\mathbb{R}}^{m}$具有$\left( {\Omega \left( {\log \left| X\right| }\right) ,\varepsilon /4}\right)$ - 受限等距性质（RIP），那么$f \circ  \sigma$大概率是一个约翰逊 - 林登斯特劳斯变换（JLT）。巴拉纽克（Baraniuk）等人[Bar + 08]的早期结果表明，从联合拉普拉斯分布（JLD）中采样的变换大概率具有$\left( {\mathcal{O}\left( {{\varepsilon }^{2}m/\log d}\right) ,\varepsilon }\right)$ - 受限等距性质（RIP）。因此，正如从它们的表现所预期的那样，约翰逊 - 林登斯特劳斯引理（Johnson - Lindenstrauss Lemma）和受限等距性质（Restricted Isometry Property）确实是一脉相承的。

压缩感知文献中的另一种变换使用了所谓的托普利茨（Toeplitz）矩阵或部分循环矩阵[Baj+07: Rau09: Rom09: Hau+10: RRT12: Baj12: DJR19]，它们可以按以下方式定义。对于$m,d \in  {\mathbb{N}}_{1}$，若存在${t}_{-\left( {m - 1}\right) },{t}_{-\left( {m - 2}\right) }\ldots ,{t}_{d - 1} \in  \mathbb{R}$使得${T}_{ij} = {t}_{j - i}$ ，则称$T \in  {\mathbb{R}}^{m \times  d}$为实托普利茨矩阵。这意味着任意一条对角线上的元素都相同（见图1），并且计算矩阵 - 向量乘积相当于计算与由$t$组成的向量的卷积。部分循环矩阵是托普利茨矩阵的特殊情况，其中对角线会“环绕”矩阵的两端，即对于所有的$i \in  \left\lbrack  {m - 1}\right\rbrack$ ，有${t}_{-i} = {t}_{d - i}$ 。

作为一个联合限制特征值（JLT），托普利茨（Toeplitz）构造为$f\left( x\right)  \mathrel{\text{:=}} {TDx}$，其中$T \in  \{  - 1,1{\} }^{m \times  d}$是一个具有独立同分布拉德马赫（Rademacher）元素的托普利茨矩阵，而$D \in  \{  - 1,0,1{\} }^{d \times  d}$是一个像往常一样具有拉德马赫元素的对角矩阵。请注意，两个向量的卷积对应于傅里叶（Fourier）空间中的逐元素乘积，因此我们可以采用快速傅里叶变换（FFT）在时间$O\left( {d\log d}\right)$内用托普利茨构造嵌入一个向量。当我们意识到将$T$划分为$\frac{d}{m}$个大小为$m \times  m$的连续块时，每个块也是一个托普利茨矩阵，并且通过分别应用，嵌入时间变为$O\left( {\frac{d}{m}m\log m}\right)$，此时这个时间甚至可以减少到$O\left( {d\log m}\right)$。

将[KW11]的结果与托普利茨矩阵（Toeplitz matrices）的受限等距性质（RIP）界[RRT12]相结合，可以得出，$m = \Theta \left( {{\varepsilon }^{-1}{\log }^{3/2}\left| X\right| {\log }^{3/2}d + {\varepsilon }^{-2}\log \left| X\right| {\log }^{4}d}\right)$足以使托普利茨构造以高概率成为约翰逊 - 林登斯特劳斯引理（JLT）。然而，托普利茨构造也被直接作为约翰逊 - 林登斯特劳斯维数约简（JLD）进行研究，而无需通过其受限等距性质界。欣里克斯（Hinrichs）和维比拉尔（Vybíral）[HV11]表明，$m = \Theta \left( {{\varepsilon }^{-2}{\log }^{3}\frac{1}{\delta }}\right)$足以用于托普利茨构造，此后不久，[Vyb11]将此界改进为$m = \Theta \left( {{\varepsilon }^{-2}{\log }^{2}\frac{1}{\delta }}\right)$。那么问题是，我们是否可以加强分析，去掉最后的对数因子，从而得到一个具有最优目标维度和$O\left( {d\log d}\right)$嵌入时间的约翰逊 - 林登斯特劳斯维数约简的难以捉摸的结果，即使当$m$接近$d$时也是如此。遗憾的是，情况并非如此，因为弗雷克森（Freksen）和拉森（Larsen）[FL20]表明，存在向量${}^{27}$，使得托普利茨构造必须满足$m = \Omega \left( {{\varepsilon }^{-2}{\log }^{2}\frac{1}{\delta }}\right)$。

正如约翰逊 - 林登施特劳斯变换（JLTs）被用作预处理器来加速解决我们实际关心问题的算法一样，我们还可以使用一个JLT来加速其他JLT，这可以称为复合JLT。更明确地说，如果${f}_{1} : {\mathbb{R}}^{d} \rightarrow  {\mathbb{R}}^{{d}^{\prime }}$和${f}_{2} : {\mathbb{R}}^{{d}^{\prime }} \rightarrow  {\mathbb{R}}^{m}$（其中$m \ll  {d}^{\prime } \ll  d$）是两个JLT，并且计算${f}_{1}\left( x\right)$的速度很快，我们可以期望计算$\left( {{f}_{2} \circ  {f}_{1}}\right) \left( x\right)$的速度也很快，因为${f}_{2}$只需要处理${d}^{\prime }$维向量，并且希望$\left( {{f}_{2} \circ  {f}_{1}}\right)$能充分保留向量的范数，因为${f}_{1}$和${f}_{2}$各自都能近似地保留范数。如这里所介绍的，${f}_{1}$的一个明显候选者是基于受限等距性质（RIP）的约翰逊 - 林登施特劳斯降维（JLD）方法之一，该方法已在[BK17]中成功应用。在他们的构造中，我们将其称为${\mathrm{{GRHD}}}^{28},{f}_{1}$，其中${\mathrm{{GRHD}}}^{28},{f}_{1}$是稀疏随机哈达玛变换（SRHT），${f}_{2}$是稠密拉德马赫构造（即$f\left( x\right)  \mathrel{\text{:=}} {A}_{\text{Rad }}{SHDx}$），并且对于任何固定的$\gamma  > 0$，它可以在时间$O\left( {d\log m}\right)$内嵌入一个向量，其中$m = O\left( {d}^{1/2 - \gamma }\right)$。这与艾隆（Ailon）和利伯蒂（Liberty）[AL09]的构造结果类似，但与该构造不同的是，广义随机哈达玛降维（GRHD）能更优雅地处理$m$的剩余范围，因为对于任何$r \in  \left\lbrack  {1/2,1}\right\rbrack$和$m = O\left( {d}^{r}\right)$，GRHD的嵌入时间变为$O\left( {{d}^{2r}{\log }^{4}d}\right)$。然而，GRHD构造的主要卖点在于，通过利用快速矩阵 - 矩阵乘法技术[LR83]，即使对于任何固定的$\gamma  > 0$有$m = \Theta \left( {d}^{1 - \gamma }\right)$，它也允许在总时间$O\left( {\left| X\right| d\log m}\right)$内同时嵌入足够大的点集$X$。

另一种化合物JLD基于所谓的精简沃尔什变换（LWT）[LAS11]，该变换是基于所谓的种子矩阵定义的。对于$r,c \in  {\mathbb{N}}_{1}$，若$r < c$，其列向量长度为单位长度，且其行向量两两正交并具有相同的${\ell }_{2}$范数，则称${A}_{1} \in  {\mathbb{C}}^{r \times  c}$为种子矩阵。因此，部分沃尔什 - 哈达玛矩阵（partial Walsh - Hadamard matrices）和部分傅里叶矩阵（partial Fourier matrices）（经归一化处理后）均为种子矩阵；不过，为简便起见，我们将专注于部分沃尔什 - 哈达玛矩阵以保持实数运算。然后，我们可以基于此种子定义阶数为$l \in  {\mathbb{N}}_{1}$的精简沃尔什变换为${A}_{l} \mathrel{\text{:=}} {A}_{1}^{\otimes l} = {A}_{1} \otimes  \cdots  \otimes  {A}_{1}$，其中$\otimes$表示克罗内克积（Kronecker product），下面我们将快速给出其定义。设$A$为一个$m \times  n$矩阵，$B$为一个$p \times  q$矩阵，则克罗内克积$A \otimes  B$是定义为如下形式的${mp} \times  {nq}$分块矩阵

$$
A \otimes  B \mathrel{\text{:=}} \left( \begin{matrix} {A}_{11}B & \cdots & {A}_{1n}B \\  \vdots &  \ddots  & \vdots \\  {A}_{m1}B & \cdots & {A}_{mn}B \end{matrix}\right) .
$$

---

<!-- Footnote -->

${}^{27}$ 奇怪的是，托普利茨构造（Toeplitz construction）的困难实例与[FKL18]中使用的特征哈希（Feature Hashing）的困难实例非常相似。

${}^{28}$ 这是由于[BK17]中矩阵名称的选择。

<!-- Footnote -->

---

请注意，${A}_{l}$ 是一个 ${r}^{l} \times  {c}^{l}$ 矩阵，并且任何沃尔什 - 哈达玛矩阵（Walsh - Hadamard matrix）都可以写成 ${A}_{l}$ 的形式，其中 $l$ 为某个值，而 $2 \times  2$ 沃尔什 - 哈达玛矩阵 ${}^{29}$ 可写成 ${A}_{1}$ 的形式。此外，对于固定大小的种子，使用类似于快速傅里叶变换（FFT）的算法将 ${A}_{l}$ 应用于向量的时间复杂度为 $\bar{O}\left( {c}^{l}\right)$。然后，我们可以定义复合变换，我们将其称为 LWTJL，即 $f\left( x\right)  \mathrel{\text{:=}} G{A}_{l}{Dx}$，其中 $D \in  \{  - 1,1{\} }^{d \times  d}$ 是一个具有拉德马赫（Rademacher）元素的对角矩阵，${A}_{l} \in  {\mathbb{R}}^{{r}^{l} \times  d}$ 是一个 LWT，$G \in  {\mathbb{R}}^{m \times  {r}^{l}}$ 是一个 JLT，并且 $r$ 和 $c$ 是常数。一种看待 LWTJL 的方式是将其视为广义随机哈达玛设计（GRHD）的一种变体，其中子采样发生在种子矩阵上，而不是最终的沃尔什 - 哈达玛矩阵上。如果 $G$ 可以在 $O\left( {{r}^{l}\log {r}^{l}}\right)$ 时间内应用，例如，如果 $G$ 是 BCHJL 构造 [AL09] 且 $m = O\left( {r}^{l\left( {1/2 - \gamma }\right) }\right)$，则总嵌入时间变为 $O\left( d\right)$，因为对于某个 $\alpha  < 1$ 有 ${r}^{l} = {d}^{\alpha }$。然而，为了证明 LWTJL 满足引理 1.2，[LAS11] 的分析对 $r,c$ 以及我们希望嵌入的向量施加了一些要求，即 $\log r/\log c \geq  1 - {2\delta }$ 和 $v = O\left( {{m}^{-1/2}{d}^{-\delta }}\right)$，其中 $v$ 是第 1.4.1 节末尾引入的 ${\ell }_{\infty }/{\ell }_{2}$ 比率的上界。如命题 1.11 所示，$v$ 的界有些严格

命题1.11。对于任何种子矩阵，将LWT定义为以该矩阵为种子的LWTJL分布。那么对于所有$\delta  \in  \left( {0,1}\right)$，存在一个向量$x \in  {\mathbb{C}}^{d}$（如果种子矩阵是实矩阵，则为$x \in  {\mathbb{R}}^{d}$）满足$\parallel x{\parallel }_{\infty }/\parallel x{\parallel }_{2} = \Theta \left( {{\log }^{-1/2}\frac{1}{\delta }}\right)$，使得

$$
\mathop{\Pr }\limits_{{f \sim  \mathrm{{LWT}}}}\left\lbrack  {f\left( x\right)  = \mathbf{0}}\right\rbrack   > \delta  \tag{11}
$$

命题1.11的证明可在第2.3节中找到，它基于将$x$构造为与种子矩阵的行正交的向量的若干副本。

我们将介绍的最后一种联合线性设计（JLD）基于所谓的卡茨随机游走（Kac random walks）。尽管艾隆（Ailon）和查泽尔（Chazelle）[AC09]猜想这种构造可能满足引理1.1，但直到贾恩（Jain）等人[Jai+20]才最终给出了证明。与上述精简沃尔什变换（lean Walsh transforms）一样，在描述如何用它们构造联合线性设计之前，让我们先定义卡茨随机游走。卡茨随机游走是一个线性变换的马尔可夫链，在每一步中，我们随机选择两个坐标，并对这两个坐标所张成的平面进行随机旋转，更正式的定义如下：

定义1.12（卡茨随机游走 [Kac56]）。对于给定的维度 $d \in  {\mathbb{N}}_{1}$，设 ${K}^{\left( 0\right) } \mathrel{\text{:=}} I \in  \{ 0,1{\} }^{d \times  d}$ 为单位矩阵，并且对于每个 $t > 0$，独立且均匀地随机采样 $\left( {{i}_{t},{j}_{t}}\right)  \in  \left( \begin{matrix} \left\lbrack  d\right\rbrack  \\  2 \end{matrix}\right)$ 和 ${\theta }_{t} \in  \lbrack 0,{2\pi })$。然后将长度为 $t$ 的卡茨随机游走定义为 ${K}^{\left( t\right) } \mathrel{\text{:=}} {R}^{\left( {i}_{t},{j}_{t},{\theta }_{t}\right) }{K}^{\left( t - 1\right) }$，其中 ${R}^{\left( i,j,\theta \right) } \in  {\mathbb{R}}^{d \times  d}$ 是在 (i,j) 平面上旋转 $\theta$ 角度，其表达式为

$$
{R}^{\left( i,j,\theta \right) }{e}_{k} \mathrel{\text{:=}} {e}_{k}
$$

$\forall k \notin  \{ i,j\}$

$$
{R}^{\left( i,j,\theta \right) }\left( {a{e}_{i} + b{e}_{j}}\right)  \mathrel{\text{:=}} \left( {a\cos \theta  - b\sin \theta }\right) {e}_{i} + \left( {a\sin \theta  + b\cos \theta }\right) {e}_{j}.
$$

---

<!-- Footnote -->

${}^{29}$ 这里我们忽略种子矩阵的$r < c$要求。

<!-- Footnote -->

---

文献[Jai+20]中引入的主要联合线性降维（JLD）方法，我们将其称为KacJL，是一种复合JLD，其中${f}_{1}$和${f}_{2}$均由Kac随机游走（Kac random walk）后接子采样组成，可按以下方式更正式地定义。设${T}_{1} \mathrel{\text{:=}} \Theta \left( {d\log d}\right)$为第一次Kac随机游走的长度，${d}^{\prime } \mathrel{\text{:=}} \min \left\{  {d,\Theta \left( {{\varepsilon }^{-2}\log \left| X\right| {\log }^{2}\log \left| X\right| {\log }^{3}d}\right) }\right\}$为中间维度，${T}_{2} \mathrel{\text{:=}} \Theta \left( {{d}^{\prime }\log \left| X\right| }\right)$为第二次Kac随机游走的长度，$m \mathrel{\text{:=}} \Theta \left( {{\varepsilon }^{-2}\log \left| X\right| }\right)$为目标维度，然后将联合线性变换（JLT）定义为$f\left( x\right)  = \left( {{f}_{2} \circ  {f}_{1}}\right) \left( x\right)  \mathrel{\text{:=}} {S}^{\left( m,{d}^{\prime }\right) }{K}^{\left( {T}_{2}\right) }{S}^{\left( {d}^{\prime },d\right) }{K}^{\left( {T}_{1}\right) }x$，其中${K}^{\left( {T}_{1}\right) } \in  {\mathbb{R}}^{d \times  d}$和${K}^{\left( {T}_{2}\right) } \in  {\mathbb{R}}^{{d}^{\prime } \times  {d}^{\prime }}$分别是长度为${T}_{1}$和${T}_{2}$的独立Kac随机游走，${S}^{\left( {d}^{\prime },d\right) } \in  \{ 0,1{\} }^{{d}^{\prime } \times  d}$和${S}^{\left( m,{d}^{\prime }\right) } \in  \{ 0,1{\} }^{m \times  {d}^{\prime }}$分别投影到前${d}^{\prime }$和$m$个坐标${}^{30}$上。由于${K}^{\left( T\right) }$可以在时间$O\left( T\right)$内应用，KacJL构造是一种嵌入时间为$O\left( {d\log d + \min \left\{  {d\log \left| X\right| ,{\varepsilon }^{-2}{\log }^{2}\left| X\right| {\log }^{2}\log \left| X\right| {\log }^{3}d}\right\}  }\right)$且目标维度渐近最优的JLD，并且仅应用第一部分$\left( {f}_{1}\right)$时，KacJL的嵌入时间为$O\left( {d\log d}\right)$，但目标维度为$O\left( {{\varepsilon }^{-2}\log \left| X\right| {\log }^{2}\log \left| X\right| {\log }^{3}d}\right)$，并非最优。

贾恩等人（Jain et al.）[Jai+20]还提出了他们的JLD构造的一个版本，该版本通过从集合$\{ \pi /4,{3\pi }/4,{5\pi }/4,{7\pi }/4\}$中甚至是单元素集合$\{ \pi /4\}$中均匀随机地选择角度${\theta }_{t}$，避免了计算三角函数${}^{31}$。这样做的${\cos }^{32}$是将${T}_{1}$增加$\log \log d$倍，将${T}_{2}$增加$\log d$倍，并且在单元素集合的情况下，与随机符号相乘（就像我们在之前的许多构造中对$D$矩阵所做的那样），并投影到坐标的随机子集上，而不是投影到前${d}^{\prime }$个或前$m$个坐标上。

$$
\text{-} \rightarrow  
$$

至此，我们完成了对约翰逊 - 林登斯特劳斯分布（Johnson - Lindenstrauss distributions）和变换的概述，不过仍有许多方面我们未作讨论，比如空间使用、预处理时间、随机性使用以及除 ${\ell }_{2}$ 之外的范数。然而，我们所涵盖的主要方面（JLD 的嵌入时间和目标维度）的总结可在表 1 中找到。

---

<!-- Footnote -->

${}^{30}$ 论文将 ${d}^{\prime }$ 和 $m$ 设为随机变量，但就此处所呈现的 JLD 而言，确定性投影就足够了，尽管这可能会影响隐藏在大 $O$ 表达式中的常数。

${}^{31}$ 回顾 $\sin \left( {\pi /4}\right)  = {2}^{-1/2}$，并且类似的结果对于余弦以及其他角度也成立。

${}^{32}$ 请注意，各种卡茨游走（Kac walk）长度仅被证明是足够的，因此更严谨的分析可能会缩短它们，并且或许能消除使用更简单角度的成本。

<!-- Footnote -->

---

<!-- Media -->

表1：挂毯数据表格化。

<!-- Media -->

## 2 延迟证明

### 2.1 k - 均值成本即成对距离

让我们先重复一下引理，以提醒自己需要证明什么。

引理1.6。设$k,d \in  {\mathbb{N}}_{1}$且对于$i \in  \{ 1,\ldots ,k\}$有${X}_{i} \subset  {\mathbb{R}}^{d}$，则

$$
\mathop{\sum }\limits_{{i = 1}}^{k}\mathop{\sum }\limits_{{x \in  {X}_{i}}}{\begin{Vmatrix}x - \frac{1}{\left| {X}_{i}\right| }\mathop{\sum }\limits_{{y \in  {X}_{i}}}y\end{Vmatrix}}_{2}^{2} = \frac{1}{2}\mathop{\sum }\limits_{{i = 1}}^{k}\frac{1}{\left| {X}_{i}\right| }\mathop{\sum }\limits_{{x,y \in  {X}_{i}}}\parallel x - y{\parallel }_{2}^{2}.
$$

为了证明引理1.6，我们需要以下引理。

引理2.1。设$d \in  {\mathbb{N}}_{1}$和$X \subset  {\mathbb{R}}^{d}$，并将$\mu  \mathrel{\text{:=}} \frac{1}{\left| X\right| }\mathop{\sum }\limits_{{x \in  X}}x$定义为$X$的均值，则有如下结论成立

$$
\mathop{\sum }\limits_{{x,y \in  X}}\langle x - \mu ,y - \mu \rangle  = 0.
$$

引理2.1的证明。该引理可由$\mu$的定义以及实内积的线性性质推出。

$$
\mathop{\sum }\limits_{{x,y \in  X}}\langle x - \mu ,y - \mu \rangle  = \mathop{\sum }\limits_{{x,y \in  X}}\left( {\langle x,y\rangle -\langle x,\mu \rangle -\langle y,\mu \rangle +\langle \mu ,\mu \rangle }\right) 
$$

$$
 = \mathop{\sum }\limits_{{x,y \in  X}}\langle x,y\rangle  - \mathop{\sum }\limits_{{x \in  X}}2\left| X\right| \langle x,\mu \rangle  + {\left| X\right| }^{2}\langle \mu ,\mu \rangle 
$$

$$
 = \mathop{\sum }\limits_{{x,y \in  X}}\langle x,y\rangle  - 2\mathop{\sum }\limits_{{x \in  X}}\left\langle  {x,\mathop{\sum }\limits_{{y \in  X}}y}\right\rangle   + \left\langle  {\mathop{\sum }\limits_{{x \in  X}}x,\mathop{\sum }\limits_{{y \in  X}}y}\right\rangle  
$$

$$
 = \mathop{\sum }\limits_{{x,y \in  X}}\langle x,y\rangle  - 2\mathop{\sum }\limits_{{x,y \in  X}}\langle x,y\rangle  + \mathop{\sum }\limits_{{x,y \in  X}}\langle x,y\rangle 
$$

$$
 = 0\text{.}
$$

引理1.6的证明。我们首先将证明每个划分的一个恒等式，因此，设${X}_{i} \subseteq  X \subset  {\mathbb{R}}^{d}$为数据集$X$的任意划分，并将${\mu }_{i} \mathrel{\text{:=}} \frac{1}{\left| {X}_{i}\right| }\mathop{\sum }\limits_{{x \in  {X}_{i}}}x$定义为${X}_{i}$的均值。

$$
\frac{1}{2\left| {X}_{i}\right| }\mathop{\sum }\limits_{{x,y \in  {X}_{i}}}\parallel x - y{\parallel }_{2}^{2} = \frac{1}{2\left| {X}_{i}\right| }\mathop{\sum }\limits_{{x,y \in  {X}_{i}}}{\begin{Vmatrix}\left( x - {\mu }_{i}\right)  - \left( y - {\mu }_{i}\right) \end{Vmatrix}}_{2}^{2}
$$

$$
 = \frac{1}{2\left| {X}_{i}\right| }\mathop{\sum }\limits_{{x,y \in  {X}_{i}}}\left( {{\begin{Vmatrix}x - {\mu }_{i}\end{Vmatrix}}_{2}^{2} + {\begin{Vmatrix}y - {\mu }_{i}\end{Vmatrix}}_{2}^{2} - 2\left\langle  {x - {\mu }_{i},y - {\mu }_{i}}\right\rangle  }\right) 
$$

$$
 = \mathop{\sum }\limits_{{x \in  {X}_{i}}}{\begin{Vmatrix}x - {\mu }_{i}\end{Vmatrix}}_{2}^{2} - \frac{1}{2\left| {X}_{i}\right| }\mathop{\sum }\limits_{{x,y \in  {X}_{i}}}2\left\langle  {x - {\mu }_{i},y - {\mu }_{i}}\right\rangle  
$$

$$
 = \mathop{\sum }\limits_{{x \in  {X}_{i}}}{\begin{Vmatrix}x - {\mu }_{i}\end{Vmatrix}}_{2}^{2}
$$

最后一个等式由引理2.1成立。我们现在使用刚刚推导出的恒等式来替换引理1.6中求和式里的每一项：

$$
\mathop{\sum }\limits_{{i = 1}}^{k}\mathop{\sum }\limits_{{x \in  {X}_{i}}}{\begin{Vmatrix}x - \frac{1}{\left| {X}_{i}\right| }\mathop{\sum }\limits_{{y \in  {X}_{i}}}y\end{Vmatrix}}_{2}^{2} = \frac{1}{2}\mathop{\sum }\limits_{{i = 1}}^{k}\frac{1}{\left| {X}_{i}\right| }\mathop{\sum }\limits_{{x,y \in  {X}_{i}}}\parallel x - y{\parallel }_{2}^{2}.
$$

### 2.2 超稀疏DKS（Super Sparse DKS）

定理1.8中给出的特征哈希性能的紧界可以扩展到DKS构造（DKS construction）的紧性能界。回顾一下，由所谓的列稀疏度 $s \in  {\mathbb{N}}_{1}$ 参数化的DKS构造的工作方式是，首先将向量 $x \in  {\mathbb{R}}^{d}$ 映射到 ${x}^{\prime } \in  {\mathbb{R}}^{sd}$，方法是将 ${xs}$ 中的每个元素复制若干次，然后用 $1/\sqrt{s}$ 进行缩放，再对 ${x}^{\prime }$ 应用特征哈希，因为与 $x$ 相比，${x}^{\prime }$ 具有更合适的 ${\ell }_{\infty }/{\ell }_{2}$ 比率。扩展结果的设定是，如果我们希望使用DKS构造，但只需要处理 $\parallel x{\parallel }_{\infty }/\parallel x{\parallel }_{2}$ 比率较小的向量，我们可以选择比通常的 $\Theta \left( {{\varepsilon }^{-1}\log \frac{1}{\delta }\log \frac{m}{\delta }}\right)$ 更小的列稀疏度，仍然能获得约翰逊 - 林登斯特劳斯（Johnson - Lindenstrauss）保证。这在推论1.9中得到了形式化表述。我们在推论1.9的证明中使用的定理1.8的两个支柱是，特征哈希的权衡是紧的，并且我们可以迫使DKS构造为特征哈希创建困难实例。

推论1.9。设${v}_{\mathrm{{DKS}}} \in  \left\lbrack  {1/\sqrt{d},1}\right\rbrack$表示所需的最大${\ell }_{\infty }/{\ell }_{2}$比率，${v}_{\mathrm{{FH}}}$表示定理1.8中所定义的特征哈希的${\ell }_{\infty }/{\ell }_{2}$约束，${s}_{\mathrm{{DKS}}} \in  \left\lbrack  m\right\rbrack$表示最小列稀疏度，使得具有该稀疏度的DKS构造是满足$\parallel x{\parallel }_{\infty }/\parallel x{\parallel }_{2} \leq  {v}_{\mathrm{{DKS}}}$的向量子集$x \in  {\mathbb{R}}^{d}$的JLD。那么

$$
{s}_{\mathrm{{DKS}}} = \Theta \left( \frac{{v}_{\mathrm{{DKS}}}^{2}}{{v}_{\mathrm{{FH}}}^{2}}\right) . \tag{12}
$$

推论1.9中$\Theta$的上界部分表明，我们可以将DKS构造（DKS construction）选择得多么稀疏，同时仍能为我们关注的数据提供约翰逊 - 林登斯特劳斯（Johnson - Lindenstrauss）保证；而下界则表明，如果我们选择的稀疏度低于此界限，那么存在一些向量，尽管其${\ell }_{\infty }/{\ell }_{2}$比率至多为${v}_{\mathrm{{DKS}}}$，但它们仍会经常出现过度失真的情况。

推论1.9的证明 让我们首先证明上界：${s}_{\mathrm{{DKS}}} = O\left( \frac{{v}_{\mathrm{{DKS}}}^{2}}{{v}_{\mathrm{{FH}}}^{2}}\right)$。

设 $s \mathrel{\text{:=}} \Theta \left( \frac{{v}_{\mathrm{{DES}}}^{2}}{{v}_{\mathrm{{FH}}}^{2}}\right)  \in  \left\lbrack  m\right\rbrack$ 为列稀疏度，并设 $x \in  {\mathbb{R}}^{d}$ 为一个单位向量，且

$\parallel x{\parallel }_{\infty } \leq  {v}_{\mathrm{{DKS}}}$ 。现在的目标是证明，稀疏度为 $s$ 的 DKS 构造（DKS construction）能够嵌入 $x$ ，同时以至少 $1 - \delta$ 的概率（如引理 1.2 所定义）将其范数保持在 $1 \pm  \varepsilon$ 范围内。设 ${x}^{\prime } \in  {\mathbb{R}}^{sd}$ 为按照 DKS 构造方式，将 ${xs}$ 中的每个元素复制若干次并按 $1/\sqrt{s}$ 进行缩放后得到的单位向量。现在我们有

$$
{\begin{Vmatrix}{x}^{\prime }\end{Vmatrix}}_{\infty } \leq  \frac{{v}_{\mathrm{{DKS}}}}{\sqrt{s}} = \Theta \left( {v}_{\mathrm{{FH}}}\right) . \tag{13}
$$

让DKS表示通过DKS构造得到的列稀疏度为$s$的联合低密度奇偶校验码（JLD），并让FH表示特征哈希JLD。然后我们可以得出结论

$$
\mathop{\Pr }\limits_{{f \sim  \mathrm{{DKS}}}}\left\lbrack  {\left| {\parallel f\left( x\right) {\parallel }_{2}^{2} - 1}\right|  \leq  \varepsilon }\right\rbrack   = \mathop{\Pr }\limits_{{g \sim  \mathrm{{FH}}}}\left\lbrack  {\left| {{\begin{Vmatrix}g\left( {x}^{\prime }\right) \end{Vmatrix}}_{2}^{2} - 1}\right|  \leq  \varepsilon }\right\rbrack   \geq  1 - \delta ,
$$

其中该不等式由等式(13)和定理1.8推出

现在让我们证明下界：${s}_{\mathrm{{DKS}}} = \Omega \left( \frac{{v}_{\mathrm{{DKS}}}^{2}}{{v}_{\mathrm{{FH}}}^{2}}\right)$ 。

设$s \mathrel{\text{:=}} o\left( \frac{{v}_{\mathrm{{DKS}}}^{2}}{{v}_{\mathrm{{FH}}}^{2}}\right)$，并设$x = {\left( {v}_{\mathrm{{DKS}}},\ldots ,{v}_{\mathrm{{DKS}}},0,\ldots ,0\right) }^{\top } \in  {\mathbb{R}}^{d}$为单位向量。我们现在要证明，稀疏度为$s$的DKS构造（DKS construction）将以严格小于$1 - \delta$的概率把$x$的范数保持在$1 \pm  \varepsilon$的范围内。和之前一样，将${x}^{\prime } \in  {\mathbb{R}}^{sd}$定义为DKS构造在把$xs$中的每个元素复制$1/\sqrt{s}$倍并进行缩放时所计算出的单位向量。这

给出

$$
{\begin{Vmatrix}{x}^{\prime }\end{Vmatrix}}_{\infty } = \frac{{v}_{\mathrm{{DKS}}}}{\sqrt{s}} = \omega \left( {v}_{\mathrm{{FH}}}\right) . \tag{14}
$$

最后，令DKS表示通过DKS构造得到的具有列稀疏性$s$的联合局部敏感哈希函数族（JLD），令FH表示特征哈希JLD。然后我们可以得出结论

$$
\mathop{\Pr }\limits_{{f \sim  \mathrm{{DKS}}}}\left\lbrack  {\left| {\parallel f\left( x\right) {\parallel }_{2}^{2} - 1}\right|  \leq  \varepsilon }\right\rbrack   = \mathop{\Pr }\limits_{{g \sim  \mathrm{{FH}}}}\left\lbrack  {\left| {{\begin{Vmatrix}g\left( {x}^{\prime }\right) \end{Vmatrix}}_{2}^{2} - 1}\right|  \leq  \varepsilon }\right\rbrack   < 1 - \delta ,
$$

其中该不等式由等式(14)和定理1.8以及${x}^{\prime }$具有特征哈希的渐近最坏情况实例的形式这一事实所蕴含。

### 2.3 轻量级张量联合局部敏感哈希（LWTJL）对过于稀疏的向量失效

命题1.11。对于任何种子矩阵，将LWT定义为以该矩阵为种子的LWTJL分布。那么对于所有的$\delta  \in  \left( {0,1}\right)$，存在一个向量$x \in  {\mathbb{C}}^{d}$（或者如果种子矩阵是实矩阵，则为$x \in  {\mathbb{R}}^{d}$）满足$\parallel x{\parallel }_{\infty }/\parallel x{\parallel }_{2} = \Theta \left( {{\log }^{-1/2}\frac{1}{\delta }}\right)$，使得

$$
\mathop{\Pr }\limits_{{f \sim  \mathrm{{LWT}}}}\left\lbrack  {f\left( x\right)  = \mathbf{0}}\right\rbrack   > \delta 
$$

证明。主要思路是从以一定概率与种子矩阵正交的线段中构建向量 $x$，然后证明向量 $x$ 同时与种子矩阵的所有副本正交的概率大于 $\delta$。

设 $r,c \in  {\mathbb{N}}_{1}$ 为常数，${A}_{1} \in  {\mathbb{C}}^{r \times  c}$ 为种子矩阵。设 $d$ 为 LWTJL 构造的源维度，$D \in  \{  - 1,0,1{\} }^{d \times  d}$ 为具有独立同分布拉德马赫（Rademacher）变量的随机对角矩阵，$l \in  {\mathbb{N}}_{1}$ 使得 ${c}^{l} = d$ ，且 ${A}_{l} \in  {\mathbb{C}}^{{r}^{l} \times  {c}^{l}}$ 为 LWT，即 ${A}_{l} \mathrel{\text{:=}} {A}_{1}^{\otimes l}$ 。由于 $r < c$ ，存在一个非平凡向量 $z \in  {\mathbb{C}}^{c} \smallsetminus  \{ \mathbf{0}\}$ 与 ${A}_{1}$ 的所有 $r$ 行以及 $\parallel z{\parallel }_{\infty } = \Theta \left( 1\right)$ 正交。现在将 $x \in  {\mathbb{C}}^{d}$ 定义为 $z$ 的 $k \in  {\mathbb{N}}_{1}$ 个副本，后面再填充 $0\mathrm{\;s}$ ，其中 $k = \left\lfloor  {\frac{1}{c}\lg \frac{1}{\delta } - 1}\right\rfloor$ 。注意，如果种子矩阵是实矩阵，我们也可以选择 $z$ ，从而 $x$ 也为实矩阵。

首先需要注意的是

$$
\parallel x{\parallel }_{0} \leq  {ck} < \lg \frac{1}{\delta },
$$

这意味着

$$
\mathop{\Pr }\limits_{D}\left\lbrack  {{Dx} = x}\right\rbrack   = {2}^{-\parallel x{\parallel }_{0}} > \delta .
$$

其次，由于${A}_{l}$的克罗内克结构（Kronecker structure）以及$z$与${A}_{1}$的行正交这一事实，我们有

$$
{Ax} = \mathbf{0}\text{.}
$$

综上所述，我们可以得出结论

$$
\mathop{\Pr }\limits_{{f \sim  \mathrm{{LWT}}}}\left\lbrack  {f\left( x\right)  = \mathbf{0}}\right\rbrack   \geq  \mathop{\Pr }\limits_{D}\left\lbrack  {{A}_{l}{Dx} = \mathbf{0}}\right\rbrack   \geq  \mathop{\Pr }\limits_{D}\left\lbrack  {{Dx} = x}\right\rbrack   > \delta .
$$

现在我们只需证明$\parallel x{\parallel }_{\infty }/\parallel x{\parallel }_{2} = \Theta \left( {{\log }^{-1/2}\frac{1}{\delta }}\right)$。由于$c$是一个常数，且$x$由$k = \Theta \left( {\log \frac{1}{\delta }}\right)$个$z$的副本后面跟着零组成，

$$
\parallel x{\parallel }_{\infty } = \parallel z{\parallel }_{\infty } = \Theta \left( 1\right) ,
$$

$$
\parallel z{\parallel }_{2} = \Theta \left( 1\right) 
$$

$$
\parallel x{\parallel }_{2} = \sqrt{k}\parallel z{\parallel }_{2} = \Theta \left( \sqrt{\log \frac{1}{\delta }}\right) ,
$$

这意味着所声称的比率

$$
\frac{\parallel x{\parallel }_{\infty }}{\parallel x{\parallel }_{2}} = \Theta \left( {{\log }^{-1/2}\frac{1}{\delta }}\right) .
$$

以下推论只是根据引理1.2对命题1.11的重述，因此其证明可直接从命题1.11得出

推论2.2。对于每一个$m,d, \in  {\mathbb{N}}_{1}$、$\delta ,\varepsilon  \in  \left( {0,1}\right)$，以及在$f : {\mathbb{K}}^{d} \rightarrow  {\mathbb{K}}^{m}$上的LWTJL分布LWT（其中$\mathbb{K} \in  \{ \mathbb{R},\mathbb{C}\}$且$m < d$），存在一个向量$x \in  {\mathbb{K}}^{d}$，满足$\parallel x{\parallel }_{\infty }/\parallel x{\parallel }_{2} = \Theta \left( {{\log }^{-1/2}\frac{1}{\delta }}\right)$，使得

$$
\mathop{\Pr }\limits_{{f \sim  \operatorname{LWT}}}\left\lbrack  {\left| {\parallel f\left( x\right) {\parallel }_{2}^{2} - \parallel x{\parallel }_{2}^{2}}\right|  \leq  \varepsilon \parallel x{\parallel }_{2}^{2}}\right\rbrack   < 1 - \delta .
$$