### 1.4 The Tapestry of Johnson-Lindenstrauss Transforms

As mentioned in section 1.2, the original JLD from [JL84] is a distribution over functions $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}$, where ${ }^{15} f(x)=(d / m)^{1 / 2} A x$ and $A$ is a random $m \times d$ matrix whose rows form an orthonormal basis of some $m$-dimensional subspace of $\mathbb{R}^{d}$, i.e. the rows are unit vectors and pairwise orthogonal. While Johnson and Lindenstrauss [JL84] showed that $m=\Theta\left(\varepsilon^{-2} \log |X|\right)$ suffices to prove lemma 1.1, they did not give any bounds on the constant in the big- $O$ expression. This was remedied in [|FM88|, which proved that $m=\left\lceil 9\left(\varepsilon^{2}-2 \varepsilon^{3} / 3\right)^{-1} \ln |X|\right\rceil+1$ suffices for the same JLD if $m<\sqrt{|X|}$. This bound was further improved in FM90| by removing the $m<\sqrt{|X|}$ restriction and lowering the bound to $m=\left\lceil 8\left(\varepsilon^{2}-2 \varepsilon^{3} / 3\right)^{-1} \ln |X| \mid\right.$.

---

The next thread of JL research worked on simplifying the JLD constructions as Indyk and Motwani [HIM12] showed that sampling each entry in the matrix i.i.d. from a properly scaled Gaussian distribution is a JLD. The rows of such a matrix do not form a basis as they are with high probability not orthogonal; however, the literature still refer to this and most other JLDs as random projections. Shortly thereafter Arriaga and Vempala [AV06] constructed a JLD by sampling i.i.d. from a Rademacher ${ }^{16}$ distribution, and Achlioptas [Ach03] sparsified the Rademacher construction such that the entries $a_{i j}$ are sampled i.i.d. with $\operatorname{Pr}\left[a_{i j}=0\right]=$ $2 / 3$ and $\operatorname{Pr}\left[a_{i j}=-1\right]=\operatorname{Pr}\left[a_{i j}=1\right]=1 / 6$. We will refer to such sparse i.i.d. Rademacher constructions as Achlioptas constructions. The Gaussian and Rademacher results have later been generalised [Mat08; IN07; KM05] to show that a JLD can be constructed by sampling each entry in a $m \times d$ matrix i.i.d. from any distribution with mean 0 , variance 1 , and a subgaussian tai ${ }^{17}$. It should be noted that these developments have a parallel in the streaming literature as the previously mentioned AMS Sketch [AMS99; Alo+02] is identical to the Rademacher construction [AV06], albeit with constant error probability.

---

As for the target dimension for these constructions, [HIM12] proved that the Gaussian construction is a JLD if $m \geq 8\left(\varepsilon^{2}-2 \varepsilon^{3} / 3\right)^{-1}(\ln |X|+O(\log m))$, which roughly corresponds to an additional additive $O\left(\varepsilon^{-2} \log \log |X|\right)$ term over the original construction. This additive $\log \log$ term was shaved off by the proof in [DG02], which concerns itself with the original JLD construction but can easily ${ }^{18}$ be adapted to the Gaussian construction, and the proof in [AV06], which also give the same $\log \log$ free bound for the dense Rademacher construction. Achlioptas [Ach03] showed that his construction also achieves $m=\left\lceil 8\left(\varepsilon^{2}-2 \varepsilon^{3} / 3\right)^{-1} \ln |X|\right\rceil$. The constant of 8 has been improved for the Gaussian and dense Rademacher constructions in the sense that Rojo and Nguyen [RN10; Ngu09] have been able to replace the bound with more intricate 19 expressions, which yield a 10 to $40 \%$ improvement for many sets of parameters. However, in the distributional setting it has been shown in [BGK18] that $m \geq 4 \varepsilon^{-2} \ln \frac{1}{\delta}(1-o(1))$ is necessary for any JLD to satisfy lemma 1.2 , which corresponds to a constant of 8 if we prove lemma 1.1 the usual way by setting $\delta=n^{-2}$ and union bounding over all pairs of vectors.

---

There seems to have been some confusion in the literature regarding the improvements in target dimension. The main pitfall was that some papers [e.g. Ach03; HIM12; DG02; RN10; BGK18] were only referring to [FM88] when referring to the target dimension bound of the original construction. As such, [Ach03: HIM12] mistakenly claim to improve the constant for the target dimension with their constructions. Furthermore, [Ach01] is sometimes [e.g. in AC09; Mat08; Sch18| the only work credited for the Rademacher construction, despite it being developed independently and published 2 years prior in [AV99].

---

All the constructions that have been mentioned so far in this section, embed a vector by performing a relatively dense and unstructured matrix-vector multiplication, which takes $\Theta\left(m\|x\|_{0}\right)=O(m d)$ time ${ }^{20}$ to compute. This sparked two distinct but intertwined strands of research seeking to reduce the embedding time, namely the sparsity-based JLDs which dealt with the density of the embedding matrix and the fast Fourier transform-based which introduced more structure to the matrix.

### 1.4.1 Sparse Johnson-Lindenstrauss Transforms

The simple fact underlying the following string of results is that if $A$ has $s$ nonzero entries per column, then $f(x)=A x$ can be computed in $\Theta\left(s\|x\|_{0}\right)$ time. The first result here is the Achlioptas construction [Ach03] mentioned above, whose column sparsity $s$ is $m / 3$ in expectancy, which leads to an embedding time that is a third of the full Rademacher construction ${ }^{[21]}$ However, the first superconstant improvement is due to Dasgupta, Kumar, and Sarlós [DKS10], who based on heuristic approaches [Wei+09; Shi+09b; LLS07; GD08] constructed a JLD with $s=O\left(\varepsilon^{-1} \log \frac{1}{\delta} \log ^{2} \frac{m}{\delta}\right)$. Their construction, which we will refer to as the DKS construction, works by sampling $s$ hash functions $h_{1}, \ldots, h_{s}:[d] \rightarrow\{-1,1\} \times[m]$ independently, such that each source entry $x_{i}$ will be hashed to $s$ random signs $\sigma_{i, 1}, \ldots, \sigma_{i, s}$ and $s$ target coordinates $j_{i, 1}, \ldots, j_{i, s}$ (with replacement). The embedding can then be defined as $f(x):=\sum_{i} \sum_{k} e_{j_{i, k}} \sigma_{i, k} x_{i,}$ which is to say that every source coordinate is hashed to $s$ output coordinates and randomly added to or subtracted from those output coordinates. The sparsity analysis was later tightened to show that $s=O\left(\varepsilon^{-1} \log \frac{1}{\delta} \log \frac{m}{\delta}\right)$ suffices KN10; KN14] and even $s=O\left(\varepsilon^{-1}\left(\frac{\log \frac{1}{\delta} \log \log \log \frac{1}{\delta}}{\log \log \frac{1}{\delta}}\right)^{2}\right)$ suffices for the DKS construction assuming $\varepsilon<\log ^{-2} \frac{1}{\delta}$ [BOR10], while [KN14] showed that $s=\Omega\left(\varepsilon^{-1} \log ^{2} \frac{1}{\delta} / \log ^{2} \frac{1}{\varepsilon}\right)$ is neccessary for the DKS construction.

---

Kane and Nelson [KN14] present two constructions that circumvent the DKS lower bound by ensuring that the hash functions do not collide within a column, i.e. $j_{i, a} \neq j_{i, b}$ for all $i, a$, and $b$. The first construction, which we will refer to as the graph construction, simply samples the $s$ coordinates without replacement. The second construction, which we will refer to as the block construction, partitions the output vector into $s$ consecutive blocks of length $\mathrm{m} / \mathrm{s}$ and samples one output coordinate per block. Note that the block construction is the same as Count Sketch from the streaming literature [CG05: CCF04], though the hash functions differ and the output is interpreted differently. Kane and Nelson [KN14| prove that $s=\Theta\left(\varepsilon^{-1} \log \frac{1}{\delta}\right)$ is both neccessary and sufficient in order for their two constructions to satisfy lemma 1.2 Note that while Count Sketch is even sparser than the lower bound for the block construction, it does not contradict it as Count sketch does not embed into $\ell_{2}^{m}$ as it computes the median, which is nonlinear. As far as general sparsity lower bounds go, [DKS10] shows that an average column sparsity of $s_{\text {avg }}=\Omega\left(\min \left\{\varepsilon^{-2}, \varepsilon^{-1} \sqrt{\log _{m} d}\right\}\right)$ is neccessary for a sparse JLD, while Nelson and Nguyễn [NN13b] improves upon this by showing that there exists a set of points $X \in \mathbb{R}^{d}$ such that any JLT for that set must have column sparsity $s=\Omega\left(\varepsilon^{-1} \log |X| / \log \frac{1}{\varepsilon}\right)$ in order to satisfy lemma 1.1. And so it seems that we have almost reached the limit of the sparse JL approach, but why should theory be in the way of a good result? Let us massage the definitions so as to get around these lower bounds.

---

The hard instances used to prove the lower bounds [NN13b; KN14] consist of very sparse vectors, e.g. $x=(1 / \sqrt{2}, 1 / \sqrt{2}, 0, \ldots, 0)^{\top}$, but the vectors we are interested in applying a JLT to might not be so unpleasant, and so by restricting the input vectors to be sufficiently "nice", we can get meaningful result that perform better than what the pessimistic lower bound would indicate. The formal formulation of this niceness is bounding the $\ell_{\infty} / \ell_{2}$ ratio of the vectors lemmas 1.1 and 1.2 need apply to. Let us denote this norm ratio as $v \in[1 / \sqrt{d}, 1]$, and revisit some of the sparse JLDs. The Achlioptas construction [Ach03] can be generalised so that the expected number of nonzero entries per column is $q m$ rather than $\frac{1}{3} m$ for a parameter $q \in[0,1]$. Ailon and Chazelle $|\mathrm{AC} 09|$ show that if $v=O\left(\sqrt{\log \frac{1}{\delta}} / \sqrt{d}\right)$ then choosing $q=\Theta\left(\frac{\log ^{2} 1 / \delta}{d}\right)$ and sampling the nonzero entries from a Gaussian distribution suffices. This result is generalised in [Mat08] by proving that for all $v \in[1 / \sqrt{d}, 1]$ choosing $q=\Theta\left(v^{2} \log \frac{d}{\varepsilon \delta}\right)$ and sampling the nonzero entries from a Rademacher distribution is a JLD for the vectors constrained by that $v$.

---

Be aware that sometimes [e.g. in DKS10 BOR10 this bound ${ }^{22}$ on $q$ is misinterpreted as a lower bound stating that $q m=\tilde{\Omega}\left(\varepsilon^{-2}\right)$ is neccessary for the Achlioptas construction when $v=1$. However, Matoušek [Mat08] only loosely argues that his bound is tight for $v \leq d^{-0.1}$, and if it indeed was tight at $v=1$, the factors hidden by the $\tilde{\Omega}$ would lead to the contradiction that $m \geq q m=\Omega\left(\varepsilon^{-2} \log \frac{1}{\delta} \log \frac{d}{\varepsilon \delta}\right)=\omega(m)$.

---

The heuristic Wei+09; Shi+09b; LLS07; GD08] that DKS10] is based on is called Feature Hashing a.k.a. the hashing trick a.k.a. the hashing kernel and is a sparse JL construction with exactly 1 nonzero entry per column ${ }^{23}$ The block construction can then be viewed as the concatenation of $s=\Theta\left(\varepsilon^{-1} \log \frac{1}{\delta}\right)$ feature hashing instances, and the DKS construction can be viewed as the sum of $s=O\left(\varepsilon^{-1} \log \frac{1}{\delta} \log \frac{m}{\delta}\right)$ Feature Hashing instances or alternatively as first duplicating each entry of $x \in \mathbb{R}^{d} s$ times before applying Feature Hashing to the enlarged vector $x^{\prime} \in \mathbb{R}^{s d}$ : Let $f_{\text {dup }}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{s d}$ be a function that duplicates each entry in its input $s$ times, i.e. $f_{\text {dup }}(x)_{(i-1) s+j}=x_{(i-1) s+j}^{\prime}:=x_{i}$ for $i \in[d], j \in[s]$, then $f_{\text {DKS }}=f_{\text {FH }} \circ f_{\text {dup }}$.

---

This duplication is the key to the analysis in [DKS10] as $f_{\text {dup }}$ is isometric (up to normalisation) and it ensures that the $\ell_{\infty} / \ell_{2}$ ratio of $x^{\prime}$ is small, i.e. $v \leq 1 / \sqrt{s}$ from the point of view of the Feature Hashing data structure ( $f_{\mathrm{FH}}$ ). And so, any lower bound on the sparsity of the DKS construction (e.g. the one given in [KN14]) gives an upper bound on the values of $v$ for which Feature Hashing is a JLD: If $u$ is a unit vector such that a DKS instance with sparsity $\hat{s}$ fails to preserve $u$ s norm within $1 \pm \varepsilon$ with probability $\delta$, then it must be the case that Feature Hashing fails to preserve the norm of $f_{\text {dup }}(u)$ within $1 \pm \varepsilon$ with probability $\delta$, and therefore the $\ell_{\infty} / \ell_{2}$ ratio for which Feature Hashing can handle all vectors is strictly less than $1 / \sqrt{\hat{s}}$.

---

Written more concisely the statement is $s_{\mathrm{DKS}}=\Omega(a) \Longrightarrow v_{\mathrm{FH}}=O(1 / \sqrt{a})$ and by contraposition ${ }^{24} v_{\mathrm{FH}}=\Omega(1 / \sqrt{a}) \Longrightarrow s_{\mathrm{DKS}}=O(a)$, where $s_{\mathrm{DKS}}$ is the minimum column sparsity of a DKS construction that is a JLD, $v_{\mathrm{FH}}$ is the maximum $\ell_{\infty} / \ell_{2}$ constraint for which Feature Hashing is a JLD, and $a$ is any positive expression. Furthermore, if we prove an upper bound on $v_{\mathrm{FH}}$ using a hard instance that is identical to an $x^{\prime}$ that the DKS construction can generate after duplication, we can replace the previous two implications with bi-implications.

---

Wei+09] claims to give a bound on $v_{\mathrm{FH}}$, but it sadly contains an error in its proof of this bound [DKS10; Wei+10]. Dahlgaard, Knudsen, and Thorup [DKT17] improve the $v_{\text {FH }}$ lower bound to $v_{\mathrm{FH}}=\Omega\left(\sqrt{\frac{\varepsilon \log \left(1+\frac{4}{\delta}\right)}{\log \frac{1}{\delta} \log \frac{m}{\delta}}}\right)$, and Freksen, Kamma, and Larsen [FKL18] give an intricate but tight bound for $v_{\mathrm{FH}}$ shown in theorem 1.8, where the hard instance used to prove the upper bound is identical to an $x^{\prime}$ from the DKS construction.

---

Theorem 1.8 ([|FKL18]). There exist constants $C \geq D>0$ such that for every $\varepsilon, \delta \in(0,1)$ and $m \in \mathbb{N}_{1}$ the following holds. If $\frac{\mathrm{Clg} \frac{1}{\delta}}{\varepsilon^{2}} \leq m<\frac{2}{\varepsilon^{2} \delta}$ then

$$
\nu_{\mathrm{FH}}(m, \varepsilon, \delta)=\Theta\left(\sqrt{\varepsilon} \min \left\{\frac{\log \frac{\varepsilon m}{\log \frac{1}{\delta}}}{\log \frac{1}{\delta}}, \sqrt{\frac{\log \frac{\varepsilon^{2} m}{\log \frac{1}{\delta}}}{\log \frac{1}{\delta}}}\right\}\right) .
$$

Otherwise, if $m \geq \frac{2}{\varepsilon^{2} \delta}$ then $\mathcal{v}_{\mathrm{FH}}(m, \varepsilon, \delta)=1$. Moreover if $m<\frac{D \lg \frac{1}{\delta}}{\varepsilon^{2}}$ then $v_{\mathrm{FH}}(m, \varepsilon, \delta)=0$.

---

Furthermore, if an $x \in\{0,1\}^{d}$ satisfies $v_{\mathrm{FH}}<\|x\|_{2}^{-1}<1$ then

$$
\operatorname{Pr}_{f \sim \mathcal{F H}}\left[\left|\|f(x)\|_{2}^{2}-\|x\|_{2}^{2}\right|>\varepsilon\|x\|_{2}^{2}\right]>\delta .
$$

This bound gives a tight tradeoff between target dimension $m$, distortion $\varepsilon$, error probability $\delta$, and $\ell_{\infty} / \ell_{2}$ constraint $v$ for Feature Hashing, while showing how to construct hard instances for Feature Hashing: Vectors with the shape $x=(1, \ldots, 1,0, \ldots, 0)^{\top}$ are hard instances if they contain few 1 s , meaning that Feature Hashing cannot preserve their norms within $1 \pm \varepsilon$ with probability $\delta$. Theorem 1.8 is used in corollary 1.9 to provide a tight tradeoff between $m, \varepsilon, \delta, v$, and column sparsity $s$ for the DKS construction.

---

Corollary 1.9. Let $v_{\mathrm{DKS}} \in[1 / \sqrt{d}, 1]$ denote the largest $\ell_{\infty} / \ell_{2}$ ratio required, $v_{\mathrm{FH}}$ denote the $\ell_{\infty} / \ell_{2}$ constraint for Feature Hashing as defined in theorem 1.8. and $s_{\mathrm{DKS}} \in[m]$ as the minimum column sparsity such that the DKS construction with that sparsity is a JLD for the subset of vectors $x \in \mathbb{R}^{d}$ that satisfy $\|x\|_{\infty} /\|x\|_{2} \leq v_{\text {DKS }}$. Then

$$
s_{\mathrm{DKS}}=\Theta\left(\frac{v_{\mathrm{DKS}}^{2}}{v_{\mathrm{FH}}^{2}}\right) .
$$

The proof of this corollary is deferred to section 2.2 .

---

Jagadeesan [Jag19] generalised the result from [FKL18] to give a lower bound ${ }^{25]}$ on the $m, \varepsilon, \delta$, $v$, and $s$ tradeoff for any sparse Rademacher construction with a chosen column sparsity, e.g. the block and graph constructions, and gives a matching upper bound for the graph construction.

### 1.4.2 Structured Johnson-Lindenstrauss Transforms

As we move away from the sparse JLDs we will slightly change our idea of what an efficient JLD is. In the previous section the JLDs were especially fast when the vectors were sparse, as the running time scaled with $\|x\|_{0}$, whereas we in this section will optimise for dense input vectors such that an embedding time of $O(d \log d)$ is a satisfying result.

---

The chronologically first asymptotic improvement over the original JLD construction is due to Ailon and Chazelle [AC09] who introduced the so-called Fast Johnson-Lindenstrauss Transform (FJLT). As mentioned in the previous section, [AC09] showed that we can use a very sparse (and therefore very fast) embedding matrix as long as the vectors have a low $\ell_{\infty} / \ell_{2}$ ratio, and furthermore that applying a randomised Walsh-Hadamard transform to a vector results in a low $\ell_{\infty} / \ell_{2}$ ratio with high probability. And so, the FJLT is defined as $f(x):=P H D x$, where $P \in \mathbb{R}^{m \times d}$ is a sparse Achlioptas matrix with Gaussian entries and $q=\Theta\left(\frac{\log ^{2} 1 / \delta}{d}\right), H \in\{-1,1\}^{d \times d}$ is a Walsh-Hadamard matrix ${ }^{26}$, and $D \in\{-1,0,1\}^{d \times d}$ is a random diagonal matrix with i.i.d. Rademachers on the diagonal. As the Walsh-Hadamard transform can be computed using a simple recursive formula, the expected embedding time becomes $O\left(d \log d+m \log ^{2} \frac{1}{\delta}\right)$. And as mentioned, [Mat08] showed that we can sample from a Rademacher rather than a Gaussian distribution when constructing the matrix $P$. The embedding time improvement of FJLT over previous constructions depends on the relationship between $m$ and $d$. If $m=\Theta\left(\varepsilon^{-2} \log \frac{1}{\delta}\right)$ and $m=O\left(\varepsilon^{-4 / 3} d^{1 / 3}\right)$, FJLT's embedding time becomes bounded by the Walsh-Hadamard transform at $O(d \log d)$, but at $m=\Theta\left(d^{1 / 2}\right)$ FJLT is only barely faster than the original construction.

---

Ailon and Liberty [AL09] improved the running time of the FJLT construction to $O(d \log m)$ for $m=O\left(d^{1 / 2-\gamma}\right)$ for any fixed $\gamma>0$. The increased applicable range of $m$ was achieved by applying multiple randomised Walsh-Hadamard transformations, i.e. replacing HD with $\prod_{i} H D^{(i)}$, where the $D^{(i)}$ s are a constant number of independent diagonal Rademacher matrices, as well as by replacing $P$ with $B D$ where $D$ is yet another diagonal matrix with Rademacher entries and $B$ is consecutive blocks of specific partial Walsh-Hadamard matrices (based on so-called binary dual BCH codes [see e.g. MS77]). The reduction in running time comes from altering the transform slightly by partitioning the input into consecutive blocks of length poly $(m)$ and applying the randomised Walsh-Hadamard transforms to each of them independently. We will refer to this variant of FJLT as the BCHJL construction.

---

The next pattern of results has roots in compressed sensing and approaches the problem from another angle: Rather than being fast only when $m \ll d$, they achieve $O(d \log d)$ embedding time even when $m$ is close to $d$, at the cost of $m$ being suboptimal. Before describing these constructions, let us set the scene by briefly introducing some concepts from compressed sensing.

---

Roughly speaking, compressed sensing concerns itself with recovering a sparse signal via a small number of linear measurements and a key concept here is the Restricted Isometry Property [CT05; CT06: CRT06; Don06].

---

Definition 1.10 (Restricted Isometry Property). Let $d, m, k \in \mathbb{N}_{1}$ with $m, k<d$ and $\varepsilon \in(0,1)$. A linear function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}$ is said to have the Restricted Isometry Property of order $k$ and level $\varepsilon$ (which we will denote as $(k, \varepsilon)$-RIP) if for all $x \in \mathbb{R}^{d}$ with $\|x\|_{0} \leq k$,

$$
\begin{equation*}
\left|\|f(x)\|_{2}^{2}-\|x\|_{2}^{2}\right| \leq \varepsilon\|x\|_{2}^{2} . \tag{10}
\end{equation*}
$$

---

In the compressed sensing literature it has been shown [CT06; RV08] that the subsampled Hadamard transform (SHT) defined as $f(x):=S H x$, has the $(k, \varepsilon)$-RIP with high probability

$$
\left(\begin{array}{ccccc}
t_{0} & t_{1} & t_{2} & \cdots & t_{d-1} \\
t_{-1} & t_{0} & t_{1} & \cdots & t_{d-2} \\
t_{-2} & t_{-1} & t_{0} & \cdots & t_{d-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
t_{-(m-1)} & t_{-(m-2)} & t_{-(m-3)} & \cdots & t_{d-m}
\end{array}\right)
$$

for $m=\Omega\left(\varepsilon^{-2} k \log ^{4} d\right)$ while allowing a vector to be embedded in $O(d \log d)$ time. Here $H \in\{-1,1\}^{d \times d}$ is the Walsh-Hadamard matrix and $S \in\{0,1\}^{m \times d}$ samples $m$ entries of $H x$ with replacement, i.e. each row in $S$ has one non-zero entry per row, which is chosen uniformly and independently, i.e. $S$ is a uniformly random feature selection matrix. Inspired by this transform and the FJLT mentioned previously, Ailon and Liberty [AL13] were able to show that the subsampled randomised Hadamard transform (SRHT) defined as $f(x):=S H D x$, is a JLT if $m=\Theta\left(\varepsilon^{-4} \log |X| \log ^{4} d\right)$. Once again $D$ denotes a random diagonal matrix with Rademacher entries, and $S$ and $H$ is as in the SHT. Some related results include Do et al. [Do+09] who before [AL13] were able to get a bound of $m=\Theta\left(\varepsilon^{-2} \log ^{3}|X|\right)$ in the large set case where $|X| \geq d$, [Tro11] which showed how the SRHT construction approximately preserves the norms of a subspace of vectors, and [LL20] which modified the sampling matrix $S$ to improve precision when used as a preprocessor for support vector machines (SVMs) by sacrificing input data independence.

---

This target dimension bound of [AL13] was later tightened by Krahmer and Ward [KW11], who showed that $m=\Theta\left(\varepsilon^{-2} \log |X| \log ^{4} d\right)$ suffices for the SRHT. This was a corollary of a more general result, namely that if $\sigma: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ applies random signs equivalently to the $D$ matrices mentioned previously and $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}$ has the $(\Omega(\log |X|), \varepsilon / 4)$-RIP then $f \circ \sigma$ is a JLT with high probability. An earlier result by Baraniuk et al. |Bar+08| showed that a transform sampled from a JLD has the $\left(O\left(\varepsilon^{2} m / \log d\right), \varepsilon\right)$-RIP with high probability. And so, as one might have expected from their appearance, the Johnson-Lindenstrauss Lemma and the Restricted Isometry Property are indeed cut from the same cloth.

---

Another transform from the compressed sensing literature uses so-called Toeplitz or partial circulant matrices [Baj+07; Rau09; Rom09; Hau+10; RRT12; Baj12; DJR19], which can be defined in the following way. For $m, d \in \mathbb{N}_{1}$ we say that $T \in \mathbb{R}^{m \times d}$ is a real Toeplitz matrix if there exists $t_{-(m-1)}, t_{-(m-2)} \ldots, t_{d-1} \in \mathbb{R}$ such that $T_{i j}=t_{j-i}$. This has the effect that the entries on any one diagonal are the same (see fig. (1) and computing the matrix-vector product corresponds to computing the convolution with a vector of the $t$ s. Partial circulant matrices are special cases of Toeplitz matrices where the diagonals "wrap around" the ends of the matrix, i.e. $t_{-i}=t_{d-i}$ for all $i \in[m-1]$.

---

As a JLT, the Toeplitz construction is $f(x):=T D x$, where $T \in\{-1,1\}^{m \times d}$ is a Toeplitz matrix with i.i.d. Rademacher entries and $D \in\{-1,0,1\}^{d \times d}$ is a diagonal matrix with Rademacher entries as usual. Note that the convolution of two vectors corresponds to the entrywise product in Fourier space, and we can therefore employ fast Fourier transform (FFT) to embed a vector with the Toeplitz construction in time $O(d \log d)$. This time can even be reduced to $O(d \log m)$ as we realise that by partitioning $T$ into $\frac{d}{m}$ consecutive blocks of size $m \times m$, each block is also a Toeplitz matrix, and by applying each individually the embedding time becomes $O\left(\frac{d}{m} m \log m\right)$.

---

Combining the result from [KW11] with RIP bounds for Toeplitz matrices [RRT12] gives that $m=\Theta\left(\varepsilon^{-1} \log ^{3 / 2}|X| \log ^{3 / 2} d+\varepsilon^{-2} \log |X| \log ^{4} d\right)$ is sufficient for the Toeplitz construction to be a JLT with high probability. However, the Toeplitz construction has also been studied directly as a JLD without going via its RIP bounds. Hinrichs and Vybíral [HV11] showed that $m=\Theta\left(\varepsilon^{-2} \log ^{3} \frac{1}{\delta}\right)$ is sufficient for the Toeplitz construction, and this bound was improved shortly thereafter in [Vyb11] to $m=\Theta\left(\varepsilon^{-2} \log ^{2} \frac{1}{\delta}\right)$. The question then is if we can tighten the analysis to shave off the last $\log$ factor and get the elusive result of a JLD with optimal target dimension and $O(d \log d)$ embedding time even when $m$ is close to $d$. Sadly, this is not the case as Freksen and Larsen [FL20] showed that there exists vector ${ }^{277}$ that necessitates $m=\Omega\left(\varepsilon^{-2} \log ^{2} \frac{1}{\delta}\right)$ for the Toeplitz construction.

---

Just as JLTs are used as preprocessors to speed up algorithms that solve the problems we actually care about, we can also use a JLT to speed up other JLTs in what one could refer to as compound JLTs. More explicitely if $f_{1}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d^{\prime}}$ and $f_{2}: \mathbb{R}^{d^{\prime}} \rightarrow \mathbb{R}^{m}$ with $m \ll d^{\prime} \ll d$ are two JLTs and computing $f_{1}(x)$ is fast, we could hope that computing $\left(f_{2} \circ f_{1}\right)(x)$ is fast as well as $f_{2}$ only need to handle $d^{\prime}$ dimensional vectors and hope that $\left(f_{2} \circ f_{1}\right)$ preserves the norm sufficiently well since both $f_{1}$ and $f_{2}$ approximately preserve norms individually. As presented here, the obvious candidate for $f_{1}$ is one of the RIP-based JLDs, which was succesfully applied in [BK17]. In their construction, which we will refer to as GRHD ${ }^{28}$, $f_{1}$ is the SRHT and $f_{2}$ is the dense Rademacher construction (i.e. $f(x):=A_{\text {Rad }} S H D x$ ), and it can embed a vector in time $O(d \log m)$ for $m=O\left(d^{1 / 2-\gamma}\right)$ for any fixed $\gamma>0$. This is a similar result to the construction of Ailon and Liberty [AL09], but unlike that construction, GRHD handles the remaining range of $m$ more gracefully as for any $r \in[1 / 2,1]$ and $m=O\left(d^{r}\right)$, the embedding time for GRHD becomes $O\left(d^{2 r} \log ^{4} d\right)$. However the main selling point of the GRHD construction is that it allows the simultaneous embedding of sufficiently large sets of points $X$ to be computed in total time $O(|X| d \log m)$, even when $m=\Theta\left(d^{1-\gamma}\right)$ for any fixed $\gamma>0$, by utilising fast matrix-matrix multiplication techniques [LR83].

---

Another compound JLD is based on the so-called lean Walsh transforms (LWT) [LAS11], which are defined based on so-called seed matrices. For $r, c \in \mathbb{N}_{1}$ we say that $A_{1} \in \mathbb{C}^{r \times c}$ is a seed matrix if $r<c$, its columns are of unit length, and its rows are pairwise orthogonal and have the same $\ell_{2}$ norm. As such, partial Walsh-Hadamard matrices and partial Fourier matrices are seed matrices (up to normalisation); however, for simplicity's sake we will keep it real by focusing on partial Walsh-Hadamard matrices. We can then define a LWT of order $l \in \mathbb{N}_{1}$ based on this seed as $A_{l}:=A_{1}^{\otimes l}=A_{1} \otimes \cdots \otimes A_{1}$, where $\otimes$ denotes the Kronecker product, which we will quickly define. Let $A$ be a $m \times n$ matrix and $B$ be a $p \times q$ matrix, then the Kronecker product $A \otimes B$ is the $m p \times n q$ block matrix defined as

$$
A \otimes B:=\left(\begin{array}{ccc}
A_{11} B & \cdots & A_{1 n} B \\
\vdots & \ddots & \vdots \\
A_{m 1} B & \cdots & A_{m n} B
\end{array}\right) .
$$

---

Note that $A_{l}$ is a $r^{l} \times c^{l}$ matrix and that any Walsh-Hadamard matrix can be written as $A_{l}$ for some $l$ and the $2 \times 2$ Walsh-Hadamard matrix ${ }^{29}$ as $A_{1}$. Furthermore, for a constant sized seed the time complexity of applying $A_{l}$ to a vector is $O\left(c^{l}\right)$ by using an algorithm similar to FFT. We can then define the compound transform which we will refer to as LWTJL, as $f(x):=G A_{l} D x$, where $D \in\{-1,1\}^{d \times d}$ is a diagonal matrix with Rademacher entries, $A_{l} \in \mathbb{R}^{r^{l} \times d}$ is a LWT, and $G \in \mathbb{R}^{m \times r^{l}}$ is a JLT, and $r$ and $c$ are constants. One way to view LWTJL is as a variant of GRHD where the subsampling occurs on the seed matrix rather than the final Walsh-Hadamard matrix. If $G$ can be applied in $O\left(r^{l} \log r^{l}\right)$ time, e.g. if $G$ is the BCHJL construction |AL09| and $m=O\left(r^{l(1 / 2-\gamma)}\right)$, the total embedding time becomes $O(d)$, as $r^{l}=d^{\alpha}$ for some $\alpha<1$. However, in order to prove that LWTJL satisfies lemma 1.2 the analysis of [LAS11] imposes a few requirements on $r, c$, and the vectors we wish to embed, namely that $\log r / \log c \geq 1-2 \delta$ and $v=O\left(m^{-1 / 2} d^{-\delta}\right)$, where $v$ is an upper bound on the $\ell_{\infty} / \ell_{2}$ ratio as introduced at the end of section 1.4.1. The bound on $v$ is somewhat tight as shown in proposition 1.11

---

Proposition 1.11. For any seed matrix define LWT as the LWTJL distribution seeded with that matrix. Then for all $\delta \in(0,1)$, there exists a vector $x \in \mathbb{C}^{d}$ (or $x \in \mathbb{R}^{d}$, if the seed matrix is a real matrix) satisfying $\|x\|_{\infty} /\|x\|_{2}=\Theta\left(\log ^{-1 / 2} \frac{1}{\delta}\right)$ such that

$$
\begin{equation*}
\operatorname{Pr}_{f \sim L W T}[f(x)=0]>\delta . \tag{11}
\end{equation*}
$$

The proof of proposition 1.11 can be found in section 2.3 and it is based on constructing $x$ as a few copies of a vector that is orthogonal to the rows of the seed matrix.

---

The last JLD we will cover is based on so-called Kac random walks, and despite Ailon and Chazelle [AC09] conjecturing that such a construction could satisfy lemma 1.1] it was not until Jain et al. [Jai+20] that a proof was finally at hand. As with the lean Walsh transforms above, let us first define Kac random walks before describing how they can be used to construct JLDs. A Kac random walk is a Markov chain of linear transformations, where for each step we choose two coordinates at random and perform a random rotation on the plane spanned by these two coordinates, or more formally:

---

Definition 1.12 (Kac random walk [Kac56]). For a given dimention $d \in \mathbb{N}_{1}$, let $K^{(0)}:=I \in\{0,1\}^{d \times d}$ be the identity matrix, and for each $t>0$ sample $\left(i_{t}, j_{t}\right) \in\binom{[d]}{2}$ and $\theta_{t} \in[0,2 \pi)$ independently and uniformly at random. Then define the Kac random walk of length $t$ as $K^{(t)}:=R^{\left(i_{t}, j_{t}, \theta_{t}\right)} K^{(t-1)}$, where $R^{(i, j, \theta)} \in \mathbb{R}^{d \times d}$ is the rotation in the $(i, j)$ plane by $\theta$ and is given by

$$
\begin{array}{rlrl}
R^{(i, j, \theta)} e_{k} & :=e_{k} & \forall k \notin\{i, j\}, \\
R^{(i, j, \theta)}\left(a e_{i}+b e_{j}\right) & :=(a \cos \theta-b \sin \theta) e_{i}+(a \sin \theta+b \cos \theta) e_{j} .
\end{array}
$$

---

The main JLD introduced in $[J a \mathrm{Jai}+20]$, which we will refer to as KacJL , is a compound JLD where both $f_{1}$ and $f_{2}$ consists of a Kac random walk followed by subsampling, which can be defined more formally in the following way. Let $T_{1}:=\Theta(d \log d)$ be the length of the first Kac random walk, $d^{\prime}:=\min \left\{d, \Theta\left(\varepsilon^{-2} \log |X| \log ^{2} \log |X| \log ^{3} d\right)\right\}$ be the intermediate dimension, $T_{2}:=\Theta\left(d^{\prime} \log |X|\right)$ be the length of the second Kac random walk, and $m:=\Theta\left(\varepsilon^{-2} \log |X|\right)$ be the target dimension, and then define the JLT as $f(x)=\left(f_{2} \circ f_{1}\right)(x):=S^{\left(m, d^{\prime}\right)} K^{\left(T_{2}\right)} S^{\left(d^{\prime}, d\right)} K^{\left(T_{1}\right)} x$, where $K^{\left(T_{1}\right)} \in \mathbb{R}^{d \times d}$ and $K^{\left(T_{2}\right)} \in \mathbb{R}^{d^{\prime} \times d^{\prime}}$ are independent Kac random walks of length $T_{1}$ and $T_{2}$, respectively, and $S^{\left(d^{\prime}, d\right)} \in\{0,1\}^{d^{\prime} \times d}$ and $S^{\left(m, d^{\prime}\right)} \in\{0,1\}^{m \times d^{\prime}}$ projects onto the first $d^{\prime}$ and $m$ coordinates ${ }^{30}$, respectively. Since $K^{(T)}$ can be applied in time $O(T)$, the KacJL construction is JLD with embedding time $O\left(d \log d+\min \left\{d \log |X|, \varepsilon^{-2} \log ^{2}|X| \log ^{2} \log |X| \log ^{3} d\right\}\right)$ with asymptotically optimal target dimension, and by only applying the first part $\left(f_{1}\right)$, KacJL achieves an embedding time of $O(d \log d)$ but with a suboptimal target dimension of $O\left(\varepsilon^{-2} \log |X| \log ^{2} \log |X| \log ^{3} d\right)$.

---

Jain et al. [Jai+20] also proposes a version of their JLD construction that avoids computing trigonometric functions ${ }^{31}$ by choosing the angles $\theta_{t}$ uniformly at random from the set $\{\pi / 4,3 \pi / 4,5 \pi / 4,7 \pi / 4\}$ or even the singleton set $\{\pi / 4\}$. This comes at the $\cos \sqrt{32}$ of increasing $T_{1}$ by a factor of $\log \log d$ and $T_{2}$ by a factor of $\log d$, and for the singleton case multiplying with random signs (as we have done with the $D$ matrices in many of the previous constructions) and projecting down onto a random subset of coordinates rather than the $d^{\prime}$ or $m$ first.

---

This concludes the overview of Johnson-Lindenstrauss distributions and transforms, though there are many aspects we did not cover such as space usage, preprocessing time, randomness usage, and norms other than $\ell_{2}$. However, a summary of the main aspects we did cover (embedding times and target dimensions of the JLDs) can be found in table 1 .