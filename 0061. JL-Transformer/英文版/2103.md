### An Introduction to Johnson-Lindenstrauss Transforms 


#### Abstract

Johnson-Lindenstrauss Transforms are powerful tools for reducing the dimensionality of data while preserving key characteristics of that data, and they have found use in many fields from machine learning to differential privacy and more. This note explains what they are; it gives an overview of their use and their development since they were introduced in the 1980s; and it provides many references should the reader wish to explore these topics more deeply.

---

The text was previously a main part of the introduction of my PhD thesis [Fre20], but it has been adapted to be self contained and serve as a (hopefully good) starting point for readers interested in the topic.


## 1 The Why, What, and How

### 1.1 The Problem

Consider the following scenario: We have some data that we wish to process but the data is too large, e.g. processing the data takes too much time, or storing the data takes too much space. A solution would be to compress the data such that the valuable parts of the data are kept and the other parts discarded. Of course, what is considered valuable is defined by the data processing we wish to apply to our data. To make our scenario more concrete let us say that our data consists of vectors in a high dimensional Euclidean space, $\mathbb{R}^{d}$, and we wish to find a transform to embed these vectors into a lower dimensional space, $\mathbb{R}^{m}$, where $m \ll d$, so that we can apply our data processing in this lower dimensional space and still get meaningful results. The problem in this more concrete scenario is known as dimensionality reduction.

---

As an example, let us pretend to be a library with a large corpus of texts and whenever a person returns a novel that they liked, we would like to recommend some similar novels for them to read next. Or perhaps we wish to be able to automatically categorise the texts into groups such as fiction/non-fiction or child/young-adult/adult literature. To be able to use the wealth of research on similarity search and classification we need a suitable representation of our texts, and here a common choice is called bag-of-words. For a language or vocabulary with $d$ different words, the bag-of-words representation of a text $t$ is a vector $x \in \mathbb{R}^{d}$ whose $i$ th entry is the number of times the $i$ th word occurs in $t$. For example, if the language is just ["be", "is", "not", "or", "question", "that", "the", "to"] then the text "to be or not to be" is represented as $(2,0,1,1,0,0,0,2)^{\top}$. To capture some of the context of the words, we can instead represent a text as the count of so-called $n$-grams 1 , which are sequences of $n$ consecutive words, e.g. the 2 -grams of "to be or not to be" are ["to be", "be or", "or not", "not to", "to be"], and we represent such a bag-of- $n$-grams as a vector in $\mathbb{R}^{\left(d^{n}\right)}$. To compare two texts we compute the distance between the vectors of those texts, because the distance between vectors of texts with mostly the same words (or $n$-grams) is small/ ${ }^{2}$ For a more realistic language such as English with $d \approx 171000$ words [SW89] or that of the "English" speaking internet at $d \gtrsim 4790000$ words [WZ05], the dimension quickly becomes infeasable. While we only need to store the nonzero counts of words (or $n$-grams) to represent a vector, many data processing algorithms have a dependency on the vector dimension $d$ (or $\left.d^{n}\right)$, e.g. using nearest-neighbour search to find similar novels to recommend [AI17] or using neural networks to classify our texts [Sch18]. These algorithms would be infeasible for our library use case if we do not first reduce the dimension of our data.

---

A seemingly simple approach would be to select a subset of the coordinates, say if the data contained redundant or irrelevant coordinates. This is known as feature selection [JS08; HTF17; Jam+13a], and can be seen as projecting ${ }^{3}$ onto an axis aligned subspace, i.e. a subspace whose basis is a subset of $\left\{e_{1}, \ldots, e_{d}\right\}$.

---

We can build upon feature selection by choosing the basis from a richer set of vectors. For instance, in principal component analysis as dimensionality reduction (PCA) [Pea01: Hot33] we let the basis of the subspace be the $m$ first eigenvectors (ordered decreasingly by eigenvalue) of $X^{\top} X$, where the rows of $X \in \mathbb{R}^{n \times d}$ are our $n$ high dimensional vectors $4^{4}$. This subspace maximises the variance of the data in the sense that the first eigenvector is the axis that maximises variance and subsequent eigenvectors are the axes that maximise variance subject to being orthogonal to all previous eigenvectors [LRU20b; HTF17: Jam+13b].

But what happens if we choose a basis randomly?

### 1.2 The Johnson-Lindenstrauss Lemma(s)

In 1984 ${ }^{5}$ it was discovered that projecting onto a random basis approximately preserves pairwise distances with high probability. In order to prove a theorem regarding Lipschitz extensions of functions from metric spaces into $\ell_{2}$, Johnson and Lindenstrauss [JL84] proved the following lemma.

---

Lemma 1.1 (Johnson-Lindenstrauss lemma [JL84]). For every $d \in \mathbb{N}_{1}, \varepsilon \in(0,1)$, and $X \subset \mathbb{R}^{d}$, there exists a function $f: X \rightarrow \mathbb{R}^{m}$, where $m=\Theta\left(\varepsilon^{-2} \log |X|\right)$ such that for every $x, y \in X$,

$$
\begin{equation*}
\left|\|f(x)-f(y)\|_{2}^{2}-\|x-y\|_{2}^{2}\right| \leq \varepsilon\|x-y\|_{2}^{2} . \tag{1}
\end{equation*}
$$

Proof. The gist of the proof is to first define $f(x):=(d / m)^{1 / 2} A x$, where $A \in \mathbb{R}^{m \times d}$ are the first $m$ rows of a random orthogonal matrix. They then showed that $f$ preserves the norm of any vector with high probability, or more formally that the distribution of $f$ satisfies the following lemma.

---

Lemma 1.2 (Distributional Johnson-Lindenstrauss lemma [JL84]). For every $d \in \mathbb{N}_{1}$ and $\varepsilon, \delta \in$ $(0,1)$, there exists a probability distribution $\mathcal{F}$ over linear functions $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}$, where $m=$ $\Theta\left(\varepsilon^{-2} \log \frac{1}{\delta}\right)$ such that for every $x \in \mathbb{R}^{d}$,

$$
\begin{equation*}
\operatorname{Pr}_{f \sim \mathcal{F}}\left[\left|\|f(x)\|_{2}^{2}-\|x\|_{2}^{2}\right| \leq \varepsilon\|x\|_{2}^{2}\right] \geq 1-\delta . \tag{2}
\end{equation*}
$$

By choosing $\delta=1 /|X|^{2}$, we can union bound over all pairs of vectors $x, y \in X$ and show that their distance (i.e. the $\ell_{2}$ norm of the vector $x-y$ ) is preserved simultaneously for all pairs with probability at least $1-\binom{|X|}{2} /|X|^{2}>1 / 2$.

---

We will use the term Johnson-Lindenstrauss distribution (JLD) to refer to a distribution $\mathcal{F}$ that is a witness to lemma 1.2 , and the term Johnson-Lindenstrauss transform (JLT) to a function $f$ witnessing lemma 1.1, e.g. a sample of a JLD.

---

A few things to note about these lemmas are that when sampling a JLT from a JLD it is independent of the input vectors themselves; the JLT is only dependendent on the source dimension $d$, number of vectors $|X|$, and distortion $\varepsilon$. This allows us to sample a JLT without having access to the input data, e.g. to compute the JLT before the data exists, or to compute the JLT in settings where the data is too large to store on or move to a single machinef Secondly, the target dimension $m$ is independent from the source dimension $d$, meaning there are potentially very significant savings in terms of dimensionality, which will become more apparent shortly.

---

Compared to PCA, the guarantees that JLTs give are different: PCA finds an embedding with optimal average distortion of distances between the original and the embedded vectors, i.e. $A_{\text {PCA }}=\arg \min _{A \in \mathbb{R}^{m \times d}} \sum_{x \in X}\left\|A^{\top} A x-x\right\|_{2}^{2}$ [Jol02], whereas a JLT bounds the worst case distortion between the distances within the original space and distances within the embedded space. As for computing the transformations, a common ${ }^{7}$ way of performing PCA is done by computing the covariance matrix and then performing eigenvalue decomposition, which results in a running time ${ }^{8}$ of $\mathcal{O}\left(|X| d^{2}+d^{\omega}\right)$ DDH07], compared to $\Theta(|X| d \log d)$ and $9 \Theta\left(\|X\|_{0} \varepsilon^{-1} \log |X|\right)$ that can be achieved by the JLDs "FJLT" and "Block SparseJL", respectively, which will be introduced in section 1.4 As such, PCA and JLDs are different tools appropriate for different scenarios (see e.g. [Bre+19] where the two techniques are compared empirically in the domain of medicinal imaging; see also [Das00; BM01; FB03; FM03; Tan+05; DB06; Arp+14; Woj+16; Bre+20]). That is not to say the two are mutually exclusive, as one could apply JL to quickly shave off some dimensions followed by PCA to more carefully reduce the remaining dimensions [e.g. RST09; HMT11; Xie+16; Yan+20]. For more on PCA, we refer the interested reader to [Jol02], which provides an excellent in depth treatment of the topic.

---

One natural question to ask with respect to JLDs and JLTs is if the target dimension is optimal. This is indeed the case as Kane, Meka, and Nelson [KMN11] and Jayram and Woodruff [JW13] independently give a matching lower bound of $m=\Omega\left(\varepsilon^{-2} \log \frac{1}{\delta}\right)$ for any JLD that satisfies lemma 1.2, and Larsen and Nelson [LN17] showed that the bound in lemma 1.1]is optimal up constant factors for almost the entire range of $\varepsilon$ with the following theorem.

---

Theorem 1.3 ([LN17]). For any integers $n, d \geq 2$ and $\lg ^{0.5001} n / \sqrt{\min \{d, n\}}<\varepsilon<1$ there exists a set of points $X \subset \mathbb{R}^{d}$ of size $n$ such that any function $f: X \rightarrow \mathbb{R}^{m}$ satisfying eq. (1) must have

$$
\begin{equation*}
m=\Omega\left(\varepsilon^{-2} \log \left(\varepsilon^{2} n\right)\right) . \tag{3}
\end{equation*}
$$

Note that if $\varepsilon \leq \sqrt{\lg n / \min \{d, n\}}$ then $\varepsilon^{-2} \lg n \geq \min \{d, n\}$, and embedding $X$ into dimension $\min \{d,|X|\}$ can be done isometrically by the identity function or by projecting onto $\operatorname{span}(X)$, respectively.

---

Alon and Klartag [AK17] extended the result in [LN17] by providing a lower bound for the gap in the range of $\varepsilon$.

Theorem 1.4 ([|AK17]). There exists an absolute positive constant $0<c<1$ so that for any $n \geq d>c d \geq m$ and for all $\varepsilon \geq 2 / \sqrt{n}$, there exists a set of points $X \subset \mathbb{R}^{d}$ of size $n$ such that any function $f: X \rightarrow \mathbb{R}^{m}$ satisfying eq. (1) must have

$$
\begin{equation*}
m=\Omega\left(\varepsilon^{-2} \log \left(2+\varepsilon^{2} n\right)\right) \tag{4}
\end{equation*}
$$

---


It is, however, possible to circumvent these lower bounds by restricting the set of input vectors we apply the JLTs to. For instance, Klartag and Mendelson [KM05], Dirksen [Dir16], and Bourgain, Dirksen, and Nelson [BDN15b] provide target dimension upper bounds for JLTs that are dependent on statistical properties of the input set X. Similarly, JLTs can be used to approximately preserve pairwise distances simultaneously for an entire subspace using $m=\Theta\left(\varepsilon^{-2} t \log (t / \varepsilon)\right)$, where $t$ denotes the dimension of the subspace [Sar06], which is a great improvement when $t \ll|X|, d$.

---

Another useful property of JLTs is that they approximately preserve dot products. Corollary 1.5 formalises this property in terms of lemma 1.1] though it is sometimes [Sar06; AV06] stated in terms of lemma 1.2 Corollary 1.5 has a few extra requirements on $f$ and $X$ compared to lemma 1.1, but these are not an issue if the JLT is sampled from a JLD, or if we add the negations of all our vectors to $X$, which only slightly increases the target dimension.

---

Corollary 1.5. Let $d, \varepsilon, X$ and $f$ be as defined in lemma 1.1 and furthermore let $f$ be linear. Then for every $x, y \in X$, if $-y \in X$ then

$$
\begin{equation*}
|\langle f(x), f(y)\rangle-\langle x, y\rangle| \leq \varepsilon\|x\|_{2}\|y\|_{2} . \tag{5}
\end{equation*}
$$

Proof. If at least one of $x$ and $y$ is the 0 -vector, then eq. (5) is trivially satisfied as $f$ is linear. If $x$ and $y$ are both unit vectors then we assume w.l.o.g. that $\|x+y\|_{2} \geq\|x-y\|_{2}$ and we proceed as follows, utilising the polarisation identity: $4\langle u, v\rangle=\|u+v\|_{2}^{2}-\|u-v\|_{2}^{2}$.

$$
\begin{aligned}
4|\langle f(x), f(y)\rangle-\langle x, y\rangle| & =\left|\|f(x)+f(y)\|_{2}^{2}-\|f(x)-f(y)\|_{2}^{2}-4\langle x, y\rangle\right| \\
& \leq\left|(1+\varepsilon)\|x+y\|_{2}^{2}-(1-\varepsilon)\|x-y\|_{2}^{2}-4\langle x, y\rangle\right| \\
& =\left|4\langle x, y\rangle+\varepsilon\left(\|x+y\|_{2}^{2}+\|x-y\|_{2}^{2}\right)-4\langle x, y\rangle\right| \\
& =\varepsilon\left(2\|x\|_{2}^{2}+2\|y\|_{2}^{2}\right) \\
& =4 \varepsilon .
\end{aligned}
$$

Otherwise we can reduce to the unit vector case.

$$
\begin{aligned}
|\langle f(x), f(y)\rangle-\langle x, y\rangle| & =\left|\left\langle f\left(\frac{x}{\|x\|_{2}}\right), f\left(\frac{y}{\|y\|_{2}}\right)\right\rangle-\left\langle\frac{x}{\|x\|_{2}}, \frac{y}{\|y\|_{2}}\right)\right|\|x\|_{2}\|y\|_{2} \\
& \leq \varepsilon\|x\|_{2}\|y\|_{2} .
\end{aligned}
$$

---

Before giving an overview of the development of JLDs in section 1.4. let us return to our scenario and example in section 1.1 and show the wide variety of fields where dimensionality reduction via JLTs have found use. Furthermore, to make us more familiar with lemma 1.1 and its related concepts, we will pick a few examples of how the lemma is used.

### 1.3 The Use(fulness) of Johnson-Lindenstrauss

JLDs and JLTs have found uses and parallels in many fields and tasks, some of which we will list below. Note that there are some overlap between the following categories, as e.g. [FB03] uses a JLD for an ensemble of weak learners to learn a mixture of Gaussians clustering, and [PW15] solves convex optimisation problems in a way that gives differential privacy guarantees.

---

Nearest-neighbour search. have benefited from the Johnson-Lindenstrauss lemmas on multiple occasions, including [Kle97; KOR00], which used JL to randomly partition space rather than reduce the dimension, while others [AC09; HIM12] used the dimensionality reduction properties of JL more directly. Variations on these results include consructing locality sensitive hashing schemes [Dat+04] and finding nearest neighbours without false negatives [SW17].

---

Clustering. with results in various sub-areas such as mixture of Gaussians [Das99; FB03; UDS07], subspace clustering [HTB17], graph clustering [SI09; Guo+20], self-organising maps [RK89: Kas98], and $k$-means [Bec+19: Coh+15; Bou+14] Liu+17: SF18], which will be explained in more detail in section 1.3.1.

Outlier detection. where there have been works for various settings of outliers, including approximate nearest-neighbours [dVCH10; SZK15] and Gaussian vectors [NC20], while [Zha+20] uses JL as a preprocessor for a range of outlier detection algorithms in a distributed computational model, and [AP12] evaluates the use of JLTs for outlier detection of text documents.

---

Ensemble learning. where independent JLTs can be used to generate training sets for weak learners for bagging [SR09] and with the voting among the learners weighted by how well a given JLT projects the data [CS17: Can20]. The combination of JLTs with multiple learners have also found use in the regime of learning high-dimensional distributions from few datapoints (i.e. $|X| \ll d$ ) [DK13; ZK19: Niy+20].

---

Adversarial machine learning. where Johnson-Lindenstrauss can both be used to defend against adversarial input [ $\mathrm{Ngu+16}$; Wee+19; Tar+19] as well as help craft such attacks [i+20].

---

Miscellaneous machine learning. where, in addition to the more specific machine learning topics mentioned above, Johnson-Lindenstrauss has been used together with support vector machines [CJS09: Pau+14; LL20], Fisher's linear discriminant [DK10], and neural networks [Sch18], while [KY20] uses JL to facilitate stochastic gradient descent in a distributed setting.

---

Numerical linear algebra. with work focusing on low rank approximation [Coh+15; MM20], canonical correlation analysis [Avr+14], and regression in a local [THM17]: MM09:| Kab14; Sla17] and a distributed [HMM16] computational model. Futhermore, as many of these subfields are related some papers tackle multiple numerical linear algebra problems at once, e.g. low rank approximation, regression, and approximate matrix multiplication [Sar06], and a line of work [MM13; CW17; NN13a] have used JLDs to perform subspace embeddings which in turn gives algorithms for $\ell_{p}$ regression, low rank approximation, and leverage scores.

For further reading, there are surveys [Mah11; HMT11; Woo14] covering much of JLDs' use in numerical linear algebra.

---

Convex optimisation. in which Johnson-Lindenstrauss has been used for (integer) linear programming [VPL15] and to improve a cutting plane method for finding a point in a convex set using a separation oracle [TSC15; Jia+20]. Additionally, [Zha+13] studies how to recover a high-dimensional optimisation solution from a JL dimensionality reduced one.

---

Differential privacy. have utilised Johnson-Lindenstrauss to provide sanitised solutions to the linear algebra problems of variance estimation [Blo+12], regression [She19; SKD19; ZLW09], Euclidean distance estimation [Ken+13; LKR06; GLK13; Tur $\mathbf{Z}$ 08; Xu+17], and low-rank factorisation [Upa18], as well as convex optimisation [PW15; [KJ16], collaborative filtering [Yan+17] and solutions to graph-cut queries [Blo+12: Upa13]. Furthermore, [Upa15] analysis various JLDs with respect to differential privacy and introduces a novel one designed for this purpose.

---

Neuroscience. where it is used as a tool to process data in computational neuroscience [GS12, ALG13], but also as a way of modelling neurological processes [GS12; ALG13; All+14; PP14]. Interestingly, there is some evidence [MFL08; SA09; Car+13] to suggest that JL-like operations occur in nature, as a large set of olifactory sensory inputs (projection neurons) map onto a smaller set of neurons (Kenyon cells) in the brains of fruit flies, where each Kenyon cell is connected to a small and seemingly random subset of the projection neurons. This is reminiscent of sparse JL constructions, which will be introduced in section 1.4.1. though I am not neuroscientifically adept enough to judge how far these similarities between biological constructs and randomised linear algebra extend.

---

Other topics. where Johnson-Lindenstrauss have found use include graph sparsification [SS11], graph embeddings in Euclidean spaces [FM88], integrated circuit design [Vem98], biometric authentication [Arp+14], and approximating minimum spanning trees [HI00].
For further examples of Johnson-Lindenstrauss use cases, please see [Ind01; Vem04].

Now, let us dive deeper into the areas of clustering and streaming algorithms to see how Johnson-Lindenstrauss can be used there.

### 1.3.1 Clustering

Clustering can be defined as partitioning a dataset such that elements are similar to elements in the same partition while being dissimilar to elements in other partitions. A classic clustering problem is the so-called $k$-means clustering where the dataset $X \subset \mathbb{R}^{d}$ consists of points in Euclidean space. The task is to choose $k$ cluster centers $c_{1}, \ldots, c_{k}$ such that they minimise the sum of squared distances from datapoints to their nearest cluster center, i.e.

$$
\begin{equation*}
\underset{c_{1}, \ldots, c_{k}}{\arg \min } \sum_{x \in X} \min _{i}\left\|x-c_{i}\right\|_{2}^{2} . \tag{6}
\end{equation*}
$$

---

This creates a Voronoi partition, as each datapoint is assigned to the partition corresponding to its nearest cluster center. We let $X_{i} \subseteq X$ denote the set of points that have $c_{i}$ as their closest center. It is well known that for an optimal choice of centers, the centers are the means of their corresponding partitions, and furthermore, the cost of any choice of centers is never lower than the sum of squared distances from datapoints to the mean of their assigned partition, i.e.

$$
\begin{equation*}
\sum_{x \in X} \min _{i}\left\|x-c_{i}\right\|_{2}^{2} \geq \sum_{i=1}^{k} \sum_{x \in X_{i}}\left\|x-\frac{1}{\left|X_{i}\right|} \sum_{y \in X_{i}} y\right\|_{2}^{2} \tag{7}
\end{equation*}
$$

---

It has been shown that finding the optimal centers, even for $k=2$, is NP-hard [Alo+09; Das08|; however, various heuristic approaches have found success such as the commonly used Lloyd's algorithm [Llo82]. In Lloyd's algorithm, after initialising the centers in some way we iteratively improve the choice of centers by assigning each datapoint to its nearest center and then updating the center to be the mean of the datapoints assigned to it. These two steps can then be repeated until some termination criterion is met, e.g. when the centers have converged. If we let $t$ denote the number of iterations, then the running time becomes $O(t|X| k d)$, as we use $O(|X| k d)$ time per iteration to assign each data point to its nearest center. We can improve this running time by quickly embedding the datapoints into a lower dimensional space using a JLT and then running Lloyd's algorithm in this smaller space. The Fast Johnson-Lindenstrauss Transform, which we will introduce later, can for many sets of parameters embed a vector in $O(d \log d)$ time reducing the total running time to $O\left(|X| d \log d+t|X| k \varepsilon^{-2} \log |X|\right)$. However, for this to be useful we need the partitioning of points in the lower dimensional space to correspond to an (almost) equally good partition in the original higher dimensional space.

---

In order to prove such a result we will use the following lemma, which shows that the cost of a partitioning, with its centers chosen as the means of the partitions, can be written in terms of pairwise distances between datapoints in the partitions.

Lemma 1.6. Let $k, d \in \mathbb{N}_{1}$ and $X_{i} \subset \mathbb{R}^{d}$ for ${ }^{10} i \in[k]$.

$$
\begin{equation*}
\sum_{i=1}^{k} \sum_{x \in X_{i}}\left\|x-\frac{1}{\left|X_{i}\right|} \sum_{y \in X_{i}} y\right\|_{2}^{2}=\frac{1}{2} \sum_{i=1}^{k} \frac{1}{\left|X_{i}\right|} \sum_{x, y \in X_{i}}\|x-y\|_{2}^{2} \tag{8}
\end{equation*}
$$

The proof of lemma 1.6 consists of various linear algebra manipulations and can be found in section 2.1. Now we are ready to prove the following proposition, which states that if we find a partitioning whose cost is within $(1+\gamma)$ of the optimal cost in low dimensional space, that partitioning when moving back to the high dimensional space is within $(1+4 \varepsilon)(1+\gamma)$ of the optimal cost there.

---

Proposition 1.7. Let $k, d \in \mathbb{N}_{1}, X \subset \mathbb{R}^{d}, \varepsilon \leq 1 / 2, m=\Theta\left(\varepsilon^{-2} \log |X|\right)$, and $f: X \rightarrow \mathbb{R}^{m}$ be a JLT. Let $Y \subset \mathbb{R}^{m}$ be the embedding of $X$. Let $\kappa_{m}^{*}$ denote the optimal cost of a partitioning of $Y$, with respect to eq. (6). Let $Y_{1}, \ldots, Y_{k} \subseteq Y$ be a partitioning of $Y$ with cost $\kappa_{m}$ such that $\kappa_{m} \leq(1+\gamma) \kappa_{m}^{*}$ for some $\gamma \in \mathbb{R}$. Let $\kappa_{d}^{*}$ be the cost of an optimal partitioning of $X$ and $\kappa_{d}$ be the cost of the partitioning $X_{1}, \ldots, X_{k} \subseteq X$, satisfying $Y_{i}=\left\{f(x) \mid x \in X_{i}\right\}$. Then

$$
\begin{equation*}
\kappa_{d} \leq(1+4 \varepsilon)(1+\gamma) \kappa_{d}^{*} \tag{9}
\end{equation*}
$$

Proof. Due to lemma 1.6 and the fact that $f$ is a JLT we know that the cost of our partitioning is approximately preserved when going back to the high dimensional space, i.e. $\kappa_{d} \leq \kappa_{m} /(1-\varepsilon)$. Furthermore, since the cost of $X$ 's optimal partitioning when embedded down to $Y$ cannot be lower than the optimal cost of partitioning $Y$, we can conclude $\kappa_{m}^{*} \leq(1+\varepsilon) \kappa_{d}^{*}$. Since $\varepsilon \leq 1 / 2$, we have $1 /(1-\varepsilon)=1+\varepsilon /(1-\varepsilon) \leq 1+2 \varepsilon$ and also $(1+\varepsilon)(1+2 \varepsilon)=\left(1+3 \varepsilon+2 \varepsilon^{2}\right) \leq 1+4 \varepsilon$. Combining these inequalities we get

$$
\begin{aligned}
\kappa_{d} & \leq \frac{1}{1-\varepsilon} \kappa_{m} \\
& \leq(1+2 \varepsilon)(1+\gamma) \kappa_{m}^{*} \\
& \leq(1+2 \varepsilon)(1+\gamma)(1+\varepsilon) \kappa_{d}^{*} \\
& \leq(1+4 \varepsilon)(1+\gamma) \kappa_{d}^{*} .
\end{aligned}
$$

---

By pushing the constant inside the $\Theta$-notation, proposition 1.7 shows that we can achieve a $(1+\varepsilon)$ approximation ${ }^{11}$ of $k$-means with $m=\Theta\left(\varepsilon^{-2} \log |X|\right)$. However, by more carefully analysing which properties are needed, we can improve upon this for the case where $k \ll|X|$. Boutsidis et al. $[B o u+14]$ showed that projecting down to a target dimension of $m=\Theta\left(\varepsilon^{-2} k\right)$ suffices for a slightly worse $k$-means approximation factor of $(2+\varepsilon)$. This result was expanded upon in two ways by Cohen et al. [Coh+15], who showed that projecting down to $m=\Theta\left(\varepsilon^{-2} k\right)$ achieves a $(1+\varepsilon)$ approximation ratio, while projecting all the way down to $m=\Theta\left(\varepsilon^{-2} \log k\right)$ still suffices for a $(9+\varepsilon)$ approximation ratio. The $(1+\varepsilon)$ case has recently been further improved upon by both Becchetti et al. [Bec +19$]$, who have shown that one can achieve the $(1+\varepsilon)$ approximation ratio for $k$-means when projecting down to $m=\Theta\left(\varepsilon^{-6}(\log k+\log \log |X|) \log \varepsilon^{-1}\right)$, and by Makarychev, Makarychev, and Razenshteyn [MMR19], who independently have proven an even better bound of $m=\Theta\left(\varepsilon^{-2} \log k / \varepsilon\right)$, essentially giving a "best of worlds" result with respect to [Coh+15].

For an overview of the history of $k$-means clustering, we refer the interested reader to [Boc08].

### 1.3.2 Streaming

The field of streaming algorithms is characterised by problems where we receive a sequence (or stream) of items and are queried on the items received so far. The main constraint is usually that we only have limited access to the sequence, e.g. that we are only allowed one pass over it, and that we have very limited space, e.g. polylogarithmic in the length of the stream. To make up for these constraints we are allowed to give approximate answers to the queries. The subclass of streaming problems we will look at here are those where we are only allowed a single pass over the sequence and the items are updates to a vector and a query is some statistic on that vector, e.g. the $\ell_{2}$ norm of the vector. More formally, and to introduce the notation, let $d \in \mathbb{N}_{1}$ be the number of different items and let $T \in \mathbb{N}$ be the length of the stream of updates $\left(i_{j}, v_{j}\right) \in[d] \times \mathbb{R}$ for $j \in[T]$, and define the vector $x$ at time $t$ as $x^{(t)}:=\sum_{j=1}^{t} v_{j} e_{i j}$. A query $q$ at time $t$ is then a function of $x^{(t)}$, and we will omit the ${ }^{(t)}$ superscript when referring to the current time.

---

There are a few common variations on this model with respect to the updates. In the cash register model or insertion only model $x$ is only incremented by bounded integers, i.e. $v_{j} \in[M]$, for some $M \in \mathbb{N}_{1}$. In the turnstile model, $x$ can only be incremented or decremented by bounded integers, i.e. $v_{j} \in\{-M, \ldots, M\}$ for some $M \in \mathbb{N}_{1}$, and the strict turnstile model is as the turnstile model with the additional constraint that the entries of $x$ are always non-negative, i.e. $x_{i}^{(t)} \geq 0$, for all $t \in[T]$ and $i \in[d]$.

---

As mentioned above, we are usually space constrained so that we cannot explicitely store $x$ and the key idea to overcome this limitation is to store a linear sketch of $x$, that is storing $y:=f(x)$, where $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}$ is a linear function and $m \ll d$, and then answering queries by applying some function on $y$ rather than $x$. Note that since $f$ is linear, we can apply it to each update individually and compute $y$ as the sum of the sketched updates. Furthermore, we can aggregate results from different streams by adding the different sketches, allowing us to distribute the computation of the streaming algorithm.

---

The relevant Johnson-Lindenstrauss lemma in this setting is lemma 1.2 as with a JLD we get linearity and are able to sample a JLT before seeing any data at the cost of introducing some failure probability.

---

Based on JLDs, the most natural streaming problem to tackle is second frequency moment estimation in the turnstile model, i.e. approximating $\|x\|_{2}^{2}$, which has found use in database query optimisation [Alo+02; WDJ91; DeW+92] and network data analysis [Gil+01; CG05] among other areas. Simply letting $f$ be a sample from a JLD and returning $\|f(x)\|_{2}^{2}$ on queries, gives a factor $(1 \pm \varepsilon)$ approximation with failure probability $\delta$ using $O\left(\varepsilon^{-2} \log \frac{1}{\delta}+|f|\right)$ words ${ }^{12}$ of space, where $|f|$ denotes the words of space needed to store and apply $f$. However, the approach taken by the streaming literature is to estimate $\|x\|_{2}^{2}$ with constant error probability using $O\left(\varepsilon^{-2}+|f|\right)$ words of space, and then sampling $O\left(\log \frac{1}{\delta}\right)$ JLTs $f_{1}, \ldots, f_{O(\log 1 / \delta)}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{O\left(\varepsilon^{-2}\right)}$ and responding to a query with median $_{k}\left\|f_{k}(x)\right\|_{2}^{2}$, which reduces the error probability to $\delta$. This allows for simpler analyses as well more efficient embeddings (in the case of Count Sketch) compared to using a single bigger JLT, but it comes at the cost of not embedding into $\ell_{2}$, which is needed for some applications outside of streaming. With this setup the task lies in constructing space efficient JLTs and a seminal work here is the AMS Sketch a.k.a. AGMS Sketch a.k.a. Tug-of-War Sketch [AMS99: Alo+02], whose JLTs can be defined as $f_{i}:=m^{-1 / 2} A x$, where $A \in\{-1,1\}^{m \times d}$ is a random matrix. The key idea is that each row $r$ of $A$ can be backed by a hash function $\sigma_{r}:[d] \rightarrow\{-1,1\}$ that need only be 4 -wise independent, meaning that for any set of 4 distinct keys $\left\{k_{1}, \ldots k_{4}\right\} \subset[d]$ and 4 (not necessarily distinct) values $v_{1}, \ldots v_{4} \in\{-1,1\}$, the probability that the keys hash to those values is $\operatorname{Pr}_{\sigma_{r}}\left[\bigwedge_{i} \sigma_{r}\left(k_{i}\right)=v_{i}\right]=|\{-1,1\}|^{-4}$. This can for instance ${ }^{13}$ be attained by implementing $\sigma_{r}$ as 3rd degree polynomial modulus a sufficiently large prime with random coefficients [WC81], and so such a JLT need only use $O\left(\varepsilon^{-2}\right)$ words of space. Embedding a scaled standard unit vector with such a JLT takes $O\left(\varepsilon^{-2}\right)$ time leading to an overall update time of the AMS Sketch of $O\left(\varepsilon^{-2} \log \frac{1}{\delta}\right)$.

---

A later improvement of the AMS Sketch is the so-called Fast-AGMS Sketch [CG05] a.k.a. Count Sketch [CCF04; TZ12], which sparsifies the JLTs such that each column in their matrix representations only has one non-zero entry. Each JLT can be represented by a pairwise independent hash function $h:[d] \rightarrow\left[O\left(\varepsilon^{-2}\right)\right]$ to choose the position of each nonzero entry and a 4 -wise independent hash function $\sigma:[d] \rightarrow\{-1,1\}$ to choose random signs as before. This reduces the standard unit vector embedding time to $O(1)$ and so the overall update time becomes $O\left(\log \frac{1}{\delta}\right)$ for Count Sketch. It should be noted that the JLD inside Count Sketch is also known as Feature Hashing, which we will return to in section 1.4.1.

---

Despite not embedding into $\ell_{2}$, due to the use of the non-linear median, AMS Sketch and Count Sketch approximately preserve dot products similarly to corollary 1.5 [CG05, Theorem 2.1 and Theorem 3.5]. This allows us to query for the (approximate) frequency of any particular item as

$$
\underset{k}{\operatorname{median}}\left\langle f_{k}(x), f_{k}\left(e_{i}\right)\right\rangle=\left\langle x, e_{i}\right\rangle \pm \varepsilon\|x\|_{2}\left\|e_{i}\right\|_{2}=x_{i} \pm \varepsilon\|x\|_{2}
$$

with probability at least $1-\delta$.

---

This can be extended to finding frequent items in an insertion only stream [CCF04]. The idea is to use a slightly larger ${ }^{14}$ Count Sketch instance to maintain a heap of the $k$ approximately most frequent items of the stream so far. That is, if we let $i_{k}$ denote the $k$ th most frequent item (i.e. $\left|\left\{j \mid x_{j} \geq x_{i_{k}}\right\}\right|=k$ ), then with probability $1-\delta$ we have $x_{j}>(1-\varepsilon) x_{i_{k}}$ for every item $j$ in our heap.

For more on streaming algorithms, we refer the reader to [Mut05] and [Nel11], which also relates streaming to Johnson-Lindenstrauss.

### 1.4 The Tapestry of Johnson-Lindenstrauss Transforms

As mentioned in section 1.2, the original JLD from [JL84] is a distribution over functions $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}$, where ${ }^{15} f(x)=(d / m)^{1 / 2} A x$ and $A$ is a random $m \times d$ matrix whose rows form an orthonormal basis of some $m$-dimensional subspace of $\mathbb{R}^{d}$, i.e. the rows are unit vectors and pairwise orthogonal. While Johnson and Lindenstrauss [JL84] showed that $m=\Theta\left(\varepsilon^{-2} \log |X|\right)$ suffices to prove lemma 1.1, they did not give any bounds on the constant in the big- $O$ expression. This was remedied in [|FM88|, which proved that $m=\left\lceil 9\left(\varepsilon^{2}-2 \varepsilon^{3} / 3\right)^{-1} \ln |X|\right\rceil+1$ suffices for the same JLD if $m<\sqrt{|X|}$. This bound was further improved in FM90| by removing the $m<\sqrt{|X|}$ restriction and lowering the bound to $m=\left\lceil 8\left(\varepsilon^{2}-2 \varepsilon^{3} / 3\right)^{-1} \ln |X| \mid\right.$.

---

The next thread of JL research worked on simplifying the JLD constructions as Indyk and Motwani [HIM12] showed that sampling each entry in the matrix i.i.d. from a properly scaled Gaussian distribution is a JLD. The rows of such a matrix do not form a basis as they are with high probability not orthogonal; however, the literature still refer to this and most other JLDs as random projections. Shortly thereafter Arriaga and Vempala [AV06] constructed a JLD by sampling i.i.d. from a Rademacher ${ }^{16}$ distribution, and Achlioptas [Ach03] sparsified the Rademacher construction such that the entries $a_{i j}$ are sampled i.i.d. with $\operatorname{Pr}\left[a_{i j}=0\right]=$ $2 / 3$ and $\operatorname{Pr}\left[a_{i j}=-1\right]=\operatorname{Pr}\left[a_{i j}=1\right]=1 / 6$. We will refer to such sparse i.i.d. Rademacher constructions as Achlioptas constructions. The Gaussian and Rademacher results have later been generalised [Mat08; IN07; KM05] to show that a JLD can be constructed by sampling each entry in a $m \times d$ matrix i.i.d. from any distribution with mean 0 , variance 1 , and a subgaussian tai ${ }^{17}$. It should be noted that these developments have a parallel in the streaming literature as the previously mentioned AMS Sketch [AMS99; Alo+02] is identical to the Rademacher construction [AV06], albeit with constant error probability.

---

As for the target dimension for these constructions, [HIM12] proved that the Gaussian construction is a JLD if $m \geq 8\left(\varepsilon^{2}-2 \varepsilon^{3} / 3\right)^{-1}(\ln |X|+O(\log m))$, which roughly corresponds to an additional additive $O\left(\varepsilon^{-2} \log \log |X|\right)$ term over the original construction. This additive $\log \log$ term was shaved off by the proof in [DG02], which concerns itself with the original JLD construction but can easily ${ }^{18}$ be adapted to the Gaussian construction, and the proof in [AV06], which also give the same $\log \log$ free bound for the dense Rademacher construction. Achlioptas [Ach03] showed that his construction also achieves $m=\left\lceil 8\left(\varepsilon^{2}-2 \varepsilon^{3} / 3\right)^{-1} \ln |X|\right\rceil$. The constant of 8 has been improved for the Gaussian and dense Rademacher constructions in the sense that Rojo and Nguyen [RN10; Ngu09] have been able to replace the bound with more intricate 19 expressions, which yield a 10 to $40 \%$ improvement for many sets of parameters. However, in the distributional setting it has been shown in [BGK18] that $m \geq 4 \varepsilon^{-2} \ln \frac{1}{\delta}(1-o(1))$ is necessary for any JLD to satisfy lemma 1.2 , which corresponds to a constant of 8 if we prove lemma 1.1 the usual way by setting $\delta=n^{-2}$ and union bounding over all pairs of vectors.

---

There seems to have been some confusion in the literature regarding the improvements in target dimension. The main pitfall was that some papers [e.g. Ach03; HIM12; DG02; RN10; BGK18] were only referring to [FM88] when referring to the target dimension bound of the original construction. As such, [Ach03: HIM12] mistakenly claim to improve the constant for the target dimension with their constructions. Furthermore, [Ach01] is sometimes [e.g. in AC09; Mat08; Sch18| the only work credited for the Rademacher construction, despite it being developed independently and published 2 years prior in [AV99].

---

All the constructions that have been mentioned so far in this section, embed a vector by performing a relatively dense and unstructured matrix-vector multiplication, which takes $\Theta\left(m\|x\|_{0}\right)=O(m d)$ time ${ }^{20}$ to compute. This sparked two distinct but intertwined strands of research seeking to reduce the embedding time, namely the sparsity-based JLDs which dealt with the density of the embedding matrix and the fast Fourier transform-based which introduced more structure to the matrix.

### 1.4.1 Sparse Johnson-Lindenstrauss Transforms

The simple fact underlying the following string of results is that if $A$ has $s$ nonzero entries per column, then $f(x)=A x$ can be computed in $\Theta\left(s\|x\|_{0}\right)$ time. The first result here is the Achlioptas construction [Ach03] mentioned above, whose column sparsity $s$ is $m / 3$ in expectancy, which leads to an embedding time that is a third of the full Rademacher construction ${ }^{[21]}$ However, the first superconstant improvement is due to Dasgupta, Kumar, and Sarlós [DKS10], who based on heuristic approaches [Wei+09; Shi+09b; LLS07; GD08] constructed a JLD with $s=O\left(\varepsilon^{-1} \log \frac{1}{\delta} \log ^{2} \frac{m}{\delta}\right)$. Their construction, which we will refer to as the DKS construction, works by sampling $s$ hash functions $h_{1}, \ldots, h_{s}:[d] \rightarrow\{-1,1\} \times[m]$ independently, such that each source entry $x_{i}$ will be hashed to $s$ random signs $\sigma_{i, 1}, \ldots, \sigma_{i, s}$ and $s$ target coordinates $j_{i, 1}, \ldots, j_{i, s}$ (with replacement). The embedding can then be defined as $f(x):=\sum_{i} \sum_{k} e_{j_{i, k}} \sigma_{i, k} x_{i,}$ which is to say that every source coordinate is hashed to $s$ output coordinates and randomly added to or subtracted from those output coordinates. The sparsity analysis was later tightened to show that $s=O\left(\varepsilon^{-1} \log \frac{1}{\delta} \log \frac{m}{\delta}\right)$ suffices KN10; KN14] and even $s=O\left(\varepsilon^{-1}\left(\frac{\log \frac{1}{\delta} \log \log \log \frac{1}{\delta}}{\log \log \frac{1}{\delta}}\right)^{2}\right)$ suffices for the DKS construction assuming $\varepsilon<\log ^{-2} \frac{1}{\delta}$ [BOR10], while [KN14] showed that $s=\Omega\left(\varepsilon^{-1} \log ^{2} \frac{1}{\delta} / \log ^{2} \frac{1}{\varepsilon}\right)$ is neccessary for the DKS construction.

---

Kane and Nelson [KN14] present two constructions that circumvent the DKS lower bound by ensuring that the hash functions do not collide within a column, i.e. $j_{i, a} \neq j_{i, b}$ for all $i, a$, and $b$. The first construction, which we will refer to as the graph construction, simply samples the $s$ coordinates without replacement. The second construction, which we will refer to as the block construction, partitions the output vector into $s$ consecutive blocks of length $\mathrm{m} / \mathrm{s}$ and samples one output coordinate per block. Note that the block construction is the same as Count Sketch from the streaming literature [CG05: CCF04], though the hash functions differ and the output is interpreted differently. Kane and Nelson [KN14| prove that $s=\Theta\left(\varepsilon^{-1} \log \frac{1}{\delta}\right)$ is both neccessary and sufficient in order for their two constructions to satisfy lemma 1.2 Note that while Count Sketch is even sparser than the lower bound for the block construction, it does not contradict it as Count sketch does not embed into $\ell_{2}^{m}$ as it computes the median, which is nonlinear. As far as general sparsity lower bounds go, [DKS10] shows that an average column sparsity of $s_{\text {avg }}=\Omega\left(\min \left\{\varepsilon^{-2}, \varepsilon^{-1} \sqrt{\log _{m} d}\right\}\right)$ is neccessary for a sparse JLD, while Nelson and Nguyễn [NN13b] improves upon this by showing that there exists a set of points $X \in \mathbb{R}^{d}$ such that any JLT for that set must have column sparsity $s=\Omega\left(\varepsilon^{-1} \log |X| / \log \frac{1}{\varepsilon}\right)$ in order to satisfy lemma 1.1. And so it seems that we have almost reached the limit of the sparse JL approach, but why should theory be in the way of a good result? Let us massage the definitions so as to get around these lower bounds.

---

The hard instances used to prove the lower bounds [NN13b; KN14] consist of very sparse vectors, e.g. $x=(1 / \sqrt{2}, 1 / \sqrt{2}, 0, \ldots, 0)^{\top}$, but the vectors we are interested in applying a JLT to might not be so unpleasant, and so by restricting the input vectors to be sufficiently "nice", we can get meaningful result that perform better than what the pessimistic lower bound would indicate. The formal formulation of this niceness is bounding the $\ell_{\infty} / \ell_{2}$ ratio of the vectors lemmas 1.1 and 1.2 need apply to. Let us denote this norm ratio as $v \in[1 / \sqrt{d}, 1]$, and revisit some of the sparse JLDs. The Achlioptas construction [Ach03] can be generalised so that the expected number of nonzero entries per column is $q m$ rather than $\frac{1}{3} m$ for a parameter $q \in[0,1]$. Ailon and Chazelle $|\mathrm{AC} 09|$ show that if $v=O\left(\sqrt{\log \frac{1}{\delta}} / \sqrt{d}\right)$ then choosing $q=\Theta\left(\frac{\log ^{2} 1 / \delta}{d}\right)$ and sampling the nonzero entries from a Gaussian distribution suffices. This result is generalised in [Mat08] by proving that for all $v \in[1 / \sqrt{d}, 1]$ choosing $q=\Theta\left(v^{2} \log \frac{d}{\varepsilon \delta}\right)$ and sampling the nonzero entries from a Rademacher distribution is a JLD for the vectors constrained by that $v$.

---

Be aware that sometimes [e.g. in DKS10 BOR10 this bound ${ }^{22}$ on $q$ is misinterpreted as a lower bound stating that $q m=\tilde{\Omega}\left(\varepsilon^{-2}\right)$ is neccessary for the Achlioptas construction when $v=1$. However, Matoušek [Mat08] only loosely argues that his bound is tight for $v \leq d^{-0.1}$, and if it indeed was tight at $v=1$, the factors hidden by the $\tilde{\Omega}$ would lead to the contradiction that $m \geq q m=\Omega\left(\varepsilon^{-2} \log \frac{1}{\delta} \log \frac{d}{\varepsilon \delta}\right)=\omega(m)$.

---

The heuristic Wei+09; Shi+09b; LLS07; GD08] that DKS10] is based on is called Feature Hashing a.k.a. the hashing trick a.k.a. the hashing kernel and is a sparse JL construction with exactly 1 nonzero entry per column ${ }^{23}$ The block construction can then be viewed as the concatenation of $s=\Theta\left(\varepsilon^{-1} \log \frac{1}{\delta}\right)$ feature hashing instances, and the DKS construction can be viewed as the sum of $s=O\left(\varepsilon^{-1} \log \frac{1}{\delta} \log \frac{m}{\delta}\right)$ Feature Hashing instances or alternatively as first duplicating each entry of $x \in \mathbb{R}^{d} s$ times before applying Feature Hashing to the enlarged vector $x^{\prime} \in \mathbb{R}^{s d}$ : Let $f_{\text {dup }}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{s d}$ be a function that duplicates each entry in its input $s$ times, i.e. $f_{\text {dup }}(x)_{(i-1) s+j}=x_{(i-1) s+j}^{\prime}:=x_{i}$ for $i \in[d], j \in[s]$, then $f_{\text {DKS }}=f_{\text {FH }} \circ f_{\text {dup }}$.

---

This duplication is the key to the analysis in [DKS10] as $f_{\text {dup }}$ is isometric (up to normalisation) and it ensures that the $\ell_{\infty} / \ell_{2}$ ratio of $x^{\prime}$ is small, i.e. $v \leq 1 / \sqrt{s}$ from the point of view of the Feature Hashing data structure ( $f_{\mathrm{FH}}$ ). And so, any lower bound on the sparsity of the DKS construction (e.g. the one given in [KN14]) gives an upper bound on the values of $v$ for which Feature Hashing is a JLD: If $u$ is a unit vector such that a DKS instance with sparsity $\hat{s}$ fails to preserve $u$ s norm within $1 \pm \varepsilon$ with probability $\delta$, then it must be the case that Feature Hashing fails to preserve the norm of $f_{\text {dup }}(u)$ within $1 \pm \varepsilon$ with probability $\delta$, and therefore the $\ell_{\infty} / \ell_{2}$ ratio for which Feature Hashing can handle all vectors is strictly less than $1 / \sqrt{\hat{s}}$.

---

Written more concisely the statement is $s_{\mathrm{DKS}}=\Omega(a) \Longrightarrow v_{\mathrm{FH}}=O(1 / \sqrt{a})$ and by contraposition ${ }^{24} v_{\mathrm{FH}}=\Omega(1 / \sqrt{a}) \Longrightarrow s_{\mathrm{DKS}}=O(a)$, where $s_{\mathrm{DKS}}$ is the minimum column sparsity of a DKS construction that is a JLD, $v_{\mathrm{FH}}$ is the maximum $\ell_{\infty} / \ell_{2}$ constraint for which Feature Hashing is a JLD, and $a$ is any positive expression. Furthermore, if we prove an upper bound on $v_{\mathrm{FH}}$ using a hard instance that is identical to an $x^{\prime}$ that the DKS construction can generate after duplication, we can replace the previous two implications with bi-implications.

---

Wei+09] claims to give a bound on $v_{\mathrm{FH}}$, but it sadly contains an error in its proof of this bound [DKS10; Wei+10]. Dahlgaard, Knudsen, and Thorup [DKT17] improve the $v_{\text {FH }}$ lower bound to $v_{\mathrm{FH}}=\Omega\left(\sqrt{\frac{\varepsilon \log \left(1+\frac{4}{\delta}\right)}{\log \frac{1}{\delta} \log \frac{m}{\delta}}}\right)$, and Freksen, Kamma, and Larsen [FKL18] give an intricate but tight bound for $v_{\mathrm{FH}}$ shown in theorem 1.8, where the hard instance used to prove the upper bound is identical to an $x^{\prime}$ from the DKS construction.

---

Theorem 1.8 ([|FKL18]). There exist constants $C \geq D>0$ such that for every $\varepsilon, \delta \in(0,1)$ and $m \in \mathbb{N}_{1}$ the following holds. If $\frac{\mathrm{Clg} \frac{1}{\delta}}{\varepsilon^{2}} \leq m<\frac{2}{\varepsilon^{2} \delta}$ then

$$
\nu_{\mathrm{FH}}(m, \varepsilon, \delta)=\Theta\left(\sqrt{\varepsilon} \min \left\{\frac{\log \frac{\varepsilon m}{\log \frac{1}{\delta}}}{\log \frac{1}{\delta}}, \sqrt{\frac{\log \frac{\varepsilon^{2} m}{\log \frac{1}{\delta}}}{\log \frac{1}{\delta}}}\right\}\right) .
$$

Otherwise, if $m \geq \frac{2}{\varepsilon^{2} \delta}$ then $\mathcal{v}_{\mathrm{FH}}(m, \varepsilon, \delta)=1$. Moreover if $m<\frac{D \lg \frac{1}{\delta}}{\varepsilon^{2}}$ then $v_{\mathrm{FH}}(m, \varepsilon, \delta)=0$.

---

Furthermore, if an $x \in\{0,1\}^{d}$ satisfies $v_{\mathrm{FH}}<\|x\|_{2}^{-1}<1$ then

$$
\operatorname{Pr}_{f \sim \mathcal{F H}}\left[\left|\|f(x)\|_{2}^{2}-\|x\|_{2}^{2}\right|>\varepsilon\|x\|_{2}^{2}\right]>\delta .
$$

This bound gives a tight tradeoff between target dimension $m$, distortion $\varepsilon$, error probability $\delta$, and $\ell_{\infty} / \ell_{2}$ constraint $v$ for Feature Hashing, while showing how to construct hard instances for Feature Hashing: Vectors with the shape $x=(1, \ldots, 1,0, \ldots, 0)^{\top}$ are hard instances if they contain few 1 s , meaning that Feature Hashing cannot preserve their norms within $1 \pm \varepsilon$ with probability $\delta$. Theorem 1.8 is used in corollary 1.9 to provide a tight tradeoff between $m, \varepsilon, \delta, v$, and column sparsity $s$ for the DKS construction.

---

Corollary 1.9. Let $v_{\mathrm{DKS}} \in[1 / \sqrt{d}, 1]$ denote the largest $\ell_{\infty} / \ell_{2}$ ratio required, $v_{\mathrm{FH}}$ denote the $\ell_{\infty} / \ell_{2}$ constraint for Feature Hashing as defined in theorem 1.8. and $s_{\mathrm{DKS}} \in[m]$ as the minimum column sparsity such that the DKS construction with that sparsity is a JLD for the subset of vectors $x \in \mathbb{R}^{d}$ that satisfy $\|x\|_{\infty} /\|x\|_{2} \leq v_{\text {DKS }}$. Then

$$
s_{\mathrm{DKS}}=\Theta\left(\frac{v_{\mathrm{DKS}}^{2}}{v_{\mathrm{FH}}^{2}}\right) .
$$

The proof of this corollary is deferred to section 2.2 .

---

Jagadeesan [Jag19] generalised the result from [FKL18] to give a lower bound ${ }^{25]}$ on the $m, \varepsilon, \delta$, $v$, and $s$ tradeoff for any sparse Rademacher construction with a chosen column sparsity, e.g. the block and graph constructions, and gives a matching upper bound for the graph construction.

### 1.4.2 Structured Johnson-Lindenstrauss Transforms

As we move away from the sparse JLDs we will slightly change our idea of what an efficient JLD is. In the previous section the JLDs were especially fast when the vectors were sparse, as the running time scaled with $\|x\|_{0}$, whereas we in this section will optimise for dense input vectors such that an embedding time of $O(d \log d)$ is a satisfying result.

---

The chronologically first asymptotic improvement over the original JLD construction is due to Ailon and Chazelle [AC09] who introduced the so-called Fast Johnson-Lindenstrauss Transform (FJLT). As mentioned in the previous section, [AC09] showed that we can use a very sparse (and therefore very fast) embedding matrix as long as the vectors have a low $\ell_{\infty} / \ell_{2}$ ratio, and furthermore that applying a randomised Walsh-Hadamard transform to a vector results in a low $\ell_{\infty} / \ell_{2}$ ratio with high probability. And so, the FJLT is defined as $f(x):=P H D x$, where $P \in \mathbb{R}^{m \times d}$ is a sparse Achlioptas matrix with Gaussian entries and $q=\Theta\left(\frac{\log ^{2} 1 / \delta}{d}\right), H \in\{-1,1\}^{d \times d}$ is a Walsh-Hadamard matrix ${ }^{26}$, and $D \in\{-1,0,1\}^{d \times d}$ is a random diagonal matrix with i.i.d. Rademachers on the diagonal. As the Walsh-Hadamard transform can be computed using a simple recursive formula, the expected embedding time becomes $O\left(d \log d+m \log ^{2} \frac{1}{\delta}\right)$. And as mentioned, [Mat08] showed that we can sample from a Rademacher rather than a Gaussian distribution when constructing the matrix $P$. The embedding time improvement of FJLT over previous constructions depends on the relationship between $m$ and $d$. If $m=\Theta\left(\varepsilon^{-2} \log \frac{1}{\delta}\right)$ and $m=O\left(\varepsilon^{-4 / 3} d^{1 / 3}\right)$, FJLT's embedding time becomes bounded by the Walsh-Hadamard transform at $O(d \log d)$, but at $m=\Theta\left(d^{1 / 2}\right)$ FJLT is only barely faster than the original construction.

---

Ailon and Liberty [AL09] improved the running time of the FJLT construction to $O(d \log m)$ for $m=O\left(d^{1 / 2-\gamma}\right)$ for any fixed $\gamma>0$. The increased applicable range of $m$ was achieved by applying multiple randomised Walsh-Hadamard transformations, i.e. replacing HD with $\prod_{i} H D^{(i)}$, where the $D^{(i)}$ s are a constant number of independent diagonal Rademacher matrices, as well as by replacing $P$ with $B D$ where $D$ is yet another diagonal matrix with Rademacher entries and $B$ is consecutive blocks of specific partial Walsh-Hadamard matrices (based on so-called binary dual BCH codes [see e.g. MS77]). The reduction in running time comes from altering the transform slightly by partitioning the input into consecutive blocks of length poly $(m)$ and applying the randomised Walsh-Hadamard transforms to each of them independently. We will refer to this variant of FJLT as the BCHJL construction.

---

The next pattern of results has roots in compressed sensing and approaches the problem from another angle: Rather than being fast only when $m \ll d$, they achieve $O(d \log d)$ embedding time even when $m$ is close to $d$, at the cost of $m$ being suboptimal. Before describing these constructions, let us set the scene by briefly introducing some concepts from compressed sensing.

---

Roughly speaking, compressed sensing concerns itself with recovering a sparse signal via a small number of linear measurements and a key concept here is the Restricted Isometry Property [CT05; CT06: CRT06; Don06].

---

Definition 1.10 (Restricted Isometry Property). Let $d, m, k \in \mathbb{N}_{1}$ with $m, k<d$ and $\varepsilon \in(0,1)$. A linear function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}$ is said to have the Restricted Isometry Property of order $k$ and level $\varepsilon$ (which we will denote as $(k, \varepsilon)$-RIP) if for all $x \in \mathbb{R}^{d}$ with $\|x\|_{0} \leq k$,

$$
\begin{equation*}
\left|\|f(x)\|_{2}^{2}-\|x\|_{2}^{2}\right| \leq \varepsilon\|x\|_{2}^{2} . \tag{10}
\end{equation*}
$$

---

In the compressed sensing literature it has been shown [CT06; RV08] that the subsampled Hadamard transform (SHT) defined as $f(x):=S H x$, has the $(k, \varepsilon)$-RIP with high probability

$$
\left(\begin{array}{ccccc}
t_{0} & t_{1} & t_{2} & \cdots & t_{d-1} \\
t_{-1} & t_{0} & t_{1} & \cdots & t_{d-2} \\
t_{-2} & t_{-1} & t_{0} & \cdots & t_{d-3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
t_{-(m-1)} & t_{-(m-2)} & t_{-(m-3)} & \cdots & t_{d-m}
\end{array}\right)
$$

for $m=\Omega\left(\varepsilon^{-2} k \log ^{4} d\right)$ while allowing a vector to be embedded in $O(d \log d)$ time. Here $H \in\{-1,1\}^{d \times d}$ is the Walsh-Hadamard matrix and $S \in\{0,1\}^{m \times d}$ samples $m$ entries of $H x$ with replacement, i.e. each row in $S$ has one non-zero entry per row, which is chosen uniformly and independently, i.e. $S$ is a uniformly random feature selection matrix. Inspired by this transform and the FJLT mentioned previously, Ailon and Liberty [AL13] were able to show that the subsampled randomised Hadamard transform (SRHT) defined as $f(x):=S H D x$, is a JLT if $m=\Theta\left(\varepsilon^{-4} \log |X| \log ^{4} d\right)$. Once again $D$ denotes a random diagonal matrix with Rademacher entries, and $S$ and $H$ is as in the SHT. Some related results include Do et al. [Do+09] who before [AL13] were able to get a bound of $m=\Theta\left(\varepsilon^{-2} \log ^{3}|X|\right)$ in the large set case where $|X| \geq d$, [Tro11] which showed how the SRHT construction approximately preserves the norms of a subspace of vectors, and [LL20] which modified the sampling matrix $S$ to improve precision when used as a preprocessor for support vector machines (SVMs) by sacrificing input data independence.

---

This target dimension bound of [AL13] was later tightened by Krahmer and Ward [KW11], who showed that $m=\Theta\left(\varepsilon^{-2} \log |X| \log ^{4} d\right)$ suffices for the SRHT. This was a corollary of a more general result, namely that if $\sigma: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d}$ applies random signs equivalently to the $D$ matrices mentioned previously and $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}$ has the $(\Omega(\log |X|), \varepsilon / 4)$-RIP then $f \circ \sigma$ is a JLT with high probability. An earlier result by Baraniuk et al. |Bar+08| showed that a transform sampled from a JLD has the $\left(O\left(\varepsilon^{2} m / \log d\right), \varepsilon\right)$-RIP with high probability. And so, as one might have expected from their appearance, the Johnson-Lindenstrauss Lemma and the Restricted Isometry Property are indeed cut from the same cloth.

---

Another transform from the compressed sensing literature uses so-called Toeplitz or partial circulant matrices [Baj+07; Rau09; Rom09; Hau+10; RRT12; Baj12; DJR19], which can be defined in the following way. For $m, d \in \mathbb{N}_{1}$ we say that $T \in \mathbb{R}^{m \times d}$ is a real Toeplitz matrix if there exists $t_{-(m-1)}, t_{-(m-2)} \ldots, t_{d-1} \in \mathbb{R}$ such that $T_{i j}=t_{j-i}$. This has the effect that the entries on any one diagonal are the same (see fig. (1) and computing the matrix-vector product corresponds to computing the convolution with a vector of the $t$ s. Partial circulant matrices are special cases of Toeplitz matrices where the diagonals "wrap around" the ends of the matrix, i.e. $t_{-i}=t_{d-i}$ for all $i \in[m-1]$.

---

As a JLT, the Toeplitz construction is $f(x):=T D x$, where $T \in\{-1,1\}^{m \times d}$ is a Toeplitz matrix with i.i.d. Rademacher entries and $D \in\{-1,0,1\}^{d \times d}$ is a diagonal matrix with Rademacher entries as usual. Note that the convolution of two vectors corresponds to the entrywise product in Fourier space, and we can therefore employ fast Fourier transform (FFT) to embed a vector with the Toeplitz construction in time $O(d \log d)$. This time can even be reduced to $O(d \log m)$ as we realise that by partitioning $T$ into $\frac{d}{m}$ consecutive blocks of size $m \times m$, each block is also a Toeplitz matrix, and by applying each individually the embedding time becomes $O\left(\frac{d}{m} m \log m\right)$.

---

Combining the result from [KW11] with RIP bounds for Toeplitz matrices [RRT12] gives that $m=\Theta\left(\varepsilon^{-1} \log ^{3 / 2}|X| \log ^{3 / 2} d+\varepsilon^{-2} \log |X| \log ^{4} d\right)$ is sufficient for the Toeplitz construction to be a JLT with high probability. However, the Toeplitz construction has also been studied directly as a JLD without going via its RIP bounds. Hinrichs and Vybíral [HV11] showed that $m=\Theta\left(\varepsilon^{-2} \log ^{3} \frac{1}{\delta}\right)$ is sufficient for the Toeplitz construction, and this bound was improved shortly thereafter in [Vyb11] to $m=\Theta\left(\varepsilon^{-2} \log ^{2} \frac{1}{\delta}\right)$. The question then is if we can tighten the analysis to shave off the last $\log$ factor and get the elusive result of a JLD with optimal target dimension and $O(d \log d)$ embedding time even when $m$ is close to $d$. Sadly, this is not the case as Freksen and Larsen [FL20] showed that there exists vector ${ }^{277}$ that necessitates $m=\Omega\left(\varepsilon^{-2} \log ^{2} \frac{1}{\delta}\right)$ for the Toeplitz construction.

---

Just as JLTs are used as preprocessors to speed up algorithms that solve the problems we actually care about, we can also use a JLT to speed up other JLTs in what one could refer to as compound JLTs. More explicitely if $f_{1}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d^{\prime}}$ and $f_{2}: \mathbb{R}^{d^{\prime}} \rightarrow \mathbb{R}^{m}$ with $m \ll d^{\prime} \ll d$ are two JLTs and computing $f_{1}(x)$ is fast, we could hope that computing $\left(f_{2} \circ f_{1}\right)(x)$ is fast as well as $f_{2}$ only need to handle $d^{\prime}$ dimensional vectors and hope that $\left(f_{2} \circ f_{1}\right)$ preserves the norm sufficiently well since both $f_{1}$ and $f_{2}$ approximately preserve norms individually. As presented here, the obvious candidate for $f_{1}$ is one of the RIP-based JLDs, which was succesfully applied in [BK17]. In their construction, which we will refer to as GRHD ${ }^{28}$, $f_{1}$ is the SRHT and $f_{2}$ is the dense Rademacher construction (i.e. $f(x):=A_{\text {Rad }} S H D x$ ), and it can embed a vector in time $O(d \log m)$ for $m=O\left(d^{1 / 2-\gamma}\right)$ for any fixed $\gamma>0$. This is a similar result to the construction of Ailon and Liberty [AL09], but unlike that construction, GRHD handles the remaining range of $m$ more gracefully as for any $r \in[1 / 2,1]$ and $m=O\left(d^{r}\right)$, the embedding time for GRHD becomes $O\left(d^{2 r} \log ^{4} d\right)$. However the main selling point of the GRHD construction is that it allows the simultaneous embedding of sufficiently large sets of points $X$ to be computed in total time $O(|X| d \log m)$, even when $m=\Theta\left(d^{1-\gamma}\right)$ for any fixed $\gamma>0$, by utilising fast matrix-matrix multiplication techniques [LR83].

---

Another compound JLD is based on the so-called lean Walsh transforms (LWT) [LAS11], which are defined based on so-called seed matrices. For $r, c \in \mathbb{N}_{1}$ we say that $A_{1} \in \mathbb{C}^{r \times c}$ is a seed matrix if $r<c$, its columns are of unit length, and its rows are pairwise orthogonal and have the same $\ell_{2}$ norm. As such, partial Walsh-Hadamard matrices and partial Fourier matrices are seed matrices (up to normalisation); however, for simplicity's sake we will keep it real by focusing on partial Walsh-Hadamard matrices. We can then define a LWT of order $l \in \mathbb{N}_{1}$ based on this seed as $A_{l}:=A_{1}^{\otimes l}=A_{1} \otimes \cdots \otimes A_{1}$, where $\otimes$ denotes the Kronecker product, which we will quickly define. Let $A$ be a $m \times n$ matrix and $B$ be a $p \times q$ matrix, then the Kronecker product $A \otimes B$ is the $m p \times n q$ block matrix defined as

$$
A \otimes B:=\left(\begin{array}{ccc}
A_{11} B & \cdots & A_{1 n} B \\
\vdots & \ddots & \vdots \\
A_{m 1} B & \cdots & A_{m n} B
\end{array}\right) .
$$

---

Note that $A_{l}$ is a $r^{l} \times c^{l}$ matrix and that any Walsh-Hadamard matrix can be written as $A_{l}$ for some $l$ and the $2 \times 2$ Walsh-Hadamard matrix ${ }^{29}$ as $A_{1}$. Furthermore, for a constant sized seed the time complexity of applying $A_{l}$ to a vector is $O\left(c^{l}\right)$ by using an algorithm similar to FFT. We can then define the compound transform which we will refer to as LWTJL, as $f(x):=G A_{l} D x$, where $D \in\{-1,1\}^{d \times d}$ is a diagonal matrix with Rademacher entries, $A_{l} \in \mathbb{R}^{r^{l} \times d}$ is a LWT, and $G \in \mathbb{R}^{m \times r^{l}}$ is a JLT, and $r$ and $c$ are constants. One way to view LWTJL is as a variant of GRHD where the subsampling occurs on the seed matrix rather than the final Walsh-Hadamard matrix. If $G$ can be applied in $O\left(r^{l} \log r^{l}\right)$ time, e.g. if $G$ is the BCHJL construction |AL09| and $m=O\left(r^{l(1 / 2-\gamma)}\right)$, the total embedding time becomes $O(d)$, as $r^{l}=d^{\alpha}$ for some $\alpha<1$. However, in order to prove that LWTJL satisfies lemma 1.2 the analysis of [LAS11] imposes a few requirements on $r, c$, and the vectors we wish to embed, namely that $\log r / \log c \geq 1-2 \delta$ and $v=O\left(m^{-1 / 2} d^{-\delta}\right)$, where $v$ is an upper bound on the $\ell_{\infty} / \ell_{2}$ ratio as introduced at the end of section 1.4.1. The bound on $v$ is somewhat tight as shown in proposition 1.11

---

Proposition 1.11. For any seed matrix define LWT as the LWTJL distribution seeded with that matrix. Then for all $\delta \in(0,1)$, there exists a vector $x \in \mathbb{C}^{d}$ (or $x \in \mathbb{R}^{d}$, if the seed matrix is a real matrix) satisfying $\|x\|_{\infty} /\|x\|_{2}=\Theta\left(\log ^{-1 / 2} \frac{1}{\delta}\right)$ such that

$$
\begin{equation*}
\operatorname{Pr}_{f \sim L W T}[f(x)=0]>\delta . \tag{11}
\end{equation*}
$$

The proof of proposition 1.11 can be found in section 2.3 and it is based on constructing $x$ as a few copies of a vector that is orthogonal to the rows of the seed matrix.

---

The last JLD we will cover is based on so-called Kac random walks, and despite Ailon and Chazelle [AC09] conjecturing that such a construction could satisfy lemma 1.1] it was not until Jain et al. [Jai+20] that a proof was finally at hand. As with the lean Walsh transforms above, let us first define Kac random walks before describing how they can be used to construct JLDs. A Kac random walk is a Markov chain of linear transformations, where for each step we choose two coordinates at random and perform a random rotation on the plane spanned by these two coordinates, or more formally:

---

Definition 1.12 (Kac random walk [Kac56]). For a given dimention $d \in \mathbb{N}_{1}$, let $K^{(0)}:=I \in\{0,1\}^{d \times d}$ be the identity matrix, and for each $t>0$ sample $\left(i_{t}, j_{t}\right) \in\binom{[d]}{2}$ and $\theta_{t} \in[0,2 \pi)$ independently and uniformly at random. Then define the Kac random walk of length $t$ as $K^{(t)}:=R^{\left(i_{t}, j_{t}, \theta_{t}\right)} K^{(t-1)}$, where $R^{(i, j, \theta)} \in \mathbb{R}^{d \times d}$ is the rotation in the $(i, j)$ plane by $\theta$ and is given by

$$
\begin{array}{rlrl}
R^{(i, j, \theta)} e_{k} & :=e_{k} & \forall k \notin\{i, j\}, \\
R^{(i, j, \theta)}\left(a e_{i}+b e_{j}\right) & :=(a \cos \theta-b \sin \theta) e_{i}+(a \sin \theta+b \cos \theta) e_{j} .
\end{array}
$$

---

The main JLD introduced in $[J a \mathrm{Jai}+20]$, which we will refer to as KacJL , is a compound JLD where both $f_{1}$ and $f_{2}$ consists of a Kac random walk followed by subsampling, which can be defined more formally in the following way. Let $T_{1}:=\Theta(d \log d)$ be the length of the first Kac random walk, $d^{\prime}:=\min \left\{d, \Theta\left(\varepsilon^{-2} \log |X| \log ^{2} \log |X| \log ^{3} d\right)\right\}$ be the intermediate dimension, $T_{2}:=\Theta\left(d^{\prime} \log |X|\right)$ be the length of the second Kac random walk, and $m:=\Theta\left(\varepsilon^{-2} \log |X|\right)$ be the target dimension, and then define the JLT as $f(x)=\left(f_{2} \circ f_{1}\right)(x):=S^{\left(m, d^{\prime}\right)} K^{\left(T_{2}\right)} S^{\left(d^{\prime}, d\right)} K^{\left(T_{1}\right)} x$, where $K^{\left(T_{1}\right)} \in \mathbb{R}^{d \times d}$ and $K^{\left(T_{2}\right)} \in \mathbb{R}^{d^{\prime} \times d^{\prime}}$ are independent Kac random walks of length $T_{1}$ and $T_{2}$, respectively, and $S^{\left(d^{\prime}, d\right)} \in\{0,1\}^{d^{\prime} \times d}$ and $S^{\left(m, d^{\prime}\right)} \in\{0,1\}^{m \times d^{\prime}}$ projects onto the first $d^{\prime}$ and $m$ coordinates ${ }^{30}$, respectively. Since $K^{(T)}$ can be applied in time $O(T)$, the KacJL construction is JLD with embedding time $O\left(d \log d+\min \left\{d \log |X|, \varepsilon^{-2} \log ^{2}|X| \log ^{2} \log |X| \log ^{3} d\right\}\right)$ with asymptotically optimal target dimension, and by only applying the first part $\left(f_{1}\right)$, KacJL achieves an embedding time of $O(d \log d)$ but with a suboptimal target dimension of $O\left(\varepsilon^{-2} \log |X| \log ^{2} \log |X| \log ^{3} d\right)$.

---

Jain et al. [Jai+20] also proposes a version of their JLD construction that avoids computing trigonometric functions ${ }^{31}$ by choosing the angles $\theta_{t}$ uniformly at random from the set $\{\pi / 4,3 \pi / 4,5 \pi / 4,7 \pi / 4\}$ or even the singleton set $\{\pi / 4\}$. This comes at the $\cos \sqrt{32}$ of increasing $T_{1}$ by a factor of $\log \log d$ and $T_{2}$ by a factor of $\log d$, and for the singleton case multiplying with random signs (as we have done with the $D$ matrices in many of the previous constructions) and projecting down onto a random subset of coordinates rather than the $d^{\prime}$ or $m$ first.

---

This concludes the overview of Johnson-Lindenstrauss distributions and transforms, though there are many aspects we did not cover such as space usage, preprocessing time, randomness usage, and norms other than $\ell_{2}$. However, a summary of the main aspects we did cover (embedding times and target dimensions of the JLDs) can be found in table 1 .


## 2 Deferred Proofs

## 2.1 k-Means Cost is Pairwise Distances

Let us first repeat the lemma to remind ourselves what we need to show.

Lemma 1.6, Let $k, d \in \mathbb{N}_{1}$ and $X_{i} \subset \mathbb{R}^{d}$ for $i \in\{1, \ldots, k\}$, then

$$
\sum_{i=1}^{k} \sum_{x \in X_{i}}\left\|x-\frac{1}{\left|X_{i}\right|} \sum_{y \in X_{i}} y\right\|_{2}^{2}=\frac{1}{2} \sum_{i=1}^{k} \frac{1}{\left|X_{i}\right|} \sum_{x, y \in X_{i}}\|x-y\|_{2}^{2}
$$

---

In order to prove lemma 1.6 we will need the following lemma.

Lemma 2.1. Let $d \in \mathbb{N}_{1}$ and $X \subset \mathbb{R}^{d}$ and define $\mu:=\frac{1}{|X|} \sum_{x \in X} x$ as the mean of $X$, then it holds that

$$
\sum_{x, y \in X}\langle x-\mu, y-\mu\rangle=0
$$

Proof of lemma 2.1 The lemma follows from the definition of $\mu$ and the linearity of the real inner product.

$$
\begin{aligned}
\sum_{x, y \in X}\langle x-\mu, y-\mu\rangle & =\sum_{x, y \in X}(\langle x, y\rangle-\langle x, \mu\rangle-\langle y, \mu\rangle+\langle\mu, \mu\rangle) \\
& =\sum_{x, y \in X}\langle x, y\rangle-\sum_{x \in X} 2|X|\langle x, \mu\rangle+|X|^{2}\langle\mu, \mu\rangle \\
& =\sum_{x, y \in X}\langle x, y\rangle-2 \sum_{x \in X}\left\langle x, \sum_{y \in X} y\right\rangle+\left\langle\sum_{x \in X} x, \sum_{y \in X} y\right\rangle \\
& =\sum_{x, y \in X}\langle x, y\rangle-2 \sum_{x, y \in X}\langle x, y\rangle+\sum_{x, y \in X}\langle x, y\rangle \\
& =0
\end{aligned}
$$

---

Proof of lemma 1.6 We will first prove an identity for each partition, so let $X_{i} \subseteq X \subset \mathbb{R}^{d}$ be any partition of the dataset $X$ and define $\mu_{i}:=\frac{1}{\left|X_{i}\right|} \sum_{x \in X_{i}} x$ as the mean of $X_{i}$.

$$
\begin{aligned}
\frac{1}{2\left|X_{i}\right|} \sum_{x, y \in X_{i}}\|x-y\|_{2}^{2} & =\frac{1}{2\left|X_{i}\right|} \sum_{x, y \in X_{i}}\left\|\left(x-\mu_{i}\right)-\left(y-\mu_{i}\right)\right\|_{2}^{2} \\
& =\frac{1}{2\left|X_{i}\right|} \sum_{x, y \in X_{i}}\left(\left\|x-\mu_{i}\right\|_{2}^{2}+\left\|y-\mu_{i}\right\|_{2}^{2}-2\left\langle x-\mu_{i}, y-\mu_{i}\right\rangle\right) \\
& =\sum_{x \in X_{i}}\left\|x-\mu_{i}\right\|_{2}^{2}-\frac{1}{2\left|X_{i}\right|} \sum_{x, y \in X_{i}} 2\left\langle x-\mu_{i}, y-\mu_{i}\right\rangle \\
& =\sum_{x \in X_{i}}\left\|x-\mu_{i}\right\|_{2}^{2}
\end{aligned}
$$

where the last equality holds by lemma 2.1 We now substitute each term in the sum in lemma 1.6 using the just derived identity:

$$
\sum_{i=1}^{k} \sum_{x \in X_{i}}\left\|x-\frac{1}{\left|X_{i}\right|} \sum_{y \in X_{i}} y\right\|_{2}^{2}=\frac{1}{2} \sum_{i=1}^{k} \frac{1}{\left|X_{i}\right|} \sum_{x, y \in X_{i}}\|x-y\|_{2}^{2}
$$

### 2.2 Super Sparse DKS

The tight bounds on the performance of feature hashing presented in theorem 1.8 can be extended to tight performance bounds for the DKS construction. Recall that the DKS construction, parameterised by a so-called column sparsity $s \in \mathbb{N}_{1}$, works by first mapping a vector $x \in \mathbb{R}^{d}$ to an $x^{\prime} \in \mathbb{R}^{s d}$ by duplicating each entry in $x s$ times and then scaling with $1 / \sqrt{s}$, before applying feature hashing to $x^{\prime}$, as $x^{\prime}$ has a more palatable $\ell_{\infty} / \ell_{2}$ ratio compared to $x$. The setting for the extended result is that if we wish to use the DKS construction but we only need to handle vectors with a small $\|x\|_{\infty} /\|x\|_{2}$ ratio, we can choose a column sparsity smaller than the usual $\Theta\left(\varepsilon^{-1} \log \frac{1}{\delta} \log \frac{m}{\delta}\right)$ and still get the Johnson-Lindenstrauss guarantees. This is formalised in corollary 1.9 The two pillars of theorem 1.8 we use in the proof of corollary 1.9 is that the feature hashing tradeoff is tight and that we can force the DKS construction to create hard instances for feature hashing.

---

Corollary 1.9. Let $v_{\mathrm{DKS}} \in[1 / \sqrt{d}, 1]$ denote the largest $\ell_{\infty} / \ell_{2}$ ratio required, $v_{\mathrm{FH}}$ denote the $\ell_{\infty} / \ell_{2}$ constraint for feature hashing as defined in theorem [1.8. and $s_{\text {DKS }} \in[m]$ as the minimum column sparsity such that the DKS construction with that sparsity is a JLD for the subset of vectors $x \in \mathbb{R}^{d}$ that satisfy $\|x\|_{\infty} /\|x\|_{2} \leq v_{\text {DKS }}$. Then

$$
\begin{equation*}
s_{\mathrm{DKS}}=\Theta\left(\frac{v_{\mathrm{DKS}}^{2}}{v_{\mathrm{FH}}^{2}}\right) . \tag{12}
\end{equation*}
$$

---

The upper bound part of the $\Theta$ in corollary 1.9 shows how sparse we can choose the DKS construction to be and still get Johnson-Lindenstrauss guarantees for the data we care about, while the lower bound shows that if we choose a sparsity below this bound, there exists vectors who get distorted too much too often despite having an $\ell_{\infty} / \ell_{2}$ ratio of at most $v_{\text {DKS }}$.

---

Proof of corollary 1.9 Let us first prove the upper bound: $s_{\mathrm{DKS}}=O\left(\frac{v_{\mathrm{KKS}}^{2}}{v_{\mathrm{FH}}^{2}}\right)$.

Let $s:=\Theta\left(\frac{v_{\mathrm{OKs}}^{2}}{v_{\mathrm{FH}}^{2}}\right) \in[m]$ be the column sparsity, and let $x \in \mathbb{R}^{d}$ be a unit vector with $\|x\|_{\infty} \leq v_{\text {DKs }}$. The goal is now to show that a DKS construction with sparsity $s$ can embed $x$ while preserving its norm within $1 \pm \varepsilon$ with probability at least $1-\delta$ (as defined in lemma 1.2). Let $x^{\prime} \in \mathbb{R}^{s d}$ be the unit vector constructed by duplicating each entry in $x s$ times and scaling with $1 / \sqrt{s}$ as in the DKS construction. We now have

$$
\begin{equation*}
\left\|x^{\prime}\right\|_{\infty} \leq \frac{v_{\mathrm{DKS}}}{\sqrt{s}}=\Theta\left(v_{\mathrm{FH}}\right) . \tag{13}
\end{equation*}
$$

---

Let DKS denote the JLD from the DKS construction with column sparsity $s$, and let FH denote the feature hashing JLD. Then we can conclude

$$
\operatorname{Pr}_{f \sim \mathrm{DKS}}\left[\left|\|f(x)\|_{2}^{2}-1\right| \leq \varepsilon\right]=\operatorname{Pr}_{g \sim \mathrm{FH}}\left[\left|\left\|g\left(x^{\prime}\right)\right\|_{2}^{2}-1\right| \leq \varepsilon\right] \geq 1-\delta
$$

where the inequality is implied by eq. (13) and theorem 1.8 .

---

Now let us prove the lower bound: $s_{\mathrm{DKS}}=\Omega\left(\frac{v_{\mathrm{DKS}}^{2}}{v_{\mathrm{FH}}^{2}}\right)$.

Let $s:=o\left(\frac{v_{\mathrm{DKS}}^{2}}{v_{\mathrm{FH}}^{2}}\right)$, and let $x=\left(v_{\mathrm{DKS}}, \ldots, v_{\mathrm{DKS}}, 0, \ldots, 0\right)^{\top} \in \mathbb{R}^{d}$ be a unit vector. We now wish to show that a DKS construction with sparsity $s$ will preserve the norm of $x$ to within $1 \pm \varepsilon$ with probability strictly less than $1-\delta$. As before, define $x^{\prime} \in \mathbb{R}^{s d}$ as the unit vector the DKS construction computes when duplicating every entry in $x s$ times and scaling with $1 / \sqrt{s}$. This gives

$$
\begin{equation*}
\left\|x^{\prime}\right\|_{\infty}=\frac{v_{\mathrm{DKS}}}{\sqrt{S}}=\omega\left(v_{\mathrm{FH}}\right) \tag{14}
\end{equation*}
$$

---

Finally, let DKS denote the JLD from the DKS construction with column sparsity $s$, and let FH denote the feature hashing JLD. Then we can conclude

$$
\operatorname{Pr}_{f \sim \mathrm{DKS}}\left[\left|\|f(x)\|_{2}^{2}-1\right| \leq \varepsilon\right]=\underset{g \sim \mathrm{FH}}{\operatorname{Pr}}\left[\left|\left\|g\left(x^{\prime}\right)\right\|_{2}^{2}-1\right| \leq \varepsilon\right]<1-\delta
$$

where the inequality is implied by eq. 14 and theorem 1.8 . and the fact that $x^{\prime}$ has the shape of an asymptotically worst case instance for feature hashing.

### 2.3 LWTJL Fails for Too Sparse Vectors

Proposition 1.11. For any seed matrix define LWT as the LWTJL distribution seeded with that matrix. Then for all $\delta \in(0,1)$, there exists a vector $x \in \mathbb{C}^{d}$ (or $x \in \mathbb{R}^{d}$, if the seed matrix is a real matrix) satisfying $\|x\|_{\infty} /\|x\|_{2}=\Theta\left(\log ^{-1 / 2} \frac{1}{\delta}\right)$ such that

$$
\operatorname{Pr}_{f \sim \mathrm{LW} T}[f(x)=0]>\delta
$$

Proof. The main idea is to construct the vector $x$ out of segments that are orthogonal to the seed matrix with some probability, and then show that $x$ is orthogonal to all copies of the seed matrix simultaneously with probability larger than $\delta$.

---

Let $r, c \in \mathbb{N}_{1}$ be constants and $A_{1} \in \mathbb{C}^{r \times c}$ be a seed matrix. Let $d$ be the source dimension of the LWTJL construction, $D \in\{-1,0,1\}^{d \times d}$ be the random diagonal matrix with i.i.d. Rademachers, $l \in \mathbb{N}_{1}$ such that $c^{l}=d$, and $A_{l} \in \mathbb{C}^{r^{l} \times c^{l}}$ be the the LWT, i.e. $A_{l}:=A_{1}^{\otimes l}$. Since $r<c$ there exists a nontrivial vector $z \in \mathbb{C}^{c} \backslash\{0\}$ that is orthogonal to all $r$ rows of $A_{1}$ and $\|z\|_{\infty}=\Theta(1)$. Now define $x \in \mathbb{C}^{d}$ as $k \in \mathbb{N}_{1}$ copies of $z$ followed by a padding of 0 s, where $k=\left\lfloor\frac{1}{c} \lg \frac{1}{\delta}-1\right\rfloor$. Note that if the seed matrix is real, we can choose $z$ and therefore $x$ to be real as well.

---

The first thing to note is that

$$
\|x\|_{0} \leq c k<\lg \frac{1}{\delta^{\prime}}
$$

which implies that

$$
\underset{D}{\operatorname{Pr}}[D x=x]=2^{-\|x\|_{0}}>\delta .
$$

Secondly, due to the Kronecker structure of $A_{l}$ and the fact that $z$ is orthogonal to the rows of $A_{1}$, we have

$$
A x=0 .
$$

Taken together, we can conclude

$$
\operatorname{Pr}_{f \sim L W T}[f(x)=0] \geq \operatorname{Pr}_{D}\left[A_{l} D x=0\right] \geq \operatorname{Pr}_{D}[D x=x]>\delta .
$$

---

Now we just need to show that $\|x\|_{\infty} /\|x\|_{2}=\Theta\left(\log ^{-1 / 2} \frac{1}{\delta}\right)$. Since $c$ is a constant and $x$ is consists of $k=\Theta\left(\log \frac{1}{\delta}\right)$ copies of $z$ followed by zeroes,

$$
\begin{aligned}
\|x\|_{\infty} & =\|z\|_{\infty} \quad=\Theta(1) \\
\|z\|_{2} & =\Theta(1) \\
\|x\|_{2} & =\sqrt{k}\|z\|_{2}=\Theta\left(\sqrt{\log \frac{1}{\delta}}\right),
\end{aligned}
$$

which implies the claimed ratio,

$$
\frac{\|x\|_{\infty}}{\|x\|_{2}}=\Theta\left(\log ^{-1 / 2} \frac{1}{\delta}\right) .
$$

---

The following corollary is just a restatement of proposition 1.11 in terms of lemma 1.2, and the proof therefore follows immediately from proposition 1.11

Corollary 2.2. For every $m, d, \in \mathbb{N}_{1}$, and $\delta, \varepsilon \in(0,1)$, and LWTJL distribution LWT over $f: \mathbb{K}^{d} \rightarrow \mathbb{K}^{m}$, where $\mathbb{K} \in\{\mathbb{R}, \mathbb{C}\}$ and $m<d$ there exists a vector $x \in \mathbb{K}^{d}$ with $\|x\|_{\infty} /\|x\|_{2}=\Theta\left(\log ^{-1 / 2} \frac{1}{\delta}\right)$ such that

$$
\operatorname{Pr}_{f \sim L W T}\left[\left|\|f(x)\|_{2}^{2}-\|x\|_{2}^{2}\right| \leq \varepsilon\|x\|_{2}^{2}\right]<1-\delta .
$$

