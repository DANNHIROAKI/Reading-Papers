### 1.3 The Use(fulness) of Johnson-Lindenstrauss

JLDs and JLTs have found uses and parallels in many fields and tasks, some of which we will list below. Note that there are some overlap between the following categories, as e.g. [FB03] uses a JLD for an ensemble of weak learners to learn a mixture of Gaussians clustering, and [PW15] solves convex optimisation problems in a way that gives differential privacy guarantees.

---

Nearest-neighbour search. have benefited from the Johnson-Lindenstrauss lemmas on multiple occasions, including [Kle97; KOR00], which used JL to randomly partition space rather than reduce the dimension, while others [AC09; HIM12] used the dimensionality reduction properties of JL more directly. Variations on these results include consructing locality sensitive hashing schemes [Dat+04] and finding nearest neighbours without false negatives [SW17].

---

Clustering. with results in various sub-areas such as mixture of Gaussians [Das99; FB03; UDS07], subspace clustering [HTB17], graph clustering [SI09; Guo+20], self-organising maps [RK89: Kas98], and $k$-means [Bec+19: Coh+15; Bou+14] Liu+17: SF18], which will be explained in more detail in section 1.3.1.

Outlier detection. where there have been works for various settings of outliers, including approximate nearest-neighbours [dVCH10; SZK15] and Gaussian vectors [NC20], while [Zha+20] uses JL as a preprocessor for a range of outlier detection algorithms in a distributed computational model, and [AP12] evaluates the use of JLTs for outlier detection of text documents.

---

Ensemble learning. where independent JLTs can be used to generate training sets for weak learners for bagging [SR09] and with the voting among the learners weighted by how well a given JLT projects the data [CS17: Can20]. The combination of JLTs with multiple learners have also found use in the regime of learning high-dimensional distributions from few datapoints (i.e. $|X| \ll d$ ) [DK13; ZK19: Niy+20].

---

Adversarial machine learning. where Johnson-Lindenstrauss can both be used to defend against adversarial input [ $\mathrm{Ngu+16}$; Wee+19; Tar+19] as well as help craft such attacks [i+20].

---

Miscellaneous machine learning. where, in addition to the more specific machine learning topics mentioned above, Johnson-Lindenstrauss has been used together with support vector machines [CJS09: Pau+14; LL20], Fisher's linear discriminant [DK10], and neural networks [Sch18], while [KY20] uses JL to facilitate stochastic gradient descent in a distributed setting.

---

Numerical linear algebra. with work focusing on low rank approximation [Coh+15; MM20], canonical correlation analysis [Avr+14], and regression in a local [THM17]: MM09:| Kab14; Sla17] and a distributed [HMM16] computational model. Futhermore, as many of these subfields are related some papers tackle multiple numerical linear algebra problems at once, e.g. low rank approximation, regression, and approximate matrix multiplication [Sar06], and a line of work [MM13; CW17; NN13a] have used JLDs to perform subspace embeddings which in turn gives algorithms for $\ell_{p}$ regression, low rank approximation, and leverage scores.

For further reading, there are surveys [Mah11; HMT11; Woo14] covering much of JLDs' use in numerical linear algebra.

---

Convex optimisation. in which Johnson-Lindenstrauss has been used for (integer) linear programming [VPL15] and to improve a cutting plane method for finding a point in a convex set using a separation oracle [TSC15; Jia+20]. Additionally, [Zha+13] studies how to recover a high-dimensional optimisation solution from a JL dimensionality reduced one.

---

Differential privacy. have utilised Johnson-Lindenstrauss to provide sanitised solutions to the linear algebra problems of variance estimation [Blo+12], regression [She19; SKD19; ZLW09], Euclidean distance estimation [Ken+13; LKR06; GLK13; Tur $\mathbf{Z}$ 08; Xu+17], and low-rank factorisation [Upa18], as well as convex optimisation [PW15; [KJ16], collaborative filtering [Yan+17] and solutions to graph-cut queries [Blo+12: Upa13]. Furthermore, [Upa15] analysis various JLDs with respect to differential privacy and introduces a novel one designed for this purpose.

---

Neuroscience. where it is used as a tool to process data in computational neuroscience [GS12, ALG13], but also as a way of modelling neurological processes [GS12; ALG13; All+14; PP14]. Interestingly, there is some evidence [MFL08; SA09; Car+13] to suggest that JL-like operations occur in nature, as a large set of olifactory sensory inputs (projection neurons) map onto a smaller set of neurons (Kenyon cells) in the brains of fruit flies, where each Kenyon cell is connected to a small and seemingly random subset of the projection neurons. This is reminiscent of sparse JL constructions, which will be introduced in section 1.4.1. though I am not neuroscientifically adept enough to judge how far these similarities between biological constructs and randomised linear algebra extend.

---

Other topics. where Johnson-Lindenstrauss have found use include graph sparsification [SS11], graph embeddings in Euclidean spaces [FM88], integrated circuit design [Vem98], biometric authentication [Arp+14], and approximating minimum spanning trees [HI00].
For further examples of Johnson-Lindenstrauss use cases, please see [Ind01; Vem04].

Now, let us dive deeper into the areas of clustering and streaming algorithms to see how Johnson-Lindenstrauss can be used there.

### 1.3.1 Clustering

Clustering can be defined as partitioning a dataset such that elements are similar to elements in the same partition while being dissimilar to elements in other partitions. A classic clustering problem is the so-called $k$-means clustering where the dataset $X \subset \mathbb{R}^{d}$ consists of points in Euclidean space. The task is to choose $k$ cluster centers $c_{1}, \ldots, c_{k}$ such that they minimise the sum of squared distances from datapoints to their nearest cluster center, i.e.

$$
\begin{equation*}
\underset{c_{1}, \ldots, c_{k}}{\arg \min } \sum_{x \in X} \min _{i}\left\|x-c_{i}\right\|_{2}^{2} . \tag{6}
\end{equation*}
$$

---

This creates a Voronoi partition, as each datapoint is assigned to the partition corresponding to its nearest cluster center. We let $X_{i} \subseteq X$ denote the set of points that have $c_{i}$ as their closest center. It is well known that for an optimal choice of centers, the centers are the means of their corresponding partitions, and furthermore, the cost of any choice of centers is never lower than the sum of squared distances from datapoints to the mean of their assigned partition, i.e.

$$
\begin{equation*}
\sum_{x \in X} \min _{i}\left\|x-c_{i}\right\|_{2}^{2} \geq \sum_{i=1}^{k} \sum_{x \in X_{i}}\left\|x-\frac{1}{\left|X_{i}\right|} \sum_{y \in X_{i}} y\right\|_{2}^{2} \tag{7}
\end{equation*}
$$

---

It has been shown that finding the optimal centers, even for $k=2$, is NP-hard [Alo+09; Das08|; however, various heuristic approaches have found success such as the commonly used Lloyd's algorithm [Llo82]. In Lloyd's algorithm, after initialising the centers in some way we iteratively improve the choice of centers by assigning each datapoint to its nearest center and then updating the center to be the mean of the datapoints assigned to it. These two steps can then be repeated until some termination criterion is met, e.g. when the centers have converged. If we let $t$ denote the number of iterations, then the running time becomes $O(t|X| k d)$, as we use $O(|X| k d)$ time per iteration to assign each data point to its nearest center. We can improve this running time by quickly embedding the datapoints into a lower dimensional space using a JLT and then running Lloyd's algorithm in this smaller space. The Fast Johnson-Lindenstrauss Transform, which we will introduce later, can for many sets of parameters embed a vector in $O(d \log d)$ time reducing the total running time to $O\left(|X| d \log d+t|X| k \varepsilon^{-2} \log |X|\right)$. However, for this to be useful we need the partitioning of points in the lower dimensional space to correspond to an (almost) equally good partition in the original higher dimensional space.

---

In order to prove such a result we will use the following lemma, which shows that the cost of a partitioning, with its centers chosen as the means of the partitions, can be written in terms of pairwise distances between datapoints in the partitions.

Lemma 1.6. Let $k, d \in \mathbb{N}_{1}$ and $X_{i} \subset \mathbb{R}^{d}$ for ${ }^{10} i \in[k]$.

$$
\begin{equation*}
\sum_{i=1}^{k} \sum_{x \in X_{i}}\left\|x-\frac{1}{\left|X_{i}\right|} \sum_{y \in X_{i}} y\right\|_{2}^{2}=\frac{1}{2} \sum_{i=1}^{k} \frac{1}{\left|X_{i}\right|} \sum_{x, y \in X_{i}}\|x-y\|_{2}^{2} \tag{8}
\end{equation*}
$$

The proof of lemma 1.6 consists of various linear algebra manipulations and can be found in section 2.1. Now we are ready to prove the following proposition, which states that if we find a partitioning whose cost is within $(1+\gamma)$ of the optimal cost in low dimensional space, that partitioning when moving back to the high dimensional space is within $(1+4 \varepsilon)(1+\gamma)$ of the optimal cost there.

---

Proposition 1.7. Let $k, d \in \mathbb{N}_{1}, X \subset \mathbb{R}^{d}, \varepsilon \leq 1 / 2, m=\Theta\left(\varepsilon^{-2} \log |X|\right)$, and $f: X \rightarrow \mathbb{R}^{m}$ be a JLT. Let $Y \subset \mathbb{R}^{m}$ be the embedding of $X$. Let $\kappa_{m}^{*}$ denote the optimal cost of a partitioning of $Y$, with respect to eq. (6). Let $Y_{1}, \ldots, Y_{k} \subseteq Y$ be a partitioning of $Y$ with cost $\kappa_{m}$ such that $\kappa_{m} \leq(1+\gamma) \kappa_{m}^{*}$ for some $\gamma \in \mathbb{R}$. Let $\kappa_{d}^{*}$ be the cost of an optimal partitioning of $X$ and $\kappa_{d}$ be the cost of the partitioning $X_{1}, \ldots, X_{k} \subseteq X$, satisfying $Y_{i}=\left\{f(x) \mid x \in X_{i}\right\}$. Then

$$
\begin{equation*}
\kappa_{d} \leq(1+4 \varepsilon)(1+\gamma) \kappa_{d}^{*} \tag{9}
\end{equation*}
$$

Proof. Due to lemma 1.6 and the fact that $f$ is a JLT we know that the cost of our partitioning is approximately preserved when going back to the high dimensional space, i.e. $\kappa_{d} \leq \kappa_{m} /(1-\varepsilon)$. Furthermore, since the cost of $X$ 's optimal partitioning when embedded down to $Y$ cannot be lower than the optimal cost of partitioning $Y$, we can conclude $\kappa_{m}^{*} \leq(1+\varepsilon) \kappa_{d}^{*}$. Since $\varepsilon \leq 1 / 2$, we have $1 /(1-\varepsilon)=1+\varepsilon /(1-\varepsilon) \leq 1+2 \varepsilon$ and also $(1+\varepsilon)(1+2 \varepsilon)=\left(1+3 \varepsilon+2 \varepsilon^{2}\right) \leq 1+4 \varepsilon$. Combining these inequalities we get

$$
\begin{aligned}
\kappa_{d} & \leq \frac{1}{1-\varepsilon} \kappa_{m} \\
& \leq(1+2 \varepsilon)(1+\gamma) \kappa_{m}^{*} \\
& \leq(1+2 \varepsilon)(1+\gamma)(1+\varepsilon) \kappa_{d}^{*} \\
& \leq(1+4 \varepsilon)(1+\gamma) \kappa_{d}^{*} .
\end{aligned}
$$

---

By pushing the constant inside the $\Theta$-notation, proposition 1.7 shows that we can achieve a $(1+\varepsilon)$ approximation ${ }^{11}$ of $k$-means with $m=\Theta\left(\varepsilon^{-2} \log |X|\right)$. However, by more carefully analysing which properties are needed, we can improve upon this for the case where $k \ll|X|$. Boutsidis et al. $[B o u+14]$ showed that projecting down to a target dimension of $m=\Theta\left(\varepsilon^{-2} k\right)$ suffices for a slightly worse $k$-means approximation factor of $(2+\varepsilon)$. This result was expanded upon in two ways by Cohen et al. [Coh+15], who showed that projecting down to $m=\Theta\left(\varepsilon^{-2} k\right)$ achieves a $(1+\varepsilon)$ approximation ratio, while projecting all the way down to $m=\Theta\left(\varepsilon^{-2} \log k\right)$ still suffices for a $(9+\varepsilon)$ approximation ratio. The $(1+\varepsilon)$ case has recently been further improved upon by both Becchetti et al. [Bec +19$]$, who have shown that one can achieve the $(1+\varepsilon)$ approximation ratio for $k$-means when projecting down to $m=\Theta\left(\varepsilon^{-6}(\log k+\log \log |X|) \log \varepsilon^{-1}\right)$, and by Makarychev, Makarychev, and Razenshteyn [MMR19], who independently have proven an even better bound of $m=\Theta\left(\varepsilon^{-2} \log k / \varepsilon\right)$, essentially giving a "best of worlds" result with respect to [Coh+15].

For an overview of the history of $k$-means clustering, we refer the interested reader to [Boc08].

### 1.3.2 Streaming

The field of streaming algorithms is characterised by problems where we receive a sequence (or stream) of items and are queried on the items received so far. The main constraint is usually that we only have limited access to the sequence, e.g. that we are only allowed one pass over it, and that we have very limited space, e.g. polylogarithmic in the length of the stream. To make up for these constraints we are allowed to give approximate answers to the queries. The subclass of streaming problems we will look at here are those where we are only allowed a single pass over the sequence and the items are updates to a vector and a query is some statistic on that vector, e.g. the $\ell_{2}$ norm of the vector. More formally, and to introduce the notation, let $d \in \mathbb{N}_{1}$ be the number of different items and let $T \in \mathbb{N}$ be the length of the stream of updates $\left(i_{j}, v_{j}\right) \in[d] \times \mathbb{R}$ for $j \in[T]$, and define the vector $x$ at time $t$ as $x^{(t)}:=\sum_{j=1}^{t} v_{j} e_{i j}$. A query $q$ at time $t$ is then a function of $x^{(t)}$, and we will omit the ${ }^{(t)}$ superscript when referring to the current time.

---

There are a few common variations on this model with respect to the updates. In the cash register model or insertion only model $x$ is only incremented by bounded integers, i.e. $v_{j} \in[M]$, for some $M \in \mathbb{N}_{1}$. In the turnstile model, $x$ can only be incremented or decremented by bounded integers, i.e. $v_{j} \in\{-M, \ldots, M\}$ for some $M \in \mathbb{N}_{1}$, and the strict turnstile model is as the turnstile model with the additional constraint that the entries of $x$ are always non-negative, i.e. $x_{i}^{(t)} \geq 0$, for all $t \in[T]$ and $i \in[d]$.

---

As mentioned above, we are usually space constrained so that we cannot explicitely store $x$ and the key idea to overcome this limitation is to store a linear sketch of $x$, that is storing $y:=f(x)$, where $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}$ is a linear function and $m \ll d$, and then answering queries by applying some function on $y$ rather than $x$. Note that since $f$ is linear, we can apply it to each update individually and compute $y$ as the sum of the sketched updates. Furthermore, we can aggregate results from different streams by adding the different sketches, allowing us to distribute the computation of the streaming algorithm.

---

The relevant Johnson-Lindenstrauss lemma in this setting is lemma 1.2 as with a JLD we get linearity and are able to sample a JLT before seeing any data at the cost of introducing some failure probability.

---

Based on JLDs, the most natural streaming problem to tackle is second frequency moment estimation in the turnstile model, i.e. approximating $\|x\|_{2}^{2}$, which has found use in database query optimisation [Alo+02; WDJ91; DeW+92] and network data analysis [Gil+01; CG05] among other areas. Simply letting $f$ be a sample from a JLD and returning $\|f(x)\|_{2}^{2}$ on queries, gives a factor $(1 \pm \varepsilon)$ approximation with failure probability $\delta$ using $O\left(\varepsilon^{-2} \log \frac{1}{\delta}+|f|\right)$ words ${ }^{12}$ of space, where $|f|$ denotes the words of space needed to store and apply $f$. However, the approach taken by the streaming literature is to estimate $\|x\|_{2}^{2}$ with constant error probability using $O\left(\varepsilon^{-2}+|f|\right)$ words of space, and then sampling $O\left(\log \frac{1}{\delta}\right)$ JLTs $f_{1}, \ldots, f_{O(\log 1 / \delta)}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{O\left(\varepsilon^{-2}\right)}$ and responding to a query with median $_{k}\left\|f_{k}(x)\right\|_{2}^{2}$, which reduces the error probability to $\delta$. This allows for simpler analyses as well more efficient embeddings (in the case of Count Sketch) compared to using a single bigger JLT, but it comes at the cost of not embedding into $\ell_{2}$, which is needed for some applications outside of streaming. With this setup the task lies in constructing space efficient JLTs and a seminal work here is the AMS Sketch a.k.a. AGMS Sketch a.k.a. Tug-of-War Sketch [AMS99: Alo+02], whose JLTs can be defined as $f_{i}:=m^{-1 / 2} A x$, where $A \in\{-1,1\}^{m \times d}$ is a random matrix. The key idea is that each row $r$ of $A$ can be backed by a hash function $\sigma_{r}:[d] \rightarrow\{-1,1\}$ that need only be 4 -wise independent, meaning that for any set of 4 distinct keys $\left\{k_{1}, \ldots k_{4}\right\} \subset[d]$ and 4 (not necessarily distinct) values $v_{1}, \ldots v_{4} \in\{-1,1\}$, the probability that the keys hash to those values is $\operatorname{Pr}_{\sigma_{r}}\left[\bigwedge_{i} \sigma_{r}\left(k_{i}\right)=v_{i}\right]=|\{-1,1\}|^{-4}$. This can for instance ${ }^{13}$ be attained by implementing $\sigma_{r}$ as 3rd degree polynomial modulus a sufficiently large prime with random coefficients [WC81], and so such a JLT need only use $O\left(\varepsilon^{-2}\right)$ words of space. Embedding a scaled standard unit vector with such a JLT takes $O\left(\varepsilon^{-2}\right)$ time leading to an overall update time of the AMS Sketch of $O\left(\varepsilon^{-2} \log \frac{1}{\delta}\right)$.

---

A later improvement of the AMS Sketch is the so-called Fast-AGMS Sketch [CG05] a.k.a. Count Sketch [CCF04; TZ12], which sparsifies the JLTs such that each column in their matrix representations only has one non-zero entry. Each JLT can be represented by a pairwise independent hash function $h:[d] \rightarrow\left[O\left(\varepsilon^{-2}\right)\right]$ to choose the position of each nonzero entry and a 4 -wise independent hash function $\sigma:[d] \rightarrow\{-1,1\}$ to choose random signs as before. This reduces the standard unit vector embedding time to $O(1)$ and so the overall update time becomes $O\left(\log \frac{1}{\delta}\right)$ for Count Sketch. It should be noted that the JLD inside Count Sketch is also known as Feature Hashing, which we will return to in section 1.4.1.

---

Despite not embedding into $\ell_{2}$, due to the use of the non-linear median, AMS Sketch and Count Sketch approximately preserve dot products similarly to corollary 1.5 [CG05, Theorem 2.1 and Theorem 3.5]. This allows us to query for the (approximate) frequency of any particular item as

$$
\underset{k}{\operatorname{median}}\left\langle f_{k}(x), f_{k}\left(e_{i}\right)\right\rangle=\left\langle x, e_{i}\right\rangle \pm \varepsilon\|x\|_{2}\left\|e_{i}\right\|_{2}=x_{i} \pm \varepsilon\|x\|_{2}
$$

with probability at least $1-\delta$.

---

This can be extended to finding frequent items in an insertion only stream [CCF04]. The idea is to use a slightly larger ${ }^{14}$ Count Sketch instance to maintain a heap of the $k$ approximately most frequent items of the stream so far. That is, if we let $i_{k}$ denote the $k$ th most frequent item (i.e. $\left|\left\{j \mid x_{j} \geq x_{i_{k}}\right\}\right|=k$ ), then with probability $1-\delta$ we have $x_{j}>(1-\varepsilon) x_{i_{k}}$ for every item $j$ in our heap.

For more on streaming algorithms, we refer the reader to [Mut05] and [Nel11], which also relates streaming to Johnson-Lindenstrauss.