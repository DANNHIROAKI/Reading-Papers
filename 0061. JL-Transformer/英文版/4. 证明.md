## 2 Deferred Proofs

## 2.1 k-Means Cost is Pairwise Distances

Let us first repeat the lemma to remind ourselves what we need to show.

Lemma 1.6, Let $k, d \in \mathbb{N}_{1}$ and $X_{i} \subset \mathbb{R}^{d}$ for $i \in\{1, \ldots, k\}$, then

$$
\sum_{i=1}^{k} \sum_{x \in X_{i}}\left\|x-\frac{1}{\left|X_{i}\right|} \sum_{y \in X_{i}} y\right\|_{2}^{2}=\frac{1}{2} \sum_{i=1}^{k} \frac{1}{\left|X_{i}\right|} \sum_{x, y \in X_{i}}\|x-y\|_{2}^{2}
$$

---

In order to prove lemma 1.6 we will need the following lemma.

Lemma 2.1. Let $d \in \mathbb{N}_{1}$ and $X \subset \mathbb{R}^{d}$ and define $\mu:=\frac{1}{|X|} \sum_{x \in X} x$ as the mean of $X$, then it holds that

$$
\sum_{x, y \in X}\langle x-\mu, y-\mu\rangle=0
$$

Proof of lemma 2.1 The lemma follows from the definition of $\mu$ and the linearity of the real inner product.

$$
\begin{aligned}
\sum_{x, y \in X}\langle x-\mu, y-\mu\rangle & =\sum_{x, y \in X}(\langle x, y\rangle-\langle x, \mu\rangle-\langle y, \mu\rangle+\langle\mu, \mu\rangle) \\
& =\sum_{x, y \in X}\langle x, y\rangle-\sum_{x \in X} 2|X|\langle x, \mu\rangle+|X|^{2}\langle\mu, \mu\rangle \\
& =\sum_{x, y \in X}\langle x, y\rangle-2 \sum_{x \in X}\left\langle x, \sum_{y \in X} y\right\rangle+\left\langle\sum_{x \in X} x, \sum_{y \in X} y\right\rangle \\
& =\sum_{x, y \in X}\langle x, y\rangle-2 \sum_{x, y \in X}\langle x, y\rangle+\sum_{x, y \in X}\langle x, y\rangle \\
& =0
\end{aligned}
$$

---

Proof of lemma 1.6 We will first prove an identity for each partition, so let $X_{i} \subseteq X \subset \mathbb{R}^{d}$ be any partition of the dataset $X$ and define $\mu_{i}:=\frac{1}{\left|X_{i}\right|} \sum_{x \in X_{i}} x$ as the mean of $X_{i}$.

$$
\begin{aligned}
\frac{1}{2\left|X_{i}\right|} \sum_{x, y \in X_{i}}\|x-y\|_{2}^{2} & =\frac{1}{2\left|X_{i}\right|} \sum_{x, y \in X_{i}}\left\|\left(x-\mu_{i}\right)-\left(y-\mu_{i}\right)\right\|_{2}^{2} \\
& =\frac{1}{2\left|X_{i}\right|} \sum_{x, y \in X_{i}}\left(\left\|x-\mu_{i}\right\|_{2}^{2}+\left\|y-\mu_{i}\right\|_{2}^{2}-2\left\langle x-\mu_{i}, y-\mu_{i}\right\rangle\right) \\
& =\sum_{x \in X_{i}}\left\|x-\mu_{i}\right\|_{2}^{2}-\frac{1}{2\left|X_{i}\right|} \sum_{x, y \in X_{i}} 2\left\langle x-\mu_{i}, y-\mu_{i}\right\rangle \\
& =\sum_{x \in X_{i}}\left\|x-\mu_{i}\right\|_{2}^{2}
\end{aligned}
$$

where the last equality holds by lemma 2.1 We now substitute each term in the sum in lemma 1.6 using the just derived identity:

$$
\sum_{i=1}^{k} \sum_{x \in X_{i}}\left\|x-\frac{1}{\left|X_{i}\right|} \sum_{y \in X_{i}} y\right\|_{2}^{2}=\frac{1}{2} \sum_{i=1}^{k} \frac{1}{\left|X_{i}\right|} \sum_{x, y \in X_{i}}\|x-y\|_{2}^{2}
$$

### 2.2 Super Sparse DKS

The tight bounds on the performance of feature hashing presented in theorem 1.8 can be extended to tight performance bounds for the DKS construction. Recall that the DKS construction, parameterised by a so-called column sparsity $s \in \mathbb{N}_{1}$, works by first mapping a vector $x \in \mathbb{R}^{d}$ to an $x^{\prime} \in \mathbb{R}^{s d}$ by duplicating each entry in $x s$ times and then scaling with $1 / \sqrt{s}$, before applying feature hashing to $x^{\prime}$, as $x^{\prime}$ has a more palatable $\ell_{\infty} / \ell_{2}$ ratio compared to $x$. The setting for the extended result is that if we wish to use the DKS construction but we only need to handle vectors with a small $\|x\|_{\infty} /\|x\|_{2}$ ratio, we can choose a column sparsity smaller than the usual $\Theta\left(\varepsilon^{-1} \log \frac{1}{\delta} \log \frac{m}{\delta}\right)$ and still get the Johnson-Lindenstrauss guarantees. This is formalised in corollary 1.9 The two pillars of theorem 1.8 we use in the proof of corollary 1.9 is that the feature hashing tradeoff is tight and that we can force the DKS construction to create hard instances for feature hashing.

---

Corollary 1.9. Let $v_{\mathrm{DKS}} \in[1 / \sqrt{d}, 1]$ denote the largest $\ell_{\infty} / \ell_{2}$ ratio required, $v_{\mathrm{FH}}$ denote the $\ell_{\infty} / \ell_{2}$ constraint for feature hashing as defined in theorem [1.8. and $s_{\text {DKS }} \in[m]$ as the minimum column sparsity such that the DKS construction with that sparsity is a JLD for the subset of vectors $x \in \mathbb{R}^{d}$ that satisfy $\|x\|_{\infty} /\|x\|_{2} \leq v_{\text {DKS }}$. Then

$$
\begin{equation*}
s_{\mathrm{DKS}}=\Theta\left(\frac{v_{\mathrm{DKS}}^{2}}{v_{\mathrm{FH}}^{2}}\right) . \tag{12}
\end{equation*}
$$

---

The upper bound part of the $\Theta$ in corollary 1.9 shows how sparse we can choose the DKS construction to be and still get Johnson-Lindenstrauss guarantees for the data we care about, while the lower bound shows that if we choose a sparsity below this bound, there exists vectors who get distorted too much too often despite having an $\ell_{\infty} / \ell_{2}$ ratio of at most $v_{\text {DKS }}$.

---

Proof of corollary 1.9 Let us first prove the upper bound: $s_{\mathrm{DKS}}=O\left(\frac{v_{\mathrm{KKS}}^{2}}{v_{\mathrm{FH}}^{2}}\right)$.

Let $s:=\Theta\left(\frac{v_{\mathrm{OKs}}^{2}}{v_{\mathrm{FH}}^{2}}\right) \in[m]$ be the column sparsity, and let $x \in \mathbb{R}^{d}$ be a unit vector with $\|x\|_{\infty} \leq v_{\text {DKs }}$. The goal is now to show that a DKS construction with sparsity $s$ can embed $x$ while preserving its norm within $1 \pm \varepsilon$ with probability at least $1-\delta$ (as defined in lemma 1.2). Let $x^{\prime} \in \mathbb{R}^{s d}$ be the unit vector constructed by duplicating each entry in $x s$ times and scaling with $1 / \sqrt{s}$ as in the DKS construction. We now have

$$
\begin{equation*}
\left\|x^{\prime}\right\|_{\infty} \leq \frac{v_{\mathrm{DKS}}}{\sqrt{s}}=\Theta\left(v_{\mathrm{FH}}\right) . \tag{13}
\end{equation*}
$$

---

Let DKS denote the JLD from the DKS construction with column sparsity $s$, and let FH denote the feature hashing JLD. Then we can conclude

$$
\operatorname{Pr}_{f \sim \mathrm{DKS}}\left[\left|\|f(x)\|_{2}^{2}-1\right| \leq \varepsilon\right]=\operatorname{Pr}_{g \sim \mathrm{FH}}\left[\left|\left\|g\left(x^{\prime}\right)\right\|_{2}^{2}-1\right| \leq \varepsilon\right] \geq 1-\delta
$$

where the inequality is implied by eq. (13) and theorem 1.8 .

---

Now let us prove the lower bound: $s_{\mathrm{DKS}}=\Omega\left(\frac{v_{\mathrm{DKS}}^{2}}{v_{\mathrm{FH}}^{2}}\right)$.

Let $s:=o\left(\frac{v_{\mathrm{DKS}}^{2}}{v_{\mathrm{FH}}^{2}}\right)$, and let $x=\left(v_{\mathrm{DKS}}, \ldots, v_{\mathrm{DKS}}, 0, \ldots, 0\right)^{\top} \in \mathbb{R}^{d}$ be a unit vector. We now wish to show that a DKS construction with sparsity $s$ will preserve the norm of $x$ to within $1 \pm \varepsilon$ with probability strictly less than $1-\delta$. As before, define $x^{\prime} \in \mathbb{R}^{s d}$ as the unit vector the DKS construction computes when duplicating every entry in $x s$ times and scaling with $1 / \sqrt{s}$. This gives

$$
\begin{equation*}
\left\|x^{\prime}\right\|_{\infty}=\frac{v_{\mathrm{DKS}}}{\sqrt{S}}=\omega\left(v_{\mathrm{FH}}\right) \tag{14}
\end{equation*}
$$

---

Finally, let DKS denote the JLD from the DKS construction with column sparsity $s$, and let FH denote the feature hashing JLD. Then we can conclude

$$
\operatorname{Pr}_{f \sim \mathrm{DKS}}\left[\left|\|f(x)\|_{2}^{2}-1\right| \leq \varepsilon\right]=\underset{g \sim \mathrm{FH}}{\operatorname{Pr}}\left[\left|\left\|g\left(x^{\prime}\right)\right\|_{2}^{2}-1\right| \leq \varepsilon\right]<1-\delta
$$

where the inequality is implied by eq. 14 and theorem 1.8 . and the fact that $x^{\prime}$ has the shape of an asymptotically worst case instance for feature hashing.

### 2.3 LWTJL Fails for Too Sparse Vectors

Proposition 1.11. For any seed matrix define LWT as the LWTJL distribution seeded with that matrix. Then for all $\delta \in(0,1)$, there exists a vector $x \in \mathbb{C}^{d}$ (or $x \in \mathbb{R}^{d}$, if the seed matrix is a real matrix) satisfying $\|x\|_{\infty} /\|x\|_{2}=\Theta\left(\log ^{-1 / 2} \frac{1}{\delta}\right)$ such that

$$
\operatorname{Pr}_{f \sim \mathrm{LW} T}[f(x)=0]>\delta
$$

Proof. The main idea is to construct the vector $x$ out of segments that are orthogonal to the seed matrix with some probability, and then show that $x$ is orthogonal to all copies of the seed matrix simultaneously with probability larger than $\delta$.

---

Let $r, c \in \mathbb{N}_{1}$ be constants and $A_{1} \in \mathbb{C}^{r \times c}$ be a seed matrix. Let $d$ be the source dimension of the LWTJL construction, $D \in\{-1,0,1\}^{d \times d}$ be the random diagonal matrix with i.i.d. Rademachers, $l \in \mathbb{N}_{1}$ such that $c^{l}=d$, and $A_{l} \in \mathbb{C}^{r^{l} \times c^{l}}$ be the the LWT, i.e. $A_{l}:=A_{1}^{\otimes l}$. Since $r<c$ there exists a nontrivial vector $z \in \mathbb{C}^{c} \backslash\{0\}$ that is orthogonal to all $r$ rows of $A_{1}$ and $\|z\|_{\infty}=\Theta(1)$. Now define $x \in \mathbb{C}^{d}$ as $k \in \mathbb{N}_{1}$ copies of $z$ followed by a padding of 0 s, where $k=\left\lfloor\frac{1}{c} \lg \frac{1}{\delta}-1\right\rfloor$. Note that if the seed matrix is real, we can choose $z$ and therefore $x$ to be real as well.

---

The first thing to note is that

$$
\|x\|_{0} \leq c k<\lg \frac{1}{\delta^{\prime}}
$$

which implies that

$$
\underset{D}{\operatorname{Pr}}[D x=x]=2^{-\|x\|_{0}}>\delta .
$$

Secondly, due to the Kronecker structure of $A_{l}$ and the fact that $z$ is orthogonal to the rows of $A_{1}$, we have

$$
A x=0 .
$$

Taken together, we can conclude

$$
\operatorname{Pr}_{f \sim L W T}[f(x)=0] \geq \operatorname{Pr}_{D}\left[A_{l} D x=0\right] \geq \operatorname{Pr}_{D}[D x=x]>\delta .
$$

---

Now we just need to show that $\|x\|_{\infty} /\|x\|_{2}=\Theta\left(\log ^{-1 / 2} \frac{1}{\delta}\right)$. Since $c$ is a constant and $x$ is consists of $k=\Theta\left(\log \frac{1}{\delta}\right)$ copies of $z$ followed by zeroes,

$$
\begin{aligned}
\|x\|_{\infty} & =\|z\|_{\infty} \quad=\Theta(1) \\
\|z\|_{2} & =\Theta(1) \\
\|x\|_{2} & =\sqrt{k}\|z\|_{2}=\Theta\left(\sqrt{\log \frac{1}{\delta}}\right),
\end{aligned}
$$

which implies the claimed ratio,

$$
\frac{\|x\|_{\infty}}{\|x\|_{2}}=\Theta\left(\log ^{-1 / 2} \frac{1}{\delta}\right) .
$$

---

The following corollary is just a restatement of proposition 1.11 in terms of lemma 1.2, and the proof therefore follows immediately from proposition 1.11

Corollary 2.2. For every $m, d, \in \mathbb{N}_{1}$, and $\delta, \varepsilon \in(0,1)$, and LWTJL distribution LWT over $f: \mathbb{K}^{d} \rightarrow \mathbb{K}^{m}$, where $\mathbb{K} \in\{\mathbb{R}, \mathbb{C}\}$ and $m<d$ there exists a vector $x \in \mathbb{K}^{d}$ with $\|x\|_{\infty} /\|x\|_{2}=\Theta\left(\log ^{-1 / 2} \frac{1}{\delta}\right)$ such that

$$
\operatorname{Pr}_{f \sim L W T}\left[\left|\|f(x)\|_{2}^{2}-\|x\|_{2}^{2}\right| \leq \varepsilon\|x\|_{2}^{2}\right]<1-\delta .
$$