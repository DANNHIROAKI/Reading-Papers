### An Introduction to Johnson-Lindenstrauss Transforms 


#### Abstract

Johnson-Lindenstrauss Transforms are powerful tools for reducing the dimensionality of data while preserving key characteristics of that data, and they have found use in many fields from machine learning to differential privacy and more. This note explains what they are; it gives an overview of their use and their development since they were introduced in the 1980s; and it provides many references should the reader wish to explore these topics more deeply.

---

The text was previously a main part of the introduction of my PhD thesis [Fre20], but it has been adapted to be self contained and serve as a (hopefully good) starting point for readers interested in the topic.


## 1 The Why, What, and How

### 1.1 The Problem

Consider the following scenario: We have some data that we wish to process but the data is too large, e.g. processing the data takes too much time, or storing the data takes too much space. A solution would be to compress the data such that the valuable parts of the data are kept and the other parts discarded. Of course, what is considered valuable is defined by the data processing we wish to apply to our data. To make our scenario more concrete let us say that our data consists of vectors in a high dimensional Euclidean space, $\mathbb{R}^{d}$, and we wish to find a transform to embed these vectors into a lower dimensional space, $\mathbb{R}^{m}$, where $m \ll d$, so that we can apply our data processing in this lower dimensional space and still get meaningful results. The problem in this more concrete scenario is known as dimensionality reduction.

---

As an example, let us pretend to be a library with a large corpus of texts and whenever a person returns a novel that they liked, we would like to recommend some similar novels for them to read next. Or perhaps we wish to be able to automatically categorise the texts into groups such as fiction/non-fiction or child/young-adult/adult literature. To be able to use the wealth of research on similarity search and classification we need a suitable representation of our texts, and here a common choice is called bag-of-words. For a language or vocabulary with $d$ different words, the bag-of-words representation of a text $t$ is a vector $x \in \mathbb{R}^{d}$ whose $i$ th entry is the number of times the $i$ th word occurs in $t$. For example, if the language is just ["be", "is", "not", "or", "question", "that", "the", "to"] then the text "to be or not to be" is represented as $(2,0,1,1,0,0,0,2)^{\top}$. To capture some of the context of the words, we can instead represent a text as the count of so-called $n$-grams 1 , which are sequences of $n$ consecutive words, e.g. the 2 -grams of "to be or not to be" are ["to be", "be or", "or not", "not to", "to be"], and we represent such a bag-of- $n$-grams as a vector in $\mathbb{R}^{\left(d^{n}\right)}$. To compare two texts we compute the distance between the vectors of those texts, because the distance between vectors of texts with mostly the same words (or $n$-grams) is small/ ${ }^{2}$ For a more realistic language such as English with $d \approx 171000$ words [SW89] or that of the "English" speaking internet at $d \gtrsim 4790000$ words [WZ05], the dimension quickly becomes infeasable. While we only need to store the nonzero counts of words (or $n$-grams) to represent a vector, many data processing algorithms have a dependency on the vector dimension $d$ (or $\left.d^{n}\right)$, e.g. using nearest-neighbour search to find similar novels to recommend [AI17] or using neural networks to classify our texts [Sch18]. These algorithms would be infeasible for our library use case if we do not first reduce the dimension of our data.

---

A seemingly simple approach would be to select a subset of the coordinates, say if the data contained redundant or irrelevant coordinates. This is known as feature selection [JS08; HTF17; Jam+13a], and can be seen as projecting ${ }^{3}$ onto an axis aligned subspace, i.e. a subspace whose basis is a subset of $\left\{e_{1}, \ldots, e_{d}\right\}$.

---

We can build upon feature selection by choosing the basis from a richer set of vectors. For instance, in principal component analysis as dimensionality reduction (PCA) [Pea01: Hot33] we let the basis of the subspace be the $m$ first eigenvectors (ordered decreasingly by eigenvalue) of $X^{\top} X$, where the rows of $X \in \mathbb{R}^{n \times d}$ are our $n$ high dimensional vectors $4^{4}$. This subspace maximises the variance of the data in the sense that the first eigenvector is the axis that maximises variance and subsequent eigenvectors are the axes that maximise variance subject to being orthogonal to all previous eigenvectors [LRU20b; HTF17: Jam+13b].

But what happens if we choose a basis randomly?

### 1.2 The Johnson-Lindenstrauss Lemma(s)

In 1984 ${ }^{5}$ it was discovered that projecting onto a random basis approximately preserves pairwise distances with high probability. In order to prove a theorem regarding Lipschitz extensions of functions from metric spaces into $\ell_{2}$, Johnson and Lindenstrauss [JL84] proved the following lemma.

---

Lemma 1.1 (Johnson-Lindenstrauss lemma [JL84]). For every $d \in \mathbb{N}_{1}, \varepsilon \in(0,1)$, and $X \subset \mathbb{R}^{d}$, there exists a function $f: X \rightarrow \mathbb{R}^{m}$, where $m=\Theta\left(\varepsilon^{-2} \log |X|\right)$ such that for every $x, y \in X$,

$$
\begin{equation*}
\left|\|f(x)-f(y)\|_{2}^{2}-\|x-y\|_{2}^{2}\right| \leq \varepsilon\|x-y\|_{2}^{2} . \tag{1}
\end{equation*}
$$

Proof. The gist of the proof is to first define $f(x):=(d / m)^{1 / 2} A x$, where $A \in \mathbb{R}^{m \times d}$ are the first $m$ rows of a random orthogonal matrix. They then showed that $f$ preserves the norm of any vector with high probability, or more formally that the distribution of $f$ satisfies the following lemma.

---

Lemma 1.2 (Distributional Johnson-Lindenstrauss lemma [JL84]). For every $d \in \mathbb{N}_{1}$ and $\varepsilon, \delta \in$ $(0,1)$, there exists a probability distribution $\mathcal{F}$ over linear functions $f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}$, where $m=$ $\Theta\left(\varepsilon^{-2} \log \frac{1}{\delta}\right)$ such that for every $x \in \mathbb{R}^{d}$,

$$
\begin{equation*}
\operatorname{Pr}_{f \sim \mathcal{F}}\left[\left|\|f(x)\|_{2}^{2}-\|x\|_{2}^{2}\right| \leq \varepsilon\|x\|_{2}^{2}\right] \geq 1-\delta . \tag{2}
\end{equation*}
$$

By choosing $\delta=1 /|X|^{2}$, we can union bound over all pairs of vectors $x, y \in X$ and show that their distance (i.e. the $\ell_{2}$ norm of the vector $x-y$ ) is preserved simultaneously for all pairs with probability at least $1-\binom{|X|}{2} /|X|^{2}>1 / 2$.

---

We will use the term Johnson-Lindenstrauss distribution (JLD) to refer to a distribution $\mathcal{F}$ that is a witness to lemma 1.2 , and the term Johnson-Lindenstrauss transform (JLT) to a function $f$ witnessing lemma 1.1, e.g. a sample of a JLD.

---

A few things to note about these lemmas are that when sampling a JLT from a JLD it is independent of the input vectors themselves; the JLT is only dependendent on the source dimension $d$, number of vectors $|X|$, and distortion $\varepsilon$. This allows us to sample a JLT without having access to the input data, e.g. to compute the JLT before the data exists, or to compute the JLT in settings where the data is too large to store on or move to a single machinef Secondly, the target dimension $m$ is independent from the source dimension $d$, meaning there are potentially very significant savings in terms of dimensionality, which will become more apparent shortly.

---

Compared to PCA, the guarantees that JLTs give are different: PCA finds an embedding with optimal average distortion of distances between the original and the embedded vectors, i.e. $A_{\text {PCA }}=\arg \min _{A \in \mathbb{R}^{m \times d}} \sum_{x \in X}\left\|A^{\top} A x-x\right\|_{2}^{2}$ [Jol02], whereas a JLT bounds the worst case distortion between the distances within the original space and distances within the embedded space. As for computing the transformations, a common ${ }^{7}$ way of performing PCA is done by computing the covariance matrix and then performing eigenvalue decomposition, which results in a running time ${ }^{8}$ of $\mathcal{O}\left(|X| d^{2}+d^{\omega}\right)$ DDH07], compared to $\Theta(|X| d \log d)$ and $9 \Theta\left(\|X\|_{0} \varepsilon^{-1} \log |X|\right)$ that can be achieved by the JLDs "FJLT" and "Block SparseJL", respectively, which will be introduced in section 1.4 As such, PCA and JLDs are different tools appropriate for different scenarios (see e.g. [Bre+19] where the two techniques are compared empirically in the domain of medicinal imaging; see also [Das00; BM01; FB03; FM03; Tan+05; DB06; Arp+14; Woj+16; Bre+20]). That is not to say the two are mutually exclusive, as one could apply JL to quickly shave off some dimensions followed by PCA to more carefully reduce the remaining dimensions [e.g. RST09; HMT11; Xie+16; Yan+20]. For more on PCA, we refer the interested reader to [Jol02], which provides an excellent in depth treatment of the topic.

---

One natural question to ask with respect to JLDs and JLTs is if the target dimension is optimal. This is indeed the case as Kane, Meka, and Nelson [KMN11] and Jayram and Woodruff [JW13] independently give a matching lower bound of $m=\Omega\left(\varepsilon^{-2} \log \frac{1}{\delta}\right)$ for any JLD that satisfies lemma 1.2, and Larsen and Nelson [LN17] showed that the bound in lemma 1.1]is optimal up constant factors for almost the entire range of $\varepsilon$ with the following theorem.

---

Theorem 1.3 ([LN17]). For any integers $n, d \geq 2$ and $\lg ^{0.5001} n / \sqrt{\min \{d, n\}}<\varepsilon<1$ there exists a set of points $X \subset \mathbb{R}^{d}$ of size $n$ such that any function $f: X \rightarrow \mathbb{R}^{m}$ satisfying eq. (1) must have

$$
\begin{equation*}
m=\Omega\left(\varepsilon^{-2} \log \left(\varepsilon^{2} n\right)\right) . \tag{3}
\end{equation*}
$$

Note that if $\varepsilon \leq \sqrt{\lg n / \min \{d, n\}}$ then $\varepsilon^{-2} \lg n \geq \min \{d, n\}$, and embedding $X$ into dimension $\min \{d,|X|\}$ can be done isometrically by the identity function or by projecting onto $\operatorname{span}(X)$, respectively.

---

Alon and Klartag [AK17] extended the result in [LN17] by providing a lower bound for the gap in the range of $\varepsilon$.

Theorem 1.4 ([|AK17]). There exists an absolute positive constant $0<c<1$ so that for any $n \geq d>c d \geq m$ and for all $\varepsilon \geq 2 / \sqrt{n}$, there exists a set of points $X \subset \mathbb{R}^{d}$ of size $n$ such that any function $f: X \rightarrow \mathbb{R}^{m}$ satisfying eq. (1) must have

$$
\begin{equation*}
m=\Omega\left(\varepsilon^{-2} \log \left(2+\varepsilon^{2} n\right)\right) \tag{4}
\end{equation*}
$$

---


It is, however, possible to circumvent these lower bounds by restricting the set of input vectors we apply the JLTs to. For instance, Klartag and Mendelson [KM05], Dirksen [Dir16], and Bourgain, Dirksen, and Nelson [BDN15b] provide target dimension upper bounds for JLTs that are dependent on statistical properties of the input set X. Similarly, JLTs can be used to approximately preserve pairwise distances simultaneously for an entire subspace using $m=\Theta\left(\varepsilon^{-2} t \log (t / \varepsilon)\right)$, where $t$ denotes the dimension of the subspace [Sar06], which is a great improvement when $t \ll|X|, d$.

---

Another useful property of JLTs is that they approximately preserve dot products. Corollary 1.5 formalises this property in terms of lemma 1.1] though it is sometimes [Sar06; AV06] stated in terms of lemma 1.2 Corollary 1.5 has a few extra requirements on $f$ and $X$ compared to lemma 1.1, but these are not an issue if the JLT is sampled from a JLD, or if we add the negations of all our vectors to $X$, which only slightly increases the target dimension.

---

Corollary 1.5. Let $d, \varepsilon, X$ and $f$ be as defined in lemma 1.1 and furthermore let $f$ be linear. Then for every $x, y \in X$, if $-y \in X$ then

$$
\begin{equation*}
|\langle f(x), f(y)\rangle-\langle x, y\rangle| \leq \varepsilon\|x\|_{2}\|y\|_{2} . \tag{5}
\end{equation*}
$$

Proof. If at least one of $x$ and $y$ is the 0 -vector, then eq. (5) is trivially satisfied as $f$ is linear. If $x$ and $y$ are both unit vectors then we assume w.l.o.g. that $\|x+y\|_{2} \geq\|x-y\|_{2}$ and we proceed as follows, utilising the polarisation identity: $4\langle u, v\rangle=\|u+v\|_{2}^{2}-\|u-v\|_{2}^{2}$.

$$
\begin{aligned}
4|\langle f(x), f(y)\rangle-\langle x, y\rangle| & =\left|\|f(x)+f(y)\|_{2}^{2}-\|f(x)-f(y)\|_{2}^{2}-4\langle x, y\rangle\right| \\
& \leq\left|(1+\varepsilon)\|x+y\|_{2}^{2}-(1-\varepsilon)\|x-y\|_{2}^{2}-4\langle x, y\rangle\right| \\
& =\left|4\langle x, y\rangle+\varepsilon\left(\|x+y\|_{2}^{2}+\|x-y\|_{2}^{2}\right)-4\langle x, y\rangle\right| \\
& =\varepsilon\left(2\|x\|_{2}^{2}+2\|y\|_{2}^{2}\right) \\
& =4 \varepsilon .
\end{aligned}
$$

Otherwise we can reduce to the unit vector case.

$$
\begin{aligned}
|\langle f(x), f(y)\rangle-\langle x, y\rangle| & =\left|\left\langle f\left(\frac{x}{\|x\|_{2}}\right), f\left(\frac{y}{\|y\|_{2}}\right)\right\rangle-\left\langle\frac{x}{\|x\|_{2}}, \frac{y}{\|y\|_{2}}\right)\right|\|x\|_{2}\|y\|_{2} \\
& \leq \varepsilon\|x\|_{2}\|y\|_{2} .
\end{aligned}
$$

---

Before giving an overview of the development of JLDs in section 1.4. let us return to our scenario and example in section 1.1 and show the wide variety of fields where dimensionality reduction via JLTs have found use. Furthermore, to make us more familiar with lemma 1.1 and its related concepts, we will pick a few examples of how the lemma is used.