## SPLATE: Sparse Late Interaction Retrieval 

### Abstract

The late interaction paradigm introduced with ColBERT stands out in the neural Information Retrieval space, offering a compelling effectiveness-efficiency trade-off across many benchmarks. Efficient late interaction retrieval is based on an optimized multi-step strategy, where an approximate search first identifies a set of candidate documents to re-rank exactly. In this work, we introduce SPLATE, a simple and lightweight adaptation of the ColBERTv2 model which learns an "MLM adapter", mapping its frozen token embeddings to a sparse vocabulary space with a partially learned SPLADE module. This allows us to perform the candidate generation step in late interaction pipelines with traditional sparse retrieval techniques, making it particularly appealing for running ColBERT in CPU environments. Our SPLATE ColBERTv2 pipeline achieves the same effectiveness as the PLAID ColBERTv2 engine by re-ranking 50 documents that can be retrieved under 10 ms .

# 1 INTRODUCTION

In the landscape of neural retrieval models based on Pre-trained Language Models (PLMs), the late interaction paradigm - introduced with the ColBERT model [16] - delivers state-of-the-art results across many benchmarks. ColBERT - and its variants [11,12,21,25,33,38,45,48]$ - enjoys many good properties, ranging from interpretability [8, 46] to robustness [ $10,26,47,49$ ]. The fine-grained interaction mechanism, based on a token-level dense vector representation of documents and queries, alleviates the inherent limitation of single-vector models such as DPR [15]. Due to its MaxSim formulation, late interaction retrieval requires a dedicated multi-step search pipeline. In the meantime, Learned Sparse Retrieval [30] has emerged as a new paradigm to reconcile the traditional search infrastructure with PLMs. In particular, SPLADE models $[6,7,9]$ exhibit strong in-domain and zero-shot capabilities at a fraction of the cost of late interaction approaches - both in terms of memory footprint and search latency [18, 19, 34, 35].

---

In this work, we draw a parallel between these two lines of works, and show how we can simply "adapt" ColBERTv2 frozen representations with a light SPLADE module to effectively map queries and documents in a sparse vocabulary space. Based on this idea, we introduce SPLATE - for SParse LATE interaction - as an alternative approximate scoring method for late interaction pipelines. Contrary to optimized engines like PLAID [37], our method relies on traditional sparse techniques, making it particularly appealing to run ColBERT in mono-CPU environments.

# 2 RELATED WORKS

Efficient Late Interaction Retrieval. Late interaction retrieval is a powerful paradigm, that requires complex engineering to scale up efficiently. Specifically, it resorts to a multi-step pipeline, where an initial set of candidate documents is retrieved based on approximate scores [16]. While it is akin to the traditional retrieve-and-rank pipeline in IR, it still fundamentally differs in that the same (PLM) model is used for both steps ${ }^{1}$. Late interaction models offer advantages over cross-encoders because they allow for pre-computation of document representations offline, thus improving efficiency in theory. However, this comes at the cost of storing large indexes of dense term representations. Various optimizations of the ColBERT engine have thus been introduced [5, 12, 20, 23, 27, 29, 33, 37, 38, 41, 43]. ColBERTv2 [38] refines the original ColBERT by introducing residual compression to reduce the space footprint of late interaction approaches. Yet, search speed remains a bottleneck, mostly due to the large number of candidates to re-rank exactly (> 10k) [27]. Santhanam et al. identify the major bottlenecks - in terms of search speed - of the vanilla ColBERTv2 pipeline, and introduce PLAID [37], a new optimized late interaction pipeline that can largely reduce the number of candidate passages without impacting ColBERTv2's effectiveness. In particular, PLAID candidate generation is based on three steps that leverage centroid interaction and centroid pruning - emulating traditional Bag-of-Words (BoW) retrieval - as well as dedicated CUDA kernels. It reduces the large number of candidate documents to re-rank, greatly offloading subsequent steps (index lookup, decompression, and scoring).

---

Hybrid Models. Several works have identified similarities between the representations learned by different neural ranking models. For instance, UNIFIER [40] jointly learns dense and sparse single-vector bi-encoders by sharing intermediate transformer layers. Similarly, the BGE-M3 embedding model [3] can perform dense, multi-vector, and sparse retrieval indifferently. SparseEmbed [17] extends SPLADE with dense contextual embeddings - borrowing ideas from ColBERT and COIL [11]. SLIM [22] adapts ColBERT to perform late interaction on top of SPLADE-like representations making it fully compatible with traditional search techniques. Ram et al. [36] show that mapping representations of a dense bi-encoder to the vocabulary space - via the Masked Language Modeling (MLM) head - can also be used for interpretation purposes.

# 3 METHOD

SPLATE is motivated by two core ideas: (1) PLAID [37] draws inspiration from traditional BoW retrieval to optimize the late interaction pipeline; (2) dense embeddings can seemingly be mapped to the vocabulary space [36]. Rather than proposing a new standalone model, we show how SPLATE can be used to approximate the candidate generation step in late interaction retrieval, by bridging the gap between sparse and dense models.

---

Adapting Representations. SPLATE builds on the similarities between the representations learned by sparse and dense IR models. For instance, Ram et al. [36] show that mapping representations of a dense bi-encoder with the MLM head can produce meaningful BoW. We take one step further and hypothesize that effective sparse models can be derived - or at least adapted - from frozen embeddings of dense IR models in a SPLADE-like fashion. We, therefore, propose to "branch" an MLM head on top of a frozen ColBERT model.

---

SPLATE. Given ColBERT's contextual embeddings $\left(h_{i}\right)_{i \in t}$ of an input query or document $t$, we can define a simple "adapted" MLM head, by linearly mapping transformed representations back to the vocabulary. Inspired by Adapter modules [14, 32], SPLATE thus simply adapts frozen representations $\left(h_{i}\right)_{i \in t}$ by learning a simple two-layer MLP, whose output is recombined in a residual fashion before "MLM" vocabulary projection:

$$
\begin{equation*}
w_{i v}=\left(h_{i}+M L P_{\theta}\left(h_{i}\right)\right)^{T} E_{v}+b_{v} \tag{1}
\end{equation*}
$$

where $w_{i}$ corresponds to an unnormalized log-probability distribution over the vocabulary $\mathcal{V}$ for the token $t_{i}, E_{v}$ is the (Col)BERT input embedding for the token $v$ and $b_{v}$ is a token-level bias. The residual guarantees a near-identity initialization - making training stable [14]. We can then derive sparse SPLADE vectors as follows:

$$
\begin{equation*}
w_{v}=\max _{i \in t} \log \left(1+\operatorname{ReLU}\left(w_{i v}\right)\right), \quad v \in\{1, \ldots,|\mathcal{V}|\} \tag{2}
\end{equation*}
$$

---

We then train the parameters of the MLM head $(\boldsymbol{\theta}, \boldsymbol{b})$ with distillation based on the derived SPLADE vectors to reproduce ColBERT's scores - see Section 4. Our approach is very light, as the ColBERT backbone model is entirely frozen - including the (tied) projection layer $E$. In our default setting, the MLP first downprojects representations by a factor of two, then up-projects back to the original dimension. This corresponds to a latent dimension of $768 / 2=384-$ early experiments indicate that the choice of this hyperparameter is not critical - and amounts to roughly 0.6 M trainable parameters only (yellow blocks in Figure 1, (Left)).

- Figure 1: (Left) SPLATE relies on the same representations $\left(h_{i}\right)_{i \in t}$ to learn sparse BoW with SPLADE (candidate generation) and to compute late interactions (re-ranking). (Right) Inference: SPLATE ColBERTv2 maps the representations of the query tokens to a sparse vector, which is used to retrieve $k$ documents from a pre-computed sparse index ( R setting). In the e2e setting, representations are gathered from the ColBERT index to re-rank the candidates exactly with MaxSim.

---

Efficient Candidate Generation for Late Interaction. By adapting ColBERT's frozen dense representations with a SPLADE module, SPLATE aims to approximate late interaction scoring with an efficient sparse dot product. Thus, the same representations $\left(h_{i}\right)_{i \in t}$ can function in both retrieval (SPLATE module) and re-ranking (ColBERT's MaxSim) scenarios - requiring a single transformer inference step on query and document sides. Thus, it becomes possible to replace the existing candidate generation step in late retrieval pipelines such as PLAID with traditional sparse retrieval to efficiently provide ColBERT with documents to re-rank. SPLATE is therefore not a model per se, but rather offers an alternative implementation to late-stage pipelines by bridging the gap between sparse and dense models. SPLATE however differs from PLAID in various aspects:

- While PLAID implicitly derives sparse BoW representations from ColBERTv2's centroid mapping, SPLATE explicitly learns such representations by adapting a pseudo-MLM head to ColBERT frozen representations. The approximate step becomes supervised rather than (yet efficiently) "engineered".

- The candidate generation can benefit from the long-standing efficiency of inverted indexes and query processing techniques such as MaxScore [44] or WAND [2], making end-toend ColBERT more "CPU-friendly" - see Table 1.

- It is more controllable and directly amenable to all sorts of recent optimizations for learned sparse models [18, 19].

- ColBERT's pipeline becomes even more interpretable, as SPLATE's candidate generation simply operates in the vocabulary space - rather than representing documents as a lightweight bag of centroids - see Table 3 for examples.

---

Nonetheless, SPLATE requires an additional - although light training round for the parameters of the Adapter module. It also requires indexing SPLATE's sparse document vectors, therefore adding a small memory footprint overhead ${ }^{2}$. Also, note that hybrid approaches like BGE-M3 [3] - that can output sparse and multivector representations - could in theory be used in late interaction pipelines. However, SPLATE is directly optimized to approximate ColBERTv2, and we leave for future work the study of jointly training the candidate generation and re-ranking modules.

# 4 EXPERIMENTS

Setting. We initialize SPLATE with ColBERTv2 [38] weights which are kept frozen. We rely on top $-k_{q, d}$ pooling to obtain respectively query and document BoW SPLADE representations ${ }^{3}$. We train the MLM parameters $(\boldsymbol{\theta}, \boldsymbol{b})$ on the MS MARCO passage dataset [1], using both distillation and hard negative sampling. More specifically, we distill ColBERTv2's scores based on a weighted combination of marginMSE [13] and KLDiv [24] losses for 3 epochs. We set the batch size to 24 , and select 20 hard negatives per query coming from ColBERTv2's top-1000. By using ColBERTv2 as both the teacher and the source of hard negatives, SPLATE aims to approximate late interaction with sparse retrieval. SPLATE models are trained with the SPLADE codebase on 2 Tesla V100 GPUs with 32GB memory in less than two hours ${ }^{4}$. SPLATE can be evaluated as a standalone sparse retriever ( R ), but more interestingly in an end-to-end late interaction pipeline (e2e) where it provides ColBERTv2 with candidates to re-rank (see Figure 1, (Right) $)^{5}$. For the former, we rely on the PISA engine [28] to conduct sparse retrieval with block-max WAND and provide latency measurements as the Mean Response Time (MRT), i.e., the average search latency measured on the MS MARCO dataset using one core of an $\operatorname{Intel}(\mathrm{R}) \mathrm{Xeon}(\mathrm{R})$ Gold 6338 CPU @ 2.00 GHz CPU. For the latter, we perform on-the-fly re-ranking with the ColBERT library ${ }^{6}$. Note that naive re-ranking with ColBERT is sub-optimal - compared to pipelines that precompute document term embeddings. We leave the end-to-end latency measurements for future work - but we believe the integration of SPLATE into ColBERT's pipelines such as PLAID should be seamless, as it would only require modifying the candidate generation step. We evaluate models on the MS MARCO dev set and the TREC DL19 queries [4] (in-domain), and provide out-of-domain evaluations on the 13 readily available BEIR datasets [42], as well as the test pooled Search dataset of the LoTTE benchmark [38].

---

The following experiments investigate three different Research Questions: (1) How does the sparsity of SPLATE vectors affect latency and re-ranking performance? (2) How accurate SPLATE candidate generation is compared to ColBERTv2? (3) How does it perform overall for in-domain and out-of-domain scenarios?

---

Latency Results. Table 1 reports in-domain results on MS MARCO, in both retrieval-only (R) and end-to-end (e2e) settings. Overall, the results show that it is possible to "convert" a frozen ColBERTv2 model to an effective SPLADE, with a lightweight residual adaptation of its token embeddings. We consider several SPLATE models trained with varying pooling sizes $\left(k_{q}, k_{d}\right)$ - those parameters controlling the size of the query and document representations. We observe the standard effectiveness-efficiency trade-off for SPLADE, where pooling affects both the performance and average latency. These results indicate that one can easily control the latency of the candidate generation step by selecting appropriate pooling sizes. However, after re-ranking with ColBERTv2, all the models perform comparably, which is interesting from an efficiency perspective, as it becomes possible to use very lightweight models to cheaply provide candidates (e.g., as low as 2.9 ms Mean Response Time), while achieving performance on par with the original ColBERTv2 (see Table 2). For comparison, the end-to-end latency reported in PLAID [37] (single CPU core, less conservative setting with $k=10$ ) is around 186 ms on MS MARCO. Given that candidate generation accounts for around two-thirds of the complete pipeline [37], SPLATE thus offers an interesting alternative for running ColBERT on mono-CPU environments.

- Table 2: Evaluation of SPLATE with $\left(k_{q}, k_{d}\right)=(10,100)$ and $k=50$. ${ }^{\text {abcde }}$ denote significant improvements over the corresponding rows, for a paired $t$-test with $p$-value $=0.01$ and Bonferroni correction (MS MARCO dev set and DL19). PLAID ColBERTv2 [37] ( $k=1000$ ) reports the $\operatorname{dev}$ LoTTE* S@5.

---

Approximation Quality. To assess the quality of SPLATE approximation, we compare the top- $k$ passages retrieved by PLAID ColBERTv2 to the ones retrieved by SPLATE (R). We report in Figure 2 the average fraction $R(k)$ of documents in SPLATE's top- $k^{\prime}$ that also appear in the top- $k$ documents retrieved by ColBERTv2 on MS MARCO, for $k \in\{10,100\}$ and $k^{\prime}=i \times k, i \in\{1, \ldots, 5\}$. When $k=10$, SPLATE can retrieve more than $90 \%$ of ColBERTv2's documents in its top-50 $(i=5)$, for all levels of $\left(k_{q}, k_{d}\right)$. This explains the ability of SPLATE to fully recover ColBERT's performance by re-ranking a handful of documents (e.g., 50 only). We additionally observe that the quality of approximation falls short for efficient models (i.e., lower $\left.\left(k_{q}, k_{d}\right)\right)$ when $k$ is higher.

---

Figure 3 further reports the performance of SPLATE (e2e) on out-of-domain. We observe similar trends, where increasing both the number $k$ of documents to re-rank and $\left(k_{q}, k_{d}\right)$ leads to better generalization. Overall, re-ranking only 50 documents provides a good trade-off across all settings - echoing previous findings [27, 37]. Yet, the most efficient scenario $\left(\left(k_{q}, k_{d}\right)=(5,50), k=10\right)$ still leads to impressive results: 38.4 MRR@10 on MS MARCO dev (not shown), $70.0 S @ 5$ on LoTTE (purple line on Figure 3).

---

Overall Results. Finally, Table 2 compares SPLATE ColBERTv2 with the reference points ColBERTv2 [38] and PLAID ColBERTv2 $(k=1000)$ [37] - in both R and e2e settings. We also include results from SPLADE++ [7], as well as the hybrid methods SparseEmbed [17] and SLIM ++ [22] - even though they are not entirely comparable to SPLATE. While SparseEmbed and SLIM introduce new models, SPLATE rather proposes an alternative implementation to ColBERT's late retrieval pipeline. We further report the two baselines consisting of retrieving documents with BM25 (resp. SPLADE ++ ) and re-ranking those with ColBERTv2 (BM25 $\gg \mathrm{C}$ and $\mathrm{S} \gg \mathrm{C}$ respectively, with $k=50$ ). Note that we expect SPLATE to perform in between, as BM25 $\gg$ C relies on a less effective retriever, while $\mathrm{S} \gg \mathrm{C}$ fundamentally differs from SPLATE, as it is based on two different models. Specifically, it requires feeding the query to a PLM twice at inference time. Overall, SPLATE (R) is effective as a standalone retriever (e.g., reaching almost 37 MRR@10 on MS MARCO dev). On the other hand, SPLATE (e2e) performs comparably to ColBERTv2 and PLAID on MS MARCO, BEIR, and LoTTE. Additionally, we conducted a meta-analysis against PLAID with RANGER [39] over the 13 BEIR datasets, and found no statistical differences on 10 datasets, and statistical improvement (resp. loss) on one (resp. two) dataset(s). Finally, we provide in Table 3 some examples of predicted BoW for queries in MS MARCO dev - highlighting the interpretable nature of the retrieval step in SPLATE-based ColBERT's pipeline.

# 5 CONCLUSION

We propose SPLATE, a new lightweight candidate generation technique simplifying ColBERTv2's candidate generation for late interaction retrieval. SPLATE adapts ColBERTv2's frozen embeddings to conduct efficient sparse retrieval with SPLADE. When evaluated end-to-end, the SPLATE implementation of ColBERTv2 performs comparably to ColBERTv2 and PLAID on several benchmarks, by re-ranking a handful of documents. The sparse term-based nature of the candidate generation step makes it particularly appealing in mono-CPU environments efficiency-wise. Beyond optimizing late interaction retrieval, our work opens the path to a deeper study of the link between the representations trained from different architectures.

