# 0. Abstract  

密集检索技术利用预训练的大型语言模型来构建查询和文档的高维表示。这些表示通过高效的相似性度量来计算文档与查询的相关性。在这方面，多向量表示通过在每个词汇级别上对查询和文档进行编码，尽管提高了效果，但也带来了内存占用和查询延迟的十倍增加。最近，PLAID通过引入基于质心的术语表示，解决了这些问题，从而减少了多向量系统的内存影响。通过利用质心交互机制，PLAID能够过滤掉不相关的文档，从而减少后续排序阶段的成本。本文提出了“基于位向量的高效多向量密集检索”（EMVB），这是一个用于多向量密集检索中高效查询处理的新框架。首先，EMVB通过优化的位向量进行高效的文档预过滤。其次，质心交互计算以列为单位进行，利用SIMD指令，从而减少了延迟。第三，EMVB利用产品量化（PQ）来减少存储向量表示的内存占用，同时允许快速的后期交互。第四，我们引入了一种每文档术语过滤方法，进一步提高了最后一步的效率。在MS MARCO和LoTTE上的实验表明，与PLAID相比，EMVB在不损失检索准确度的情况下，速度提高了最多2.8倍，内存占用减少了1.8倍。 

# 1. Introduction  

预训练的大型语言模型（LLM）的引入显著提高了信息检索系统的效果 [13,8,26,2]，这得益于LLM在建模语义和上下文方面的知名能力 [12,1,3]。在密集检索中，LLM被成功地用于学习文档和查询的高维密集表示。这些学习到的表示通过快速相似度操作（即内积或L2距离）来回答用户查询。在此方面，多向量技术 [14,20] 利用LLM为文档中的每个词汇构建密集表示。这些方法相比于单向量技术 [24,27] 或稀疏检索技术 [5] 提供了更好的效果。在这个背景下，查询与文档之间的相似度通过后期交互机制 [14,20] 进行度量，该机制通过计算查询的每个词汇与候选文档的每个词汇之间的最大相似度之和来工作。多向量检索系统的效果提升是以其增加的计算负担为代价的。首先，为每个词汇生成一个向量导致嵌入的数量比单向量表示大几个数量级。此外，由于嵌入数量庞大，识别候选文档变得非常耗时。此外，后期交互步骤需要计算查询与所有候选嵌入之间的最大相似度，这也是一个耗时的过程。

---

早期的多向量检索系统，例如ColBERT [14]，利用倒排索引来存储嵌入并检索候选文档。然后，检索到文档的表示并用来计算与查询的最大相似度得分。尽管这种方法相当高效，但它需要在内存中保持每个文档术语的全精度表示。在MS MARCO [17]，一个广泛采用的文档检索基准数据集上，ColBERT使用的整个嵌入集合需要超过140 GB的存储空间 [14]。ColBERTv2 [20] 引入了一种基于质心的压缩技术来高效地存储文档嵌入。每个嵌入通过保存最接近质心的ID并用1或2位/分量压缩残差（即逐元素差异）来存储。与ColBERT相比，ColBERTv2节省了最多10倍的空间，但在现代CPU上显著低效，执行查询处理时需要最多3秒钟 [19]。查询处理时间的减少是通过Santhanam等人提出的PLAID [19]实现的。PLAID利用了ColBERTv2的嵌入压缩器，并使用基于质心的表示来丢弃不相关的文档（质心交互 [19]），因此仅在精心挑选的文档批次上执行后期交互。与ColBERTv2相比，PLAID允许大幅加速，但其平均查询延迟在单线程执行的CPU上可达到400毫秒 [19]。

---

本文提出了EMVB，一个用于高效查询处理的多向量密集检索新框架。首先，我们识别了PLAID中最耗时的步骤。这些步骤包括：i) 在候选文档选择过程中提取每个查询词的前n个最接近质心，ii) 计算质心交互机制，iii) 解压量化后的残差。我们的方法通过引入基于优化位向量的高效文档过滤方法来解决前两个步骤。我们的过滤器识别出一小组关键的质心得分，从而降低了前n个质心提取的成本。同时，它减少了需要计算质心交互的文档数量。此外，我们引入了一种高效的列式降维方法，利用SIMD指令来加速这一步骤。最后，我们通过引入产品量化（PQ）[9]来提高后期交互的效率。与PLAID的位压缩器相比，PQ在性能上达到相同或更优的表现，同时速度提高了最多3倍。最后，为了进一步提高管道最后一步的效率，我们引入了一种动态文档-术语选择标准，从而将该步骤的成本降低最多30%。

------

我们在两个数据集上对EMVB进行实验评估，分别是MS MARCO passage [17]（用于领域内评估）和LoTTE [20]（用于领域外评估）。在MS MARCO上的结果表明，EMVB比PLAID快最多2.8倍，同时将内存占用减少了1.8倍，且检索精度没有损失。在领域外评估中，EMVB相较于PLAID提供了最多2.9倍的加速，且检索质量的损失最小。

------

本文其余部分的组织结构如下：第2节讨论相关工作；第3节描述PLAID [19]，多向量密集检索的当前最先进技术；第4节介绍EMVB；第5节对EMVB与PLAID进行实验评估；最后，第6节总结本文工作。

# 2. Related Work  

密集检索编码器大致可以分为单向量和多向量技术。单向量编码器允许将整个文档编码为一个单一的密集向量 [11]。在这方面，ANCE [25] 和 STAR/ADORE [26] 采用了硬负样本，通过教会检索器区分在词汇上相似的正负文档，从而改善了密集检索模型的训练。多向量编码器通过ColBERT引入。ColBERT的局限性以及克服这些局限性所做的努力（ColBERTv2、PLAID）在第1节中讨论过。COIL [6] 通过将词汇之间的匹配限制为查询和文档之间的词汇匹配，重新发现了经典检索系统（如BM25）的经验教训。CITADEL [16] 是一种最近提出的方法，它通过使用动态词汇路由引入了条件词元交互。条件词元交互意味着，特定文档的查询相关性仅通过查看部分词元来估算。这些词元通过所谓的词汇路由选择，排名架构的一个模块被训练来确定哪些词汇（即词汇表中的单词）被查询/文档激活。CITADEL显著减少了GPU上的执行时间，但在相同的检索质量下，比PLAID在CPU上慢了2倍。多向量密集检索还在ColBERT-PRF [23] 和 CWPRF [22] 中与伪相关反馈结合使用，显示出这种结合能够提高模型的效果。

------

**我们的贡献：** 本文通过引入EMVB，一个新颖的框架，显著提升了PLAID管道的检索性能，从而推动了多向量密集检索技术的进步。根据我们所知，这是文献中首次提出基于优化位向量的高效文档过滤方法、基于列式SIMD降维的候选文档检索方法，以及结合产品量化和每文档术语过滤的后期交互机制。

# 3. Multi-vector Dense Retrieval  

考虑一个包含 $n_P$ 篇段落的语料库 $\mathcal{P}$。在多向量密集检索场景中，LLM 将 $\mathcal{P}$ 中的每个 token 编码成一个 $d$ 维的密集向量 $T_j$。对于每个段落 $P$，生成一个密集表示 $P = \left\{ T_j \right\}$，其中 $j = 0, \ldots, n_t$，$n_t$ 是段落 $P$ 中 token 的数量。采用基于 token 的密集表示可以提高检索系统的效果 [14,20,19]。然而，这种方法会生成大量的 $d$ 维向量，导致在实际搜索场景中面临显著的空间（内存需求）和时间（查询处理器的延迟）挑战。

------

为了解决内存需求问题，ColBERTv2 [20] 和随后出现的 PLAID [19] 利用了基于质心的向量压缩技术。首先，使用 K-means 算法在 $d$ 维空间中进行聚类，识别出 $k$ 个质心 $\mathcal{C} = \left\{ C_i \right\}_{i=1}^{n_c}$。然后，对于每个向量 $x$，计算其与最接近的质心 $\bar{C}$ 之间的残差 $r = x - \bar{C}$。残差 $r$ 被压缩为 $\tilde{r}$，采用 $b$ 位编码器表示残差的每一维度，其中 $b \in {1,2}$。存储一个 $d$ 维向量的内存开销为：质心索引需要 $\left\lceil \log_2 |C| \right\rceil$ 位，压缩残差需要 $d \times b$ 位。该方法需要一个时间开销较大的解压缩阶段，用于根据质心 ID 和残差编码恢复近似的全精度向量表示。

---

因此，PLAID 旨在尽可能少地解压候选文档。这通过引入基于质心近似嵌入表示的高质量过滤步骤来实现，称为质心交互 [19]。具体来说，PLAID 检索引擎由四个不同的阶段组成 [19]。第一个阶段是候选段落的检索。为每个质心构建一个候选段落列表。如果一个段落中的一个或多个 token 的最近质心是 $C_i$，则该段落属于质心 $C_i$ 的候选列表。对于每个查询词 $q_i$，其中 $i=1, \ldots, n_q$，根据点积相似度度量，计算出与查询词最接近的 top-nprobe 个质心。与这些 top-nprobe 个质心相关的独特文档集会进入第二个阶段，这个阶段作为过滤阶段。在这个阶段，token 嵌入 $T_j$（其中 $j=1, \ldots, n_p$）通过其最近的质心 $\bar{C}^{T_j}$ 进行近似。因此，$T_j$ 与第 $i$ 个查询词 $q_i$ 的距离可以通过以下公式近似计算：

$q_i \cdot T_j \simeq q_i \cdot \bar{C}^{T_j} = \tilde{T}_{i, j}$。（公式1）

考虑一个由 $n_p$ 个 token 组成的候选段落 $P$。$P$ 的近似得分是通过计算所有查询词 $q_i$ 和属于该段落的每个 token 的最近质心 $\bar{C}^{T_j}$ 之间的点积，即：

$\bar{S}*{q, P} = \sum*{i=1}^{n_q} \max_{j=1 \ldots n_t} q_i \cdot \bar{C}^{T_j}$。（公式2）

------

第三个阶段，称为解压阶段，旨在通过组合质心和残差来重构 $P$ 的全精度表示。这是在过滤阶段选择的 top-ndocs 段落上进行的 [19]。在第四个阶段，PLAID 根据延迟交互机制（公式 3）使用解压后的全精度表示重新计算每个段落相对于查询 $q$ 的最终得分。然后根据相似度得分对段落进行排序，并选择 top-$k$ 个段落。

$S_{q, P} = \sum_{i=1}^{n_q} \max_{j=1 \ldots n_t} q_i \cdot T_j$。（公式3）

------

**PLAID 执行时间。** 我们提供了 PLAID 在不同阶段的执行时间分析，分别是检索、过滤、解压和延迟交互。这个实验是在第 5 节中详细描述的实验设置下进行的。我们报告了不同 $k$ 值（即检索段落的数量）下的执行时间。

# 4. EMVB

我们现在介绍 EMVB，这是我们提出的高效多向量密集检索框架。首先，EMVB 引入了一个高效的预过滤阶段，该阶段利用了优化的位向量。其次，我们通过引入基于 SIMD 指令的按列最大值归约方法，改进了质心交互步骤（公式 1）的效率。第三，EMVB 利用产品量化（PQ）来减少存储向量表示的内存占用，同时允许快速的延迟交互阶段。第四，PQ 被应用于与一种新颖的每段落词项过滤方法结合使用，从而进一步提高延迟交互的效率。接下来的子章节中，我们将详细阐述 EMVB 的这四个贡献。

## 4.1. Retrieval of Candidate Passages  

图 1 显示了 PLAID 所需计算中，检索阶段占用了大量的时间。我们进一步细分了这些步骤，以揭示其中最耗时的部分。检索包括：i) 计算传入查询与质心集合之间的距离，ii) 为每个查询词提取 top-nprobe 个最接近的质心。前者通过利用高效的矩阵乘法工具（例如 Intel MKL [18,21]）来高效执行。在后者步骤中，PLAID 使用 numpy 的 topk 函数来提取 top-nprobe 个质心，该函数实现了 quickselect 算法。对于每个 $n_q$ 个查询词，在 $|C|=2^{18}$ 个质心中选择 top-nprobe 的成本是第一步矩阵乘法的 $3 \times$。在 4.2 节中，我们展示了我们的预过滤方法通过减少评估元素的数量，天然地加速了 top-nprobe 的选择。实际上，我们展示了如何高效地筛选出得分低于某个阈值的质心，并仅对剩余的质心执行 quickselect。因此，在 EMVB 中，top-nprobe 提取的成本变得可以忽略不计，比在完整质心集合上提取 top-nprobe 快了两个数量级。

## 4.2. Efficient Pre-Filtering of Candidate P  

图 1 显示了候选过滤阶段可能非常耗时，尤其是当 $k$ 值较大时。在本节中，我们提出了一种基于质心新型位向量表示的预过滤方法，该方法能够高效地丢弃与查询不相关的段落。

------

给定一个段落 $P$，我们的预过滤方法包括判断 $\tilde{T}_{i, j}$，对于 $i=1, \ldots, n_q, j=1, \ldots, n_t$ 是否较大。回忆一下，$\tilde{T}_{i, j}$ 表示段落 $P$ 中第 $j$ 个 token 相对于查询词 $q_i$ 的近似得分，如公式 1 中定义的那样。这可以通过检查 $\bar{C}_j^T$——与 $T_j$ 关联的质心——是否属于查询词 $q_i$ 的最接近质心集合来获得。我们引入了 close ${ }_i^{t h}$，即与查询词 $q_i$ 相关的得分大于某个阈值 $t h$ 的质心集合。给定一个段落 $P$，我们定义了质心 ID 列表 $I_P$，其中 $I_P^j$ 是 $\bar{C}^{T_j}$ 的质心 ID。段落与查询的相似度可以通过我们新提出的过滤函数 $F(P, q) \in [0, n_q]$ 来估算，具体公式如下：

$F(P, q) = \sum_{i=1}^{n_q} \mathbf{1}\left(\exists j \text{ s.t. } I_P^j \in \text{ close }_i^{t h}\right)$。（公式4）

对于一个段落 $P$，该函数计算有多少查询词在段落 $P$ 中至少有一个相似的段落词，其中“相似”指的是 $T_j$ 属于 close ${ }_i^{t h}$。

---

在图 2（左）中，我们将基于质心交互机制的我们新型预过滤方法（橙色、蓝色、绿色线条）与在整个候选文档集合上使用质心交互机制（红色虚线）的性能进行了比较，数据集为 MS MARCO。图表显示，我们的预过滤方法能够高效地丢弃与查询不相关的段落，同时不会影响后续质心交互阶段的召回率。例如，我们可以通过将阈值设为 $th = 0.4$，将候选段落集合缩小至仅 1000 个元素，并且不会损失 R@100 的表现。在本节的剩余部分，我们将展示如何高效地实现这个预过滤方法。

---

**构建位向量。** 给定 $t h$，计算 close ${ }_i^{t h}$ 的问题在概念上是简单的。然而，考虑到现代 CPU 的特性，进行高效实现对于快速计算公式 4 至关重要。

------

设 $C S = q \cdot C^T$，其中 $C S \in [-1,1]^{n_q \times |C|}$ 是查询 $q$ 和质心集合 $C$ 之间的得分矩阵（两个矩阵均为 $L_2$ 归一化），其中 $n_q$ 是查询词的数量，$|C|$ 是质心的数量。在基于 $i f$ 的朴素解法中，我们扫描 $C S$ 的第 $i$ 行，选择那些满足 $C S_{i, j} > t h$ 的 $j$ 值。通过利用 SIMD 指令，可以加速这个方法。特别地，`_mm512_cmp_epi32_` 掩码指令允许一次比较 16 个 fp32 值，并将比较结果存储在掩码变量中。如果掩码 $==$ 0，则可以跳过接下来的 16 个值，因为当前所有的 $j$ 值的比较都失败了。否则，我们提取那些索引 $J=\left\{ j \in [0, 15] \mid \operatorname{mask}_j = 1 \right\}$。

---

这种基于 `if` 的算法效率主要依赖于分支预测错误率。现代 CPU 会在计算条件之前，根据算法执行流中的模式预测 `if` 语句的结果。当错误的分支被预测时，会发生控制冒险，流水线会被刷新，延迟大约是 $15-20$ 时钟周期，即约 10 纳秒。我们通过提出一种无分支算法来解决分支预测错误的低效问题。无分支算法使用一个指针 $p$ 指向一个预分配的缓冲区。在扫描 $C S_{i, j}$ 时，它将 $j$ 写入由 $p$ 指示的位置。然后，它将比较结果加到 $p$ 上：如果 $C S_{i, j} > t h$，则为 1，否则为 0。在下一次迭代中，如果比较结果为 0，则 $j+1$ 会覆盖 $j$；否则，$j$ 会写入到下一个内存位置，并保存到缓冲区中。无分支选择不包含任何 `if` 指令，因此执行流中没有任何分支。通过利用 SIMD 指令，可以更高效地实现无分支算法。特别地，上述提到的 `_mm512_cmp_epi32_` 掩码指令允许一次比较 16 个 fp32 值，而 `_mm512_mask_compressstore` 则允许通过单条指令提取 $J$。

------

图 2（右）展示了我们不同方法的比较，分别是“朴素 `IF`”，“向量化 `IF`”，“无分支”和上面描述的“VecBranchless”。无分支算法的执行时间是恒定的，与阈值的值无关，而基于 `if` 的方法随着 $t h$ 值的增加性能会更好。当 $t h \geq 0.3$ 时，“向量化 `IF`”是最有效的方法，与朴素方法相比，性能提升可达 $3 \times$。

---

**快速集合成员测试。** 一旦计算了 close ${ }_i^{t h}$，我们就需要高效地计算公式 4。在这里，给定 $I_P$ 作为一个整数列表，我们需要测试其至少一个成员 $I_P^j$ 是否属于 close ${ }_i^{t h}$，其中 $i=1, \ldots, n_q$。这可以通过使用位向量来高效地表示 close ${ }_i^{t h}$ 来完成。位向量将最多 $N$ 个整数映射到一个 $N$ 位的数组，其中第 $e$ 位被设置为 1 当且仅当整数 $e$ 属于该集合。添加和查找任何整数 $e$ 都可以通过位操作符在常数时间内完成。此外，位向量需要 $N$ 位来存储。在我们的案例中，由于 $|C|=2^{18}$，一个位向量只需要存储 $32 K$ 字节。

------

由于我们同时遍历所有的 $n_q$ 个位向量，我们可以进一步通过将位向量垂直堆叠来利用位向量表示（图 3）。这使得可以同时测试所有查询项对应的一个质心索引。对于不同查询项的相同质心的位是连续的，并且能够适配一个 32 位字。这样，我们可以通过单一的位操作在常数时间内同时测试所有查询项的成员资格。具体来说，我们的算法通过初始化一个 $n_q=32$ 位全为零的掩码 $m$（步骤 1，图 3）开始。然后，对于每个候选文档中的项，它执行掩码与表示所有查询项成员资格的 32 位字的按位异或（步骤 2，图 3）。因此，通过计算执行结束后 $m$ 中 1 的个数，可以得到公式 4，使用现代 CPU 特有的 popent 操作（步骤 3，图 3）。

------

图 4（上）展示了我们“向量化”集合成员测试的实现，相比于依赖于位向量的朴素用法的“基准”方法，提供了从 $10 \times$ 到 $16 \times$ 的加速。特别地，我们基于位向量的预过滤可以比 PLAID [19] 提出的质心交互快 $30 \times$，参见图 4（下）。

## 4.3. Fast Filtering of Candidate Passages  

我们的预筛选方法使我们能够高效地过滤掉不相关的文档，并且它被应用于PLAID的质心交互之前（公式2）。接下来，我们将展示如何提高质心交互本身的效率。

---

考虑一个文档 $P$ 及其相关的质心得分矩阵 $\tilde{P}=q_i \cdot \bar{C}^{T_j}$。显式构建这个矩阵可以在评分阶段重复使用它，从而替代代价较高的解压步骤（参见第4.4节）。为了构建 $\tilde{P}$，我们将 $C S$ 转置为 $C S^T$，其大小为 $|C| \times n_q$。$C S^T$ 的第 $i$ 行可以访问第 $i$ 个质心对应的所有 $n_q$ 查询词的得分。给定每个文档词的最近质心的ID（在第4.2节定义为 $I_P$），我们可以检索每个质心ID的得分。我们将 $\tilde{P}^T$ 构建为 $\tilde{P}$ 的转置——这样可以使 CPU 读取和写入连续的内存位置。与直接处理 $\tilde{P}$ 相比，这种方法可以提供超过 $2 \times$ 的加速。我们现在得到的是形状为 $n_t \times n_q$ 的 $\tilde{P}^T$。接下来，我们需要沿着列进行最大化归约（max-reduce），然后将得到的值相加，以实现公式2。这个过程通过对 $\tilde{P}^T$ 的每一行进行迭代并将它们打包到 AVX512 寄存器中来完成。假设 $n_q=32$，每个 AVX512 寄存器可以容纳 512/32 $=16$ 个浮点值，因此每一行需要 2 个寄存器。我们将第一行打包到 $max\_l$ 和 $max\_h$ 中。所有后续行都打包到 $current\_l$ 和 $current\_h$ 中。在每次迭代中，我们使用之前提到的 `_mm512_cmp_ps_mask` AVX512 指令将 $max\_l$ 与 $current\_l$ 和 $max\_h$ 与 $current\_h$ 进行比较。输出的掩码 $m$ 被用来通过 `_mm512_mask_blend_ps` 指令更新 $max\_l$ 和 $max\_h$。在 IceLake Xeon CPUs 上， `_mm512_cmp_ps_mask` 的吞吐量为 2，因此每一行的 $\tilde{P}$ 都可以在同一时钟周期内与 $max\_l$ 和 $max\_h$ 进行比较，且在两个不同的端口上并行处理。同样，`_mm512_mask_blend_ps` 指令也是如此，这意味着最大归约操作在 2 个时钟周期内完成（不考虑内存加载）。最后，将 $max\_l$ 和 $max\_h$ 相加，并使用函数 `_mm512_reduce_add_ps` 完成最终计算。

---

我们用 C++ 实现了 PLAID 的质心交互，并将其过滤时间与我们基于 SIMD 的解决方案进行了比较。比较结果在图 4（下）中报告。得益于高效的读写模式和极为高效的列向最大归约，我们的方法比 PLAID 提出的过滤方案快了最多 1.8 倍。

## 4.4. Late Interaction  

之前的方法[20, 19]提出的 $b$-位残差压缩器在晚期交互阶段之前需要一个代价高昂的解压步骤。图1显示，在PLAID中，解压向量的代价是晚期交互阶段的最多 5 倍。

---

我们提出通过使用产品量化（Product Quantization，PQ）[9]来压缩残差 $r$。PQ 允许在不解压的情况下计算输入查询向量 $q$ 和压缩残差 $r_{pq}$ 之间的点积。考虑一个查询 $q$ 和一个候选段落 $P$。我们将最大相似度操作符（方程式（3））的计算分解为

$S_{q, P}=\sum_{i=1}^{n_q} \max _{j=1 \ldots n_t}\left(q_i \cdot \bar{C}^{T_j}+q_i \cdot r^{T_j}\right) \simeq \sum_{i=1}^{n_q} \max _{j=1 \ldots n_t}\left(q_i \cdot \bar{C}^{T_j}+q_i \cdot r_{p q}^{T_j}\right)$ (5)

其中，$r^{T_j}=T_j-\bar{C}^{T_j}$。 一方面，这种分解方法允许利用预计算的 $\tilde{P}$ 矩阵；另一方面，得益于PQ，它可以在不解压的情况下计算查询和残差之间的点积。

---

我们将PLAID的残差压缩替换为PQ，特别是使用JMPQ [4]，该方法在语言模型微调过程中优化了产品量化的编码，以适应检索任务。我们测试了 $m = {16, 32}$，其中 $m$ 是用来划分向量的子空间数量 [9]。我们通过实验验证，PQ相较于PLAID的$b$-bit压缩器，能够将晚期交互阶段的延迟减少最多3.6倍。此外，在使用JMPQ版本时，它在MRR@10方面提供了相同（$m = 16$）或更好的性能（$m = 32$）。

---

我们提出通过利用方程式（5）的性质进一步提高评分阶段的效率。我们通过实验观察到，在许多情况下，$q_i \cdot \bar{C}_j^T > q_i \cdot r_{pq}^{T_j}$，意味着在许多情况下，最大值操作符（max operator）在$j$上的结果主要由查询项和质心之间的分数主导，而不是查询项和残差之间的分数。因此，我们认为，只对一个减少的文档术语集合$\bar{J}_i$计算残差的分数是可行的，其中 $i$ 标识查询项的索引。特别地，$\bar{J}_i=\left\{j \mid q_i \cdot \bar{C}_j^T > t h_r\right\}$，其中 $t h_r$ 是一个第二阈值，用来确定质心的分数是否足够大。通过引入这个新的每项过滤器，方程式（5）现在变为计算在集合$\bar{J}_i$中的段落最大值操作，即：

$S_{q, P}=\sum_{i=1}^{n_q} \max _{j \in \bar{J}_i}\left(q_i \cdot \bar{C}^{T_j}+q_i \cdot r_{p q}^{T_j}\right)$ (6)

---

在实际应用中，我们仅计算那些其质心分数足够大的文档术语的残差分数。如果$\bar{J}_i=\emptyset$，则我们按照方程式（5）计算$S_{q, P}$。图5（左）展示了我们方法的有效性。在$y$轴上，我们报告了原始有效性的百分比，计算方式是通过将使用方程式（6）和方程式（5）计算得到的MRR@10值的比值来得出。根据方程式（6）过滤文档术语不会损害检索质量，因为它提供的MRR@10几乎与方程式（5）相同。在图5的右侧，我们报告了计算的分数术语相对于使用方程式（5）计算的文档术语数量的百分比。当$t h_r=0.5$时，我们能够将计算的分数术语数量减少至少30%（右），而在MRR@10方面没有任何性能下降。

# 5. Experimental Evaluation  

**实验设置。** 本节将我们的方法与目前最先进的多向量密集检索引擎PLAID [19]进行对比。我们在MS MARCO passages数据集[17]上进行领域内评估，并在LoTTE [20]数据集上进行领域外评估。我们使用ColBERTv2模型生成MS MARCO的嵌入。生成的数据集由大约$600$百万$d$维向量组成，其中$d=128$。产品量化（PQ）使用FAISS [10]库实现，并在MS MARCO上通过JMPQ技术[4]进行了优化。EMBV的实现可在GitHub上找到。我们将EMBV与原始的PLAID实现[19]进行比较，后者也在C++中实现其核心组件。实验在一台配备AVX512指令集的Intel Xeon Gold 5318Y CPU（主频为2.10 GHz）上进行，采用单线程执行。代码使用GCC 11.3.0（-O3编译选项）在Linux 5.15.0-72机器上编译。在使用AVX512指令运行实验时，我们确保避免Intel CPU上频率缩减事件[15]的发生。

------

**评估。** 表1比较了EMVB和PLAID在MS MARCO数据集上的表现，包括内存需求（每个嵌入的字节数）、平均查询延迟（以毫秒为单位）、MRR@10、Recall@100和Recall@1000。

---

结果表明，EMVB在评估的各项权衡中表现优越。当$m = 16$时，EMVB几乎将每个向量的内存负担减少了一半，与PLAID相比，查询速度提高了最多2.8倍，且几乎没有影响检索效果的性能退化。通过将每个向量的子划分数加倍，即$m = 32$，EMVB在内存占用相同的情况下，在MRR和Recall指标上超过了PLAID，速度提升最多达到2.5倍。

------

表2比较了EMVB和PLAID在LoTTE数据集上的领域外评估结果。与PLAID [19]一样，我们采用Success@5和Success@100作为检索质量指标。在该数据集上，EMVB在检索质量方面略有不如PLAID的表现。回想一下，由于缺少训练查询，JMPQ [4]无法应用于领域外评估。因此，我们采用了优化产品量化（OPQ）[7]，它通过寻找数据集向量的最优旋转来减少PQ带来的质量退化。为了减轻检索质量的损失，我们只在$m = 32$时实验PQ，因为更多的划分数能够更好地表示原始向量。另一方面，EMVB相比PLAID提供了最多2.9倍的加速。这一速度提升相较于MS MARCO更为显著，原因在于LoTTE数据集中的文档平均长度较长。在这种情况下，使用我们基于位向量的方法过滤不相关文档对效率的影响非常显著。需要注意的是，对于领域外评估，我们的预过滤方法可以被引入到PLAID中，这将允许在保持PLAID准确性的同时，提升EMVB的效率。将PLAID和EMVB结合的研究将留待未来工作。

# 6. Conclusion  

我们提出了EMVB，一个高效的多向量密集检索新框架。EMVB通过引入四个创新贡献，推进了当前最先进的PLAID方法。首先，EMVB采用了高度高效的预过滤步骤，使用优化的位向量加速候选文档的过滤过程。第二，EMVB在计算质心交互时采用了降低精度的方式。第三，EMVB利用产品量化（Product Quantization）减少存储向量表示的内存占用，同时支持快速的后期交互。第四，我们引入了针对每个文档术语的过滤器，减少了后期交互步骤的计算成本，最高可降低30%。我们在两个公开数据集（MS MARCO和LoTTE）上对EMVB进行了实验评估。结果表明，在领域内评估中，EMVB比PLAID快最多2.8倍，内存占用减少1.8倍，且检索质量没有损失。在领域外评估中，EMVB比PLAID快最多2.9倍，且几乎没有检索质量退化。

