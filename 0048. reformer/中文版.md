# Reformer：高效的 Transformer

#### 摘要

大型 Transformer 模型在许多任务上通常能够取得最先进的结果，但训练这些模型的成本可能非常高，尤其是在长序列上。我们引入了两种技术来提高 Transformer 的效率。首先，我们用局部敏感哈希替换了点积注意力，将其复杂度从 $\mathrm{O}\left(L^{2}\right)$ 降低到 $\mathrm{O}(L \log L)$，其中 $L$ 是序列的长度。此外，我们使用可逆残差层代替标准残差层，这使得在训练过程中只需存储一次激活值，而不是 $N$ 次，其中 $N$ 是层数。最终的模型 Reformer 在性能上与 Transformer 模型相当，同时在长序列上更加内存高效且速度更快。

## 1 引言

Transformer 架构（Vaswani 等，2017）被广泛应用于自然语言处理，并在许多任务上取得了最先进的结果。为了获得这些结果，研究人员不得不训练越来越大的 Transformer 模型。在（Shazeer 等，2018）中报告的最大配置中，每层的参数数量超过 0.5 B，而在（Al-Rfou 等，2018）中，层数达到了 64。Transformer 模型也用于处理越来越长的序列。在（Liu 等，2018）中，单个示例中处理了多达 1.1 万个文本标记，而在处理其他模态（如音乐（Huang 等，2018）和图像（Parmar 等，2018））时，甚至更长的序列也很常见。这些大规模长序列模型产生了出色的结果，但也使得资源紧张到有人认为这种趋势正在破坏 NLP 研究 ${ }^{1}$。许多大型 Transformer 模型实际上只能在大型工业研究实验室中训练，而使用模型并行训练的此类模型甚至无法在单个 GPU 上进行微调，因为它们的内存需求需要多加速器硬件设置，即使对于单个训练步骤也是如此。

---

大型 Transformer 模型是否从根本上需要如此巨大的资源，还是它们只是效率低下？考虑以下计算：在报告的最大 Transformer 层中使用的 0.5 B 参数占用了 2 GB 的内存。64 K 个标记的激活值，嵌入大小为 1024，批量大小为 8，占用了 $64 \mathrm{~K} \times 1 \mathrm{~K} \times 8=0.5$ B 个浮点数，需要另外 2 GB 的内存。如果我们的内存使用仅是按层计算的，那么我们相当容易地在单个加速器上容纳一个大型 Transformer，即使是长度为 64 K 的序列。此外，用于训练 BERT 的整个语料库只需要 17 GB 来存储。那么为什么我们甚至无法在单台机器上微调这些模型？

---

上述估计仅包括每层内存和输入激活成本，并未考虑 Transformer 中以下主要内存来源。

- 在具有 $N$ 层的模型中，内存是单层模型的 $N$ 倍，因为需要存储激活值以进行反向传播。

- 由于中间前馈层的深度 $d_{f f}$ 通常远大于注意力激活的深度 $d_{\text {model }}$，因此它占据了内存使用的大部分。

- 长度为 $L$ 的序列的注意力在计算和内存复杂度上都是 $\mathrm{O}\left(L^{2}\right)$，因此即使是单个 64 K 标记的序列也可能耗尽加速器内存。

---

我们引入了 Reformer 模型，该模型通过以下技术解决了这些问题：

- **可逆层**，首次由 Gomez 等（2017）提出，使得在整个模型中仅存储激活值的单个副本，因此 $N$ 因子消失。

- **拆分前馈层中的激活值**并将其分块处理，消除了 $d_{f f}$ 因子，节省了前馈层内部的内存。

- **基于局部敏感哈希的近似注意力计算**将注意力层中的 $\mathrm{O}\left(L^{2}\right)$ 因子替换为 $\mathrm{O}(L \log L)$，从而允许在长序列上操作。

---

我们研究了这些技术，并表明与标准 Transformer 相比，它们对训练过程的影响可以忽略不计。事实上，拆分激活值仅影响实现；在数值上与 Transformer 中使用的层相同。应用可逆残差而不是标准残差确实会改变模型，但在我们实验的所有配置中对训练的影响可以忽略不计。最后，注意力中的局部敏感哈希是一个更重大的变化，可能会影响训练动态，具体取决于使用的并发哈希数量。我们研究了该参数，并找到了一个既高效使用又能产生与完全注意力非常接近结果的值。

---

我们在合成任务、文本任务（enwik8，序列长度为 64 K）和图像生成任务（imagenet-64 生成，序列长度为 12 K）上进行了实验。在这两种情况下，我们展示了 Reformer 与完整 Transformer 获得的结果相匹配，但运行速度更快，尤其是在文本任务上，并且内存效率提高了几个数量级。

## 2 局部敏感哈希注意力

**点积注意力**。Transformer 中使用的标准注意力是缩放点积注意力（Vaswani 等，2017）。输入包括维度为 $d_{k}$ 的查询和键，以及维度为 $d_{v}$ 的值。计算查询与所有键的点积，缩放 $\sqrt{d_{k}}$，并应用 softmax 函数以获得值的权重。在实践中，一组查询的注意力函数是同时计算的，打包成一个矩阵 $Q$。假设键和值也打包成矩阵 $K$ 和 $V$，输出矩阵定义为：

$$
\begin{equation*}
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V \tag{1}
\end{equation*}
$$

---

**多头注意力**。在 Transformer 中，不是使用 $d_{\text {model }}$ 维的键、值和查询执行单个注意力函数，而是将查询、键和值分别线性投影 $h$ 次，使用不同的学习线性投影到 $d_{k}, d_{k}$ 和 $d_{v}$ 维度。注意力并行应用于这些投影版本的查询、键和值，生成 $d_{v^{-}}$ 维输出值。这些值被连接并再次投影，得到最终值。这种机制被称为多头注意力。

---

**内存高效的注意力**。为了计算注意力机制的内存使用，让我们关注公式 1 中的注意力计算。假设 $\mathrm{Q}, \mathrm{K}$ 和 V 都具有形状 $\left[b a t c h \_s i z e, l e n g t h, ~ d_{\text {model }}\right]$。主要问题是 $Q K^{T}$ 项，其形状为 [batch_size, length, length]。在实验部分，我们在长度为 $64 K$ 的序列上训练模型——在这种情况下，即使批量大小为 1，这也是一个 $64 K \times 64 K$ 的矩阵，以 32 位浮点数存储将占用 16 GB 内存。这是不切实际的，并且阻碍了 Transformer 在长序列上的使用。但需要注意的是，$Q K^{T}$ 矩阵不需要完全存储在内存中。实际上可以为每个查询 $q_{i}$ 单独计算注意力，只需在内存中计算一次 $\operatorname{softmax}\left(\frac{q_{i} K^{T}}{\sqrt{d_{k}}}\right) V$，然后在需要梯度时在反向传播中重新计算。这种计算注意力的方式可能效率较低，但它仅使用与长度成比例的内存。我们使用这种内存高效的注意力实现来运行实验部分中展示的完全注意力基线。

---

$\mathbf{Q}, \mathbf{K}, \mathbf{V}$ 从何而来？上述多头注意力操作的是键、查询和值，但通常我们只得到一个形状为 [batch_size, length, $\left.d_{\text {model }}\right]$ 的激活张量 A——例如，将句子中的标记嵌入为向量。为了从 A 构建 $\mathrm{Q}, \mathrm{K}$ 和 V，Transformer 使用了 3 个不同的线性层，将 A 投影到具有不同参数的 $\mathrm{Q}, \mathrm{K}$ 和 V。对于具有 LSH 注意力的模型，我们希望查询和键（Q 和 K）相同。这可以通过使用相同的线性层从 A 到 Q 和 K 轻松实现，并为 V 使用单独的线性层。我们称这种行为的模型为共享 QK Transformer。事实证明，共享 QK 不会影响 Transformer 的性能，即使我们额外对键 K 的长度进行归一化，正如我们在实验部分 5 中展示的那样。

---

**哈希注意力**。对于 LSH 注意力，我们从两个张量 Q=K 和 V 开始，形状为 [batch_size, length, $\left.d_{\text {model }}\right]$。我们保持多头机制不变，并关注公式 1 中的注意力计算。如前所述，主要问题是 $Q K^{T}$ 项，其形状为 [batch_size,length, length]。但请注意，我们实际上只对 softmax $\left(Q K^{T}\right)$ 感兴趣。由于 softmax 由最大元素主导，对于每个查询 $q_{i}$，我们只需要关注 K 中最接近 $q_{i}$ 的键。例如，如果 K 的长度为 64 K，对于每个 $q_{i}$，我们可能只考虑一小部分，比如 32 或 64 个最接近的键。这要高效得多，但我们如何在键中找到最近邻居？

---

**局部敏感哈希**。在高维空间中快速找到最近邻居的问题可以通过局部敏感哈希（LSH）解决。如果附近的向量以高概率获得相同的哈希值，而远处的向量则不会，那么这种将每个向量 $x$ 分配到一个哈希 $h(x)$ 的哈希方案被称为局部敏感的。在我们的情况下，我们实际上只要求附近的向量以高概率获得相同的哈希值，并且哈希桶的大小以高概率相似。

---

我们通过以下随机投影来实现这一点（见图 1）。为了获得 $b$ 个哈希值，我们首先固定一个大小为 $\left[d_{k}, b / 2\right]$ 的随机矩阵 $R$。然后我们定义 $h(x)=\arg \max ([x R ;-x R])$，其中 $[u ; v]$ 表示两个向量的连接。这种方法是一种已知的 LSH 方案（Andoni 等，2015），易于实现并应用于批量向量。

- 图 1：角度局部敏感哈希使用球形投影点的随机旋转，通过有符号轴投影上的 argmax 来建立桶。在这个高度简化的二维描述中，两个点 $x$ 和 $y$ 不太可能共享相同的哈希桶（上），除非它们的球形投影彼此接近（下）。

---

**LSH 注意力**。了解了我们的 LSH 方案和哈希注意力的一般概念后，我们现在将正式化本文中使用的 LSH 注意力。我们首先重写正常注意力的方程 (1)，针对单个查询位置 $i$：

$$
\begin{equation*}
o_{i}=\sum_{j \in \mathcal{P}_{i}} \exp \left(q_{i} \cdot k_{j}-z\left(i, \mathcal{P}_{i}\right)\right) v_{j} \quad \text { 其中 } \mathcal{P}_{i}=\{j: i \geq j\} \tag{2}
\end{equation*}
$$

我们引入符号 $\mathcal{P}_{i}$ 来表示位置 $i$ 处的查询所关注的集合，$z$ 表示分区函数（即 softmax 中的归一化项）。为了清晰起见，我们还省略了 $\sqrt{d_{k}}$ 的缩放。

---

为了批处理目的，我们通常在一个更大的集合 $\widetilde{\mathcal{P}}_{i}=\{0,1, \ldots, l\} \supseteq \mathcal{P}_{i}$ 上执行注意力，同时屏蔽掉不在 $\mathcal{P}_{i}$ 中的元素：

$$
o_{i}=\sum_{j \in \widetilde{\mathcal{P}}_{i}} \exp \left(q_{i} \cdot k_{j}-m\left(j, \mathcal{P}_{i}\right)-z\left(i, \mathcal{P}_{i}\right)\right) v_{j} \quad \text { 其中 } m\left(j, \mathcal{P}_{i}\right)= \begin{cases}\infty & \text { 如果 } j \notin \mathcal{P}_{i}  \tag{3}\\ 0 & \text { 否则 }\end{cases}
$$

---

现在我们转向 LSH 注意力，我们可以将其理解为通过只允许在单个哈希桶内进行注意力来限制查询位置 $i$ 可以关注的目标项集合 $\mathcal{P}_{i}$。

$$
\begin{equation*}
\mathcal{P}_{i}=\left\{j: h\left(q_{i}\right)=h\left(k_{j}\right)\right\} \tag{4}
\end{equation*}
$$

图 2(a-b) 显示了完全注意力与哈希变体的示意比较。部分 (a) 描述了完全注意力的注意力矩阵通常是稀疏的，但计算并未利用这种稀疏性。在 (b) 中，查询和键已根据其哈希桶进行排序。由于相似项以高概率落入同一桶中，因此可以通过只允许在每个桶内进行注意力来近似完全注意力模式。

- 图 2：LSH 注意力的简化描述，展示了哈希分桶、排序和分块步骤以及由此产生的因果注意力。(a-d) 这些注意力变体的注意力矩阵。

---

在这种公式化中，哈希桶的大小往往不均匀，这使得跨桶批处理变得困难。此外，桶内的查询数量和键数量可能不相等，事实上，一个桶可能包含许多查询但没有键。为了缓解这些问题，我们首先通过设置 $k_{j}=\frac{q_{j}}{\left\|q_{j}\right\|}$ 来确保 $h\left(k_{j}\right)=h\left(q_{j}\right)$。接下来，我们按桶号对查询进行排序，并在每个桶内按序列位置排序；这定义了排序后的一个排列 $i \mapsto s_{i}$。在排序后的注意力矩阵中，来自同一桶的对将聚集在对角线附近（如图 2 c 所示）。我们可以采用一种批处理方法，其中 $m$ 个连续查询（排序后）相互关注，并向后关注一个块（图 2 d）。按照我们之前的符号，这对应于设置：

$$
\begin{equation*}
\widetilde{\mathcal{P}}_{i}=\left\{j:\left\lfloor\frac{s_{i}}{m}\right\rfloor-1 \leq\left\lfloor\frac{s_{j}}{m}\right\rfloor \leq\left\lfloor\frac{s_{i}}{m}\right\rfloor\right\} \tag{5}
\end{equation*}
$$

如果 $\max _{i}\left|\mathcal{P}_{i}\right|<m$，则 $\mathcal{P}_{i} \subseteq \widetilde{\mathcal{P}}_{i}$。在实践中，我们设置 $m=\frac{2 l}{n_{\text {buckets }}}$（其中 $l$ 是序列长度）。平均桶大小为 $\frac{l}{n_{\text {buckets }}}$，我们假设桶增长到两倍大小的概率足够低。LSH 注意力的整体过程总结在图 2 中。

---

**多轮 LSH 注意力**。使用哈希时，总是存在相似项落入不同桶的小概率。通过使用 $n_{\text {rounds }}$ 个不同的哈希函数 $\left\{h^{(1)}, h^{(2)}, \ldots\right\}$ 进行多轮哈希，可以减少这种概率，使得：

$$
\begin{equation*}
\mathcal{P}_{i}=\bigcup_{r=1}^{n_{\text {rounds }}} \mathcal{P}_{i}^{(r)} \quad \text { 其中 } \mathcal{P}_{i}^{(r)}=\left\{j: h^{(r)}\left(q_{i}\right)=h^{(r)}\left(q_{j}\right)\right\} \tag{6}
\end{equation*}
$$

多轮情况基本上涉及并行执行 LSH 注意力 $n_{\text {rounds }}$ 次；该过程的详细信息在附录 A 中描述。

---

**共享 QK 注意力的因果屏蔽**。在 Transformer 解码器中，屏蔽（在公式 3 中表示为 $m\left(j, \mathcal{P}_{i}\right)$）用于防止位置关注未来。为了在 LSH 注意力中实现屏蔽，我们将每个查询/键向量与一个位置索引关联，使用与排序查询/键向量相同的排列重新排序位置索引，然后使用比较操作计算屏蔽。

---

虽然不允许关注未来，但 Transformer 的典型实现确实允许一个位置关注自身。这种行为在共享 QK 公式中是不希望的，因为查询向量与自身的点积几乎总是大于查询向量与另一个位置向量的点积。因此，我们修改屏蔽以禁止一个标记关注自身，除非在标记没有其他有效注意力目标的情况下（例如序列中的第一个标记）。

### 2.1 对合成任务的分析

为了验证 LSH 注意力的性能并研究其行为，我们从以下合成任务开始：复制符号序列。在此任务中，每个训练和测试示例的形式为 $0 w 0 w$，其中 $w \in\{1, \ldots, N\}^{*}$ 是一个范围从 1 到 $N$ 的符号序列（我们在实验中使用 $N=127$）。下面给出了一个长度为 3 的单词 $w$ 的示例。

---

为了研究 LSH 注意力，我们在上述形式的示例上训练一个语言模型，其中每个 $w$ 的长度为 511（因此整个输入 $0 w 0 w$ 的长度为 1024）。由于这是一个语言建模任务，我们总是根据所有先前的符号预测下一个符号，但我们屏蔽了损失和准确性，只考虑输入后半部分的位置，即那些实际上可以预测的位置。

---

上述任务可以通过 1 层 Transformer 模型完美解决（达到 $100 \%$ 的准确性和 0 损失）。但请注意，它需要非局部注意力查找，因此任何依赖有限跨度的稀疏注意力的模型都无法解决。为了使训练简单快速，但与 NLP 中使用的模型相似，我们使用 1 层 Transformer，$d_{\text {model }}=d_{f f}=256$ 和 4 个头。我们在 4 种不同的设置中训练它 150 K 步：完全注意力，LSH 注意力，$n_{\text {rounds }}=1$，$n_{\text {rounds }}=2$ 和 $n_{\text {rounds }}=4$。

---

从表 2 中总结的结果中我们看到，使用完全注意力训练的模型可以立即用于 LSH 注意力，但会损失一些准确性。当从头开始使用 LSH 注意力训练时，使用 4 个哈希训练的模型也几乎达到了完美的准确性。有趣的是，当使用 8 个哈希进行评估时，准确性变得完美。当使用 2 或 1 个哈希进行评估时，准确性会下降。使用较少哈希训练的模型显示出较差的结果，但即使仅使用 1 个哈希训练的模型在使用 8 个哈希进行评估时也几乎表现得完美。

- 表 2：在复制任务上，使用完全注意力和使用不同数量并行哈希的局部敏感哈希注意力的 1 层 Transformer 模型的准确性。

## 3 可逆 Transformer

正如上一节所示，如果允许近似，注意力复杂度可以从长度的平方降低到线性。但从表 1 中可以清楚地看到，每个字段都以 $b \cdot n_{h} \cdot l$ 项开头：$b \cdot n_{h} \cdot l \cdot d_{k}$，或者 $b \cdot l \cdot d_{\text {model }}$ 成本无法避免。事实上，每层之前的激活大小已经是 $b \cdot l \cdot d_{\text {model }}$，因此整个模型的内存使用量至少为 $b \cdot l \cdot d_{\text {model }} \cdot n_{l}$。更糟糕的是：在 Transformer 的前馈层内部，这一成本上升到 $b \cdot l \cdot d_{f f} \cdot n_{l}$。在大型 Transformer 中，通常设置 $d_{f f}=4 K$ 和 $n_{l}=16$，因此当 $l=64 K$ 时，这将再次使用不切实际的 $16 G B$ 内存。

- 表 1：注意力变体的内存和时间复杂度。我们使用 $l$ 表示长度，$b$ 表示批量大小，$n_{h}$ 表示头数，$n_{c}$ 表示 LSH 块数，$n_{r}$ 表示哈希重复次数。

---

在本节中，我们展示了如何通过首先使用可逆层处理项的 $n_{l}$ 部分，然后展示分块如何允许我们处理 $d_{f f}$ 问题来降低这一成本。每种方法对内存和时间复杂度的影响总结在表 3 中。

- 表 3：Transformer 变体的内存和时间复杂度。我们使用 $d_{\text {model }}$ 和 $d_{f f}$ 表示模型深度，并假设 $d_{f f} \geq d_{\text {model }} ; b$ 表示批量大小，$l$ 表示长度，$n_{l}$ 表示层数。我们假设 $n_{c}=l / 32$，因此 $4 l / n_{c}=128$，我们使用 $c=128^{2}$。

---

**RevNets**。可逆残差网络由 Gomez 等（2017）引入，他们展示了它们可以替代 ResNets 进行图像分类。主要思想是允许从下一层的激活中恢复任何给定层的激活，仅使用模型参数。与必须检查中间值以用于反向传播不同，层可以随着反向传播从网络输出到输入而逐一反转。虽然正常的残差层执行一个函数 $x \mapsto y$，该函数对单个输入进行操作并产生单个输出，并且具有形式 $y=x+F(x)$，但可逆层对输入/输出对进行操作：$\left(x_{1}, x_{2}\right) \mapsto\left(y_{1}, y_{2}\right)$，并遵循以下方程：

$$
\begin{equation*}
y_{1}=x_{1}+F\left(x_{2}\right) \quad y_{2}=x_{2}+G\left(y_{1}\right) \tag{7}
\end{equation*}
$$

通过减去（而不是添加）残差来反转层：

$$
\begin{equation*}
x_{2}=y_{2}-G\left(y_{1}\right) \quad x_{1}=y_{1}-F\left(x_{2}\right) \tag{8}
\end{equation*}
$$

---

**可逆 Transformer**。我们通过将注意力和前馈层组合在 revnet 块中，将 RevNet 思想应用于 Transformer。在上面的符号中，F 成为注意力层，而 $G$ 成为前馈层。请注意，层归一化（Ba 等，2016）被移动到残差块内部。

$$
\begin{equation*}
Y_{1}=X_{1}+\operatorname{Attention}\left(X_{2}\right) \quad Y_{2}=X_{2}+\operatorname{FeedForward}\left(Y_{1}\right) \tag{9}
\end{equation*}
$$

可逆 Transformer 不需要在每层存储激活，因此消除了 $n_{l}$ 项。在第 5 节中，我们展示了当使用相同数量的参数时，它的性能与正常 Transformer 相同；我们通过使 $x_{1}$ 和 $x_{2}$ 都具有大小 $d_{\text {model }}$ 来实现这一点。

---

**分块**。虽然可逆性涵盖了 $n_{l}$ 项，但较厚的层仍然可以使用大量内存。特别是前馈层可以使用维度为 $d_{f f}=4 K$ 或更高的中间向量。然而，前馈层中的计算在序列中的位置之间是完全独立的，因此计算可以分成 $c$ 个块：

$$
\begin{equation*}
Y_{2}=\left[Y_{2}^{(1)} ; \ldots ; Y_{2}^{(c)}\right]=\left[X_{2}^{(1)}+\operatorname{FeedForward}\left(Y_{1}^{(1)}\right) ; \ldots ; X_{2}^{(c)}+\operatorname{FeedForward}\left(Y_{1}^{(c)}\right)\right] \tag{10}
\end{equation*}
$$

该层通常通过并行执行所有位置的操作进行批处理，但一次操作一个块可以减少内存。（8）中的反向计算和反向传播也被分块。除了前馈层，对于具有大词汇量（超过 $d_{\text {model }}$ 个词类型）的模型，我们还对输出的对数概率进行分块，并一次计算序列部分的损失。

---

**分块、大批量和参数重用**。通过分块和可逆层，我们在整个网络中用于激活的内存与层数无关。然而，参数的数量并非如此，因为它们的数量随着层数的增加而增加。不过，这个问题可以通过在层不计算时将层参数交换到 CPU 内存来解决。在标准 Transformer 中，这将效率低下，因为内存传输到 CPU 的速度很慢。然而，Reformer 中的批量大小乘以长度要大得多，因此使用参数完成的计算量分摊了其传输成本。

## 4 相关工作

（Vaswani 等，2017）中引入的 Transformer 模型已广泛应用于自然语言任务，并进一步扩展到建模各种数据，如乐谱（Huang 等，2018）和图像（Parmar 等，2018；Ramachandran 等，2019）。最值得注意的是，此类模型已成功应用于极大规模语言模型的自我监督训练（Devlin 等，2018，Radford 等，2019）。

---

鉴于最先进序列模型的巨大计算需求，人们越来越关注寻找减少 Transformer 模型内存占用和计算需求的方法。除了精度降低和梯度检查点（Sohoni 等，2019）等标准方法外，最近还探索了 Transformer 模型自注意力机制的更高效版本（Sukhbaatar 等，2019ab）。

---

特别是，利用注意力层中的稀疏性已被证明是有成效的。OpenAI 引入了稀疏 Transformer（Child 等，2019），它利用了注意力分解的稀疏表示。使用产品键注意力来增加键空间也被用于减少前馈层的内存需求，而不会降低性能（Lample 等，2019）。

---

据我们所知，局部敏感哈希（LSH）之前并未直接应用于 Transformer 注意力层。但以前使用外部内存与神经网络的工作已经处理过大型内存。内存网络的最初实现（Weston 等，2014）以及后来的扩展工作（Bordes 等，2015，Chandar 等，2016）使用了数百万大小的内存。这样做的成本是内存必须在训练前固定。此外，由于在训练开始时模型不太可能正确查询内存，因此使用强监督来鼓励模型查询有用的内存位置。这些提示要么由任务作为额外的监督信息给出，要么如 Hill 等（2015）中启发式地确定。Santoro 等（2016）消除了在训练前固定内存的要求，但代价是内存大小，后来 Rae 等（2016）缓解了这一问题。最后一篇论文考虑了包括 LSH 和随机 kd 树在内的近似最近邻内存查找，但仅用于外部内存中的查找。

## 5 实验

在本节中，我们展示了上述技术的实验结果。我们逐一分析这些技术，以明确哪些组合对性能有影响。我们首先展示可逆层和共享查询键空间不会影响性能，然后分析哈希注意力，最后分析完整的 Reformer 模型。

---

我们在 imagenet64 和 enwik8-64K 任务上进行了实验，其中后者是 enwik8 的变体，被分块为 $2^{16}=64 K$ 个标记的子序列。我们使用 3 层模型进行消融实验，以便与常规 Transformer 进行比较，后者内存使用率高并执行完整的 $O\left(l^{2}\right)$ 注意力。所有实验都有 $d_{\text {model }}=1024, d_{f f}=4096$，$n_{\text {heads }}=8$，总批量大小为 8 个序列。我们使用 Adafactor 优化器（Shazeer & Stern，2018）来训练这些模型。我们还按照 Vaswani 等（2017）的超参数评估了 WMT 2014 英语到德语翻译任务。所有实验的训练都在 8 个设备（8 个 GPU 或 8 个 TPU v3 核心）上并行化。我们模型的训练代码已公开提供 ${ }^{2}$。

---

**共享 QK 的影响**。我们首先考虑共享 QK 注意力对常规 Transformer 模型的影响。共享 QK 注意力设置 $k_{j}=\frac{q_{j}}{\left\|q_{j}\right\|}$ 并防止标记关注自身（除非没有其他上下文可用）。在图 3 的左侧，我们绘制了常规注意力和共享 QK 注意力的困惑度曲线。共享查询键空间的表现并不比常规注意力差；事实上，对于 enwik8，它似乎训练得更快。换句话说，我们通过切换到共享 QK 注意力并没有牺牲准确性。

- 图 3：共享查询键空间（左）和可逆性（右）对 enwik8 和 imagenet64 训练性能的影响。曲线显示了保留数据上的每维度比特数。

---

**可逆层的影响**。在图 3 右侧的两个图中，我们比较了 Vaswani 等（2017）中的常规 Transformer 和第 3 节中描述的可逆 Transformer。这两个模型具有相同的参数数量，学习曲线也几乎相同。这些结果表明，可逆 Transformer 的内存节省并不会以准确性为代价。

---

**机器翻译中的可逆层**。我们还在英语到德语的机器翻译的编码器-解码器 Transformer 模型中评估了可逆层。我们首先在 Transformer-base 架构中使编码器和解码器完全可逆，并看到生成的模型在训练 100 K 步时与 Vaswani 等（2017）的性能相当。我们还评估了更多步数的训练和更大的模型。Reformer 模型非常节省内存，因此对于后两个实验，我们不需要通过在模型中共享嵌入和输出投影权重矩阵来节省内存。结果如表 4 所示。我们在此设置中未应用 LSH 注意力，因为示例是单句，而句子往往相对较短。我们典型的 LSH 注意力配置在哈希和排序后使用 128 个标记的块，而 WMT14 测试集中的示例都短于 128 个标记。

- 表 4：WMT 英语-德语（EnDe）的 newstest2014 上的 BLEU 分数。我们还报告了由 sacreBLEU（Post，2018）计算的去标记化 BLEU 分数。

---

**Transformer 中的 LSH 注意力**。LSH 注意力是完整注意力的近似，如图 4 所示，随着哈希次数的增加，它变得更加准确。在 $n_{\text {rounds }}=8$ 时，它几乎已经与完整注意力匹配。模型的计算成本随着哈希次数的增加而增加，因此可以根据可用的计算预算调整此超参数。此外，如表 2 所示，可以在评估时增加哈希次数以产生更准确的结果。在图 5 的右半部分，我们绘制了不同注意力类型的速度与序列长度的关系，同时保持标记总数不变。我们看到，虽然常规注意力在较长序列长度时变慢，但 LSH 注意力速度保持平稳。

- 图 4：LSH 注意力性能作为哈希轮数的函数在 imagenet64 上的表现。

- 图 5：左：LSH 注意力性能作为层数的函数在 enwik8 上的表现。右：完整注意力和 LSH 注意力的评估速度作为输入长度的函数。

---

**大型 Reformer 模型**。为了验证 Reformer 确实可以在单个核心上容纳大型模型并在长序列上快速训练，我们在 enwik8 和 imagenet64 上训练了多达 20 层的大型 Reformer。如图 5 所示，这些模型可以放入内存并训练。在这种情况下，我们无法训练 Transformer 基线，因为它们太慢且内存消耗大，但我们看到随着层数的增加有明显的改进。在 enwik8 上训练的 12 层模型，训练 20 K 步，dropout 率为 0.1，在测试集上达到 1.19 bits/dim。我们还训练了一个 12 层的 Reformer 模型，进行了更长时间的调整和改进，并在 enwiki8 测试集上达到了 $1.05 \mathrm{bits} / \mathrm{dim}$。

## 6 结论

Reformer 结合了 Transformer 的建模能力和一种可以在长序列上高效执行且内存使用少的架构，即使对于具有大量层的模型也是如此。我们相信，这将有助于大型、参数丰富的 Transformer 模型变得更加广泛和易于使用。此外，处理长序列的能力为 Reformer 在许多生成任务中的使用开辟了道路。除了生成非常长的连贯文本外，Reformer 还可以将 Transformer 模型的力量带到其他领域，如时间序列预测、音乐、图像和视频生成。

## A 多轮 LSH 注意力

在本节中，我们更详细地描述了我们的 LSH 注意力机制的多哈希版本。我们首先重复主文中的公式 (3)，它描述了具有稀疏性的注意力的一般公式：

$$
o_{i}=\sum_{j \in \widetilde{\mathcal{P}}_{i}} \exp \left(q_{i} \cdot k_{j}-m\left(j, \mathcal{P}_{i}\right)-z\left(i, \mathcal{P}_{i}\right)\right) v_{j} \quad \text { 其中 } m\left(j, \mathcal{P}_{i}\right)= \begin{cases}\infty & \text { 如果 } j \notin \mathcal{P}_{i} \\ 0 & \text { 否则 }\end{cases}
$$

在多轮情况下，查询位置 $i$ 可以关注键位置 $\mathcal{P}_{i}$，如 (6) 中所定义，我们在此也重复一遍：

$$
\begin{equation*}
\mathcal{P}_{i}=\bigcup_{r=1}^{n_{\text {rounds }}} \mathcal{P}_{i}^{(r)} \quad \text { 其中 } \mathcal{P}_{i}^{(r)}=\left\{j: h^{(r)}\left(q_{i}\right)=h^{(r)}\left(q_{j}\right)\right\} \tag{6}
\end{equation*}
$$

为了批处理目的，注意力在排序的查询/键的块上执行：

$$
\begin{equation*}
\widetilde{\mathcal{P}}_{i}^{(r)}=\left\{j:\left\lfloor\frac{s_{i}^{(r)}}{m}\right\rfloor-1 \leq\left\lfloor\frac{s_{j}^{(r)}}{m}\right\rfloor \leq\left\lfloor\frac{s_{i}^{(r)}}{m}\right\rfloor\right\} \tag{11}
\end{equation*}
$$

结合 (3) 和 (6) 得到：
$$
\begin{align*}
o_{i} & =\sum_{j \in \widetilde{\mathcal{P}}_{i}} \exp \left(q_{i} \cdot k_{j}-m\left(j, \mathcal{P}_{i}\right)-z\left(i, \mathcal{P}_{i}\right)\right) v_{j}  \tag{12}\\
& =\sum_{r=1}^{n_{\text {rounds }}} \exp \left(z\left(i, \mathcal{P}_{i}^{(r)}\right)-z\left(i, \mathcal{P}_{i}\right)\right) \sum_{j \in \widetilde{\mathcal{P}}_{i}^{(r)}} \frac{1}{N_{i, j}} \exp \left(q_{i} \cdot k_{j}-m\left(j, \mathcal{P}_{i}^{(r)}\right)-z\left(i, \mathcal{P}_{i}^{(r)}\right)\right) v_{j}  \tag{13}\\
& =\sum_{r=1}^{n_{\text {rounds }}} \exp \left(z\left(i, \mathcal{P}_{i}^{(r)}\right)-z\left(i, \mathcal{P}_{i}\right)\right) o_{i}^{(r)}  \tag{14}\\
o_{i}^{(r)} & =\sum_{j \in \widetilde{\mathcal{P}}_{i}^{(r)}} \exp \left(q_{i} \cdot k_{j}-m_{i, j}^{(r)}-z\left(i, \mathcal{P}_{i}^{(r)}\right)\right) v_{j}  \tag{15}\\
& \text { where } N_{i, j}=\left|\left\{r^{\prime}: j \in \mathcal{P}_{i}^{\left(r^{\prime}\right)}\right\}\right| \text { and } m_{i, j}^{(r)}= \begin{cases}\infty & \text { if } j \notin \mathcal{P}_{i}^{(r)} \\
10^{5} & \text { if } i=j \\
\log N_{i, j} & \text { otherwise }\end{cases} \tag{16}
\end{align*}
$$

---

每一轮 LSH 注意力都会生成一个向量 $o_{i}^{(r)}$，该向量可以独立于其他轮次进行计算，除了包含一个项 $N_{i, j}$ 以避免在构建 $\mathcal{P}_{i}^{(r)}$ 集合的并集时重复计算元素。在我们的实现中，我们将 $N_{i, j}$ 因子合并到掩码项 $m_{i, j}^{(r)}$ 中。

---

我们还修改了 $m_{i, j}^{(r)}$，以引入 $i=j$ 的特殊情况。添加这种情况是因为标准 Transformer 中的因果掩码允许位置 $i$ 关注自身，这在共享 QK 公式中是不可取的。我们将掩码设置为一个较大但有限的值，以禁止原地注意力，除非在某个标记没有其他有效注意力目标的情况下。例如，序列中的第一个标记只能关注自身，因为没有可用的先前上下文。
