# ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT  

# 0. 摘要

自然语言理解（NLU）的最新进展正在快速推动信息检索（IR）的发展，主要归功于对深层语言模型（LMs）进行微调以用于文档排序。尽管效果显著，但基于这些语言模型的排序模型在计算成本上比先前的方法高出数个数量级，尤其是因为它们必须将每个查询-文档对输入到庞大的神经网络中以计算单一的相关性评分。为了解决这个问题，我们提出了ColBERT，一种新颖的排序模型，它通过深层语言模型（尤其是BERT）实现高效检索。ColBERT引入了一种后期交互架构，它使用BERT独立编码查询和文档，然后采用一种既便宜又强大的交互步骤来模拟它们的细粒度相似性。通过延迟但保留这种细粒度的交互，ColBERT能够利用深层语言模型的表达能力，同时获得离线预计算文档表示的能力，大大加快查询处理速度。除了降低对传统模型检索出的文档进行重排序的成本外，ColBERT的支持剪枝的交互机制还使其能够利用向量相似性索引直接从大型文档集合中进行端到端的检索。我们使用两个最新的段落搜索数据集对ColBERT进行了广泛评估。结果表明，ColBERT的效果与现有的基于BERT的模型竞争（并优于所有非BERT基准），同时查询执行速度提高了两个数量级，且每个查询所需的浮点运算量减少了四个数量级。

# 1. Intro

在过去的几年里，信息检索（IR）领域引入了大量的神经排序模型，包括DRMM [7]、KNRM [4, 36]和Duet [20, 22]。与依赖人工设计特征的传统排序学习方法不同，这些模型采用基于嵌入的查询和文档表示，并直接对其内容间的局部交互（即细粒度关系）进行建模。其中，一种新的方法出现了，它通过微调深度预训练语言模型（LMs），如ELMo [29]和BERT [5]，来估计相关性。通过计算查询-文档对的深度上下文语义表示，这些语言模型有助于弥合文档和查询之间普遍存在的词汇不匹配问题 [21, 42] [30]。确实，在短短几个月内，基于BERT的多个排序模型已在各种检索基准测试上取得了最新的效果 [3,18,25,39]，并被Google${ }^1$和Bing${ }^2$专有地适配用于部署。

---

然而，这些语言模型所带来的显著效果提升是以==急剧增加的计算成本为代价==的。Hofstätter等人 [9] 和MacAvaney等人 [18] 观察到，文献中的基于BERT的模型比之前的模型在计算上高出100-1000倍——其中一些模型起初也并不便宜 [13]。图1总结了这一质量-成本权衡，该图将两个基于BERT的排序器 [25, 27] 与一组代表性的排序模型进行了比较。该图使用了MS MARCO Ranking [24]，这是一个包含900万段落和100万查询的最新集合，来自Bing的日志。它报告了在官方验证集上的检索效果（MRR@10）以及平均查询延迟（对数刻度），使用的是一台高端服务器，每个查询为神经重排序模型配备一块Tesla V100 GPU。按照MS MARCO的重排序设置，ColBERT（重排序）、神经匹配模型和深层语言模型对每个查询的官方前1000个文档进行重排序。其他方法，包括ColBERT（全检索），则直接从整个集合中检索前1000个结果。

- 图1：“针对MS MARCO排名数据集[24]，展示了多个代表性排名模型的效果（MRR@10）与平均查询延迟（对数尺度）之间的关系图。图中还展示了ColBERT。神经重排序器在官方BM25的前1000条结果基础上运行，并使用Tesla V100 GPU。方法论和详细结果见第4节。”

---

如图所示，BERT显著提高了搜索精度，使MRR@10比最佳的先前方法提高了近7%；同时，即使使用高端GPU，它的延迟也增加到数万毫秒。这带来了一个棘手的权衡问题，因为查询响应时间的增加即使只有100毫秒，也会影响用户体验，甚至可测量地减少收入 [17]。为了解决这个问题，最近的研究开始探索使用自然语言理解（NLU）技术来增强传统的检索模型，如BM25 [32]。例如，Nogueira等人 [26, 28] 在使用BM25评分进行索引之前，通过NLU生成的查询扩展文档，而Dai和Callan [2] 则用NLU估计的词项重要性替代了BM25的词频。尽管这些方法成功地减少了延迟，但相较于BERT，精度通常显著降低。

---

为了在信息检索中平衡效率和上下文处理，我们提出了ColBERT，这是一种基于BERT的上下文化后期交互的排序模型。顾名思义，ColBERT提出了一种新的后期交互范式，用于估计查询$q$与文档$d$之间的相关性。在后期交互中，$q$和$d$被分别编码为两组上下文嵌入，并通过这两组嵌入之间的便捷且适合剪枝的计算来评估相关性——即快速计算，这样的计算可以在不穷尽所有候选项的情况下实现排序。

---

图2对比了我们提出的后期交互方法与现有的神经匹配范式。在左侧，图2(a)展示了基于表示的排序模型，这些模型分别为$q$和$d$独立计算一个嵌入，并将相关性估计为两个向量之间的单一相似度分数 [12, 41]。向右移动，图2(b)展示了典型的基于交互的排序模型。与将$q$和$d$总结为单个嵌入不同，这些排序模型在$q$和$d$之间建模词和短语级别的关系，并使用深度神经网络（例如CNN/MLP [22]或核函数 [36]）进行匹配。在最简单的情况下，它们将一个交互矩阵输入到神经网络中，该矩阵反映了==$q$和$d$之间每对词的相似性==。再向右，图2(c)展示了一种更强大的基于交互的范式，如BERT的transformer架构 [25]，该范式在建模$q$和$d$之间的词级交互的同时，也建模了其内部的词级交互。

---

这些日益富有表现力的架构之间存在一定的紧张关系。虽然基于交互的模型（即图2 (b)和(c)）在信息检索任务中往往表现更优 [8, 21]，但基于表示的模型——通过隔离$q$和$d$之间的计算——可以预先离线计算文档表示，从而大大降低每个查询的计算负担 [41]。在这项工作中，我们观察到，基于交互的模型的细粒度匹配能力和基于表示的模型的文档表示预计算功能可以通过保留但延迟查询-文档交互来结合。图2(d)展示了这样一种架构。正如所示，每个查询嵌入通过MaxSim运算符与所有文档嵌入进行交互，该运算符计算最大相似度（例如，余弦相似度），这些运算符的标量输出在查询项之间累加。此范式允许ColBERT在利用基于深度语言模型的表示的同时，将文档编码的成本转移到离线进行，并将查询编码的成本一次性分摊到所有排序文档上。此外，它使ColBERT能够利用向量相似性检索索引（例如，[1,15]）直接从大型文档集合中检索前$k$个结果，从而显著提升了相对于仅重排序基于词项检索输出的模型的召回率。

---

如图1所示，ColBERT可以在几十到几百毫秒内响应查询。例如，当用于重排序（如“ColBERT (re-rank)”）时，相较于现有的基于BERT的模型，它实现了超过$170 \times$的加速（并且所需的浮点运算减少了$14,000 \times$），同时效果优于所有非BERT基准 (§4.2和4.3)。ColBERT的索引构建（唯一需要将文档输入BERT的步骤）也具有实际操作性：它可以在大约3小时内使用配备四块GPU的单台服务器索引9百万篇MS MARCO段落集合 (§4.5)，并能在空间占用仅几十GiB的情况下保持其有效性。我们广泛的消融研究 (§4.4) 显示，后期交互、其通过MaxSim操作的实现以及我们基于BERT的编码器中的关键设计选择对于ColBERT的有效性都至关重要。

---

我们的主要贡献如下：

(1) 我们提出了后期交互 (§3.1) 作为一种高效且有效的神经排序范式。

(2) 我们介绍了ColBERT (§3.2和3.3)，一种高度有效的模型，它在后期交互范式下使用了新颖的基于BERT的查询和文档编码器。

(3) 我们展示了如何利用ColBERT既可用于基于词项的检索模型上的重排序 (§3.5)，也可用于使用向量相似性索引对完整集合进行搜索 (§3.6)。

(4) 我们在MS MARCO和TREC CAR两个最新的段落搜索集合上对ColBERT进行了评估。

# 2. RELATED WORK  

## 2.1. Neural Matching Models.  

在过去的几年里，信息检索（IR）研究人员引入了许多用于排序的神经架构。在这项工作中，我们比较了KNRM [4, 36]、Duet [20, 22]、ConvKNRM [4]以及fastText+ConvKNRM [10]。KNRM提出了一种可微分的核池化技术，用于从交互矩阵中提取匹配信号；Duet结合了基于精确匹配和嵌入的相似性信号进行排序。ConvKNRM在2018年推出，能够学习查询和文档中n-gram的匹配。最后，fastText+ConvKNRM（简称fT+ConvKNRM）通过采用子词嵌入来解决常见词嵌入列表中稀有词缺失的问题。

---

2018年，Zamani等人[41]提出了SNRM，这是一种以表示为中心的IR模型，将每个查询和文档编码为单一的稀疏高维“潜在词”向量。通过为每个文档生成稀疏向量表示，SNRM能够利用传统的倒排索引来表示文档，从而实现快速的端到端检索。尽管取得了颇具前景的结果和见解，但SNRM的有效性在它所评估的数据集上（例如，参见[18, 38]）明显低于最新的技术水平。SNRM利用稀疏性来支持倒排索引的使用，而我们放宽了这一假设，在我们的消融实验（第4.4节）中，将基于BERT的（密集）表示模型与我们提出的晚期交互模型ColBERT进行了对比。关于现有神经排序模型的详细概述，读者可以参考最近的两篇文献综述[8, 21]。

## 2.2. Language Model Pretraining for IR.  

近期自然语言理解（NLU）的研究强调了在无监督方式下预训练语言表示模型，然后再对下游任务进行微调的重要性。一个显著的例子是BERT [5]，这是一种基于双向Transformer的语言模型，通过微调提升了多个NLU基准的技术水平。Nogueira等人[25]、MacAvaney等人[18]以及Dai & Callan[3]研究了在不同排序数据集上整合这些语言模型（主要是BERT，但也包括ELMo [29]）。如图2（c）所示，常见的方法（也是Nogueira等人在MS MARCO和TREC CAR上的方法）是将查询和文档对输入到BERT中，并在BERT的[CLS]输出标记上使用一个MLP来生成相关性得分。随后，Nogueira等人[27]引入了duoBERT，这一模型微调了BERT以比较给定查询的两个文档的相关性。与单文档BERT相比，duoBERT在MS MARCO上获得了1%的MRR@10提升，但计算成本至少增加了1.4倍。

## 2.3. BERT Optimizations  

如第1节所述，这些基于语言模型的排序器在实际应用中可能非常昂贵。尽管在自然语言理解（NLU）领域中，关于BERT的蒸馏[14, 33]、压缩[40]和剪枝[19]的研究可以在缩小这一差距方面发挥重要作用，但由于其通用性质，这些方法通常无法达到我们为信息检索（IR）重新设计的架构所能实现的加速效果，而更激进的优化往往会以降低质量为代价。

## 2.4. Effcient NLU-based Models.  

最近，出现了一种方向，即离线执行昂贵的自然语言理解（NLU）计算。这包括doc2query [28]和DeepCT [2]。doc2query模型为每个文档扩展了一定数量的合成查询，这些查询由一个序列到序列（seq2seq）转换模型生成，该模型经过训练可以根据文档生成查询。然后，它依赖BM25索引从（扩展后的）文档中进行检索。DeepCT则使用BERT以上下文感知的方式生成BM25的词频组件，本质上通过神经网络实现了词独立假设[23]。最后，docTTTTTquery [26]与doc2query相同，但它微调了一个预训练模型（即T5 [31]）以生成预测的查询。

---

在我们撰写本文的同时，Hofstätter等人[11]发布了他们的Transformer-Kernel (TK)模型。总体而言，TK改进了之前描述的KNRM架构：KNRM在基于词嵌入的交互之上使用核池化，而TK则在核池化之前使用Transformer [34]组件对查询和文档进行上下文编码。TK在MS MARCO（开发集）上为非BERT模型设立了新的技术水平；然而，它的最佳非集成MRR@10仅为31%，而ColBERT可达36%。此外，由于离线索引文档表示并使用基于MaxSim的后期交互机制，ColBERT在可扩展性方面具有显著优势，能够支持端到端检索，而这是TK无法实现的。

# 3. COLBERT  

ColBERT提供了一个简单的框架，用于平衡神经信息检索（IR）中的质量和成本，特别是像BERT这样的深度语言模型。如前所述，延迟查询-文档交互可以通过预计算实现低成本的神经重排序，甚至支持实际的端到端神经检索（即通过向量相似性搜索进行剪枝）。ColBERT探讨了如何在仍能保持最新模型有效性的情况下实现这一点，而这些模型的大部分计算依赖于联合的查询-文档对。

---

尽管ColBERT的后期交互框架可以应用于各种架构（例如CNN、RNN、Transformer等），我们选择将本研究聚焦于双向Transformer编码器（即BERT），因为它们具有当前最高的有效性，但计算成本也非常高。

## 3.1. Architecture  

图3展示了ColBERT的总体架构，包括：(a) 查询编码器 $f_Q$，(b) 文档编码器 $f_D$，以及 (c) 后期交互机制。给定查询 $q$ 和文档 $d$，$f_Q$ 将 $q$ 编码为固定大小的嵌入集合 $E_q$，而 $f_D$ 将 $d$ 编码为另一个嵌入集合 $E_d$。关键在于，$E_q$ 和 $E_d$ 中的每个嵌入都是基于 $q$ 或 $d$ 中的其他词进行上下文化的。我们在第3.2节中详细介绍了基于BERT的编码器。

---

使用 $E_q$ 和 $E_d$，ColBERT通过后期交互计算 $q$ 和 $d$ 之间的相关性得分，我们将其定义为最大相似度（MaxSim）操作的求和。具体而言，我们计算每个 $v \in E_q$ 与 $E_d$ 中向量的最大余弦相似度，并通过求和结合这些输出。除了余弦相似度，我们还评估了平方L2距离作为向量相似度的度量。直观上，该交互机制在查询的上下文中“软性地”匹配每个查询词 $t_q$，并与文档的嵌入进行比对，通过 $t_q$ 和文档词 $t_d$ 之间的最大相似度得分量化“匹配”强度。基于这些词级得分，它通过对所有查询词的匹配证据求和来估计文档的相关性。

---

尽管可以通过深度卷积和注意力层等其他方法实现更复杂的匹配（即，典型的以交互为中心的模型），最大相似度计算的求和具有两个显著特征。首先，它是一种特别廉价的交互机制，我们在第4.2节中分析了其浮点运算量（FLOPs）。其次，更重要的是，它支持高效的前k项检索剪枝，我们在第4.3节中对此进行了评估。这使得可以使用向量相似度算法跳过文档，而无需具体化完整的交互矩阵，甚至不必逐个考虑每个文档。其他廉价的选择（例如相似度平均值的求和而非最大值）也可以实现；然而，许多方法不太适合剪枝。在第4.4节中，我们进行了广泛的消融研究，实证验证了基于MaxSim的后期交互相较于其他替代方案的优势。

## 3.2. Qery & Document Encoders  

在后期交互之前，ColBERT使用基于BERT的编码器将每个查询或文档编码为一组嵌入。我们在查询和文档编码器之间共享一个BERT模型，但通过在查询前添加特殊标记 [Q]、在文档前添加标记 [D] 来区分对应于查询和文档的输入序列。

### 3.2.1. Qery Encoder.  

给定一个文本查询 $q$，我们将其分词为基于BERT的WordPiece [35]标记 $q_1 q_2 \ldots q_l$。我们在查询前添加标记 [Q]，并将其放在BERT的序列起始标记 [CLS] 之后。如果查询少于预定义的标记数量 $N_q$，我们用BERT的特殊 [mask] 标记填充至长度 $N_q$（否则，我们截取前 $N_q$ 个标记）。这个填充后的输入标记序列随后被传入BERT的深层Transformer架构，BERT计算出每个标记的上下文化表示。

---

我们将这种用掩码标记填充的操作称为查询增强，这一步使BERT能够在对应于这些掩码的位置生成基于查询的嵌入。查询增强旨在提供一种软性、可微的机制，用于学习通过新词扩展查询或根据匹配查询的相关性重新加权现有词。正如我们在第4.4节所示，这一操作对于ColBERT的效果至关重要。

---

给定BERT对每个标记的表示，我们的编码器将上下文化的输出表示传递通过一个没有激活函数的线性层。该层用于控制ColBERT嵌入的维度，生成的嵌入为$m$维，其输出大小即为$m$。稍后我们会更详细地讨论这一点，通常我们将$m$设定为远小于BERT的固定隐藏维度。

---

虽然ColBERT的嵌入维度对查询编码的效率影响有限，但这一步对于控制文档的空间占用至关重要，正如我们在第4.5节中所展示的。此外，它对查询执行时间的影响也非常显著，尤其是在将文档表示从系统内存（在查询处理前文档表示存放于此）传输到GPU所需的时间。事实上，正如我们在第4.2节中所展示的，将嵌入从CPU传输到GPU的收集、堆叠和传输可能是ColBERT重新排序中最耗费的步骤。最后，输出嵌入进行归一化，使每个嵌入的L2范数等于1，这样任意两个嵌入的点积相当于它们的余弦相似度，范围为 $[-1,1]$。

### 3.2.2. Document Encoder.

我们的文档编码器具有非常类似的架构。我们首先将一个文档 $d$ 分割成其组成标记 $d_1 d_2 \ldots d_m$，并在前面添加BERT的起始标记 [CLS]，然后添加我们表示文档序列的特殊标记 [D]。与查询不同，我们不会在文档后面添加 [mask] 标记。将这个输入序列通过BERT和后续的线性层后，文档编码器会过滤掉对应于标点符号的嵌入，这些符号根据一个预定义的列表确定。此过滤旨在减少每个文档的嵌入数量，因为我们假设（即便是上下文化的）标点符号嵌入对效果来说是不必要的。

---

总之，给定 $q=q_0 q_1 \ldots q_l$ 和 $d=d_0 d_1 \ldots d_n$，我们按照以下方式计算嵌入集 $E_q$ 和 $E_d$，其中 # 表示 [mask] 标记。

- $E_q:=\operatorname{Normalize}\left(\mathrm{CNN}\left(\operatorname{BERT}\left(\text{‘‘}[Q] q_0 q_1 \ldots q_l \# \# \ldots \ldots \# \text{’’}\right)\right)\right)$ 
- $E_d:=\text { Filter( Normalize( CNN( BERT(}\text{‘‘}[D]  d_0 d_1 \ldots d_n \text{’’}))))$ 

## 3.3. Late Interaction  

给定查询 $q$ 和文档 $d$ 的表示，$d$ 对于 $q$ 的相关性得分，记作 $S_{q, d}$，是通过它们的上下文化嵌入集合之间的后期交互估算的。如前所述，这通过最大相似度计算的求和实现，具体而言是余弦相似度（由于嵌入归一化，实际实现为点积）或平方L2距离。

- $\displaystyle{}S_{q, d}:=\sum_{i \in\left[\left|E_q\right|\right]} \max _{j \in\left[\left|E_d\right|\right]} E_{q_i} \cdot E_{d_j}^T$ 

---

ColBERT是端到端可微的。我们微调BERT编码器，并使用Adam [16]优化器从头训练额外的参数（即线性层和 [Q]、[D] 标记的嵌入）。请注意，我们的交互机制没有可训练的参数。给定一个三元组 $\left\langle q, d^{+}, d^{-}\right\rangle$，其中 $q$ 是查询，$d^{+}$ 是正样本文档，$d^{-}$ 是负样本文档，ColBERT用于分别为每个文档生成得分，并通过 $d^{+}$ 和 $d^{-}$ 的计算得分上的成对softmax交叉熵损失进行优化。

## 3.4. Offline Indexing: Computing \& Storing Document Embeddings

ColBERT的设计隔离了查询和文档之间几乎所有的计算，主要目的是实现文档表示的离线预计算。从总体上看，我们的索引过程相对简单：我们对集合中的文档按批次进行处理，运行文档编码器 $f_D$ 并存储每个文档的输出嵌入。虽然对一组文档进行索引是离线的过程，但我们引入了一些简单的优化以提高索引的吞吐量。正如我们在第4.5节所示，这些优化可以显著降低索引的离线成本。

---

首先，我们利用多GPU（若可用）并行加速批次文档的编码。在批处理时，我们将所有文档填充至批次中最长文档的长度。为了使每批次的序列长度限制更为有效，我们的索引器以 $B$（例如，$B=100,000$）为单位分组处理文档，并按长度对这些文档进行排序，然后将长度相近的 $b$（例如，$b=128$）个文档分批送入编码器。基于长度的分组方法在某些库中被称为BucketIterator（例如，allenNLP）。最后，虽然大部分计算发生在GPU上，我们发现索引时间中有相当一部分用于文本序列的预处理，主要是BERT的WordPiece分词。利用批次内各文档间的独立性，我们在可用的CPU核心上并行化预处理。

---

一旦生成了文档表示，它们就会以32位或16位的值来表示每个维度并保存到磁盘中。正如我们在第3.5和3.6节中描述的，这些表示可以直接从磁盘加载用于排序，或者进一步索引以进行向量相似性搜索。

## 3.5. Top- $k$ Re-ranking with ColBERT

回顾一下，ColBERT可以用于对另一种检索模型（通常是基于词项的模型）的输出进行重新排序，也可以直接从文档集合中进行端到端检索。在本节中，我们讨论如何使用ColBERT对给定查询 $q$ 的一小组 $k$（例如，$k=1000$）个文档进行排序。由于 $k$ 较小，我们依赖批量计算对每个文档进行穷尽评分（与第3.6节中的方法不同）。首先，我们的查询服务子系统将已索引的文档表示加载到内存中，将每个文档表示为一个嵌入矩阵。

---

给定查询 $q$，我们计算其上下文化的嵌入集合 $E_q$（见公式1），并同时将文档表示收集到一个由 $k$ 个文档矩阵组成的三维张量 $D$ 中。我们将 $k$ 个文档填充到最大长度以便于批量操作，并将张量 $D$ 移动到GPU的内存中。在GPU上，我们计算 $E_q$ 和 $D$ 的批量点积，可能会分为多个小批次。输出结果是一个三维张量，包含了 $q$ 与每个文档之间的交叉匹配矩阵。为了计算每个文档的得分，我们通过最大池化在文档词项上对其矩阵进行归约（即实现我们的MaxSim计算的穷尽实现），并在查询词项上进行求和归约。最后，我们按总得分对 $k$ 个文档进行排序。

---

与现有的神经排序器（尤其是基于BERT的排序器）相比，这种计算非常便宜。事实上，其成本主要集中在收集和传输预计算的嵌入上。举例来说，通过典型的BERT排序器对 $k$ 个文档进行排序，需要将查询 $q$ 和文档 $d_i$ 的 $k$ 个不同输入（长度为 $l=|q|+\left|d_i\right|$）输入到BERT中，其中注意力机制的成本随序列长度呈二次增长。相比之下，ColBERT只将一个更短的序列（长度 $l=|q|$）输入到BERT中。因此，ColBERT不仅成本更低，而且在 $k$ 增加时具有更好的扩展性，正如我们在第4.2节中所讨论的那样。

## 3.6. End-to-end Top- $k$ Retrieval with ColBERT

如前所述，ColBERT的后期交互操作专门设计用于从大型文档集合中进行端到端检索，主要是为了相较于基于词项的检索方法提升召回率。本节关注的情形是，当需要排序的文档数量过多以致无法对每个候选文档进行穷尽性评估时，尤其是当我们只对得分最高的文档感兴趣的情况。具体来说，我们在这里聚焦于直接从包含 $N$（例如，$N=10,000,000$）个文档的大型文档集合中检索前 $k$ 个结果，其中 $k \ll N$。

---

为此，我们利用了后期交互中MaxSim操作的剪枝友好特性。我们并非在一个查询嵌入和一个文档的所有嵌入之间应用MaxSim，而是利用快速向量相似性数据结构在查询嵌入和整个集合中的所有文档嵌入之间高效地进行搜索。为此，我们使用了一个用于大规模向量相似性搜索的现成库，即Facebook的faiss [15]。在离线索引结束时（第3.4节），我们维护了每个嵌入与其所属文档的映射，并将所有文档嵌入索引到faiss中。

---

随后，在查询服务阶段，我们使用一个两阶段流程从整个集合中检索前 $k$ 个文档。两阶段都依赖于ColBERT的评分机制：第一阶段是一个近似过滤阶段，第二阶段是一个精细化阶段。在第一阶段中，我们并行向faiss索引发出 $N_q$ 个向量相似性查询（对应于 $E_q$ 中的每个嵌入），从而在所有文档嵌入中检索该向量的前 $k^{\prime}$（例如，$k^{\prime}=k/2$）个匹配项。我们将每个匹配项映射回其所属文档，生成 $N_q \times k^{\prime}$ 个文档ID，其中只有 $K \leq N_q \times k^{\prime}$ 个是唯一的。这些 $K$ 个文档很可能包含一个或多个与查询嵌入高度相似的嵌入。在第二阶段，我们仅对这些 $K$ 个文档进行穷尽性重新排序，以精确化结果，方法如第3.5节中所述。

---

在基于faiss的实现中，我们使用了IVFPQ索引（“倒排文件和乘积量化”）。该索引基于 $k$ 均值聚类将嵌入空间划分为 $P$（例如，$P=1000$）个单元，并根据选择的向量相似度度量将每个文档嵌入分配到其最近的单元。在查询服务阶段，当为一个查询嵌入搜索前 $k^{\prime}$ 个匹配项时，仅搜索最近的 $p$（例如，$p=10$）个分区。为提高内存效率，每个嵌入被划分为 $s$（例如，$s=16$）个子向量，每个子向量使用一个字节表示。此外，索引在这个压缩域中进行相似性计算，从而降低了计算成本，并加速了搜索。

# 4. EXPERIMENTAL EVALUATION  

我们现在开始对ColBERT进行实证测试，回答以下研究问题。

$\mathbf{RQ}_1$：在典型的重新排序设置中，ColBERT在多大程度上可以弥合（第1节中指出的）高效和高效性神经模型之间的差距？（§4.2）

$\mathbf{RQ}_2$：除了重新排序，ColBERT能否有效地支持从大型集合中直接进行端到端检索？（§4.3）

$\mathbf{RQ}_3$：ColBERT的每个组成部分（例如，后期交互、查询增强）对其质量分别有何贡献？（§4.4）

$\mathbf{RQ}_4$：ColBERT在离线计算和内存开销方面的索引相关成本是多少？（§4.5）

## 4.1. Methodology  

## 4.1.1. Datasets & Metrics  

与相关研究[2, 27, 28]类似，我们在MS MARCO Ranking[24]（以下简称MS MARCO）和TREC Complex Answer Retrieval（TREC-CAR）[6]数据集上进行了实验。这两个最新数据集都提供了大规模的训练数据，有助于对深度神经网络进行训练和评估。下面详细介绍这两个数据集。

#### 4.1.1.1. MS MARCO.  

MS MARCO是由微软在2016年推出的一个用于阅读理解的数据集（和相应的竞赛），并在2018年被改编用于检索。它包含了来自网页的8.8百万段文本，这些文本是从Bing对100万真实查询的结果中收集的。每个查询都与一个（或极少数）被标记为相关的文档关联，并且没有明确标记为不相关的文档。根据官方评估，我们使用MRR@10来衡量效果。

---

我们使用三个查询集进行评估。官方的开发集和评估集包含大约7000个查询。然而，评估集的相关性判断由微软保留，只有向竞赛组织者提交结果后才能获得效果评估。我们提交了主要的重新排序ColBERT模型以获得第4.2节中的结果。此外，该集合还包括大约55,000个查询（带标签），作为额外的验证数据。我们从中随机抽取了5000个查询（即不在我们的开发集或训练集中的查询）作为“本地”评估集。我们使用此保留集以及官方开发集来测试我们的模型和第4.3节中的基线模型。这样做是为了避免一次提交多个模型变体，因为组织者不鼓励同一团队过多提交。

#### 4.1.1.2. TREC CAR.  

TREC CAR由Dietz等人[6]于2017年推出，这是一个基于维基百科的合成数据集，包含约2900万段文本。与相关研究[25]类似，我们使用了五个预定义折叠中的前四个进行训练，第五个用于验证。这样可以生成大约300万条查询，方法是将维基页面标题与其中一个章节的标题拼接起来。该章节的段落被标记为与相应查询相关。我们的评估在TREC 2017 CAR测试集上进行，该测试集包含2,254个查询。

### 4.1.2 Implementation.  

我们的ColBERT模型使用Python 3和PyTorch 1实现。预训练的BERT模型使用了流行的transformers库${ }^5$。与[25]类似，我们将所有ColBERT模型微调的学习率设为 $3 \times 10^{-6}$，批量大小为32。我们将每个查询的嵌入数量固定为 $N_q=32$，并将ColBERT嵌入维度 $m$ 设为128；第4.5节展示了ColBERT在宽范围嵌入维度下的鲁棒性。

---

对于MS MARCO，我们使用Google官方的预训练BERT ${ }*{\text {base }}$ 模型初始化ColBERT查询和文档编码器中的BERT组件。此外，我们将所有模型训练200,000次迭代。对于TREC CAR，我们遵循相关研究[2, 25]，使用不同于官方的预训练模型。具体来说，官方的BERT模型是在维基百科上预训练的，而TREC CAR的训练集和测试集正是来源于此。为避免测试数据泄露到训练中，Nogueira和Cho[25]在对应于TREC CAR训练子集的Wiki页面上，对随机初始化的BERT模型进行预训练。他们发布了其预训练的BERT ${ }*{\text {large }}$ 模型，我们将其用于TREC CAR的ColBERT实验的微调。由于微调该模型的速度显著慢于 $\mathrm{BERT}_{\text {base }}$，我们在TREC CAR上的训练仅进行125,000次迭代。

---

在我们的重新排序结果中，除非另有说明，否则我们在嵌入中每个维度使用4字节，并采用余弦作为向量相似度函数。对于端到端排序，我们使用（平方）L2距离，因为我们发现faiss索引在基于L2的检索上速度更快。对于我们的faiss索引，我们将分区数设为 $P=2,000$，并搜索每个查询嵌入的最近 $p=10$ 个分区，以检索每个查询嵌入的 $k^{\prime}=k=1000$ 个文档向量。我们将每个嵌入分为 $s=16$ 个子向量，每个子向量使用1字节编码。为了表示用于端到端检索过程第二阶段的索引，我们在每个维度使用16位值。

### 4.1.3 Hardware & Time Measurements.  

为了评估神经重新排序模型在第4.2节中的延迟，我们使用了一台配备单个Tesla V100 GPU（32 GiB内存）的服务器，服务器还配有两个Intel Xeon Gold 6132 CPU，每个CPU有14个物理核心（24个超线程），以及469 GiB的RAM。对于第4.3节中主要基于CPU的检索实验和第4.5节中的索引实验，我们使用另一台具有相同CPU和系统内存配置的服务器，但其配备了四个Titan V GPU，每个GPU具有12 GiB的内存。在所有实验中，每次检索仅分配一个GPU处理一个查询（即，对具有神经计算的方法），但在索引过程中我们最多使用四个GPU。

## 4.2. Quality-Cost Tradeoff: Top- $k$ Re-ranking

在本节中，我们评估ColBERT在重新排序由词袋模型检索的前 $k$ 个结果时的效率和效果，这是测试和部署神经排序模型的最常见设置。我们从MS MARCO数据集开始。我们与KNRM、Duet和fastText+ConvKNRM进行了比较，这些都是之前在MS MARCO上测试过的代表性神经匹配模型。此外，我们还与Nogueira和Cho [25]提出的BERT排序模型进行了比较，特别是$\mathrm{BERT}*{\text {base }}$及其更深的版本$\mathrm{BERT}*{\text {large }}$。我们还报告了“BERT ${ }_{\text {base }}$ (our training)”的结果，该模型基于Nogueira和Cho的基础模型（包括超参数），但使用与ColBERT相同的损失函数（第3.3节）进行训练，共训练200,000次迭代，从而可以更直接地比较结果。

---

我们在验证集（Dev）和评估集（Eval）上报告了该竞赛的官方指标，即MRR@10。我们还报告了重新排序的延迟（使用单个Tesla V100 GPU测量）以及每个神经排序模型每个查询的FLOPs。对于ColBERT，我们报告的延迟包括了从收集文档表示、将其移至GPU、对查询进行分词和编码、以及应用后期交互计算文档得分的整个计算过程。对于基线模型，我们仅测量在GPU上的评分计算，并排除基于CPU的文本预处理（类似于[9]）。原则上，基线模型可以在离线预计算大部分预处理（例如，文档分词），并在线上在文档间并行化其余部分，因此成本几乎可以忽略不计。我们使用torchprofile ${ }^6$库估算了每个模型每个查询的FLOPs。

---

我们现在开始分析结果，见表1。首先，我们注意到从2017年的KNRM到2019年的基于BERT的模型的快速进步，在MRR@10上增加了超过16%。如第1节所述，计算成本的同步增加显而易见。从成本和效果呈单调增加的模式来看，这些结果似乎表明，高质量的排序需要昂贵的模型。

---

与这一趋势相反，ColBERT（在$\mathrm{BERT}*{\text {base }}$之上进行后期交互）在效果上并不逊色于Nogueira和Cho [25, 27]对BERT ${ }*{\text {base }}$ 的排序改编，且仅略微低于$\mathrm{BERT}*{\text {large }}$ 和我们训练的$\mathrm{BERT}*{\text {base }}$（见上文）。在效果上竞争力极高的同时，ColBERT的成本比BERT ${ }_{\text {base }}$低了几个数量级，尤其是在延迟上减少了170多倍，在FLOPs上减少了13,900倍。这突显了我们提出的后期交互机制的表达能力，尤其是与像BERT这样的强大预训练语言模型相结合时。尽管ColBERT的重新排序延迟比非BERT重新排序模型稍高（即高出几十毫秒），但这一差异可以通过收集、堆叠和传输文档嵌入到GPU的时间来解释。具体来说，ColBERT中的查询编码和交互仅耗时13毫秒。我们注意到，通过将查询填充至更短长度、使用更小的向量维度（其MRR@10在第4.5节中测试）、对文档向量进行量化、并在内存充足时将嵌入存储在GPU上，ColBERT的延迟和FLOPs可以大幅降低。我们将这些方向留待未来研究。

---

更深入地探讨BERT与ColBERT在质量和成本之间的权衡，图4展示了在BM25的前 $k$ 个结果中重新排序时，以重新排序深度 $k$ 为函数的FLOPs和效果（MRR@10）关系，对比了ColBERT和 $\mathrm{BERT}_{\text {base }}$（我们训练的版本）。我们在MS MARCO的开发集上进行了此实验。需要注意的是，由于官方的前1000名排名未提供BM25顺序（且每个查询仅包含前1000个文档），本实验中的模型重新排序的是Anserini [37]工具包的BM25输出。因此，$k=1000$时的MRR@10值略高于表1中的报告值。

---

分析图4中的结果，我们注意到ColBERT不仅比BERT便宜得多（对于相同的模型大小，即12层“base”Transformer编码器），它在排序文档数量增加时的扩展性也更好。部分原因在于ColBERT只需对查询进行一次处理，而与评估的文档数量无关。例如，当 $k=10$ 时，BERT所需的FLOPs几乎是ColBERT的180倍；当 $k=1000$ 时，BERT的开销跃升至13,900倍，达到 $k=2000$ 时则增至23,000倍。事实上，我们的非正式实验表明，这种数量级的FLOPs差距使得在CPU上完全运行ColBERT成为可能，尽管基于CPU的重新排序不在我们的研究范围内。

---

在研究了MS MARCO上的结果后，我们接下来考虑TREC CAR，其官方指标为MAP。表3总结了结果，除了在该数据集上测试的重新排序基线外，还包括了一些重要的基线（BM25、doc2query和DeepCT）。这些结果与MS MARCO上的结果直接对应。

## 4.3. End-to-end Top- $k$ Retrieval

除了廉价的重新排序，ColBERT还可以直接从完整集合中进行前 $k$ 项检索。表2展示了完整检索的结果，其中每个模型直接从MS MARCO的880万文档中为每个查询检索前1000个文档。除了MRR@10和毫秒级延迟，表中还报告了Recall@50、Recall@200和Recall@1000，这些是完整检索模型的重要指标，实际上是按查询从一个大型集合中筛选出相关文档。

---

我们与BM25进行了对比，特别是MS MARCO官方的BM25排名以及基于Anserini工具包的调优基线${ }^7$。尽管存在许多其他传统模型，但我们不知道有任何模型能够显著超越Anserini的BM25实现（例如，参见[28]中的RM3、[2]中的LMDir，或排行榜上微软的基于特征的RankSVM）。

---

我们还与doc2query、DeepCT和docTTTTquery进行了比较。三者均依赖于传统的词袋模型（主要是BM25）进行检索。然而，重要的是，它们在构建BM25索引之前，对每个文档的词频进行重新加权和/或扩展每个文档的词项集合。具体而言，doc2query使用序列到序列转换模型为每个文档扩展了预定义数量的合成查询（docTTTTquery则用预训练语言模型T5 [31]替代了此模型）。相对而言，DeepCT使用BERT以上下文感知的方式生成BM25的词频组件。

---

对于Anserini的BM25、doc2query和docTTTTquery的延迟，我们使用了作者[26, 28]基于Anserini的实现。虽然该实现支持多线程，但仅在不同查询之间实现了并行化。因此，我们报告了这些模型的单线程延迟，并指出通过对索引分片进行并行计算可以显著降低其已经很低的延迟。对于DeepCT，我们仅通过BM25的延迟（在表中以(est.)表示）来估算其延迟，因为DeepCT仅重新加权BM25的词频，未对索引做其他修改${ }^8$。如第4.1节所述，对于端到端检索，我们使用ColBERT$_{\mathrm{L}2}$，它采用负平方L2距离作为向量相似度函数。对于其延迟，我们测量了基于faiss的候选筛选和后续重新排序的时间。在该实验中，faiss使用了所有可用的CPU核心。

---

观察表2，我们首先看到Anserini的BM25基线，其MRR@10为18.7，延迟非常低，这是因为Anserini基于著名的Lucene系统实现了非常廉价的操作，并得益于多年来对词袋模型的前 $k$ 项检索优化。接下来的三个基线，即doc2query、DeepCT和docTTTTquery，每一个都显著提升了效果。这些改进几乎不增加延迟，因为它们最终依赖于BM25检索。这三者中效果最好的docTTTTquery通过微调最新的语言模型T5，在原始BM25上取得了9%的巨大提升。

---

将注意力转向ColBERT的端到端检索效果，我们看到其在MRR@10上对所有这些端到端模型都有显著提升。事实上，端到端设置下使用ColBERT在MRR@10上的效果优于使用相同模型的重新排序，这得益于更高的召回率。除了MRR@10，我们在Recall@50、Recall@200和Recall@1000上也看到了显著提升。例如，ColBERT的Recall@50实际上超过了官方BM25的Recall@1000，甚至在Recall@200上也仅次于docTTTTquery，突显了使用ColBERT进行端到端检索（而不仅仅是重新排序）的价值。

## 4.4. Ablation Studies  

第4.2节的结果表明，尽管ColBERT的后期交互机制简单且成本低廉，但它的效果非常出色。为了更好地理解其高效性的来源，我们分析了ColBERT交互和编码器架构中的一些重要细节。对于此次消融实验，我们在图5中报告了MS MARCO验证集上的MRR@10，展示了我们的主要重新排序ColBERT模型[E]，其MRR@10为34.9%。

---

由于训练所有模型的成本较高，我们训练了一个主要模型的副本，仅保留了BERT的12层中的前5层（即模型[D]），并以五层BERT的配置训练了所有消融模型共200,000次迭代。首先，我们探讨后期交互中的精细交互是否必要。模型[A]处理了这个问题：它使用BERT为查询生成一个单一嵌入向量，为文档生成另一个嵌入向量，这些嵌入从BERT的[CLS]上下文化嵌入中提取，并通过线性层扩展到4096维（等于 $N_q \times 128=32 \times 128$）。相关性估算为查询和文档嵌入的内积，我们发现对于单向量重新排序，内积的效果优于余弦相似度。结果显示，该模型效果明显低于ColBERT，强调了后期交互的重要性。

---

接下来，我们探讨基于MaxSim的后期交互是否优于其他简单的替代方案。我们测试了模型[B]，用平均相似度替换了ColBERT的最大相似度。结果表明查询中的各个词对文档中特定词给予特别关注的重要性。同样，图中也突出了查询增强机制的重要性：没有查询增强的模型[C]的MRR@10明显较低。最后，我们看到端到端检索不仅对召回率有影响，对MRR@10也有影响。通过直接从完整集合中检索，ColBERT能够检索到BM25前1000名中完全遗漏的前10个文档。

## 4.5. Indexing Throughput & Footprint

最后，我们分析了ColBERT的索引吞吐量和空间占用。图6展示了在MS MARCO文档上，ColBERT及另外四种消融设置的索引吞吐量，这些设置在基本批处理索引的基础上，分别启用了第3.4节中描述的优化。基于这些吞吐量，ColBERT大约可以在三小时内完成MS MARCO的索引。需要注意的是，任何基于BERT的模型都必须承担至少一次处理每个文档的计算成本。虽然ColBERT对每个文档仅进行一次BERT编码，但现有的基于BERT的排序器可能会对每个查询的数百个文档重复类似计算。

---

表4报告了在不同设置下ColBERT的空间占用情况，我们在嵌入维度和/或每个维度的字节数上进行了减少。值得注意的是，最节省空间的设置，即使用余弦相似度进行重新排序、并将24维向量存储为2字节浮点数的设置，其MRR@10仅比占用空间最多的设置低1%，而前者仅需27 GiB来表示MS MARCO集合。

# 5. CONCLUSIONS  

在本文中，我们介绍了ColBERT，这是一种新型排序模型，使用深度语言模型（特别是BERT）进行上下文化的后期交互以实现高效检索。通过将查询和文档分别编码为细粒度表示，并通过廉价且易于剪枝的计算方式进行交互，ColBERT能够利用深度语言模型的表达能力，同时大大加快查询处理。此外，这使得ColBERT能够直接从大型文档集合中进行端到端神经检索。我们的结果显示，ColBERT比现有的基于BERT的模型快170多倍，每查询FLOPs减少了14,000倍，同时质量几乎没有影响，并且效果优于所有非BERT基线。