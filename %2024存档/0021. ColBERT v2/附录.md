# A. Analysis of ColBERT’s Semantic Space

ColBERT（Khattab和Zaharia，2020）在词级别分解表示和相似度计算。基于这种组合架构，我们假设ColBERT展示了一个“轻量级”语义空间：即使不进行特别的重新训练，每个词义对应的向量会非常接近，仅因上下文而产生轻微变化。

---

如果该假设成立，我们预计词汇中每个词的嵌入将局限于嵌入空间中的少数区域，对应于词的上下文“词义”。为验证该假设，我们分析了MS MARCO Passage Ranking（Nguyen等人，2016）集合中对应词的ColBERT嵌入：我们对近6亿个嵌入（对应27,000个唯一词）进行了 $k$-均值聚类，分为 $k=2^{18}$ 个簇。作为对比，我们使用随机嵌入重复该聚类过程，但保持词的真实分布。图2展示了每个簇中出现的非停用词数（图2a）和每个词出现在的不同簇数（图2b）的经验累积分布函数（eCDF）图${ }^6$。大多数词出现在非常少的质心数量中：特别是，我们观察到约90%的簇包含 $\leq 16$ 个不同的词，ColBERT嵌入，而随机嵌入的情况下，少于50%的簇包含 $\leq 16$ 个不同词。这表明质心有效地映射了ColBERT的语义空间。

---

表6展示了一些质心捕捉的语义空间示例。簇#917中最常见的词与摄影相关，例如“photos”和“photographs”。若进一步检查这些词出现的其他簇，会发现这些新簇之间有显著的语义重叠（例如，Photos-Photo、Photo-Image-Picture）和簇#917。我们在簇#216932中出现的龙卷风相关词也观察到类似的效果。

---

该分析表明质心能够高精度地概括ColBERT的表示。在第3.3节中，我们提出了一种利用这些质心并在维度级别进行细微调整的残差压缩机制，以有效编码后期交互向量。

# B. Impact of Compression  

我们的残差压缩方法（第3.3节）大致保留了未压缩嵌入的质量。具体而言，应用于MS MARCO上的原始ColBERT模型时，其MRR@10为36.2%，Recall@50为82.1%；采用2位压缩后，模型的MRR@10为36.2%，Recall@50为82.3%。在1位压缩时，模型达到35.5%的MRR@10和81.6%的Recall@50。

- 我们与ColBERT的早期压缩实现对比，该实现使用类似BPR（Yamada等人，2021a）的二进制表示，没有使用残差质心，1位（二进制）和2位（二进制）压缩分别实现了34.8%（35.7%）的MRR@10和80.5%（81.8%）的Recall@50。与原始ColBERT一样，这种压缩形式依赖于一个单独的FAISS索引来生成候选项。

---

我们还在执行下游任务的后期交互检索器上测试了残差压缩方法，即ColBERT-QA（Khattab等人，2021b）用于开放域问答任务NaturalQuestions和Baleen（Khattab等人，2021a）用于HoVer上的多跳推理以验证声明。在NQ开发集上，ColBERT-QA的成功率@5（成功率@20）仅略微下降，从75.3%（84.3%）降至74.3%（84.2%），而使用2位压缩检索时，开放问答答案精确匹配从47.9%降至47.7%。

---

类似地，在HoVer（Jiang等人，2020）开发集上，Baleen的检索R@100从92.2%下降到90.6%，但其句子级别精确匹配几乎保持不变，从39.2%升至39.4%。我们假设ColBERTv2中应用的监督方法（第3.2节）也可以用于提升下游任务的质量，通过提高这些任务的检索召回率。我们将此类探索留待未来研究。

# C. Retrieval Latency  

图3评估了ColBERTv2在不同规模的三个集合上的延迟表现，即MS MARCO、LoTTE Pooled（dev）和LoTTE Lifestyle（dev），分别包含约900万段落、240万答案帖子和27万答案帖子。我们在MS MARCO开发集和LoTTE“搜索”查询的三次运行中平均计算延迟。搜索在具有两颗Intel Xeon Gold 6132 CPU的服务器上执行，每颗CPU有28个硬件执行上下文，使用Titan V GPU。

---

图中展示了ColBERTv2的三种设置，特别是我们评估了1位和2位编码（第3.4节）的索引，以及查询向量探测最近1、2或4个质心（第3.5节）时的搜索情况。在每个向量探测probe个质心时，我们为每个查询评分probe $\times 2^{12}$或probe $\times 2^{14}$个候选项${ }^8$。

---

首先，我们注意到$x$轴上报告的质量范围相对较窄。例如，MS MARCO的质量范围从38.50到39.75，除了两个成本最低的设置外，所有设置的得分都在39.00以上。同样，$y$轴的范围在每次查询大约50毫秒到250毫秒之间（大多数低于150毫秒），这使用了我们相对简单的基于Python的实现。

---

进一步深入分析，我们看到在这三个数据集中，最优质量或接近最佳质量的结果均可在大约100毫秒的延迟内实现，尽管它们的规模和特性各异。2位索引稳定地优于1位索引，但更激进的压缩导致的质量损失很小。

# D. LoTTE  

## D.1. Domain coverage  

表9展示了LoTTE开发数据集中社区的完整分布。LoTTE所覆盖的主题涵盖了广泛的语言现象，反映了多样化的主题和社区。然而，由于所有帖子都是匿名用户提交的，我们没有关于贡献者身份的具体人口统计信息。所有帖子均以英语书写。

## D.2. Passages  

如第4节所述，我们通过从StackExchange存档中选择评分为正的段落来构建LoTTE集合。我们从段落中移除HTML标签并过滤掉空段落。对于每个段落，我们记录其对应的查询并保存查询到段落的映射，以跟踪与每个查询对应的已发布答案。

## D.3. Search queries  

我们从GooAQ中抽取出现在StackExchange帖子存档中的查询列表，构建LoTTE搜索查询。首先对GooAQ查询列表进行随机排序，这样在同一答案段落对应多个查询的情况下，我们可以随机选择要包含在LoTTE中的查询，而不是总是选择第一个出现的查询。我们确认每个查询至少有一个对应的答案段落。

## D.4. Forum queries  

对于每个LoTTE主题及其组成社区，首先计算每个社区占总查询量的比例。然后我们根据此分布构建一个截断的查询集，从每个社区中选择得分最高的查询，这些查询按1）查询评分和2）查询浏览量排序。我们仅使用具有被接受答案的查询。尽可能确保每个社区至少为截断集贡献50个查询。我们将截断集的总体大小设置为2000个查询，但由于四舍五入和/或每个社区的最低查询数要求，总数可能会超过2000。我们移除所有引号和HTML标签。

## D.5. Statistics   

图4绘制了每个LoTTE开发语料库中每个段落的单词数量。图5和图6分别绘制了每个查询的单词数量和对应答案段落数量，分为搜索查询和论坛查询。 

## D.6. Dev Results  

表7展示了LoTTE开发查询的域外评估结果。延续我们在第5节观察到的趋势，ColBERTv2在所有测试的模型中表现出色，持续优于其他模型。

## D.7. Licensing and Anonymity  

原始StackExchange帖子存档受知识共享署名-相同方式共享4.0许可协议（CC BY-SA 4.0）许可（sta）。在上传前，存档中的个人数据已被移除，尽管所有帖子均为公开的；当我们公开发布LoTTE时，将包含指向原始帖子的URL，以满足许可要求的适当归属。GooAQ数据集受Apache 2.0许可（Khashabi等人，2021）。我们也将以CC BY-SA 4.0许可发布LoTTE。根据GooAQ许可，搜索查询仅限用于非商业研究目的。

# E. BEIR数据集

表8列出了我们在评估中使用的BEIR数据集，包括其相应的许可信息、文档数量和测试集查询数量。关于每个数据集的详细描述，请参考Thakur等人（2021）。

---

我们的Touché评估使用了BEIR中的更新数据版本，用于评估我们运行的模型（即ColBERTv2和RocketQAv2）以及SPLADEv2。

---

我们还在开放问答基准NQ、TQ和SQuAD上进行了测试，每个基准大约有9000个开发集问题，以及多跳HoVer，其开发集中包含4000个声明。在压缩评估第B节中，我们使用了在NQ和HoVer域内训练的模型，这些训练集分别包含79,000个和18,000个查询。

# F. 实现与超参数

我们使用Python 3.7、PyTorch 1.9和HuggingFace Transformers 4.10（Wolf等人，2020）实现了ColBERTv2，基于Khattab和Zaharia（2020）最初的ColBERT实现进行扩展。我们使用FAISS 1.7（Johnson等人，2019）进行 $k$-均值聚类，但与ColBERT不同，我们未使用其进行最近邻搜索。相反，我们使用Python中的PyTorch原语实现了候选生成机制（第3.5节）。

---

我们在内部集群上进行了实验，通常在每个推理任务（如索引、计算蒸馏得分和检索）上使用多达四个12GB的Titan V GPU，训练时则使用四个80GB的A100 GPU。对于内存较小的GPU，可通过梯度累积实现。借助此基础设施，计算蒸馏得分的时间不到一天，在MS MARCO上以64路模型进行40万步训练约需五天，而索引约需两小时。我们大致估算在几个月的工作期内，实验、开发和评估的总耗时上限为20个GPU月。

---

与ColBERT一样，我们的编码器是一个共享的bert-base-uncased模型，用于查询和段落编码器，包含1.1亿个参数。我们保留Khattab和Zaharia（2020）建议的默认向量维度以及后续工作中使用的维度，即 $d=128$。本文报告的实验在MS MARCO训练集上进行。学习率 $\left(10^{-5}\right)$、批量大小（32个样本）和线性衰减的暖启动（20,000步）采用简单默认值，仅进行了有限的手动探索。

---

检索相关的超参数探索详见第C节。默认情况下，探测数设为probe $=2$，但在MS MARCO和Wikipedia等大型数据集上使用probe $=4$。默认设置下候选数为probe $* 2^{12}$，但在Wikipedia上设置为probe $* 2^{13}$，在MS MARCO上设置为probe $* 2^{14}$。我们将更广泛的超参数调优留待未来工作。

---

我们在MS MARCO上进行64路蒸馏训练，从每个查询的前500个检索段落中采样。MS MARCO的训练集包含约80万个查询，但只有约50万有标签。我们对全部80万查询应用蒸馏，每个训练样本包含一个“正样本”，定义为Token为正的段落或由跨编码器教师得分最高的段落，无论其标签如何。

---

我们进行了40万步训练，从经过预微调的检查点开始，使用32路训练样本和15万步。为生成每个训练查询的top-k段落，我们进行了两轮训练，遵循Khattab等人（2021b）。我们从硬三元组训练模型开始（类似于Khattab等人，2021b），然后进行蒸馏训练，最后使用蒸馏模型进行第二轮训练。初步实验表明，该初始化和两轮训练对质量影响较小，这表明可以避免它们以降低训练成本。

---

除非另有说明，展示的结果均代表单次运行。第3节中的延迟结果为三次运行的平均值。为了开放问答检索评估，我们使用了Khattab等人（2021b）的评估脚本，该脚本检查短答案字符串是否出现在（有标题的）Wikipedia段落中。这适配了DPR（Karpukhin等人，2020）的评估代码。我们使用了Karpukhin等人（2020）发布的预处理后的2018年12月的Wikipedia数据集。

---

对于域外评估，我们选择遵循Thakur等人（2021），在BEIR和LoTTE上将ColBERT、RocketQAv2和ColBERTv2的最大文档长度设置为300个词元。Formal等人（2021a）在SPLADEv2的MS MARCO和BEIR上为查询和文档都选择了最大序列长度256，我们在LoTTE上测试其系统时保留了这一默认设置。除非另有说明，我们在ColBERTv2和RocketQAv2中保持默认查询最大序列长度为32个词元。在BEIR的ArguAna测试中，由于查询本身是长文档，我们将ColBERTv2和RocketQAv2的最大查询长度设置为300。在Climate-FEVER中，由于查询是较长的声明句，我们将ColBERTv2的最大查询长度设置为64。

---

我们使用了BEIR的开源实现${ }^{11}$和SPLADEv2的评估代码${ }^{12}$，用于评估SPLADEv2和ANCE以及LoTTE上的BM25。我们使用Anserini（Yang等人，2018a）工具包在Wikipedia开放问答检索测试中执行BM25评估，如Khattab等人（2021b）。我们使用RocketQAv2作者开发的实现评估RocketQAv2。${ }^{13}$