# Billion-Scale Similarity Search with GPUs  



# 0. Abstract

相似性搜索在处理复杂数据(如图像或视频)的数据库系统中有广泛应用，这些数据通常由高维特征表示，并需要特定的索引结构。本文旨在更好地利用GPU来解决这个问题。虽然GPU在距离计算等数据并行任务上表现出色，但该领域的先前方法受限于暴露较少并行性的算法，例如$k$-最小选择，或对内存层次结构的利用不佳。我们提出了一种新颖的$k$-选择设计。我们将其应用于不同的相似性搜索场景，通过基于产品量化的优化，改进了暴力搜索、近似搜索和压缩域搜索。在所有这些设置中，我们的性能均大幅超越了现有最先进技术。我们的实现可达到理论峰值性能的55%，使得最近邻实现比之前的GPU最先进技术快8.5倍。我们能够在35分钟内在YFcc100M数据集中构建一个高精度的$k$-NN图，且在不到12小时内在4个Maxwell Titan X GPU上构建一个连接10亿个向量的图。为了比较和可重复性，我们已经开源了我们的方法。



# 1. INTRODUCTION  

图像和视频构成了一种新的大规模数据源，用于索引和搜索。传统的媒体管理系统基于关系数据库，处理结构化数据。例如，图像通过捕获时间和位置等元数据进行索引，并可以手动添加例如人物姓名等信息。因此，图像可以通过名称、日期或位置进行查询。这些元数据使得自动组织相册成为可能。

---

对于大型媒体集合来说，这类元数据更难获得；内容生成变得如此简单，以至于数据标注成为一个显著的瓶颈。各种机器学习和深度学习算法正在被用于自动解释和标注这些复杂的现实世界实体。它们生成的表示或嵌入通常是实值、高维向量，维度在50到1000以上。常见的例子包括文本表示的word2vec和FastText，从卷积神经网络提取的图像表示，以及用于实例搜索的图像描述符。传统的关系数据库无法有效处理这些描述符，因为它们需要模糊匹配、分类器和相似性搜索等机器学习工具。

---

许多这些向量表示只能在GPU系统上有效生成，因为底层过程要么具有高算术复杂度，要么对数据带宽的需求很高，要么由于通信开销或表示质量无法有效分区。一旦生成，这些向量的操作本身也计算密集。如何有效利用GPU资源并不简单。更一般而言，如何利用新的异构磁盘/CPU/GPU/FPGA架构是数据库社区的一个关键课题。

---

在这种情况下，通过数值相似性而非结构化关系进行搜索更为合适。这可以是寻找与某张图片最相似的内容，或是找到对线性分类器响应最高的向量。在大型集合上执行的最耗费资源的操作之一是计算 $k-\mathrm{NN}$ 图，这是一个有向图，其中数据库的每个向量都是一个节点，每条边连接到其 $k$ 个最近邻。这是我们的旗舰应用。像NN-Descent这样的最先进方法在执行此任务时对数据集本身具有较大的内存开销，且无法轻易扩展到我们考虑的十亿规模的数据库。

---

这样的应用必须应对维度诅咒，使得在十亿规模数据库上进行全面搜索和针对非全面搜索的精确索引都变得不切实际。这就是为什么有大量关于近似搜索和/或图构建的研究工作。为了处理无法放入RAM中的巨大数据集，几种方法采用了使用编码的向量压缩表示。这对内存有限的设备(如GPU)尤其方便。事实证明，接受最小的准确性损失可以实现几个数量级的压缩。最流行的向量压缩方法可以分为二进制编码和量化方法。两者都具有一个理想的特性，即搜索邻居时不需要重建向量。

---

传统关系数据库存储在磁盘上。然而，我们的目标是使操作的响应时间约为10毫秒，而这些操作需要访问数百兆字节的数据。因此，我们只考虑存储在RAM中的数据库。现代服务器通常拥有几十到几百GB的内存，这对于拥有数十亿条记录的数据集是可行的，在多个服务器上分布时可以达到数万亿条记录。因此，在接下来的讨论中，我们关注的是在给定内存预算下的搜索速度，磁盘I/O性能并不相关。

---

本文集中讨论基于乘积量化(PQ)编码的方法。研究表明，它们比局部敏感哈希(LSH)或其他产生二进制编码的变体更有效。LSH的二进制编码变体是次优的，因为它丢弃了向量分量的范数，而PQ则满足量化器的Lloyd最优性条件。LSH也被用作一种分区技术，特别是 $E^2$ LSH 提供了一组适用于欧几里得搜索的哈希函数。然而，这种方法需要将完整数据集编码成多个表，造成显著的内存开销。相比之下，PQ变体为每个向量使用一个代码。LSH和PQ的并行比较验证了PQ的优越性。PQ在查询向量未编码的情况下特别有效。该算法有一个自然的扩展，能够进行非全面搜索。

---

在原始的PQ技术上提出了一些改进，但大多数在GPU上难以高效实现。倒排多索引对于高速度/低质量的操作点非常有用，但依赖复杂的“多序列”算法。优化的乘积量化(OPQ)是一种对输入向量进行线性变换的方法，可以提高乘积量化的准确性，并可以作为预处理应用。André等人的SIMD优化实现仅在次优参数(较少的粗量化中心)下运行。许多其他方法，如LOPQ和多义编码，过于复杂，无法在GPU上高效实现。

---

在GPU上有许多相似性搜索的实现，但大多数使用二进制编码、小型数据集，或进行全面搜索。根据我们所知，只有Wieschollek等人的工作适用于具有量化编码的十亿规模数据集。这是GPU上之前的最先进技术，我们将在第6.4节中与之进行比较，使用最大(十亿规模)的公开基准进行相似性搜索。

---

本文的贡献如下：

- 提出了一种GPU $k$-选择算法，运行在快速寄存器内存中，灵活到可以与其他内核融合，并提供了复杂性分析。这在此之前一直是类似GPU数据库应用的限制因素；
- 提出了近似最优的算法布局，用于在GPU上进行精确和近似的 $k$-最近邻搜索；
- 一系列实验表明，这些改进在中到大规模最近邻搜索任务中，单GPU或多GPU配置下的性能大幅超越了之前的技术。

---

本论文算法的精心设计实现可以在开源的Faiss库中找到。该库实现了许多最先进的索引方法，最相关的算法已被翻译到GPU上。Faiss库的源代码(https://github.com/facebookresearch/faiss)包含了再现本文大部分结果的脚本。

---

本文中在Faiss中实现的设计已应用于多种任务。在最近的一项自然语言处理工作中，$k$-NN搜索用于匹配词嵌入，实现了不同语言之间的翻译，无需并行文本。Caron等人利用GPU聚类方法进行嵌入的无监督训练。在其他研究中，巨大的$k$-NN图被用于图像分类。这些应用得益于GPU实现带来的数量级性能提升。

---

本文的组织结构如下：第二部分介绍上下文和符号；第三部分回顾GPU架构并讨论在进行相似性搜索时出现的问题；第四部分介绍我们的主要贡献之一，即GPU $k$-选择方法；第五部分涵盖整体算法实现；最后，第六部分提供了我们方法的广泛实验，比较其与现有技术的差异，并展示了图像集合的具体应用案例。



# 2. PROBLEM STATEMENT  

我们关注向量集合中的相似性搜索。给定一个查询向量 $\mathbf{x} \in \mathbb{R}^d$ 和一个数据库向量集合 $\left[\mathbf{y}_i\right]_{i=0: \ell}\left(\mathbf{y}_i \in \mathbb{R}^d\right)$，我们按 $L_2$(欧几里得)距离搜索 $\mathbf{x}$ 的 $k$ 个最近邻：

$L=k-\text{argmin}_{i=0: \ell}\left\|\mathbf{x}-\mathbf{y}_i\right\|_2$

$L_2$ 距离最常用，因为在学习多个嵌入时，它在设计上得到了优化，且具有吸引人的线性代数性质。

注${ }^1$: 为了避免在0索引中出现混乱，我们使用Python数组表示法 $0: \ell$ 来表示范围 $\{0, \ldots, \ell-1\}$。

---

最小距离通过 $k$-选择进行收集。对于标量数组 $\left[a_i\right]_{i=0: \ell}$，$k$-选择找到 $k$ 个最低值元素 $\left[a_{s_i}\right]_{i=0: k}$，其中 $ a_{s_i} \leq a_{s_{i+1}}$，以及这些元素在输入数组中的索引 $\left[s_i\right]_{i=0: k}$，满足 $0 \leq s_i<\ell$。$a_i$ 将是32位浮点值；$s_i$ 是32位或64位整数。有时需要其他比较器；对于余弦相似度，我们搜索最高值。相等值之间的顺序 $a_{s_i}=a_{s_i}$ 不作规定。

## 2.1. Exact Search with Batching  

通常，搜索是在批量的 $n_{\mathrm{q}}$ 查询向量 $\left[\mathbf{x}_j\right]_{j=0: n_{\mathrm{q}}}\left(\mathbf{x}_{\mathbf{j}} \in \mathbb{R}^d\right)$ 中并行进行的，这样可以在多个CPU线程或GPU上执行时提供更大的灵活性。$k$-选择的批处理涉及从 $n_{\mathrm{q}}$ 个独立数组中选择 $n_{\mathrm{q}} \times k$ 个元素和索引，其中每个数组的长度 $\ell_i \geq k$ 可能不同。

---

精确解计算完整的成对距离矩阵 $D=\left[\left\|\mathbf{x}_j-\mathbf{y}_i\right\|_2^2\right]_{j=0: n_{\mathrm{q}}, i=0: \ell} \in \mathbb{R}^{n_{\mathrm{q}} \times \ell}$。实际上，我们使用分解

$\left\|\mathbf{x}_j-\mathbf{y}_i\right\|_2^2=\left\|\mathbf{x}_j\right\|^2+\left\|\mathbf{y}_i\right\|^2-2\left\langle\mathbf{x}_j, \mathbf{y}_i\right\rangle$

前两个项在对矩阵 $X$ 和 $Y$ 进行一次遍历时预先计算，其中行分别是 $\left[\mathbf{x}_j\right]$ 和 $\left[\mathbf{y}_i\right]$。瓶颈在于计算 $\left\langle\mathbf{x}_j, \mathbf{y}_i\right\rangle$，这相当于矩阵乘法 $X Y^{\top}$。每个 $n_q$ 查询的 $k$-最近邻在 $D$ 的每一行中进行 $k$-选择。

## 2.2. Compressed-Domain Search  

从现在开始，我们专注于近似最近邻搜索。我们特别考虑IVFADC索引结构。IVFADC索引依赖于两个层次的量化，数据库向量被编码。数据库向量$\mathbf{y}$的近似表示为：

$\mathbf{y} \approx q(\mathbf{y})=q_1(\mathbf{y})+q_2\left(\mathbf{y}-q_1(\mathbf{y})\right)$

其中$q_1: \mathbb{R}^d \rightarrow \mathcal{C}_1 \subset \mathbb{R}^d$和$q_2: \mathbb{R}^d \rightarrow \mathcal{C}_2 \subset \mathbb{R}^d$是量化器；即输出有限集合中元素的函数。由于这些集合是有限的，$q(\mathbf{y})$被编码为$q_1(\mathbf{y})$和$q_2\left(\mathbf{y}-q_1(\mathbf{y})\right)$的索引。第一层量化器是粗量化器，第二层是精量化器，编码第一层后的残差向量。

---

不对称距离计算(ADC)搜索方法返回一个近似结果：

$L_{\mathrm{ADC}}=k-\text{argmin}_{i=0: \ell}\left\|\mathbf{x}-q\left(\mathbf{y}_i\right)\right\|_2$

对于IVFADC，搜索不是全面的。计算距离的向量是根据第一层量化器$q_1$预先选择的：

$L_{\mathrm{IVF}}=\tau-\text{argmin}_{\mathbf{c} \in \mathcal{C}_1}\|\mathbf{x}-\mathbf{c}\|_2$ 

---

多探测参数$\tau$是我们考虑的粗级质心的数量。量化器在再现值的集合中进行最近邻搜索，计算IVFADC搜索：

$L_{\mathrm{IVFADC}}=\underset{i=0: \ell \text { s.t. } q_1\left(\mathbf{y}_i\right) \in L_{\mathrm{IVF}}}{k \text {-argmin }}\left\|\mathbf{x}-q\left(\mathbf{y}_i\right)\right\|_2.$ 

因此，IVFADC依赖于与ADC的两步量化相同的距离估计，但仅在向量的子集上计算这些距离。

---

对应的数据结构，倒排文件，将向量$\mathbf{y}_{\mathbf{i}}$分组为$\left|\mathcal{C}_1\right|$个倒排列表$\mathcal{I}_1, \ldots, \mathcal{I}_{\left|\mathcal{C}_1\right|}$，这些列表具有相同的$q_1\left(\mathbf{y}_i\right)$。因此，最耗费内存的操作是计算$L_{\text {IVFADC}}$，这相当于线性扫描$\tau$个倒排列表。

---

**The Quantizers**. $q_1$和$q_2$具有不同的属性。量化器$q_1$需要具有相对较少的再现值，以防止倒排列表的数量激增。我们通常使用$\left|C_1\right| \approx \sqrt{\ell}$，通过$k$-均值训练得到。对于$q_2$，我们可以为更广泛的表示分配更多内存。向量索引(一个4字节或8字节整数)也存储在倒排列表中，因此没有必要使用比这更短的编码；即，$\log _2\left|\mathcal{C}_2\right|>4 \times 8$。

---

**Product Quantizer**. 我们使用产品量化器(PQ)[32]作为$q_2$，以有限的内存和计算成本提供大量的再现值。它将$\mathbf{y}$解释为$b$个子向量$\mathbf{y}=\left[\mathbf{y}^1 \ldots \mathbf{y}^b\right]$，其中$b$是维度$d$的偶数因子。每个子向量用其自身的量化器进行量化，得到$\left(q^1\left(\mathbf{y}^1\right), \ldots, q^b\left(\mathbf{y}^b\right)\right)$。子量化器通常具有256个再现值，以适应一个字节。产品量化器的量化值为$q_2(\mathbf{y})=q^1\left(\mathbf{y}^1\right)+256 \times q^2\left(\mathbf{y}^2\right)+\ldots+256^b \times q^b\left(\mathbf{y}^b\right)$，从存储的角度看，这只是每个子量化器产生的字节的串联。因此，产品量化器生成的代码为$b$字节，具有$\left|\mathcal{C}_2\right|=256^b$个再现值。量化器的$k$-均值字典较小，量化计算成本低廉。



# 3. GPU: OVERVIEW AND K-SELECTION  

本节回顾了Nvidia GPU架构和编程模型的相关细节 [40]。随后，我们将重点关注与相似性搜索相关的较少符合GPU要求的部分，即$k$-选择，并讨论相关文献和面临的挑战。

## 3.1. Architecture  

**GPU线程和波浪。**Nvidia GPU是一种通用计算机，通过32宽的CUDA线程向量(称为波浪)执行指令流。波浪中的每个线程称为通道，通道ID范围为0到31。单个波浪中的通道共享一个波浪宽度的指令计数器。当波浪通道希望采取不同的执行路径时，尽管共享指令计数器，这种现象称为波浪分歧，会降低性能。每个通道在共享寄存器文件中最多有255个32位寄存器。类比于CPU，每个波浪相当于一个独立的CPU硬件线程，具有多达255个宽度为32的SIMD向量寄存器，而波浪通道则作为SIMD向量通道。

---

**波浪集合。**1到32个波浪的可配置集合组成一个块或协作线程数组(CTA)。每个块有一个高速共享内存，大小可达48 KiB。单个CUDA线程具有相对于块的ID，称为线程ID，可用于划分和分配工作。每个块在GPU的单个核心上运行，该核心称为流处理器(SM)，具有用于执行的功能单元，如算术逻辑单元(ALUs)。GPU通过在所有SM上并行处理多个波浪中的许多操作来隐藏执行延迟。每个单独的波浪通道的指令吞吐量较低，延迟较高，但所有SM的总算术吞吐量是典型CPU的5到10倍。

---

**网格和内核。**块在内核中组织成一个块的网格。每个块被分配一个相对于网格的ID。内核是主机CPU为GPU调度执行的工作单元(指令流及其参数)。当一个块执行完成后，可以调度新的块。来自不同内核的块可以并发运行。内核之间的顺序可以通过流和事件等排序原语进行控制。

---

**资源和占用率。**并发执行的块数取决于每个块使用的共享内存和寄存器资源。CUDA线程的寄存器使用在编译时确定，而共享内存的使用可以在运行时选择。这会影响GPU的占用率；使用更多的寄存器或共享内存资源会减少执行并发性。

---

**内存类型。**不同的块和内核通过全局内存进行通信，通常大小为4到32 GB，带宽比CPU主内存高5到10倍。共享内存在速度上类似于CPU的L1缓存。GPU寄存器文件内存是带宽最高的内存。为了维持GPU上大量指令的并发执行，还需要一个庞大的寄存器文件：最新的Pascal P100有14 MB，而CPU仅有几十KB。GPU上的寄存器、共享和全局内存的聚合横截面带宽比率通常为250:6.25:1，这使得寄存器文件的带宽达到数十TB/s。

## 3.2. GPU Register File Usage  

**结构化寄存器数据。**共享内存和寄存器内存的使用涉及效率权衡。它们降低了占用率，但通过在更快的内存中保留更大的工作集来提高整体性能。大量使用寄存器驻留数据，虽然牺牲了占用率或共享内存的使用，通常是有利的。

---

由于GPU寄存器文件非常大，存储结构化数据(不仅仅是临时操作数)是有用的。单个通道可以使用其(标量)寄存器来解决局部任务，但并行性和存储有限。相反，warp中的通道可以通过warp洗牌交换寄存器数据，从而实现全warp的并行性和存储。提供了多种访问模式(移位、任意到任意)。特别地，我们广泛使用蝶形置换。

---

**通道步幅寄存器数组。**warp洗牌常用于操作通道步幅寄存器数组。即，对于元素$\left[a_i\right]_{i=0: \ell^{\prime}}$，每个后续值由相邻的通道存储在寄存器中。数组存储在每个通道的$\ell / 32$个寄存器中，其中$\ell$是32的倍数。通道$j$存储$\left\{a_j, a_{32+j}, \ldots, a_{\ell-32+j}\right\}$，而寄存器$r$存储$\left\{a_{32 r}, a_{32 r+1}, \ldots, a_{32 r+31}\right\}$。

---

为了操作$\left[a_i\right]$，必须在汇编时知道存储$a_i$的寄存器(即$\lfloor i / 32\rfloor$)和$\ell$，而通道(即$i \bmod 32$)可以在运行时获取。因此，所有配置必须在编译时通过广泛的C++模板化进行处理。

## 3.3. k-Selection on CPU versus GPU  

$k$-选择一直是之前GPU相似性搜索应用中的性能限制因素(见第6节)，因此值得讨论。常见的CPU $k$-选择算法，通常适用于任意大的$\ell$和$k$，可以被翻译到GPU上，包括基数选择和桶选择[1]、概率选择[44]、快速选择[17]以及截断排序[50]。它们的性能受到多次遍历输入的影响。在相似性搜索中，输入距离通常是即时计算的或仅存储在小块中，而不是完整存储。完整的距离数组可能太大，无法适应任何内存，其大小在处理开始时也可能未知，这使得多次遍历算法不切实际。此外，像快速选择那样根据值在全局内存中对元素进行分区的算法会导致过多的内存事务，因为warp宽的数据访问模式并不均匀。基数选择虽然没有分区，但仍然需要多次遍历。

---

**堆并行性。** 在相似性搜索中，通常只关注少量结果($k<1000$)。在这种情况下，使用最大堆进行选择在CPU上是典型的，但由于树更新的串行性，堆未能充分利用数据并行性，也无法饱和SIMD执行单元。ad-heap [41] 更好地利用了异构系统中的并行性，但仍试图在适当的执行单元之间划分串行和并行工作。尽管堆更新是串行的，但对于小的$k$，CPU可以很轻松地将所有状态保持在L1缓存中，而L1缓存的延迟和带宽仍然是一个限制因素。其他相似性搜索组件，如PQ代码操作，往往对CPU性能的影响更大[3]。

---

**GPU堆。** 堆可以在GPU上实现[9]，然而，直接的GPU实现会导致高程度的warp分歧和不规则的数据依赖内存移动，因为每个插入元素所采取的路径依赖于堆中其他值的存在。

---

GPU并行优先队列[31]通过允许多个并发更新来改善串行堆更新，但它们需要为每个插入执行潜在数量的小型排序，并且涉及数据依赖的内存移动。同时，它们还需要与CPU主机进行显著的协调。

---

此外，还有其他更为新颖的GPU算法可用于小$k$，例如fgknn库中的选择算法[51]。这是一个复杂的算法，可能面临过多的同步点、更高的内核启动开销、使用较慢的内存、过度使用层次结构、分区和缓冲等问题。然而，我们从这个特定算法中获得了灵感，利用并行合并的方法，如在其合并队列结构中所见。



# 4. FAST K-SELECTION ON THE GPU  

根据屋顶性能模型[59]，对于任何CPU或GPU算法，内存或算术吞吐量应该是限制因素。对于来自全局内存的输入，$k$-选择算法的运行速度不能超过在峰值内存带宽下扫描输入一次所需的时间。我们希望尽可能接近这个限制。因此，我们希望对输入数据进行单次遍历。

---

我们希望将中间状态保存在最快的内存中，即寄存器文件。这样做的主要缺点是第3.2节提到的lane-stride寄存器数组索引限制，这对算法的可行性造成了限制。

## 4.1. In-Register Sorting  

我们使用一种注册内排序原语作为构建模块。排序网络在SIMD架构上常被使用，因为它们利用了向量并行性。这些网络在GPU上实现起来很简单，我们利用lane-stride寄存器数组构建排序网络。

---

我们使用Batcher的比特onic排序网络的一个变体，它是对大小为$2^k$的数组进行的一组并行合并。每次合并将$s$个长度为$t$的数组($s$和$t$均为2的幂)合并为$s/2$个长度为$2t$的数组，使用$\log_2(t)$个并行步骤。比特onic排序递归地应用这种合并：要对长度为$\ell$的数组进行排序，首先将$\ell$个长度为1的数组合并为$\ell/2$个长度为2的数组，再合并为$\ell/4$个长度为4的数组，依此类推，最终得到一个长度为$\ell$的已排序数组，总共需要$\cfrac{1}{2}\left(\log_2(\ell)^2+\log_2(\ell)\right)$个并行合并步骤。

---

**奇数大小的合并和排序网络。** 如果某些输入数据已经排序，我们可以修改网络以避免合并步骤。如果我们没有满的2的幂数据集，我们可以高效地跳过处理较小的大小。

---

**算法1**是一个奇数大小的合并网络，用于合并已经排序的左数组和右数组，每个数组的长度均可为任意值。与比特onic网络合并比特onic序列不同，我们从单调序列开始：即按单调顺序排序的序列。通过反转第一个比较器阶段，将比特onic合并变为单调合并。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923200510413.png" alt="image-20240923200510413" style="zoom: 57%;" /> 

---

奇数大小算法的推导是通过考虑将数组填充到下一个最高的2的幂大小，使用从不交换的虚拟元素(合并是单调的)并且已经正确定位；与虚拟元素的任何比较都被省略。左数组被视为在开始处填充了虚拟元素；右数组则在末尾填充虚拟元素。将长度为$\ell_L$和$\ell_R$的两个已排序数组合并为一个长度为$\ell_L+\ell_R$的已排序数组需要$\left\lceil\log_2\left(\max \left(\ell_L,\ell_R\right)\right)\right\rceil+1$个并行步骤。**图1**展示了算法1的合并网络，适用于大小为5和3的数组，共有4个并行步骤。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923194125869.png" alt="image-20240923194125869" style="zoom: 25%;" /> 

图1. 奇数大小网络合并大小为5和3的数组。圆点表示并行比较/交换。虚线表示省略的元素或比较。 

---

COMPARE-SWAP通过在lane-stride寄存器数组上使用warp shuffle实现。当交换的步长是32的倍数时，交换直接在一个lane内进行，因为该lane本地持有两个元素。步长小于等于16或不是32的倍数的交换则通过warp shuffle实现。实际上，使用的数组长度是32的倍数，因为它们存储在lane-stride数组中。

---

**算法2**将合并扩展为完整的排序。假设输入数据中没有任何结构，它对长度为$\ell$的数据数组进行排序时，需要$\cfrac{1}{2}\left(\left\lceil\log_2(\ell)\right\rceil^2+\left\lceil\log_2(\ell)\right\rceil\right)$个并行步骤。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923200606997.png" alt="image-20240923200606997" style="zoom:50%;" /> 

## 4.2. WarpSelect  

我们的$k$-选择实现，WARPSELECT，完全在寄存器中维护状态，只需对输入进行一次扫描。它使用MERGE-ODD和SORT-ODD作为原语。由于寄存器文件提供的存储空间远大于共享内存，因此支持$k \leq 1024$。每个warp专注于对单个数组$\left[a_i\right]$进行$k$-选择。如果$n$足够大，每个$\left[a_i\right]$分配一个warp将导致GPU的完全占用。对于每个warp的大$\ell$，如果事先知道$\ell$，则通过递归分解进行处理。

---

**概述。** 我们的方法(**算法3**和图2)操作值，并携带相关索引(为简化描述而省略)。它选择来自全局内存的$k$个最小值，或者如果与另一个提供值的内核融合，则来自中间值寄存器。设$\left[a_i\right]_{i=0: \ell}$为提供的选择序列。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923200950969.png" alt="image-20240923200950969" style="zoom: 50%;" /> 

---

**图2**左侧的元素按32个一组处理，即warp的大小。Lane $j$负责处理元素$\left\{a_j, a_{32+j}, \ldots\right\}$。因此，如果元素来自全局内存，读取是连续的，并合并为最少数量的内存事务。每当其中一个lane满时(触发wARP-BALLOT)，就会调用合并例程对所有lane进行处理，并刷新线程队列。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923194230354.png" alt="image-20240923194230354" style="zoom:25%;" /> 

图2. WARPSELECT的概述。输入值从左侧流入，右侧的warp队列保存输出结果。

---

**数据结构。** warp共享一个大小为$k$的lane-stride寄存器数组，称为warp队列，表示已见到的最小元素$\left[W_i\right]_{i=0: k^{\prime}}$。该数组按从小到大的顺序排列(即$W_i \leq W_{i+1}$)；如果请求的$k$不是32的倍数，则将其向上取整。

---

每个lane $j$在寄存器中维护一小组$t$个元素，称为线程队列$\left[T_i^j\right]_{i=0: t^{\prime}}$，同时还维护当前有效元素的计数$C_j\left(0 \leq C_j \leq t\right)$。线程队列中索引小于$C_j$的所有元素保证小于等于$W_{k-1}$。其他元素初始化为最大哨兵值，例如$+\infty$。

---

线程队列是新值进入的第一层过滤器；在扫描过程中，仅保留新的潜在最小$k$元素，直到我们在任一lane中收集到一组$t$个需要考虑的可能真实最小$k$元素。$t$的选择相对于$k$进行，详见第4.3节。warp队列是第二层，维护所有观察到的最小$k$的warp宽度值。warp队列也初始化为最大哨兵值。

---

**更新。** 维护的三个不变式是：

- 所有$T_i^j \leq W_{k-1}$对于$i<C_j$，否则$T_i^j=+\infty$；
- 没有线程会在其线程队列中累积超过$t$个元素(即$C_j \leq t$)；
- 目前为止看到的所有真实最小$k$的$a_i$都包含在某个lane的线程队列中$\left(\left[T_i^j\right]_{i=0: C_j, j=0: 32}\right)$，或者在warp队列中。

---

lane $j$接收到一个新的$a_{32 i+j}$，并与当前的$W_k$进行比较。如果$a_{32 i+j}>W_k$，则新元素定义上不在最小$k$中，被拒绝。否则，我们将其添加到线程队列中，覆盖之前位于$W_{C_j}^j$的哨兵值。如果任何lane已经累积了$t$个值，则我们无法处理任何新的$a_i$，因为我们无法确定它是否在当前看到的所有元素中的最小$k$，因为如果它小于$W_k$就没有地方存放该元素。通过使用warp投票指令，我们确定是否有任何lane已经累积了$t$个值，如果有，则该投票“获胜”。如果没有，我们可以继续处理新元素。

---

**维护不变式。** 线程队列中的一些或所有元素现在可能在真实的最小$k$中。为了使$W_k$成为目前为止看到的真实第$k$个最小元素，warp使用ODD-MERGE将线程队列和warp队列合并和排序。新的warp队列将是合并和排序后的最小$k$元素，线程队列被重新初始化为最大哨兵值。这样，我们可以继续处理后续元素，而不会违反不变式。

---

warp队列已经是排序的，但线程队列不是。线程队列将一起排序，$32t$个已排序元素与长度为$k$的已排序warp队列合并。支持奇数大小的合并非常重要，因为Batcher的公式要求$32t=k$且是2的幂。因此，如果$k=1024$，则$t$必须是32。我们发现最优的$t$要小得多(见下文)，这意味着我们在合并两种不同的大小。

---

**处理剩余元素。** 如果存在剩余元素，因为$\ell$不是32的倍数，这些元素将被考虑到覆盖剩余部分的线程队列中，之后我们进入输出阶段。

---

**输出。** 处理完所有元素后，对线程队列和warp队列进行最后的排序和合并，之后warp队列中将保存所有$a_i$的最小$k$值。

## 4.3. Complexity and Parameter Selection  

对于每组 incoming 的32个元素，WarpSeleCT执行1、2或3个常数时间操作，所有操作都在warp宽的并行时间内进行：

1) 读取32个元素，与$W_k$进行比较，开销为$\mathcal{C}_1$，发生$N_1$次；
2) 插入线程队列，开销为$\mathcal{C}_2$，发生$N_2$次；
3) 如果存在$j$使得$\mathcal{C}_j=t$，则排序并合并队列，开销为$\mathcal{C}_3=\mathcal{O}\left(t \log (32 t)^2+k \log (\max (k, 32 t))\right)$，发生$N_3$次。

---

因此，总开销为$N_1 \mathcal{C}_1 + N_2 \mathcal{C}_2 + N_3 \mathcal{C}_3$。$N_1=\ell / 32$。对于$N_2$，由于我们在寄存器中保留$\left[T_i^j\right]$，这需要编译时索引，因此我们使用一个展开的循环来找到合适的寄存器以覆盖当前的$C_j$值，所以$N_2=\mathcal{O}(t)$。对于$N_3$，我们对独立抽取的随机数据进行估计。

---

设输入的$k$选择为一个序列$\left\{a_1, a_2, \ldots, a_{\ell}\right\}$(1-based索引)，这是一个随机选择的不同元素集合的排列。元素按顺序分为$c$组，每组大小为$w$(warp大小，所以$w=32$)。假设$\ell$是$w$的倍数，因此$c=\ell / w$。回想一下，$t$是最大线程队列长度。我们称在当前的min-$k$中位置$n$之前或位置$n$的元素为当前的min-$k$(在$n$时刻)。元素$a_n$在当前的min-$k$中的可能性为：

$\alpha(n, k):= \begin{cases}1 & \text { if } n \leq k \\ k / n & \text { if } n>k\end{cases}$ 

当$n>k$时，每个$a_n$的可能性为$k/n$，因为所有排列都是等可能的，前$k$个元素都符合条件。

---

**计算线程队列插入次数。** 在给定的lane中，如果incoming值在当前的$\min -k+t$值中，但该lane只“见过”$w c_0 + (c - c_0)$个值(其中$c_0$是之前的获胜warp投票)，则会触发插入。发生这种情况的概率为：

$\alpha\left(w c_0+\left(c-c_0\right), k+t\right) \approx \cfrac{k+t}{w c} \text { for } c>k \text {. }$


这个近似考虑了线程队列已经见过所有$w c$值，而不仅仅是分配给其lane的值。任何lane触发队列插入的概率为：

$1-\left(1-\cfrac{k+t}{w c}\right)^w \approx \cfrac{k+t}{c}$ 

这里的近似是一级泰勒展开。对$c$的概率求和得出插入的期望次数为：$N_2 \approx(k+t) \log (c)=\mathcal{O}(k \log (\ell / w))$ 

---

**计数完整排序。** 我们寻求 $N_3=\pi(\ell, k, t, w)$，即 WarpSelect 所需的完整排序的期望数量。

**单通道。** 目前假设 $w=1$，因此 $c=\ell$。设 $\gamma(\ell, m, k)$ 为在序列 ${a_1, \ldots, a_{\ell}}$ 中，恰好有 $m$ 个元素被顺序扫描器($w=1$)遇到，并且这些元素属于连续的 $\min -k$ 的概率。给定 $m$，可以选择 $\binom{\ell}{m}$ 个位置来放置这些连续的最小 $k$ 元素。该概率由以下递推关系给出：

$\gamma(\ell, m, k):= \begin{cases}1 \,,\,\,  \ell=0 \text { and } m=0 \\ 0 \,,\,\, \ell=0 \text { and } m>0 \\ 0 \,,\,\, \ell>0 \text { and } m=0 \\ (\gamma(\ell-1, m-1, k) \cdot \alpha(\ell, k)+  \gamma(\ell-1, m, k) \cdot(1-\alpha(\ell, k))) \,,\,\, \text { otherwise. }\end{cases}$ 

最后一个情况是指遇到以下情况的概率：在 $\ell-1$ 的序列中，恰好有 $m-1$ 个连续的最小 $k$ 元素在我们之前，而当前元素在连续的 $\min -k$ 中，或者当前元素不在连续的 $\min -k$ 中，$m$ 个元素在我们之前。

---

我们接着为 $\pi(\ell, k, t, 1)$ 发展一个递推关系。我们首先注意到

$\displaystyle{}\delta(\ell, b, k, t):=\sum_{m=b t}^{\min ((b t+\max (0, t-1)), \ell)} \gamma(\ell, m, k)$

对于 $b$，其中 $0 \leq b t \leq \ell$ 是长度为 $\ell$ 的所有序列中，因赢得线程队列投票而强制产生 $b$ 次数据排序的比例，因为这些排序需要有 $b t$ 到 $(b t+\max (0, t-1))$ 个元素在连续的最小 $k$ 中(因为最小 $k$ 元素会溢出线程队列)。最多可以发生 $\lfloor\ell / t\rfloor$ 次赢得的投票，因为需要 $t$ 个单独的顺序当前最小 $k$ 元素来赢得投票。因此，$\pi(\ell, k, t, 1)$ 是所有可能的 $b$ 的期望值：

$\displaystyle{}\pi(\ell, k, t, 1)=\sum_{b=1}^{\lfloor\ell / t\rfloor} b \cdot \delta(\ell, b, k, t)$ 

---

这个量可以通过动态规划来计算。分析上，注意到当 $t=1, k=1$ 时，$\pi(\ell, 1, 1, 1)$ 是调和数 $H_{\ell}=1+\cfrac{1}{2}+\cfrac{1}{3}+\ldots+\cfrac{1}{\ell}$，当 $\ell \rightarrow \infty$ 时，它收敛于 $\ln (\ell)+\gamma$(欧拉-马歇罗尼常数 $\gamma$)。

对于 $t=1, k>1, \ell>k$，有 $\pi(\ell, k, 1, 1)=k+k\left(H_{\ell}-H_k\right)$ 或 $\mathcal{O}(k \log (\ell))$，因为前 $k$ 个元素在连续的最小 $k$ 中，剩下部分的期望为 $\cfrac{k}{k+1}+\cfrac{k}{k+2}+\ldots+\cfrac{k}{\ell}$。

对于 $t>1, k>1, \ell>k$，注意对于每个可能的 $\{a_1, \ldots, a_{\ell}\}$，有一些数量 $D, k \leq D \leq \ell$ 的连续最小 $k$ 确定。每种情况的赢得的投票数定义为 $\lfloor D / t\rfloor$，因为线程队列必须填满 $t$ 次。因此，$\pi(\ell, k, t, 1)=\mathcal{O}(k \log (\ell) / t)$。

---

**多个通道。** 当 $w>1$ 时，由于需要考虑联合概率，这一情况变得复杂(如果 $w$ 个工作者中的多个触发了对某个组的排序，则仅进行一次排序)。然而，可以对这个可能性进行界定。设 $\pi^{\prime}(\ell, k, t, w)$ 为在假设 $w$ 个工作者之间没有相互干扰的情况下，赢得的投票的期望值(即，如果有 $b \leq w$ 个工作者在一个步骤中独立赢得了投票，则我们赢得 $b$ 次投票)，但在每次排序后仍然共享最小 $k$ 集合。假设 $k \geq w$，那么我们有

$\displaystyle\begin{aligned}
\pi^{\prime}(\ell, k, 1, w) & \leq w\left(\left\lceil\cfrac{k}{w}\right\rceil+\sum_{i=1}^{\lceil\ell / w\rceil-\lceil k / w\rceil} \cfrac{k}{w(\lceil k / w\rceil+i)}\right)  \leq w \pi(\lceil\ell / w\rceil, k, 1,1)=\mathcal{O}(w k \log (\ell / w))
\end{aligned}$

这里，$w$ 个工作者在每一步看到连续的 $\mathrm{min}-k$ 元素的可能性上限等于第一个工作者在每一步的上限。与之前一样，赢得的投票数按 $t$ 进行缩放，因此 $\pi^{\prime}(\ell, k, t, w)=\mathcal{O}(w k \log (\ell / w) / t)$。相互干扰只能减少投票数量，因此我们为 $\pi(\ell, k, t, w)$ 得到相同的上界。假设 $w$ 固定用于波段大小，我们有 $N_3=\pi(\ell, k, t, 32)=\mathcal{O}(k \log (\ell) / t)$。

---

**选择 $t$。** 权衡是在 $N_2 \mathcal{C}_2$ 和 $N_3 \mathcal{C}_3$ 之间平衡成本。根据对各种 $k$-最近邻数据的实验，对于 $k \leq 32$，我们使用 $t=2$；对于 $k \leq 128$，使用 $t=3$；对于 $k \leq 256$，使用 $t=4$；对于 $k \leq 1024$，使用 $t=8$，而与 $\ell$ 无关。



# 5. IMPLEMENTING THE INDEX ON A GPU  

本节解释了第二部分相似性搜索方法的高效 GPU 实现，特别关注 IVFADC，这种方法针对的是最大的数据库。它是最初基于产品量化的索引方法之一 [32]。距离计算的细节和与 $k$-选择的结合是理解此方法为何能够超过最近的 GPU 近似最近邻策略的关键 [58]。

## 5.1. Exact Search  

我们简要回到穷举搜索方法，通常称为精确暴力搜索。对于小数据集的精确最近邻搜索，它本身就很有趣。它也是文献中许多索引的组成部分；我们将其用于 IVFADC 粗量化器 $q_1$。

---

如第二部分所述，距离计算归结为矩阵乘法。我们使用 cuBLAS 库中的优化 GEMM 例程计算 $-2\left\langle\mathbf{x}_j, \mathbf{y}_i\right\rangle$ 项，以获得相对于 $L_2$ 距离的部分距离矩阵 $D^{\prime}$。为了完成距离计算，我们使用一个融合的 $k$-选择内核，将 $\left\|\mathbf{y}_i\right\|^2$ 项添加到每个部分距离结果中，并立即将该值提交给寄存器中的 $k$-选择。在 $k$-选择之前，不需要考虑 $\left\|\mathbf{x}_j\right\|^2$ 项。能够与其他 GPU 计算融合的 $k$-选择允许对矩阵 $D^{\prime}$ 仅进行 2 次遍历(GEMM 写入，$k$-选择读取)，而其他实现通常需要 3 次或更多。

---

由于矩阵 $D^{\prime}$ 对于现实问题规模来说无法放入 GPU 内存，因此问题在查询批次上进行分块，最多 $t_q \leq n_q$ 个查询在单个块中运行。

## 5.2. IVFADC Indexing  

**PQ 查找表。** IVFADC 需要计算一个向量与一组 PQ 重构值之间的距离。通过展开公式 (6) 对于数据库向量 $y$，我们得到：$\|\mathbf{x}-q(\mathbf{y})\|_2^2=\left\|\mathbf{x}-q_1(\mathbf{y})-q_2\left(\mathbf{y}-q_1(\mathbf{y})\right)\right\|_2^2$

如果将 $q_1$ 后剩余的残差向量分解为：$\mathbf{y}-q_1(\mathbf{y})=\left[\tilde{\mathbf{y}^1} \cdots \widetilde{\mathbf{y}^b}\right]$ 和 $\mathbf{x}-q_1(\mathbf{y})=\left[\widetilde{\mathbf{x}^1} \cdots \widetilde{\mathbf{x}^b}\right]$

那么距离可以重写为：$\|\mathbf{x}-q(\mathbf{y})\|_2^2=\left\|\widetilde{\mathbf{x}^1}-q^1\left(\widetilde{\mathbf{y}^1}\right)\right\|_2^2+\ldots+\left\|\widetilde{\mathbf{x}^b}-q^b\left(\widetilde{\mathbf{y}^b}\right)\right\|_2^2$

每个量化器 $q^1, \ldots, q^b$ 具有 256 个重构值，因此当 $\mathbf{x}$ 和 $q_1(\mathbf{y})$ 已知时，所有距离可以预计算并存储在每个大小为 256 的表 $T_1, \ldots, T_b$ 中 [32]。计算总和 (17) 包括 $b$ 次查找和加法。比较计算 $n$ 个距离的成本：

- 显式计算：$n \times d$ 次乘加；
- 使用查找表：$256 \times d$ 次乘加和 $n \times b$ 次查找加。

这就是 PQ 效率的关键。在我们的 GPU 实现中，$b$ 是 4 的任意倍数，最大为 64。这些代码按向量在列表中以每个向量 $b$ 字节的顺序存储。

---

**IVFADC 查找表。** 在扫描倒排列表元素 $\mathcal{I}_L$ 时，可以应用查找表方法，因为查询 $\mathbf{x}$ 是已知的，并且根据定义 $q_1(\mathbf{y})$ 是常数。此外，表 $T_1 \ldots T_b$ 的计算也得到了进一步优化 [6]。方程 (14) 中 $\|\mathbf{x}-q(\mathbf{y})\|_2^2$ 的表达式被分解为：

$\underbrace{\left\|q_2(\ldots)\right\|_2^2+2\left\langle q_1(\mathbf{y}), q_2(\ldots)\right\rangle}_{\text{项 } 1}+\underbrace{\left\|\mathbf{x}-q_1(\mathbf{y})\right\|_2^2}_{\text{项 } 2}-2 \underbrace{\left\langle\mathbf{x}, q_2(\ldots)\right\rangle}_{\text{项 } 3}$ 

目标是最小化内循环计算。我们可以提前计算并存储在查找表中的计算如下：

- 项 1 与查询无关。它可以从量化器中预计算，并存储在大小为 $\left|\mathcal{C}_1\right| \times 256 \times b$ 的表 $\mathcal{T}$ 中；
- 项 2 是与 $q_1{ }^{\prime}$ 的重构值的距离。因此，它是第一层量化器 $q_1$ 的副产品；
- 项 3 独立于倒排列表计算。其计算成本为 $d \times 256$ 次乘加。

---

这种分解用于生成在扫描倒排列表时使用的查找表 $T_1 \ldots T_b$。对于单个查询，从头计算 $\tau \times b$ 表的成本为 $\tau \times d \times 256$ 次乘加，而这种分解的成本为 $256 \times d$ 次乘加和 $\tau \times b \times 256$ 次加法。在 GPU 上，$\mathcal{T}$ 的内存使用可能是一个问题，因此只有在内存不是问题时，我们才启用这种分解。

## 5.3. GPU Implementation  

算法 4 概述了在 CPU 上实现该过程的步骤。倒排列表被存储为两个独立的数组，分别用于 PQ 代码和相关 ID。只有在 $k$-选择确定 $k$-最近邻成员时，才会解析 ID。这种查找是在大数组中进行少量稀疏内存读取，因此对于 GPU，ID 可以选择存储在 CPU 上，代价非常小。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923212002580.png" alt="image-20240923212002580" style="zoom: 45%;" /> 

---

**列表扫描。** 一个内核扫描每个查询的 $\tau$ 个最近倒排列表，并使用查找表 $T_i$ 计算每对向量之间的距离。$T_i$ 被存储在共享内存中：对于一个查询集，最多需要进行 $n_q \times \tau \times \max_i |\mathcal{I}_i| \times b$ 次查找(在实践中为数万亿次访问)，且这些访问是随机的。这将 $b$ 限制在最多 48(32 位浮点数)或 96(16 位浮点数)以内，适用于当前的 GPU 架构。如果不使用公式 (18) 的分解，$T_i$ 会在扫描之前由单独的内核计算。

---

**多遍核。** 每个 $n_q \times \tau$ 对查询与倒排列表的配对可以独立处理。在极端情况下，为每个配对分配一个块，导致最多写回 $n_q \times \tau \times \max_i |\mathcal{I}_i|$ 个部分结果到全局内存，然后进行 $k$-选择以得到 $n_q \times k$ 个最终结果。这提供了高并行性，但可能超出可用的 GPU 全局内存。与精确搜索一样，我们选择一个块大小 $t_q \leq n_q$ 来减少内存消耗，将其复杂度界定为 $\mathcal{O}(2 t_q \tau \max_i |\mathcal{I}_i|)$，并使用多流技术。

---

**端到端性能。** 对于我们的 IVFADC + PQ 实现，表 1 显示了相对于 Maxwell Titan X GPU 峰值的屋顶线模型的全局内存带宽和算术利用率。它是在 YFCC100M 索引(第 6.4 节)上评估的，查询 4096 个向量，$\tau=32$ 和 $k=100$，且没有进行 $T_i$ 的分解。端到端工作负载相当异构，同时对算术(IVFADC $q_1$)和内存带宽的需求都很高。$k$-选择仅占 1.3 秒总时间的 20.6%，但 $k$-选择 $q_1$ 和 $k$-选择列表的内核都利用了内核融合。否则，将需要在全局内存中进行多次遍历，导致工作负载主要由 $k$-选择所主导。

| $\%$ of time (1.3s total) |      kernel      | arithmetic (% peak) |     gmem (% peak)     | limit factor |
| :-----------------------: | :--------------: | :-----------------: | :-------------------: | :----------: |
|         $19.4 \%$         |   IVFADC $q_1$   |       $95 \%$       |        $44 \%$        |  arithmetic  |
|         $5.9 \%$          | $k$-select $q_1$ |       $40 \%$       |        $64 \%$        |   gmem b/w   |
|         $23.4 \%$         |  $T_i$ distance  |       $60 \%$       |        $66 \%$        |   gmem b/w   |
|         $34.5 \%$         |  list scanning   |       $26 \%$       |        $87 \%$        |   gmem b/w   |
|         $14.7 \%$         | $k$-select lists |       $55 \%$       |        $52 \%$        |  arithmetic  |
|         $2.1 \%$          |    ID lookup     |       $23 \%$       |        $13 \%$        | inst latency |
|     weighted average      |                  |      $52.4 \%$      | $\mathbf{6 5 . 7 \%}$ |   gmem b/w   |

表1. GPU IVFADC + PQ End-to-End Performance  

## 5.4. Multi-GPU Parallelism  

现代服务器可以支持多个 GPU。我们利用这一能力来提升速度和内存。

**复制。** 如果一个索引能够适应单个 GPU 的内存，它可以在 $\mathcal{R}$ 个不同的 GPU 上进行复制。对于查询 $n_{\mathrm{q}}$ 个向量，每个副本处理 $n_{\mathrm{q}} / \mathcal{R}$ 的查询。复制几乎能实现线性加速，但对于较小的 $n_{\mathrm{q}}$ 可能会导致效率损失。

---

**分片。** 如果一个索引无法适应单个 GPU 的内存，可以将索引在 $\mathcal{S}$ 个不同的 GPU 上进行分片。在添加 $\ell$ 个向量时，每个分片接收 $\ell / \mathcal{S}$ 个向量，而在查询时，每个分片处理完整的查询集 $n_q$，并在单个 GPU 或 CPU 内存中合并部分结果(仍需要额外的一轮 $k$-选择)。对于给定的索引大小 $\ell$，分片会带来加速(分片情况下查询为 $n_{\mathrm{q}}$ 对 $\ell / \mathcal{S}$，而复制情况下查询为 $n_{\mathrm{q}} / \mathcal{R}$ 对 $\ell$)，但通常由于固定开销和后续 $k$-选择的成本，速度提升低于纯复制。

复制和分片可以结合使用($\mathcal{S}$ 个分片，每个分片有 $\mathcal{R}$ 个副本，总共使用 $\mathcal{S} \times \mathcal{R}$ 个 GPU)。分片或复制都是相对简单的操作，相同的原理也可以用于在多个机器上分配索引。

# 6. EXPERIMENTS AND APPLICATIONS  

本节将我们的 GPU $k$-选择和最近邻方法与现有库进行比较。除非另有说明，实验在一台配备 4 块 Maxwell Titan X GPU 的 $2 \times 2.8 \mathrm{GHz}$ Intel Xeon E5-2680v2 服务器上使用 CUDA 8.0 进行。

## 6.1. k-Selection Performance  

我们将与另外两种 GPU 小 $k$-选择实现进行比较：来自 Tang 等人 [51] 的基于行的合并队列与缓冲搜索(Merge Queue with Buffered Search)和来自 Sismanis 等人 [50] 的截断比托尼克排序(Truncated Bitonic Sort，TBiS)。这两种实现均提取自各自的精确搜索库。这些实现的选择是因为它们不需要对输入数据进行多次遍历。任何需要对输入数据进行超过一次遍历的实现将主要受限于全局内存带宽，而在我们利用率超过 50% 的情况下，这些实现无法获胜。与我们的实现不同，fgknn 和 TBiS 需要额外的临时全局内存用于中间计算。

---

我们评估 $k$-选择，对于 $k=100$ 和 $k=1000$ 的每一行，从一个 $n_q \times \ell$ 的行主矩阵中随机选择 32 位浮点值，测试在单个 Titan X 上进行。批大小 $n_q$ 固定为 10000，数组长度 $\ell$ 从 1000 变化到 128000。问题的输入和输出保持在 GPU 内存中，输出为大小为 $n_q \times k$ 的结果，包含相应的索引。因此，输入问题的大小范围从 $40 \mathrm{MB}(\ell=1000)$ 到 $5.12 \mathrm{~GB}(\ell=128 \mathrm{k})$。在我们的测试中，TBiS 需要较大的辅助存储，限制为 $\ell \leq 48 \mathrm{k}$。

---

**图 3** 显示了我们与 TBiS 和 fgknn 的相对性能比较。它还包括 Titan X 的内存带宽限制所提供的峰值性能。WARPSELECT 相对于 fgknn 的相对性能随着 $k$ 的增大而增加；即使是 TBiS 在 $k=1000$ 时也开始在较大的 $\ell$ 下超过 fgknn。我们特别关注最大的 $\ell=128000$。在 $k=100$ 时，WarpSelect 的性能快 1.62 倍，在 $k=1000$ 时快 2.01 倍。所有实现的性能在更大的 $k$ 下都低于峰值性能。WarpSelect 在 $k=100$ 时的性能达到峰值的 55%，但在 $k=1000$ 时仅为峰值的 16%。这主要是由于与较大 $k$ 相关的额外开销，例如更大的线程队列和合并/排序网络。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923195120398.png" alt="image-20240923195120398" style="zoom: 25%;" />  

图3. 不同$k$选择方法的运行时间，作为数组长度$\ell$的函数。实线表示同时处理的数组数量$n_q=10000$和$k=100$，虚线表示$k=1000$。

---

**与 fgknn 的区别。** WARPSELECT 受到 fgknn 的影响，但有几个改进：所有状态保存在寄存器中(不使用共享内存)，没有跨 Warp 的同步，未使用多次内核启动或缓冲，没有“分层划分”，而且奇数大小的网络提供了更高效的合并和排序。这些改进使得 $k$-选择可以直接与其他 GPU 内核融合，这在精确相似性搜索(第 5.1 节)和 IVFADC 列遍历(第 5.2 节)中具有显著的性能优势。

## 6.2. k-Means Clustering  

精确搜索 $k=1$ 可以被用于 $k$-均值聚类方法的分配阶段，将 $n_{\mathrm{q}}$ 个训练向量分配给 $\left|\mathcal{C}_1\right|$ 个中心点。尽管它不使用 IVFADC，且 $k=1$ 的选择是微不足道的(并行最小化，不是 WarpSelect)，$k$-均值仍然是训练量化器 $q_1$ 时用于聚类的良好基准。

---

我们在 MNIST8m 图像上应用该算法。这 8.1M 图像为灰度数字，尺寸为 $28 \times 28$ 像素，线性化为 $784$ 维向量。在表 2 中，我们将这个 $k$-均值实现与 BIDMach [13] 的 GPU $k$-均值进行比较，后者已被证明比需要数十台机器的多个分布式 $k$-均值实现更高效。两种算法均运行 20 次迭代。我们的实现速度超过 2 倍，尽管两者都基于 cuBLAS。我们的实现受益于 $k$-选择与 $L_2$ 距离计算的融合。对于通过副本进行的多 GPU 执行，当问题足够大时，速度提升接近线性(4 个 GPU 和 4096 个中心点的加速比为 3.16)。需要注意的是，这个基准测试在一定程度上是不现实的，因为在请求如此少的中心点时，通常会随机对数据集进行子采样。

| method       | # GPUs | \# centroids 256 | \# centroids 4096 |
| :----------- | :----: | :--------------: | :---------------: |
| BIDMach [13] |   1    |      320 s       |       735 s       |
| Ours         |   1    |      140 s       |       316 s       |
| Ours         |   4    |       84 s       |       100 s       |

表2. MNIST8m $k$-Means Performance

---

**大规模。** 我们还与 Avrithis 等人 [4] 的近似方法进行比较。该方法将 $10^8$ 个 $128$ 维向量聚类到 85K 个中心点。它们的聚类方法运行时间为 46 分钟，但至少需要 56 分钟的预处理来编码向量。我们的办法在 4 个 GPU 上执行精确 $k$-均值，耗时 52 分钟，无需任何预处理。

## 6.3. Exact Nearest Neighbor Search  

我们考虑一个经典的数据集用于评估最近邻搜索：SIFT1M [32]。其特征大小为 $\ell=10^6, d=128, n_{\mathrm{q}}=10^4$。计算部分距离矩阵 $D^{\prime}$ 的成本为 $n_{\mathrm{q}} \times \ell \times d=1.28$ Tflop，在当前 GPU 上运行时间不到一秒。图 4 显示了距离计算的成本与我们在公式 (2) 中的 $-2<\mathbf{x}_{\mathbf{j}}, \mathbf{y}_{\mathbf{i}}> $ 项的 GEMM 瓦片化成本的对比，以及在大小为 $n_{\mathrm{q}} \times \ell$ 的距离矩阵上的峰值 $k$-选择性能，这还考虑了以峰值内存带宽读取瓦片结果矩阵 $D^{\prime}$ 的成本。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923195158810.png" alt="image-20240923195158810" style="zoom: 25%;" /> 

图4. 在1个Titan X GPU上，SIFT1M数据集的精确搜索$k$-NN时间，随着$k$的变化。

---

除了我们在第 5 节中的方法外，我们还包括了在第 6.1 节评估的两个 GPU 库在 $k$-选择性能方面的时间。我们做出以下观察：

- 对于 $k$-选择，使用 thrust::sort_by_key 排序完整结果数组的简单算法比比较方法慢了超过 10 倍；
- 除了我们的方法外，$L_2$ 距离和 $k$-选择的成本在所有方法中占主导地位，假设 GEMM 使用及我们对部分距离矩阵 $D^{\prime}$ 的瓦片化接近最优，该方法的性能达到了峰值性能的 85%；
- 我们的融合 $L_2$ 和 $k$-选择内核非常重要。没有融合的相同精确算法(需要额外通过 $D^{\prime}$ 进行一次)至少慢 25%。

---

在使用近似方法计算距离的情况下，效率高的 $k$-选择显得尤为重要，因为 $k$-选择相对于距离计算的相对成本会增加。

## 6.4. Billion-Scale Approximate Search  

关于大数据集上的近似最近邻搜索($\ell \gg 10^6$)的研究相对较少。我们在这里报告一些基于标准数据集和评估协议的索引搜索比较点。这些数据集的统计信息见表 3。我们最感兴趣的是 SIFT1B 和 DEEP1B，据我们所知，这两个数据集是当前可用于相似性搜索的最大公开数据集。

| dataset  | \# dataset  vectors |     \# query  vectors     |   \# training  vectors    | dims | data size |
| :------: | :-----------------: | :-----------------------: | :-----------------------: | :--: | :-------: |
|  SIF1M   |       1000000       |           10000           |          100000           | 128  |  128 MiB  |
|  SIFT1B  |     1000000000      |           10000           |         100000000         | 128  |  128 GiB  |
|  DEEP1B  |     1000000000      |           10000           |         350000000         |  96  |  384 GiB  |
| YFCC100M |      95074575       | $\mathrm{n} / \mathrm{a}$ | $\mathrm{n} / \mathrm{a}$ | 128  | 48.6 GiB  |

表3. Properties of the Datasets in Our Evaluation  

---

**SIFT1M。** 为了完整性，我们首先将我们的 GPU 搜索速度与 Wieschollek 等人 [58] 的实现进行比较。他们在 Titan X 上获得的最近邻召回率为 $R @ 1=0.51$(真实最近邻在前 1 个结果中的查询比例)和 $R @ 100=0.86$，每个查询耗时 0.02 毫秒。在相同的时间预算下，我们的实现获得了 $R@1 = 0.80$ 和 $R@100 = 0.95。

---

**SIFT1B。** 我们再次与 Wieschollek 等人进行比较，数据集为 SIFT1B，包含 10 亿个 SIFT 图像特征，$n_{\mathrm{q}}=10^4$。我们比较在相似精度下的相同内存使用的搜索性能(更精确的方法可能涉及更大的搜索时间或内存使用)。在单个 GPU 上，使用每个向量 8 字节的存储，查询向量的 $R @ 10=0.376$，耗时 $17.7 \mu \mathrm{~s}$，而他们报告的 $R @ 10=0.35$ 耗时 $150 \mu$ s。因此，我们的实现更精确，速度快了 $8.5 \times$。

---

**DEEP1B。** 我们还在 DeEp1B 数据集 [8] 上进行了实验，该数据集包含 $\ell=10^9$ 的图像 CNN 表示，$n_{\mathrm{q}}=10^4$。引入该数据集的论文报告了 CPU 结果(1 个线程)：$R@1 = 0.45$，每个向量的搜索时间为 20 毫秒。我们使用了 PQ 编码，$m=20$，通过 OPQ [23] 降至 $d=80$，并且 $\left|\mathcal{C}_1\right|=2^{18}$，这与原论文的存储数据集(20 GB)相当。由于数据集过大，无法放入单个 GPU 的全局内存，因此我们考虑使用 4 个 GPU，$\mathcal{S}=2, \mathcal{R}=2$。我们获得了 $R@1 = 0.4517$，每个向量的搜索时间为 0.0133 毫秒。虽然硬件平台不同，但这表明在 GPU 上进行搜索在单台机器上实现的速度是一个变革。

---

**YFCC100M。** 该数据集 [52] 包含 9920 万张图片和 80 万个视频。我们能够下载 9500 万张图片。我们计算 CNN 描述符，作为 ResNet [30] 的倒数第二层，使用 PCA 降至 $d=128$。由于我们仅将其用于 $k$-NN 图实验，因此不区分训练集和查询集。

## 6.5. The k-NN Graph  

我们相似性搜索方法的一个示例用法是通过暴力搜索(所有向量与整个索引进行查询)构建数据集的 $k$-最近邻图。

**实验设置。** 我们在 Yfcc100M 和 Deep1B 数据集上评估速度、精度和内存之间的权衡：

- 速度：从头构建 IVFADC 索引并通过搜索数据集中所有向量的最近邻来构建整个 $k$-NN 图($k=10$)所需的时间。因此，这是一项端到端测试，包含索引和搜索时间；
- 质量：我们抽样 10,000 张图像，以计算其精确的最近邻。我们测量返回的 10 个最近邻中有多少是在真实的 10 个最近邻之内。

对于 Yfcc100M，我们使用粗量化器($2^{16}$ 个中心点)，并考虑每个向量的 $m=16, 32$ 和 64 字节 PQ 编码。对于 Deep1B，我们通过 OPQ 将向量预处理为 $d=120$，使用 $\left|\mathcal{C}_1\right|=2^{18}$ 并考虑 $m=20, 40$。对于给定的编码，我们将 $\tau$ 从 1 变化到 256，以获得效率和质量之间的权衡，如**图 5** 所示。在 Deep1B 的 kNN 图实验中，我们没有使用训练集和查询向量，而是从主数据集中抽样。我们尝试了两种相对常见的数据密集型应用的多 GPU 工作站配置：4 个 Maxwell 级别的 Titan X GPU 或 8 个 M40 GPU。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923195541949.png" alt="image-20240923195541949" style="zoom: 25%;" />  <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923195602934.png" alt="image-20240923195602934" style="zoom:25%;" />  

图5. 对于YFCC100M和DEEP1B数据集，暴力法10-NN图构建的速度/准确性权衡。

---

**讨论。** 对于 Yfcc100M，我们使用了 $\mathcal{S}=1, \mathcal{R}=4$。在 35 分钟内获得了超过 0.8 的准确率。对于 Deep1B，可以在 6 小时内构建较低质量的图，而高质量的图大约需要半天。我们还通过将副本集翻倍，使用 8 个 Maxwell M40 GPU 进行了更多实验(M40 的性能大致相当于 Titan $X$)。性能以次线性方式提高($m=20$ 时约为 $1.6 \times$，$m=40$ 时约为 $1.7 \times$)。

---

作为比较，我们知道的最大的 $k$-NN 图构建使用了一个包含 3650 万个 384 维向量的数据集，这个过程耗费了 128 台 CPU 服务器 108.7 小时的计算时间 [56]，使用了 NN-Descent [18]。需要注意的是，NN-Descent 也可以构建或优化我们考虑的数据集的 $k$-NN 图，但它在图存储上有较大的内存开销，对于 Deep1B，存储已经达到 80 GB。此外，它还需要在所有向量之间进行随机访问(Deep1B 的数据量为 384 GB)。

---

我们找到的最大的 GPU $k$-NN 图构建是使用 GEMM 进行精确搜索的暴力构建，涉及一个包含 2000 万个 15000 维向量的数据集，耗费了 32 个 Tesla C2050 GPU 集群 10 天 [17]。假设计算成本与距离矩阵的 GEMM 成本成比例，那么在他们的集群上进行 Deep1B 的这种方法将需要不切实际的 200 天计算时间。

## 6.6. Using the k-NN Graph  

当为图像数据集构建了 $k$-NN 图时，我们可以在任何两幅图像之间找到路径，前提是图中存在一个连通分量(在本例中是这样的)。例如，我们可以通过从起始图像传播邻居到目标图像，来搜索两幅花卉图像之间的最短路径。我们用 $S$ 和 $D$ 表示源图像和目标图像，用 $d_{ij}$ 表示节点之间的距离，我们搜索路径 $P=\{p_1, \ldots, p_n\}$，其中 $p_1=S$ 和 $p_n=D$，使得：$\displaystyle{}\min_P \max_{i=1 . . n} d_{p_i p_{i+1}}$ 

即，我们希望偏向于平滑过渡。**图 6** 显示了一个结果，来自 Yfcc100M 数据集 (DEEP1B 中无法获取向量到图像的映射)。该结果是在一个 $k=15$ 邻居的 $k$-NN 图中经过 20 秒传播获得的。由于数据集中有很多花卉图像，过渡非常平滑。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923195729102.png" alt="image-20240923195729102" style="zoom:19%;" />       

图6. YFCC100M中9500万张图像的$k$-NN图中的路径。给出了第一张和最后一张图像；算法计算它们之间的最平滑路径。



## 7. CONCLUSION  

GPU 的算术吞吐量和内存带宽已经达到太菲级(teraflops)和数百吉字节每秒。然而，实现接近这些性能水平的算法是复杂且反直觉的。本文提出了相似性搜索方法的算法结构，能够在 GPU 上实现近乎最佳的性能。

---

这项工作使得以前需要复杂近似算法的应用成为可能。例如，本文提出的方法可以更快地进行精确的 $k$-均值聚类或计算 $k$-最近邻图，甚至比 CPU(或一组 CPU)用近似方法所需的时间还要短。

---

这项工作的局限性是 GPU 架构固有的。面向吞吐量、非延迟优化的 GPU 执行模型在暴力计算或线性扫描内存数组(如 IVFADC)时效率高。其他近似的 $k$-NN 方法，如近期的基于图的方法 HNSW 和 NSG，依赖于指针追踪和稀疏内存访问，可能无法高效映射到 GPU 硬件上。就像我们在这里对 GPU SIMD 架构进行高效 $k$-选择的工作一样，需要探索从表面上看是串行算法中提取并行性的方法，以评估 HNSW 和 NSG 在 GPU 上的可行性。此外，GPU 的内存仍比典型 CPU 服务器的 RAM 少一个数量级。额外的量化和压缩技术，以扩展可用的内存/速度权衡工具箱，在这种内存受限的环境中也会很有用。

---

由于机器学习算法的普及，GPU 硬件现在在科学工作站上非常常见。我们相信我们的工作进一步证明了它们在大数据应用中的重要性。































