### Approximate Nearest Neighbors Towards Removing the Curse of Dimensionality

Piotr Indyk的文章🤔

# 0. Abstract

最近邻问题是指：给定一个包含 $n$ 个点的集合 $P={p_1, \ldots, p_n}$，位于某个度量空间 $X$ 中，预处理 $P$ 以便高效地回答查询，要求找到与查询点 $q \in X$ 最近的点。在这里，我们关注 $d$ 维欧几里得空间的特定情况，其中 $X=\mathcal{R}^d$，并采用某种 $l_p$ 范数。尽管经过数十年的努力，现有的解决方案仍然远未令人满意；事实上，在大维度的情况下，理论上或实践中，它们对比暴力算法的改进微乎其微，后者比较查询点与每个数据点。近年来，对近似最近邻问题的兴趣增加，即：找到一个点 $p \in P$，使其为查询 $q$ 的 $\epsilon$-近似最近邻，满足对于所有 $p^{\prime} \in P$，$d(p, q) \leq(1+\epsilon) d\left(p^{\prime}, q\right)$。

---

我们提出了两项算法结果，显著改善了已知的界限：(a)预处理成本在 $n$ 和 $d$ 上为多项式，并且查询时间真正为次线性；(b)查询时间在 $\log n$ 和 $d$ 上为多项式，而预处理成本仅为轻微的指数级 $\tilde{O}(n) \times O(1 / \epsilon)^d$。此外，通过应用一个经典的几何引理关于随机投影(我们给出一个更简单的证明)，我们得到了第一个已知的算法，其预处理和查询时间在 $d$ 和 $\log n$ 上为多项式。然而，对于小的 $\epsilon$，后者仅是一个理论结果，因为指数依赖于 $1 / \epsilon$。实验结果表明，我们的第一个算法在真实数据集上提供了显著的运行时间改进。其关键成分是局部敏感哈希的概念，这在信息检索、模式识别、动态最近点对和快速聚类算法等应用中可能具有独立的兴趣。



# 1. Introduction

最近邻搜索(NNS)问题是：给定一个包含 $n$ 个点 $P=\left\{p_1, \ldots, p_n\right\}$ 的度量空间 $X$，其中具有距离函数 $d$，对 $P$ 进行预处理，以有效地回答查找与查询点 $q \in X$ 最近的点的问题。我们关注的特别有趣的情况是 $d$ 维欧几里得空间，其中 $X=\mathcal{R}^d$，并采用某种 $l_p$ 范数。低维情况已得到很好解决 [27]，因此主要问题在于应对“维度诅咒” [17]。这个问题最早是在 1960 年代由 Minsky 和 Papert 提出的 [54, pp. 222-225]，尽管经过数十年的努力，目前的解决方案仍然远未令人满意。事实上，对于较大的 $d$，无论在理论上还是实践中，它们相比于逐一比较查询 $q$ 与每个 $p \in P$ 的暴力算法，几乎没有改进。已知的算法有两种类型：(a)预处理成本低，但查询时间在 $n$ 和 $d$ 上线性；(b)查询时间在 $n$ 上亚线性且在 $d$ 上多项式，但预处理成本严重呈指数级增长，达到 $n^d$。这种不幸的情况在平均情况分析中也有所体现，甚至在 $\epsilon$-近似最近邻($\epsilon$-NNS)问题中也是如此：寻找一个点 $p \in P$，使其为查询 $q$ 的 $\epsilon$-近似最近邻，即对于所有的 $p^{\prime} \in P$，都有 $d(p, q) \leq(1+\epsilon) d\left(p^{\prime}, q\right)$。

---

我们提出了两个近似版本的算法，显著改善了已知的界限：(a)预处理成本在 $n$ 和 $d$ 上是多项式的，并且查询时间是真正的亚线性；(b)查询时间在 $\log n$ 和 $d$ 上是多项式的，而预处理成本仅为轻微指数级 $\tilde{O}(n) \times O(1 / \epsilon)^d$。进一步地，通过应用一个关于随机投影的经典几何引理(我们提供了一个更简单的证明)，我们获得了第一个已知的算法，其预处理和查询时间在 $d$ 和 $\log n$ 上都是多项式的。不幸的是，对于小的 $\epsilon$，这是一个纯理论结果，因为指数依赖于 $1 / \epsilon$。实验结果 [37] 表明，第一个算法在实际数据集上的运行时间有几个数量级的改善。其关键成分是局部敏感哈希的概念，这可能具有独立的兴趣；我们给出了在信息检索、模式识别、动态最近点对和快速聚类中的应用。

## 1.1. Motivation

最近邻问题在各种应用中具有重要意义，通常涉及相似性搜索。一些例子包括：数据压缩 [36]；数据库和数据挖掘 [13,39]；信息检索 [11,21,58]；图像和视频数据库 [29,31,56,61]；机器学习 [19]；模式识别 [20,26]；以及统计和数据分析 [22,45]。通常，感兴趣对象(文档、图像等)的特征被表示为 $\mathcal{R}^d$ 中的点，并使用距离度量来测量对象之间的(不)相似性。因此，基本问题是对查询对象进行索引或相似性搜索。特征的数量(即维度)从几十到几千不等。例如，在多媒体应用中，如 IBM 的 QBIC(按图像内容查询)，特征数量可能达到几百个 [29,31]。在文本文档的信息检索中，向量空间表示涉及几千维，而降维技术，如 LSI(潜在语义索引) [9, 11, 21]、主成分分析 [40] 或 Karhunen-Loéve 变换 [44, 50]，能够将维度减少到仅几百，这被认为是一个显著的改进！

---

最近，越来越多的人开始关注通过近似最近邻搜索来避免维度灾难。由于在应用中对特征的选择和距离度量的使用相当具有启发性，仅仅是试图将本质上是审美的相似性概念进行数学精确化，因此坚持绝对最近邻似乎有些过于苛刻；实际上，确定一个合理的 $\epsilon$-近似最近邻，对于一个小常数值的 $\epsilon$，对于大多数实际应用来说应该就足够了。不幸的是，即使是这种目标的放松也没有消除维度灾难，尽管 Kleinberg 的近期研究结果 [46] 提供了一些改进。

## 1.2. Previous Work  

Samet [59] 综述了多种最近邻的数据结构，包括 $k$-d 树、$R$-树的变种，以及基于填充空间曲线的结构；更近期的结果见于 [60]。虽然一些算法在 2-3 维空间中表现良好，但在高维空间中，它们在最坏情况下和典型情况下都表现不佳(例如，见 Arya、Mount 和 Narayan [4])。Dobkin 和 Lipton [23] 首次为 $\mathcal{R}^d$ 提供了最近邻算法，其查询时间为 $O\left(2^d \log n\right)$，预处理(在整个预处理过程中，成本通常指的是空间需求，通常预处理时间大致相同)成本为 $O\left(n^{2^{d+1}}\right)$。Clarkson [16] 将预处理成本降低到 $O\left(n^{\lceil d / 2\rceil(1+\delta)}\right)$，但将查询时间提高到 $O\left(2^{O(d \log d)} \log n\right)$。后来的结果，例如 Agarwal 和 Matoušek [1]、Matoušek [51]，以及 Yao 和 Yao [65]，都面临着查询时间在 $d$ 上呈指数增长的问题。Meiser [52] 获得了查询时间为 $O\left(d^5 \log n\right)$，但预处理需要 $O\left(n^{d+\delta}\right)$。所谓的“参照点”技术 [12,13,62,63] 是最近流行的启发式方法，但我们不知道其在高维欧几里得空间中的分析结果。一般而言，即使是对分布在 $\mathcal{R}^d$ 区域内的点的启发式平均情况分析，也给出了指数级的查询时间 [7,35,59]。

---

对于近似最近邻的问题，情况只稍微好一些。Arya 和 Mount [3] 提出了一个算法，其查询时间为 $O(1 / \epsilon)^d O(\log n)$，预处理时间为 $O(1 / \epsilon)^d O(n)$。Clarkson [17] 和 Chan [15] 后来将对 $\epsilon$ 的依赖减少到 $\epsilon^{-(d-1) / 2}$。Arya、Mount、Netanyahu、Silverman 和 Wu [5] 实现了最优的 $O(n)$ 预处理成本，但查询时间随着 $O(d^d)$ 增长。Bern [8] 和 Chan [15] 考虑了多项式的误差 $\epsilon$ 和 $d$，并成功避免了在这种情况下的指数依赖。最近，Kleinberg [46] 提出了一个算法，其预处理时间为 $O(n \log d)^{2 d}$，查询时间在 $d$、$\epsilon$ 和 $\log n$ 中为多项式，而另一个算法的预处理时间为 $d$、$\epsilon$ 和 $n$ 的多项式，但查询时间为 $O\left(n+d \log ^3 n\right)$。后者改进了暴力算法的 $O(d n)$ 时间界限。

---

对于汉明空间 $\{0,1\}^d$，Dolev、Harari 和 Parnas [25] 以及 Dolev、Harari、Linial、Nisan 和 Parnas [24] 提出了检索与查询 $q$ 距离不超过 $r$ 的所有点的算法。不幸的是，对于任意的 $r$，这些算法在查询时间或预处理上都是指数级的。Greene、Parnas 和 Yao [38] 提出了一个方案，对于随机均匀选择的二进制数据，可以在时间 $O\left(d n^{r / d}\right)$ 内检索与 $q$ 距离不超过 $r$ 的所有点，使用 $O\left(d n^{1+r / d}\right)$ 的预处理。

最近，Kushilevitz、Ostrovsky 和 Rabani [47] 得到了类似于下面命题 3 的结果。

## 1.3. Overview of Results and Techniques  

我们的主要结果是关于 $\epsilon\text{-NNS}$ 的算法，如下所述。

**注1**：我们的算法是随机化的，并以恒定概率返回一个近似最近邻。为了将错误概率降低到 $\alpha$，我们可以并行使用多个数据结构并返回最佳结果，复杂度增加一个 $O(\log \alpha)$ 的因子。

**注2**：为清晰起见，使用 $\tilde{O}$ 表示法来隐藏与 $n$ 的多对数相关的项。

**命题 1**：对于 $\epsilon>0$，存在一个算法用于 $\epsilon$-NNS，在 $\mathcal{R}^d$ 下使用 $l_p$ 范数($p \in[1,2]$)，其预处理成本为 $\tilde{O}\left(n^{1+1 /(1+\epsilon)}+d n\right)$，查询时间为 $\tilde{O}\left(d n^{1 /(1+\epsilon)}\right)$。

**命题 2**：对于 $0<\epsilon<1$，存在一个算法用于 $\epsilon$-NNS，在 $\mathcal{R}^d$ 下使用任意 $l_p$ 范数，其预处理成本为 $\tilde{O}(n) \times O(1 / \epsilon)^d$，查询时间为 $\tilde{O}(d)$。

**命题 3**：对于任何 $\epsilon>0$，存在一个算法用于 $\epsilon$-NNS，在 $\mathcal{R}^d$ 下使用 $l_p$ 范数($p \in[1,2]$)，其预处理成本为 $(n d)^{O(1)}$，查询时间为 $\tilde{O}(d)$。

---

我们通过将 $\epsilon$-NNS 问题简化为一个新问题，即在等半径球内的点定位，来获得这些结果。这是通过一种新颖的数据结构称为环覆盖树实现的，具体描述见第3节。我们的方法可以视为参数化搜索 [53] 的一种变体，因为它允许我们将一个优化问题简化为其决策版本。主要区别在于，在回答查询时，我们只能请求一个属于预先指定集合的决策问题的解决方案，因为解决该决策问题(即在等半径球内的点定位)需要在预处理期间创建的数据结构。我们相信这种技术将在参数化搜索有帮助的问题中找到进一步的应用。

---

在第4节中，我们给出了点定位问题的两种解决方案。一种是基于类似于 Elias 桶算法 [64] 的方法——我们将每个球体分解成有限数量的单元，并将它们存储在字典中。这使我们能够实现 $\tilde{O}(d)$ 的查询时间，而预处理的复杂度在 $d$ 上是指数级的，这意味着命题2。对于第二种解决方案，我们引入了局部敏感哈希技术。关键思想是使用哈希函数，使得靠近的对象碰撞的概率远高于远离的对象。我们证明了对于任何领域(不一定是度量空间)存在这样的函数，意味着对于该领域存在快速的 $\epsilon$-NNS 算法，预处理成本仅在 $d$ 上线性，而在 $n$ 上次线性。然后，我们展示了两类这样的函数——一类用于 Hamming 空间，另一类用于 Broder 等人 [10] 用于聚类网页文档的相似度度量下的集合子集。基于第一类的算法用于从 $\mathcal{R}^d$ 中获得最近邻算法，通过保持距离的方式将来自 $\mathcal{R}^d$ 的点嵌入到 Hamming 立方体中。相似度度量的算法被证明在信息检索和模式识别方面有多种应用。我们还给出了局部敏感哈希在动态最近点对问题和快速聚类算法中的其他应用。所有基于该方法的算法都易于实现，并且具有其他优点——它们利用了数据的稀疏性，实际运行时间 [37] 远低于理论分析的预测。我们预计这些结果将对实际应用产生重大影响。

---

由于维度引起的复杂性降低的一个优雅技术是将点投影到较低维度的随机子空间，例如，通过将 $P$ 投影到通过原点的小集合的随机直线上。具体来说，我们可以采用 Frankl 和 Maehara [33] 的结果，该结果改进了 Johnson-Lindenstrauss 引理 [42]，表明将 $P$ 投影到由大约 $9 \epsilon^{-2} \ln n$ 条随机直线定义的子空间可以以高概率保持所有点间距离在相对误差 $\epsilon$ 内。将该结果应用于查询时间为 $O(1)^d$ 的算法，我们得到一个查询时间为 $n^{9 \epsilon^{-2}}$ 的算法。不幸的是，这只会在较大值的 $\epsilon$ 下导致次线性查询时间。在附录的 A 节中，我们提供了一个随机投影结果的版本，其证明比 Frankl 和 Maehara 的更简单。我们还考虑了随机投影方法在 $l_p$ 范数($p \neq 2$)上的扩展。通过使用随机投影和命题 2，我们获得了命题 3 中描述的算法。不幸的是，高预处理成本(其指数随 $1 / \epsilon$ 增长)使得该算法在小 $\epsilon$ 时不实用。



# 2. Preliminaries  

我们使用 $l_p^d$ 来表示在 $l_p$ 范数下的空间 $\mathcal{R}^d$。对于任意点 $v \in \mathcal{R}^d$，我们用 $\|\vec{v}\|_p$ 表示向量 $\vec{v}$ 的 $l_p$ 范数；当 $p=2$ 时省略下标。同时，$H^d=\left(\{0,1\}^d, d_H\right)$ 表示维数为 $d$ 的汉明度量空间。设 $\mathcal{M}=(X, d)$ 为任意度量空间，$P \subset X$，$p \in X$。我们将采用以下符号：$d(p, P)=\min _{q \in P} d(p, q)$，而 $\Delta(P)=\max _{p, q \in P} d(p, q)$ 是 $P$ 的直径。

**定义 1** 以 $p$ 为中心的半径为 $r$ 的球体定义为 $B(p, r)=\{q \in X \mid d(p, q) \leq r\}$。以 $p$ 为中心的环 $R\left(p, r_1, r_2\right)$ 定义为 $R\left(p, r_1, r_2\right)=B\left(p, r_2\right)-B\left(p, r_1\right)=\left\{q \in X \mid r_1 \leq d(p, q) \leq r_2\right\}$。

令 $V_p^d(r)$ 表示在 $l_p^d$ 中半径为 $r$ 的球的体积。以下事实是标准的 [57, 第 11 页]。

**事实 1** 设 $\Gamma($.$)$ 表示伽马函数。那么 $V_p^d(r)=\cfrac{(2 \Gamma(1+1 / p))^d}{\Gamma(1+n / p)} r^d$，而 $V_2^d(r)=\cfrac{2 \pi^{d / 2}}{d \Gamma(d / 2)} r^d$。



# 3. Reduction to Point Location in Equal Balls  

关键思想是将 $\epsilon$-最近邻搜索($\epsilon$-NNS)简化为以下等球体中的点定位问题。

**定义 2 (等球体中的点定位(PLEB))** 给定半径为 $r$ 的球体，其中心为 $C=\left\{c_1, \ldots, c_n\right\}$，在度量空间 $\mathcal{M}=(X, d)$ 中，设计一种数据结构，对于任意查询点 $q \in X$，执行以下操作：如果存在 $c_i \in C$ 使得 $q \in B\left(c_i, r\right)$，则返回 $c_i$，否则返回 NO。

**定义 3 ($\epsilon$-等球体中的点定位($\epsilon$-PLEB))** 给定 $n$ 个半径为 $r$ 的球体，其中心为 $C=\left\{c_1, \ldots, c_n\right\}$，在度量空间 $\mathcal{M}=(X, d)$ 中，设计一种数据结构，对于任意查询点 $q \in X$，执行以下操作：

- 如果存在 $c_i \in C$ 使得 $q \in B\left(c_i, r\right)$，则返回 YES 和一个点 $c_i^{\prime}$，使得 $q \in B\left(c_i^{\prime},(1+\epsilon) r\right)$；
- 如果对于所有 $c_i \in C$，都有 $q \notin B\left(c_i,(1+\epsilon) r\right)$，则返回 NO；
- 如果对于离 $q$ 最近的点 $c_i$，满足 $r \leq d\left(q, c_i\right) \leq((1+\epsilon) r)$，则返回 YES 或 NO。

---

注意到 PLEB($\epsilon$-PLEB)可以简化为 NNS($\epsilon$-NNS)，其预处理和查询成本相同，如下所示：只需找到一个精确的($\epsilon$-近似)最近邻，然后将其与 $q$ 的距离与 $r$ 进行比较。本节的主要观点是展示如何从 $\epsilon$-NNS 反向简化到 $\epsilon$-PLEB，且仅在预处理和查询成本上增加少量开销。这种简化依赖于一种称为环覆盖树的数据结构。该结构利用了以下事实：对于任意点集 $P$，我们可以找到一个环分隔符或覆盖。任一构造都可以将 $P$ 分解为更小的集合 $S_1, \ldots, S_l$，使得对于所有 $i$，有 $\left|S_i\right| \leq c|P|$，其中 $c<1$，并且 $\sum_i\left|S_i\right| \leq b|P|$，其中 $b<1+1 / \log^2 n$。这种分解的性质在于，在搜索 $P$ 时，可以快速将搜索限制在某个集合 $S_i$ 中。

---

从 $\epsilon$-最近邻($\epsilon$-NN)到 $\epsilon$-等球体中的点定位($\epsilon$-PLEB)有一个更简单但更弱的简化。设 $R$ 为 $P$ 中最小和最大点间距离的比率。对于每个 $l \in \{(1+\epsilon)^0, (1+\epsilon)^1, \ldots, R\}$，生成一系列半径为 $l$ 的球体 $B^l=\{B_1^l, \ldots, B_n^l\}$，其中心为 $p_1, \ldots, p_n$。每个序列 $B^l$ 形成一个 PLEB 的实例。然后，对于给定的查询点 $q$，我们通过二分查找找到最小的 $l$，使得存在某个 $i$ 使得 $q \in B_i^l$，并将 $p_i$ 返回作为近似最近邻。整体简化参数为：查询时间开销因子为 $O(\log \log R)$，空间开销因子为 $O(\log R)$。这种简化的简单性在实践中非常有用。另一方面，当 $R$ 较大时，$O(\log R)$ 的空间开销是不可接受的；一般来说，$R$ 可能是无界的。在最终版本中，我们将展示通过使用这种方法的变体，可以将存储减少到 $O\left(n^2 \log n\right)$，这仍然无法提供所需的 $O\left(\cfrac{1}{\epsilon}\right)^d \tilde{O}(n)$ 界限。

---

**定义 4** 一个环 $R\left(p, r_1, r_2\right)$ 是 $P$ 的一个 $\left(\alpha_1, \alpha_2, \beta\right)$-环分隔符，如果满足 $\left|P \cap B\left(p, r_1\right)\right| \geq \alpha_1|P|$ 且 $\left|P \backslash B\left(p, r_2\right)\right| \geq \alpha_2|P|$，其中 $r_2 / r_1=\beta>1$，且 $\alpha_1, \alpha_2>0$。

**定义 5** 集合 $S \subset P$ 是 $P$ 的一个 $(\gamma, \delta)$-聚类，如果对于每个 $p \in S$，满足 $|P \cap B(p, \gamma \Delta(S))| \leq \delta|P|$，其中 $0<\gamma, \delta<1$。

**定义 6** 一系列集合 $A_1, \ldots, A_l$，其中 $A_i \subset P$，称为 $S \subset P$ 的 $(b, c, d)$-覆盖，如果存在一个 $r \geq d \Delta(A)$，其中 $A=\cup_i A_i$，使得 $S \subset A$，且对于 $i=1, \ldots, l$，

- $\left|P \cap\left(\cup_{p \in A_i} B(p, r)\right)\right| \leq b\left|A_i\right|$，
- $\left|A_i\right| \leq c|P|$。

其中 $b>1$，$0<c<1$，$d>0$。

---

**定理 1** 对于任意集合 $P$，满足 $0<\alpha<1$ 和 $\beta>1$，以下两个性质之一必须成立：

1. $P$ 存在一个 $(\alpha, \alpha, \beta)$-环分隔符，或者
2. $P$ 包含一个大小至少为 $(1-2 \alpha)|P|$ 的 $\left(\cfrac{1}{2 \beta}, \alpha\right)$-聚类。

**证明概要：** 首先注意，当 $\alpha>1 / 2$ 时，性质(1)必须为假，但此时性质(2)显然为真。一般来说，假设(1)不成立。那么，对于任意点 $p$ 和半径 $r$ 定义：

- $f_p^{\infty}(r)=|P-B(p, \beta r)|$，
- $f_p^0(r)=|P \cap B(p, r)|$。

显然，$f_p^{\infty}(0)=n, f_p^{\infty}(\infty)=0, f_p^0(0)=0$，且 $f_p^0(\infty)=n$。同时注意到 $f_p^{\infty}(r)$ 是单调递减的，而 $f_p^0(r)$ 是单调递增的。因此，必然存在一个 $r$(设为 $r_p$)，使得 $f_p^{\infty}\left(r_p\right)=f_p^0\left(r_p\right)$。由于(1)不成立，对于任何 $r$ 的值，我们必须有 $\min \left(f_p^{\infty}(r), f_p^0(r)\right) \leq \alpha n$，这意味着 $f_p^{\infty}\left(r_p\right)=f_p^0\left(r_p\right) \leq \alpha n$。

设 $q$ 为使得 $r_q$ 最小的点。定义 $S=P \cap R\left(q, r_q, \beta r_q\right)$；因此，$|S| \geq (1-2 \alpha) n$。同时注意到，对于任意 $s, s^{\prime} \in S$，有 $d\left(s, s^{\prime}\right) \leq 2 \beta r_q$，这意味着 $\Delta(S) \leq 2 \beta r_q$。最后，对于任意 $s \in S$，有 $\left|P \cap B\left(s, r_q\right)\right| \leq \left|P \cap B\left(s, r_s\right)\right| \leq \alpha n$。

---

**定理 2** 设 $S$ 是 $P$ 的一个 $(\gamma, \delta)$-聚类。那么对于任意的 $b$，存在一个算法，可以生成一系列集合 $A_1, \ldots, A_k \subset P$，构成一个 $\left(b, \delta, \cfrac{\gamma}{(1+\gamma) \log_b n}\right)$-覆盖。

**证明概要**：下面的算法贪婪地计算出 $S$ 的一个良好覆盖。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240922231445474.png" alt="image-20240922231445474" style="zoom: 40%;" /> 

为了证明算法的正确性，只需满足以下四个声明。

- $S \subset A=\cup_j A_j$ - 由外层循环的终止条件得出。
- 对于所有 $j \in \{1, \ldots, k\}$ 和任意 $p \in S$，$\left|P \cap \cup_{q \in A_j} B(p, r)\right| \leq b\left|A_j\right|$ - 由内层循环的终止条件得出。
- 对于所有 $j \in \{1, \ldots, k\}$，$\left|A_j\right| \leq \delta|P|$ - 显然，对于任何 $j$，内层循环最多重复 $\log_b n$ 次。因此，$\max_{q \in A_j} d\left(p_j, q\right) \leq r \log_b n \leq \gamma \Delta(S)$。由于 $S$ 是一个 $(\gamma, \delta)$-聚类，我们有 $\left|B\left(p_j, \gamma \Delta(S)\right) \cap P\right| \leq \delta|P|$。因此，$\left|A_j\right| \leq \delta|P|$。
- $r \leq \cfrac{\gamma \Delta(S)}{(1+\gamma) \log_b n}$ - 因为 $\Delta(A) \leq \Delta(S)+r \log_b n=\Delta(S)+\gamma \Delta(S)=(1+\gamma) \Delta(S)$。

---

**推论 1** 对于任意的 $P, 0<\alpha<1, \beta>1, b>1$，以下属性之一必须成立：

1. $P$ 具有一个 $(\alpha, \alpha, \beta)$-环分隔器 $R(p, r, \beta r)$，或者
2. 存在一个 $(b, \alpha, d)$-覆盖，针对某个 $S \subset P$，使得 $|S| \geq (1-2 \alpha) n$ 且 $d=\cfrac{1}{(2 \beta+1) \log_b n}$。

## 3.1. Constructing RingCover Trees  

环覆盖树的构造是递归的。对于给定的 $P$ 作为根节点，我们使用推论 1 中的属性 (1) 和 (2) 将 $P$ 分解为一些更小的集合 $S_1, \ldots, S_l$；这些集合被分配给 $P$ 节点的子节点。注意，当 $P$ 足够小时，基本情况被省略在此摘要中。我们还在 $P$ 节点存储一些额外的信息，使我们能够通过距离计算或点位置查询将最近邻搜索限制到 $P$ 的某个子节点。为简便起见，假设我们可以调用一个精确的 PLEB(而不是 $\epsilon$-PLEB)；该构造可以轻松修改以适应近似点位置。根据属性 (1) 和 (2) 中哪一个成立，有两种情况。设 $\beta=2\left(1+\cfrac{1}{\epsilon}\right), b=1+\cfrac{1}{\log^2 n}$，且 $\alpha=\cfrac{1-1/\log n}{2}$。

---

**情况 1**。在这种情况下，我们将 $P$ 称为环节点。我们将其子节点定义为 $S_1=P \cap B(p, \beta r)$ 和 $S_2=P-B(p, r)$。同时，我们在节点 $P$ 存储关于环分隔符 $R$ 的信息。

**情况 2**。在这里，我们将 $P$ 称为覆盖节点。我们定义 $S_i=P \cap \cup_{p \in A_i} B(p, r)$，且 $S_0=S-A$。存储在 $P$ 的信息如下。设 $r_0=(1+1/\epsilon) \Delta(A)$，且 $r_i=r_0/(1+\epsilon)^i$，其中 $i \in \{1, \ldots, k\}$，$k=\log_{1+\epsilon} \cfrac{(1+1/\epsilon) \log_b n}{\gamma}+1$。注意到 $r_k=\cfrac{\gamma \Delta(A)}{\log_b n(1+\epsilon)}=\cfrac{r}{1+\epsilon}$。对于每个 $r_i$，生成一个包含 $B(p, r_i)$ 的 PLEB 实例，所有实例都存储在 $P$。

---

**定理 3** 环覆盖树可以在确定性 $\tilde{O}(n^2)$ 时间内构造。

**证明大纲**：构造过程如下。在每一层中，我们确定一个节点是环节点还是覆盖节点；然后我们计算环或覆盖。由于层数为 $\tilde{O}(1)$，因此只需考虑根节点 $P$ 的构造。首先，对于每个 $p \in P$，我们构建一个列表 $L_p$，该列表包含所有其他点 $q \in P$，按 $d(p, q)$ 的递增顺序排序。这需要 $O(n^2 \log n)$ 时间。然后，我们应用定理 1 证明中的方法。更具体地，我们计算每个 $p \in P$ 的函数 $f_p^{\infty}$ 和 $f_p^0$。有了列表 $L_p$，可以在 $O(n^2)$ 时间内轻松实现。接下来，我们尝试找到一个环；如果找到，则完成。反之，如果没有找到，我们则找到一个簇，并应用算法 COVER 来寻找覆盖。可以验证，在给定列表 $L_p$ 的情况下，该算法的运行时间为 $\tilde{O}(n)$。PLEB 的生成只增加了一个 $\tilde{O}(1)$ 的因素。因此，总的所需时间为 $\tilde{O}(n^2)$。

---

我们现在描述如何高效地搜索环覆盖树。只需证明对于任意节点 $P$，我们可以通过少量测试将搜索限制到其某个子节点。令 $\min _q\left(p, p^{\prime}\right)$ 表示与点 $q$ 距离更近的点 $p$ 或 $p^{\prime}$。搜索过程如下；我们省略明显的基本情况。

**搜索过程**  

1. 如果 $P$ 是一个带有 $(\alpha, \alpha, \beta)$-环分隔器 $R(p, r, \beta r)$ 的环节点，则：

   - 如果 $q \in B(p, r(1+1/\epsilon))$，则返回 $\text{Search}\left(q, S_1\right)$。
   - 否则计算 $p^{\prime}=\text{Search}\left(q, S_2\right)$；返回 $\min _q\left(p, p^{\prime}\right)$。
2. 如果 $P$ 是一个覆盖节点，且 $A_1, \ldots, A_l$ 是 $S \subset P$ 的 $(b, c, d)$-覆盖，半径为 $r$，则：
   - 如果 $q \notin B\left(a, r_0\right)$，则计算 $p=\text{Search}(q, P-A)$，选择任意 $a \in A$，并返回 $\min _q(p, a)$。
   - 如果 $q \in B\left(a, r_0\right)$ 对于某些 $a \in A$，但对于所有 $a^{\prime} \in A$，$q \notin B\left(a^{\prime}, r_k\right)$，则使用二分搜索 $r_i$，在 $A$ 中找到 $q$ 的 $\epsilon$-最近邻 $p$，计算 $p^{\prime}=\text{Search}(q, P-A)$，并返回 $\min _q\left(p, p^{\prime}\right)$。
   - 如果 $q \in B\left(a, r_k\right)$ 对于某些 $a \in A_i$，则返回 $\text{Search}\left(q, S_i\right)$。

## 3.2. Analysis of RingCover Trees  

我们开始分析环覆盖树构造的有效性，首先建立搜索过程的有效性。

**引理 1** 过程 Search $(q, P)$ 在 $P$ 中产生 $q$ 的 $\epsilon$-最近邻。

**证明概要**：考虑两种情况：

1. $P$ 是一个环节点。
   - 考虑任意 $s \in P-S_1$。那么有 $d(s, p) \leq d(s, q) + d(q, p)$，这意味着 $d(s, q) \geq d(s, p) - d(q, p)$。由于 $s \notin S_1$，我们知道 $d(s, p) \geq \beta r = 2(1 + \cfrac{1}{\epsilon}) r$，而 $d(p, q) \leq r(1 + \cfrac{1}{\epsilon})$。因此，$d(s, q) \geq (1 + \cfrac{1}{\epsilon}) r \geq d(q, p)$。
   - 对于任何 $s \in B(p, r)$，有 $d(q, p) \leq d(q, s) + d(s, p)$，这意味着 $d(q, s) \geq d(q, p) - d(s, p) \geq d(q, p) - r$。因此，$\cfrac{d(q, p)}{d(q, s)} \leq \cfrac{d(q, p)}{d(q, p) - r} = 1 + \cfrac{r}{d(q, p) - r} \leq 1 + \epsilon$。
  
2. $P$ 是一个覆盖节点。
   - 类似于情况 1(b)，
   - 显然。
   - 对于任何 $p \in P-S_i$，$d(p, a) \geq r$。由于 $q \in B\left(a, r_k\right)$，我们有 $d(q, a) \leq r_k = \cfrac{r}{1 + \epsilon} \leq \cfrac{d(p, q)}{1 + \epsilon}$

引理 2 和引理 3 的证明省略。

**引理 2** 环覆盖树的深度为 $O\left(\log _{1 / 2 \alpha} n\right) = O\left(\log ^2 n\right)$。

**引理 3** 过程 Search 需要 $O\left(\log ^2 n \times \log k\right)$ 次距离计算或 PLEB 查询。

---

**引理 4** 一个环覆盖树的空间需求最多为 $O\left(k n b^{\log _{1 / 2 \alpha} n}(1+2(1-2 \alpha))^{\log _n}\right) = O(n \text{ polylog } n)$，不包括算法实现 PLEB 时使用的额外非数据存储。

**证明概要**：设 $S(n)$ 是对大小为 $n$ 的点集 $P$ 的环覆盖树空间需求的上界。那么对于一个覆盖节点：

$\displaystyle{}S(n) \leq  \max_l{ }\max_{A_1 \ldots A_l, A_i \text { disjoint, }\left|A_i\right| \leq \alpha n,|A| \geq(1-2 \alpha) n}  {\left[\sum_{i=1}^l S\left(b\left|A_i\right|\right)\right]+S(n-|A|)+|A| b k }$ 

对于一个环节点：

$S(n) \leq 2 S\left(\cfrac{n}{2}(1+2(1-2 \alpha))\right)+1$ 

通过求解这个递归可以得到上界。

**推论 2** 如果存在一个 PLEB 算法，它在大小为 $n$ 的实例上使用 $f(n)$ 空间，其中 $f(n)$ 是凸函数，则对于大小为 $n$ 的点集 $P$，一个环覆盖树的总空间需求为 $O(f(n \text{ poly log } n))$。

**事实 2** 对于由环覆盖树生成的任何 PLEB 实例 $(C, r)$，有 $\cfrac{\Delta(C)}{r} = O\left(\cfrac{1+\epsilon}{\gamma} \log_b n\right)$



# 4. Point Location in Equal Balls  

我们提出了两种解决 $\epsilon$-PLEB 问题的技术。第一种基于类似于 Elias 桶算法的方法 [64]，适用于任何 $l_p$ 范数，从而建立了命题 2。第二种使用局部敏感哈希，直接应用于汉明空间(这与 Greene、Parnas 和 Yao [38] 提出的索引技术以及 Karp、Waarts 和 Zweig [48] 的全对向量交集算法有一些相似之处，尽管技术发展非常不同)。然而，通过利用事实 2 和 4(附录 A)，在解决 $l_1^d$ 的 $\epsilon$-NN 时生成的 $\epsilon$-PLEB 实例可以简化为 $H^m$ 中的 $\epsilon$-PLEB，其中 $m=d \log_b n \times \max (1 / \epsilon, \epsilon)$。此外，依据事实 3(附录 A)，我们可以将 $l_p^d$ 简化为 $l_1^{O(d)}$，适用于任何 $p \in [1,2]$。因此，局部敏感哈希可以用于任何 $l_p$ 范数，其中 $p \in [1,2]$，从而建立了命题 1。它还可以用于 Broder 等人 [10] 提出的用于聚类网页文档的集合相似度度量。我们假设，在不失一般性的情况下，所有球体的半径为 1。

## 4.1. The Bucketing Method  

假设现在 $p=2$。我们在 $\mathbb{R}^d$ 上施加间距为 $s=\epsilon / \sqrt{d}$ 的均匀网格。显然，属于同一个网格立方体的任意两个点之间的距离最多为 $\epsilon$。根据事实 2，包含来自 $C$ 的球体的最小立方体每一边的长度至多为网格单元边长的 $O\left(\sqrt{d} \log_b n \max(1 / \epsilon, \epsilon)\right)$ 倍。对于每个球 $B_i$，定义 $\bar{B}_i$ 为与 $B_i$ 相交的网格单元集合。将所有元素 $\cup_i \bar{B}_i$ 存储在哈希表中 [34, 55]，并附带相应球的信息。(由于前面的讨论，宇宙的大小是有界的，因此我们可以使用哈希存储)在预处理之后，要回答查询 $q$，只需计算包含 $q$ 的单元，并检查该单元是否存储在表中。

---

我们声称，对于 $0<\epsilon<1$，$|\bar{B}|=O(1 / \epsilon)^d$。可以观察到，$|\bar{B}|$ 的上界由半径为 $r=2 / \epsilon \sqrt{d}$ 的 $d$ 维球体的体积决定，根据事实 1，其体积为 $2^{O(d)} r^d / d^{d / 2} \leq O(1 / \epsilon)^d$。因此，总空间需求为 $O(n) \times O(1 / \epsilon)^d$。查询时间是计算哈希函数所需的时间。我们使用如下形式的哈希函数：

$h\left(\left(x_1, \ldots, x_d\right)\right)=\left(a_1 x_1+\ldots+a_d x_d \bmod P\right) \bmod M$

其中 $P$ 是一个质数，$M$ 是哈希表的大小，$a_1, \ldots, a_d \in \mathcal{Z}_P^*$。该族哈希函数提供了一个静态字典，具有 $O(1)$ 的访问时间 [34]。哈希函数可以使用 $O(d)$ 的算术操作进行评估。对于一般的 $l_p$ 范数，我们将 $s$ 修改为 $\epsilon / d^{1 / p}$。关于 $|\bar{B}|$ 的界限保持不变。

---

**定理 4** 对于 $0<\epsilon<1$，存在一个算法可以在 $l_p^d$ 中解决 $\epsilon$-PLEB，预处理时间为 $O(n) \times O(1 / \epsilon)^d$，每个查询的哈希函数评估时间为 $O(1)$。

## 4.2. Locality Sensitive Hashing  

我们引入局部敏感哈希（locality-sensitive hashing）的概念，并将其应用于次线性时间的相似性搜索。该定义对对象相似性度量没有假设。实际上，它适用于相似性和不相似性度量；前者的一个例子是点积，而任何距离度量则是后者的一个实例。为了统一符号，我们将相似性度量 $D$ 定义的球体表示为 $B(q, r) = \{p: D(q, p) \geq r\}$。我们还将 $\epsilon$-PLEB 的概念推广到 $\left(r_1, r_2\right)$-PLEB，其中对于任何查询点 $q$，如果 $P \cap B\left(q, r_2\right) \neq \emptyset$，则要求答案为“是”，否则为“否”。

---

**定义 7** 一组 $\mathcal{H} = \{h: S \rightarrow U\}$ 称为针对 $D$ 的 $\left(r_1, r_2, p_1, p_2\right)$-敏感，如果对于任何 $q, p \in S$ 满足：

- 如果 $p \in B\left(q, r_1\right)$，则 $\text{Pr}_{\mathcal{H}}[h(q) = h(p)] \geq p_1$，
- 如果 $p \notin B\left(q, r_2\right)$，则 $\text{Pr}_{\mathcal{H}}[h(q) = h(p)] \leq p_2$。

为了使局部敏感家族有用，必须满足不等式 $p_1 > p_2$ 和 $r_1 < r_2$ 当 $D$ 为不相似性度量时，或 $p_1 > p_2$ 和 $r_1 > r_2$ 当 $D$ 为相似性度量时。

---

对于稍后指定的 $k$，定义一个函数家族 $\mathcal{G}=\left\{g: S \rightarrow U^k\right\}$，使得 $g(p)=\left(h_1(p), \ldots, h_k(p)\right)$，其中 $h_i \in \mathcal{H}$。算法如下。对于一个整数 $l$，我们从 $\mathcal{G}$ 中独立且均匀随机选择 $l$ 个函数 $g_1, \ldots, g_l$。在预处理阶段，我们将每个 $p \in P$ 存储在桶 $g_j(p)$ 中，$j=1, \ldots, l$。由于桶的总数可能很大，我们仅保留非空桶，通过哈希处理来实现 [34, 55]。为了处理查询 $q$，我们搜索所有桶 $g_1(q), \ldots, g_l(q)$；由于存储在这些桶中的总点数可能很大（尽管不太可能），我们在找到前 $2l$ 个点（包括重复）后中断搜索。令 $p_1, \ldots, p_t$ 为其中遇到的点。对于每个 $p_j$，如果 $p_j \in B\left(q, r_2\right)$，则我们返回“是”和 $p_j$，否则返回“否”。

------

参数 $k$ 和 $l$ 的选择确保以恒定的概率满足以下两个属性：

1. 如果存在 $p^* \in B\left(q, r_1\right)$，则 $g_j\left(p^*\right)=g_j(q)$ 对于某个 $j=1 \ldots l$ 成立。
2. $q$ 与来自 $P-B\left(q, r_2\right)$ 的点的总碰撞数少于 $2l$，即$\sum_{j=1}^l\left|\left(P-B\left(q, r_2\right)\right) \cap g_j^{-1}\left(g_j(q)\right)\right|<2 l$ 

观察到，如果 (1) 和 (2) 都成立，则算法是正确的。

---

**定理 5** 假设存在一个针对度量 $D$ 的 $\left(r_1, r_2, p_1, p_2\right)$-敏感家族 $\mathcal{H}$。那么存在一个算法可以在度量 $D$ 下实现 $\left(r_1, r_2\right)$-PLEB，所需空间为 $O\left(d n+n^{1+\rho}\right)$，每个查询需要 $O\left(n^\rho\right)$ 次哈希函数的评估，其中 $\rho=\cfrac{\ln 1 / p_1}{\ln 1 / p_2}$。

**证明概要**：只需确保 (1) 以概率 $P_1$ 成立，(2) 以概率 $P_2$ 成立，且 $P_1$ 和 $P_2$ 都严格大于 1/2。假设 $p^* \in B\left(q, r_1\right)$；当 $p^* \notin B\left(q, r_2\right)$ 时，证明过程类似。设 $k=\log _{1 / p_2} n$，则 $p \in P-B\left(q, r_2\right)$ 的情况下，$g\left(p^{\prime}\right)=g(q)$ 的概率至多为 $p_2^k=\cfrac{1}{n}$。因此，在固定的 $g_j$ 下，来自 $P-B\left(q, r_2\right)$ 的元素与 $q$ 碰撞的期望数量最多为 1；对于任何 $g_j$，此类碰撞的期望数量最多为 $l$。因此，根据马尔可夫不等式，该数量超过 $2 l$ 的概率小于 $1 / 2$；因此，属性 (2) 成立的概率为 $P_2>1 / 2$。

------

现在考虑 $g_j\left(p^*\right)=g_j(q)$ 的概率。显然，它的下界为$p_1^k=p_1^{\log _{1 / p_2} n}=n^{-\cfrac{\log 1 / p_1}{\log 1 / p_2}}=n^{-\rho}$

因此，存在这样的 $g_j$ 的概率至少为 $P_1=1-\left(1-n^{-\rho}\right)^l$。通过设置 $l=n^\rho$，我们得到 $P_1>1-1 / e>1 / 2$。定理得证。

---

我们将定理 5 应用到两种度量上：汉明距离和集合相似度 [10]；后者是一个定义在任意一对集合 $A$ 和 $B$ 上的相似性度量，表示为 $D(A, B)=\cfrac{|A \cap B|}{|A \cup B|}$。对于第一种度量，我们应用了一类投影函数进行快速哈希，使用 $A C^0$ 操作 [6]。对于第二种度量，我们使用之前提到的草图函数 [10] 来估计给定集合 $A$ 和 $B$ 之间的相似度。

---

**命题 4 ([6])** 设 $S=\mathcal{H}^d$，$D(p, q)$ 为 $p, q \in \mathcal{H}$ 的汉明距离。则对于任意 $r, \epsilon>0$，家族 $\mathcal{H}=\left\{h_i: h_i\left(\left(b_1, \ldots, b_d\right)\right)=b_i, i=1 \ldots n\right\}$ 是 $\left(r, r(1+\epsilon), 1-\cfrac{r}{d}, 1-\cfrac{r(1+\epsilon)}{d}\right)$-敏感的。

**推论 3** 对于任意 $\epsilon>0$，存在一个针对 $H^d$（或任何 $p \in[1,2]$ 的 $l_p^d$）的 $\epsilon$-PLEB 算法，所需空间为 $O\left(d n+n^{1+1 /(1+\epsilon)}\right)$，每个查询需要 $O\left(n^{1 /(1+\epsilon)}\right)$ 次哈希函数的评估。该哈希函数可以通过 $O(d)$ 次操作进行评估。

---

**证明概要**：我们使用命题4和定理5。首先，我们需要估计$\rho=\cfrac{\ln 1 / p_1}{\ln 1 / p_2}$的值，其中$p_1=1-\cfrac{r}{d}$，$p_2=1-\cfrac{r(1+\epsilon)}{d}$。为了不失一般性，我们假设$r<\cfrac{d}{\ln n}$，因为我们可以通过在每个点的末尾添加足够长的0字符串来增加维度。注意到：$\rho=\cfrac{\ln 1 / p_1}{\ln 1 / p_2}<\cfrac{\ln \cfrac{1}{1-r / d}}{\ln \cfrac{1}{1-(1+\epsilon) r / d}}=\cfrac{\ln (1-r / d)}{\ln (1-(1+\epsilon) r / d)}$ 


将分子和分母都乘以$\cfrac{d}{r}$，我们得到：$\rho=\cfrac{\cfrac{d}{r} \ln (1-r / d)}{\cfrac{d}{r} \ln (1-(1+\epsilon) r / d)}=\cfrac{\ln (1-r / d)^{d / r}}{\ln (1-(1+\epsilon) r / d)^{d / r}}=\cfrac{U}{L}$


为了对$\rho$进行上界，我们需要从下界$U$和上界$L$进行估计；注意到$U$和$L$都是负的。为此，我们使用以下不等式[55]：$(1-(1+\epsilon) r / d)^{d / r}<e^{-(1+\epsilon)} \quad \text {and} \quad(1-r / d)^{d / r}>e^{-1}\left(1-\cfrac{1}{d / r}\right)$

因此，

$\small\begin{aligned}
\cfrac{U}{L} & <\cfrac{\ln \left(e^{-1}\left(1-\cfrac{1}{d / r}\right)\right)}{\ln e^{-(1+\epsilon)}} =\cfrac{-1+\ln \left(1-\cfrac{1}{d / r}\right)}{-(1+\epsilon)} =1 /(1+\epsilon)-\cfrac{\ln \left(1-\cfrac{1}{d / r}\right)}{1+\epsilon} <1 /(1+\epsilon)-\ln (1-1 / \ln n)
\end{aligned}$

最后一步使用了$\epsilon>0$和$r<\cfrac{d}{\ln n}$的假设。我们得出结论：

$n^\rho<n^{1 /(1+\epsilon)} n^{-\ln (1-1 / \ln n)}=n^{1 /(1+\epsilon)}(1-1 / \ln n)^{-\ln n}=O\left(n^{1 /(1+\epsilon)}\right)$ 

---

哈希函数的评估可以比$O(d)$更快，特别是在稀疏数据的情况下，即当查询点的非零坐标数量较少时。只需从向量的非零条目中抽样位；类似的方法也适用于构建静态字典的函数。此外，我们的经验表明，预处理空间和查询时间远低于上述界限所指示的。在特别情况下，我们实现了一种上述数据结构的变体，当数据存储在磁盘上时 [37]。对于一个包含20,000个$d$-颜色直方图的数据集（$d$的范围可达64），只需进行$3-9$次磁盘访问即可实现较小的平均误差。

---

**命题 5** ([10]) 设$S$为$X=\{1 \ldots x\}$的所有子集的集合，$D$为集合相似度度量。那么，对于$1>r_1>r_2>0$，以下哈希族是$\left(r_1, r_2, r_1, r_2\right)$-敏感的：

$\mathcal{H}=\left\{h_\pi: h_\pi(A)=\max _{a \in A} \pi(a), \pi \text{是} X \text{的一个排列}\right\}$ 

**推论 4** 对于$0<\epsilon, r<1$，存在一个$(r, \epsilon r)$-PLEB算法，在集合相似度度量$D$下使用$O\left(d n+n^{1+\rho}\right)$的空间和每个查询$O\left(n^\rho\right)$次哈希函数评估，其中$\rho=\cfrac{\ln r}{\ln \epsilon r}$。

---

我们现在讨论上述推论的进一步应用。对于任意一对点$p, q \in \mathcal{H}^d$，考虑定义为点积的相似度度量$D(p, q)$。点积是信息检索应用中常用的度量 [32]；在分子聚类中也很有用 [14]。通过使用Indyk、Motwani和Venkatasubramanian的技术 [41]，它也可以用于解决近似最大公共点集问题，该问题在图像检索和模式识别中有许多应用。通过简单的参数替换，我们可以证明，对于一组大致相同权重的二进制向量，在点积度量下的PLEB（固定权重查询）可以归约为在集合相似度度量下的PLEB。固定权重的假设可以通过将数据点分成$O(\log d)$组大致相同权重来轻松满足，然后对潜在查询的权重进行相同的划分。

## 4.3. Further Applications of PLEB Algorithms  

上述描述的PLEB过程也可以用于点随着时间的插入和删除。在随机索引方法中，可以通过将点添加到所有索引中来进行插入，而通过从所有索引中删除该点来进行删除。在分桶方法中，可以通过在哈希表中添加或删除所有元素$\bar{B}$来进行插入和删除。然而，为了应用这些方法，我们必须假设点的坐标是整数，且绝对值有界，例如被限制在$M$之内。设$n$为任何时刻存在的最大点数。

**推论 5** 存在一个数据结构用于在$\{1 \ldots M\}^d$中实现$\epsilon-P L E B$，该结构可以在时间$O(1 / \epsilon)^d$ poly $(\log M, \log n)$内执行插入、删除和查询，存储需求为$O(1 / \epsilon)^d n$。

**推论 6** 存在一个数据结构用于在$\{1 \ldots M\}^d$中实现$\epsilon-P L E B$，该结构可以在时间$\tilde{O}\left(d n^{1 /(1+\varepsilon)}\right)$内执行插入、删除和查询，存储需求为$O\left(d n+n^{1+1 /(1+\varepsilon)}\right)$。

---

后一个推论源于这样的事实：为了计算$g(q)$，我们不需要明确保留$p$的单一表示。相反，只需跟踪每个坐标的断点，即采样位从0变为1的点；这显然每个坐标只需常量内存字即可。

---

通过保留多个PLEB副本，如第3节开头描述的简单方法，我们可以回答近似最近对查询。只需检查每个半径下，是否任何单元（在分桶方法中）或任何桶（在随机索引方法中）包含两个不同的点；具有此属性的最小半径即为最近对距离的近似值。所有操作的时间界限与上述推论相同，但乘以$O\left(\log \log_{1+\epsilon} M\right)$的因子。显然，双色对问题（其中点被着色且只考虑不同颜色的对）甚至多色对问题（针对两种以上颜色）也可以在相同时间界限内解决。

---

结合这两种技术，我们获得了一种动态估计最近对的方法。Eppstein [28] 最近指出，动态最近对问题在层次聚合聚类、贪婪匹配和其他问题中有很多应用，并提供了一种数据结构，使每次更新操作进行$\tilde{O}(n)$的距离计算。我们的方案以亚线性时间给出近似答案。此外，通过使用动态多色最近对数据结构轻松模拟Kruskal的最小生成树算法，近似最小生成树问题可以在近似双色最近对的成本乘以$\tilde{O}(n)$的时间界限内解决。因此，我们获得了第一个在任何维度下以亚二次时间解决此问题的算法。



# A. The Dimension Reduction Technique  

我们首先概述随机投影技术用于降维的证明。将此与命题2结合，我们得到了命题3中给出的结果。

**定义8** 设$\mathcal{M}=(X, d)$和$\mathcal{M}^{\prime}=\left(X^{\prime}, d^{\prime}\right)$是两个度量空间。如果存在一个映射$f: \mathcal{M} \rightarrow \mathcal{M}^{\prime}$，使得对于所有$p, q \in X$都有

$$(1-\epsilon) d(p, q) < d^{\prime}(f(p), f(q)) < (1+\epsilon) d(p, q)$$

那么我们称空间$\mathcal{M}$在$\mathcal{M}^{\prime}$中具有$c$-**等距嵌入**，或简单地称为$c$-**嵌入**。我们称$c$为嵌入的**失真**；如果$c=1$，我们称该嵌入为**等距的**。

---

Frankl和Maehara [33] 对Johnson-Lindenstrauss引理 [42] 提出了以下改进，涉及将任何$S \subset l_2^d$ $(1+\epsilon)$-嵌入到$l_2^{O(\log |S|)}$中。

**引理5**（Frankl-Maehara [33]）对于任意$0<\epsilon<\frac{1}{2}$，任何（足够大的）点集$S \subset \mathcal{R}^d$，以及$k=\left\lceil 9\left(\epsilon^2-2 \epsilon^3 / 3\right)^{-1} \ln |S|\right\rceil+1$，存在一个映射$f: S \rightarrow \mathcal{R}^k$，使得对于所有$u, v \in S$，

$$(1-\epsilon)\|u-v\|^2 < \|f(u)-f(v)\|^2 < (1+\epsilon)\|u-v\|^2$$

证明过程通过展示任意单位向量$v$在随机$k$维超平面上的投影长度的平方是集中在$\frac{k}{d}$附近来进行。下面我们证明一个类似的事实。然而，由于采用了不同的分布，我们能够提供一个更简单的证明，并且改进了常数。注意，这些常数是重要的，因为它们出现在命题3中结果算法的时间界限的指数中。

---

**引理6** 设$u$为$\mathcal{R}^d$中的单位向量。对于任何偶数正整数$k$，令$U_1, \ldots, U_k$为独立地从$d$维高斯分布（每个分量独立地从标准正态分布中选择）$N^d(0,1)$中选择的随机向量。定义$X_i = u \cdot U_i$，并令$W=W(u)=(X_1, \ldots, X_k)$，$L=L(u)=\|W\|^2$。则对于任意$\beta>1$，

1. $E(L)=k$，
2. $\text{Pr}[L \geq \beta k] < O(k) \times \exp \left(-\frac{k}{2}(\beta-(1+\ln \beta))\right)$，
3. $\text{Pr}[L \leq k / \beta] < O(k) \times \exp \left(-\frac{k}{2}\left(\beta^{-1}-(1-\ln \beta)\right)\right)$。

---

**证明概要**： 由于 $N^d(0,1)$ 的球对称性，每个 $X_i$ 的分布为 $N(0,1)$ [30，页面 77]。定义 $Y_i=X_{2 i-1}^2+X_{2 i}^2$，对于 $i=1, \ldots, k / 2$。因此，$Y_i$ 遵循参数为 $\lambda=\frac{1}{2}$ 的指数分布（见 [30，页面 47]）。于是有 $E(L)=\sum_{i=1}^{k / 2} E\left(Y_i\right)=(k / 2) \times 2=k$；同时可以看出 $L$ 遵循参数为 $\alpha=\frac{1}{2}$ 和 $v=k / 2$ 的伽马分布（见 [30，页面 46]）。由于该分布是泊松分布的对偶，我们得出结论$\displaystyle{}\text{Pr}[L \geq \beta k]=\text{Pr}\left[P_{\beta k}^{1 / 2} \leq v-1\right]$

其中 $P_t^\alpha$ 是一个遵循参数为 $\alpha t$ 的泊松分布的随机变量。显然，$\displaystyle{}\text{Pr}\left[P_t^\alpha \leq v-1\right]=\sum_{i=0}^{v-1} e^{-\alpha t} \frac{(\alpha t)^i}{i!}$

因此，$\displaystyle{}\text{Pr}[L \geq \beta k]=\sum_{i=0}^{v-1} e^{-\beta v} \frac{(\beta v)^i}{i!} \leq v e^{-\beta v} \frac{(\beta v)^v}{v!} \leq v e^{-\beta v} \frac{(\beta v)^v}{\frac{v^v}{e^v}}=v\left(e^{-\beta} \beta e\right)^v=v e^{-v(\beta-(1+\ln \beta)}$

这意味着所需的结果，因为 $v=k / 2$。

最后，我们声明以下界限，对于某个大的常数 $\gamma \gg 1$。

$\displaystyle{}\text{Pr}[L \leq k / \beta]=\sum_{i=v}^{\infty} e^{-v / \beta} \frac{(v / \beta)^i}{i!} \leq e^{-v / \beta} \sum_{i=v}^{\infty}\left(\frac{e v}{i \beta}\right)^i=e^{-v / \beta}\left[\sum_{i=v}^{\gamma e v / \beta}\left(\frac{e v}{i \beta}\right)^i+\sum_{i=\gamma e v / \beta+1}^{\infty}\left(\frac{e v}{i \beta}\right)^i\right]$ 

第二个求和对于 $\gamma \gg 1$ 来说非常小，因此我们只对第一个求和进行界限。由于序列 $\left(\frac{e v}{i \beta}\right)^i$ 在 $i \geq v / \beta$ 时是单调递减的，我们可以对第一个求和进行界限：$\displaystyle{}\left(\frac{\gamma e v}{\beta}\right) e^{-v / \beta}\left(\frac{e}{\beta}\right)^v=O(v) e^{-v\left(\beta^{-1}-(1-\ln \beta)\right)}$ 

由于 $v=k / 2$，我们得到了所需的结果。



# B. Auxiliary facts  

**事实 3 (Johnson-Schechtman [43])** 对于任何 $1 \leq p<2$ 和 $\epsilon>0$，存在一个常数 $\beta \geq 1$，使得对于所有 $d \geq 1$，空间 $l_p^d$ 可以在 $l_1^{\beta d}$ 中进行 $(1+\epsilon)$ 的嵌入。

**事实 4 (Linial, London, 和 Rabinovich [49])** 对于任何 $\epsilon>0$ 和每个由 $n$ 个点在 $l_1^d$ 中诱导出的 $n$ 点度量空间 $\mathcal{M}=(X, d)$，存在 $m$ 使得 $\mathcal{M}$ 可以在 $H^m$ 中进行 $(1+\epsilon)$ 的嵌入。如果所有点的坐标来自集合 $\{1 \ldots R\}$，那么 $\mathcal{M}$ 可以在 $H^m$ 中等距嵌入，且 $m=R d$。
