### SPANN: Highly-efficient Billion-scale Approximate Nearest Neighbor Search  



# 0. Abstract  

内存中近似最近邻搜索（ANNS）算法在快速高召回率搜索方面取得了巨大成功，但在处理非常大规模数据库时代价极高。因此，越来越多地要求采用小内存和廉价固态硬盘（SSD）的混合 ANNS 解决方案。本文提出了一种简单但高效的内存-磁盘混合索引和搜索系统，称为 SPANN，采用倒排索引方法。它将发布列表的中心点存储在内存中，而将大型发布列表存储在磁盘上。我们通过有效减少磁盘访问次数和检索高质量发布列表，保证了磁盘访问效率（低延迟）和高召回率。在索引构建阶段，我们采用层次平衡聚类算法来平衡发布列表的长度，并通过添加相应簇的闭包中的点来增强发布列表。在搜索阶段，我们使用查询感知方案动态剪枝不必要的发布列表访问。实验结果表明，SPANN 在三十亿规模的数据集上以相同的内存成本达到相同的召回质量 90% 时，比最新的 ANNS 解决方案 DiskANN 快 2 倍。它仅需约一毫秒就能达到 90% 的 recall@1 和 recall@10，内存成本仅为 32GB。代码可在以下链接获取：https://github.com/microsoft/SPTAG。



# 1. Introduction  

向量最近邻搜索在信息检索领域发挥了重要作用，例如多媒体搜索和网络搜索，通过搜索与查询向量距离最小的向量提供相关结果。由于计算成本高和查询延迟大，针对 K 最近邻搜索的精确解决方案 [49, 40] 不适用于大数据场景。因此，研究人员在文献中提出了多种近似最近邻搜索（ANNS）算法 [11,18,38,10,14,31,34,13,29,21,16,26,42,43,33,44,37,32,19,27,9,12,39,50,20,36]。然而，大多数算法主要关注如何在内存中使用离线预构建索引实现低延迟和高召回率搜索。当针对超大规模向量搜索场景（如网络搜索）时，内存成本将变得极为昂贵。因此，越来越多地要求采用使用小内存和廉价磁盘的混合 ANNS 解决方案，以服务于大规模数据集。

---

目前，关于混合 ANNS 解决方案的研究仅有少数，包括 DiskANN [39] 和 HM-ANN [36]。这两者都是基于图的方法。DiskANN 使用产品量化（PQ）[25] 来压缩存储在内存中的向量，同时将导航的扩展图与全精度向量放在磁盘上。当查询到达时，它根据量化向量的距离遍历图，然后根据全精度向量的距离对候选项进行重新排序。HM-ANN 利用异构内存，将主轴点放在快速内存中，而将可导航的小世界图放在慢速内存中，而不进行数据压缩。然而，它所需的快速内存比 DiskANN 大出 1.5 倍以上。此外，慢速内存的成本仍然远高于磁盘。因此，由于 DiskANN 的低成本、高召回率和低延迟优势，它已成为亿级数据集索引的最新解决方案。

---

在本文中，我们论证了简单的倒排索引方法也可以在召回率、延迟和内存成本方面对大规模数据集实现最先进的性能。我们提出了 SPANN，一种简单但出乎意料地高效的内存-磁盘混合向量索引和搜索系统，遵循倒排索引方法论。SPANN 仅在内存中存储发布列表的中心点，而将大型发布列表存储在磁盘上。通过大幅减少磁盘访问次数并提高发布列表的质量，我们保证了低延迟和高召回率。在索引构建阶段，我们使用层次平衡聚类方法来平衡发布列表的长度，并通过添加相应簇的闭包中的点来扩展发布列表。在搜索阶段，我们使用查询感知方案动态剪枝不必要的发布列表访问。实验结果表明，SPANN 在三十亿规模的数据集上以相同的内存成本达到相同的召回质量 90% 时，比最新的基于磁盘的 ANNS 算法 DiskANN 快超过两倍。它仅需约一毫秒就能达到 90% 的 recall@1 和 recall@10，内存成本仅为原始内存成本的 10%。SPANN 已被部署到微软 Bing，以支持数百亿规模的向量搜索。



# 2. Background and Related Work

给定一组数据向量 $\mathbf{X} \in \mathbb{R}^{n \times m}$（数据集包含 $n$ 个具有 $m$ 维特征的向量）和一个查询向量 $q \in \mathbb{R}^m$，向量搜索的目标是找到一个向量 $\mathbf{p}^*$，称为最近邻，使得 $\mathbf{p}^*=\arg \min _{\mathbf{p} \in \mathbf{X}} \operatorname{Dist}(\mathbf{p}, \mathbf{q})$。类似地，我们可以定义 $K$-最近邻。由于穷举搜索的计算成本高和查询延迟大，ANNS（近似最近邻搜索）算法旨在加速在大规模数据集中寻找近似 $K$-最近邻的过程，以在可接受的时间内完成。文献中大多数 ANNS 算法主要集中在内存中的快速高召回率搜索，包括基于哈希的方法 [14, 24, 47, 48, 45, 46, 51]、基于树的方法 [11, 31, 44, 33]、基于图的方法 [21, 16, 42, 32] 和混合方法 [43, 12, 23, 22]。然而，随着向量规模的爆炸性增长，内存已成为支持大规模向量搜索的瓶颈。目前只有少数几种方法在 ANNS 解决方案中针对亿级规模数据集，以最小化内存成本。它们可以分为两类：基于倒排索引的方法和基于图的方法。

---

基于倒排索引的方法，如 IVFADC [26]、FAISS [27] 和 IVFOADC $+\mathrm{G}+\mathrm{P}[9]$，通过 KMeans 聚类将向量空间划分为 $K$ 个 Voronoi 区域，仅在与查询接近的少数区域内进行搜索。为了降低内存成本，它们使用向量量化，例如产品量化（PQ）[25]，来压缩向量并将其存储在内存中。倒排多索引（IMI）[7] 也使用 PQ 来压缩向量。它将特征空间划分为多个正交子空间，并为每个子空间构建一个独立的代码本。整个特征空间作为相应子空间的笛卡尔积生成。Multi-LOPQ [28] 使用局部优化的 PQ 代码本来编码 IMI 结构中的位移。GNO-IMI [8] 通过使用非正交代码本来优化 IMI，以生成质心。尽管它们能够将一亿个 128 维向量的内存使用量降低到不到 64GB，但由于有损数据压缩，recall@1 的值非常低（仅约 $60 \%$）。虽然通过返回 10 到 100 倍的候选项进行进一步的重排序可以实现更好的召回率，但在许多场景中，这种做法通常是不可接受的。

---

基于图的方法包括 DiskANN [39] 和 HM-ANN [36]。它们都采用了混合解决方案。DiskANN 将 PQ 压缩的向量存储在内存中，同时将用于导航的扩展图和全精度向量存储在磁盘上。当查询到来时，它采用最佳优先的方式根据量化向量的距离遍历图，并根据全精度向量的距离对候选项进行重排序。同样，它使用的有损数据压缩会影响召回质量，尽管全精度向量重排序可以帮助找回一些丢失的候选项。高成本的随机磁盘访问限制了图的遍历次数和候选项的重排序。HM-ANN 利用异构内存，将底层阶段提升的枢轴点放置在快速内存中，而将可导航的小世界图放在慢速内存中，且没有进行数据压缩。然而，这会导致快速内存消耗增加超过 1.5 倍。此外，慢速内存仍然比磁盘昂贵，并且在某些平台上可能不可用。基于图的方法的限制和优点的理论分析见于 [35]。



# 3. SPANN

在本文中，我们提出了 SPANN，一个简单但高效的向量索引和搜索系统，采用了倒排索引方法。与以往利用有损数据压缩来降低内存成本的倒排索引方法不同，SPANN 采用了一种简单的内存-磁盘混合解决方案。

**索引结构：**数据向量 $\mathbf{X}$ 被划分为 $N$ 个发布列表 $\left\{\mathbf{X}_1, \mathbf{X}_2, \cdots, \mathbf{X}_N\right\}$，满足 $\mathbf{X}_1 \cup \mathbf{X}_2 \cup \ldots \cup \mathbf{X}_N=\mathbf{X}$。这些发布列表的质心 $\mathbf{c}_1, \mathbf{c}_2, \cdots, \mathbf{c}_N$ 存储在内存中，作为指向磁盘中相应发布列表位置的快速粗粒度索引。

**部分搜索：**当查询 $\mathbf{q}$ 到来时，我们找到 $K$ 个最接近的质心 $\left\{\mathbf{c}_{i 1}, \mathbf{c}_{i 2}, \ldots, \mathbf{c}_{i K}\right\}$，其中 $K \ll N$，并将与这 $K$ 个最接近质心对应的发布列表 $\mathbf{X}_{i_1}, \mathbf{X}_{i_2}, \cdots, \mathbf{X}_{i_K}$ 中的向量加载到内存中，以进行进一步的细粒度搜索。

Note: 为方便起见，我们用 $\mathbf{X}$ 来表示矩阵和向量集。

## 3.1. Challenges

**发布长度限制**：由于所有的发布列表都存储在磁盘上，为了减少磁盘访问，我们需要限制每个发布列表的长度，以便可以在少量磁盘读取中将其加载到内存中。这要求我们不仅要将数据划分为大量的发布列表，还要平衡发布列表的长度。这是非常困难的，因为聚类成本较高以及平衡划分问题本身。发布列表不均衡将导致查询延迟的高方差，特别是在发布列表存储在磁盘时。

---

**边界问题**：查询 $q$ 的最近邻向量可能位于多个发布列表的边界上。由于我们只搜索少量相关的发布列表，一些位于其他发布列表中的 $q$ 的真实邻居将会缺失（如**图 1**所示）。如果红点仅由蓝色发布列表的质心表示，那么它们将在黄色点的最近邻搜索中丢失。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923231211306.png" alt="image-20240923231211306" style="zoom:40%;" /> 

图1：由于部分搜索导致边界向量缺失的示例。如果我们搜索黄色点，我们会首先搜索绿色发布列表，因为绿色发布列表的中心点离黄色点更近，尽管在蓝色发布列表中有一些边界点（标记为红色）离黄色点更近。 

---

**搜索难度多样性**：我们发现不同的查询可能具有不同的搜索难度。有些查询只需要在一个或两个发布列表中进行搜索，而有些查询则需要在大量发布列表中进行搜索（如**图 2**所示）。如果对所有查询使用相同数量的发布列表进行搜索，将导致低召回率或长延迟。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923232429819.png" alt="image-20240923232429819" style="zoom: 33%;" /> 

图2：不同的查询需要搜索不同数量的发布列表。为了在SIFT1M数据集中召回最佳结果，我们发现$80 \%$的查询只需要搜索6个发布列表，而$99 \%$的查询需要搜索114个发布列表。

---

以上所有挑战都是之前所有倒排索引方法采用有损数据压缩解决方案，将所有压缩向量和发布列表存储在内存中的原因。

## 3.2. Key techniques to address the challenges

在本文中，我们介绍了三种关键技术，以解决上述挑战，实现内存-磁盘混合解决方案。在索引构建阶段，我们首先限制发布列表的长度，以有效减少在线搜索中每个发布列表的磁盘访问次数。然后，我们通过扩展相应发布列表闭包中的点来提高发布列表的质量。这增加了位于发布列表边界的向量的召回概率。在搜索阶段，我们提出了一种查询感知方案，以动态修剪不必要的发布列表访问，从而确保高召回率和低延迟。每项技术的详细设计将在后续章节中介绍。

### 3.2.1. Posting length limitation

限制发布列表的长度意味着我们需要将数据向量 $\mathbf{X}$ 划分为大量的发布列表 $\mathbf{X}_1, \mathbf{X}_2, \cdots, \mathbf{X}*N$。平衡发布列表的长度意味着我们需要最小化发布长度的方差 $\sum*{i=1}^N\left(\left|\mathbf{X}_i\right|-|\mathbf{X}| / N\right)^2$。

为了解决发布长度平衡问题，我们可以利用多约束平衡聚类算法 [30] 将向量均匀划分为多个发布列表：

$\displaystyle{}\min _{\mathbf{H}, \mathbf{C}}\|\mathbf{X}-\mathbf{H C}\|_{\mathrm{F}}^2+\lambda \sum_{i=1}^N\left(\sum_{l=1}^{|\mathbf{X}|} h_{l i}-|\mathbf{X}| / N\right)^2, \text { s.t. } \sum_{i=1}^N h_{l i}=1$

其中，$\mathbf{C} \in \mathbb{R}^{N \times m}$ 是质心，$\mathbf{H} \in{0,1}^{|\mathbf{X}| \times N}$ 表示聚类分配，$\sum_{l=1}^{|\mathbf{X}|} h_{l i}$ 是分配给第 $i$ 个聚类的向量数量（即 $\left|\mathbf{X}_i\right|$），$\lambda$ 是聚类与平衡约束之间的权衡超参数。

---

然而，我们发现当向量数量 $|\mathbf{X}|$ 和划分数量 $N$ 非常大时，直接使用多约束平衡聚类算法无法工作，因为大 $N$ 划分平衡聚类问题非常困难，且聚类成本极高。因此，我们引入了一种层次多约束平衡聚类技术（**图 3**），不仅将聚类的时间复杂度从 $O(|\mathbf{X}| * m * N)$ 降低到 $O\left(|\mathbf{X}| * m * k * \log _k(N)\right)$（$k$ 是一个小常数），而且还能平衡发布列表的长度。我们迭代地将向量聚类为少量（即 $k$）聚类，直到每个发布列表包含有限数量的向量。通过使用这一技术，我们不仅可以大幅减少每个发布列表的长度（磁盘访问），还可以降低索引构建的成本。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923233240001.png" alt="image-20240923233240001" style="zoom:33%;" /> 

图3：分层平衡聚类：迭代地将大簇（黄色簇）中的向量平衡分区为少量小簇（绿色簇），直到每个簇仅包含有限数量的向量（蓝色簇）。

---

此外，由于质心的数量非常大，查找查询的最近发布列表需要消耗大量的计算成本。为了使导航计算更有意义，我们用最接近质心的向量来表示每个发布列表。这样，浪费的导航计算就转化为对一小部分真实候选者的距离计算。

更重要的是，为了快速找到查询的少量最近发布列表，我们为所有代表发布列表质心的向量创建了一个内存中的 SPTAG [12]（MIT 许可证）索引。SPTAG 构建空间划分树和相对邻域图作为向量索引，可以将最近质心的搜索速度提升到亚毫秒的响应时间。

### 3.2.2. Posting list expansion

为了处理边界问题，我们需要提高位于发布列表边界的那些向量的可见性。一个简单的方法是将每个向量分配给多个相近的簇。然而，这将显著增加发布大小，从而导致大量的磁盘读取。因此，我们在最终聚类步骤中引入了一种闭包多簇分配解决方案——如果向量与这些簇之间的距离几乎相同，则将向量分配给多个最近的簇，而不仅仅是最近的一个（**图 4** 给出了一个示例）：

$\mathbf{x} \in \mathbf{X}_{i j} \Longleftrightarrow \operatorname{Dist}\left(\mathbf{x}, \mathbf{c}_{i j}\right) \leq\left(1+\epsilon_1\right) \times \operatorname{Dist}\left(\mathbf{x}, \mathbf{c}_{i 1}\right)$ 

其中$\operatorname{Dist}\left(\mathbf{x}, \mathbf{c}_{i 1}\right) \leq \operatorname{Dist}\left(\mathbf{x}, \mathbf{c}_{i 2}\right) \leq \cdots \leq \operatorname{Dist}\left(\mathbf{x}, \mathbf{c}_{i K}\right)$  

这意味着我们只对边界向量进行重复分配。对于那些非常接近簇质心的向量，它们仍然只保留一个副本。通过这样做，我们可以有效地限制由于闭包簇分配而导致的容量增加，同时提高这些边界向量的召回概率：如果搜索了它们任何一个最近的发布列表，它们都将被召回。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923234754378.png" alt="image-20240923234754378" style="zoom:33%;" /> 

图4：闭包聚类分配：将边界向量（绿色点）分配给多个最近的簇，如果它与这些簇的距离几乎相同（蓝色和黄色簇）。

---

由于每个发布列表较小，并且我们使用闭包分配，这将导致一些相互靠近的发布列表包含相同的重复向量（例如，绿色向量同时属于黄色和蓝色聚类）。在相近的发布列表中存在过多的重复向量也会浪费高成本的磁盘读取。因此，我们进一步优化闭包聚类分配，通过使用RNG规则来选择多个代表性聚类，以便为边界向量分配，从而减少两个相近发布列表之间的相似性（**图5**）。RNG规则可以简单定义为：如果$\operatorname{Dist}\left(\mathbf{c}_{i j}, \mathbf{x}\right)>\operatorname{Dist}\left(\mathbf{c}_{i j-1}, \mathbf{c}_{i j}\right)$，则跳过聚类$i j$。其背后的思想是，相互靠近的发布列表更有可能被导航索引同时召回。与其在相近的发布列表中存储相似向量，不如存储不同的向量，以增加在线搜索中可见向量的数量。从向量的角度来看，最好由位于不同方向的发布列表表示（示例中的蓝色和灰色发布列表），而不是仅由位于相同方向的发布列表表示（蓝色和黄色发布列表）。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923234834311.png" alt="image-20240923234834311" style="zoom:33%;" /> 

图5：代表性复制：使用RNG规则减少两个相近发布列表的相似性。橙色点将被分配给蓝色和灰色发布列表，尽管它与黄色列表的距离比与灰色列表的距离更近。

### 3.2.3. Query-aware dynamic pruning

在索引搜索阶段，为了有效处理不同的查询和不同的资源预算，我们引入了查询感知动态剪枝技术，根据查询与中心点之间的距离减少需要搜索的发布列表数量。我们不再对所有查询搜索最近的$K$个发布列表，而是动态决定仅在某个发布列表的中心点与查询之间的距离与查询与最近中心点之间的距离几乎相同的情况下，才进行搜索：

$\mathbf{q} \xrightarrow{\text { search }} \mathbf{X}_{i j} \Longleftrightarrow \operatorname{Dist}\left(\mathbf{q}, \mathbf{c}_{i j}\right) \leq\left(1+\epsilon_2\right) \times \operatorname{Dist}\left(\mathbf{q}, \mathbf{c}_{i 1}\right)$

其中$\operatorname{Dist}\left(\mathbf{q}, \mathbf{c}_{i 1}\right) \leq \operatorname{Dist}\left(\mathbf{q}, \mathbf{c}_{i 2}\right) \leq \cdots \leq \operatorname{Dist}\left(\mathbf{q}, \mathbf{c}_{i K}\right)$ 

通过进一步减少在最近的$K$个发布列表中的不必要发布列表，我们可以显著降低查询延迟，同时仍然通过更合理和有效地利用资源来保持较高的召回率。



# 4. Experiment

在本节中，我们首先展示SPANN与当前最先进的近似最近邻搜索（ANNS）算法的实验比较。接着，我们进行消融研究，以进一步分析每项技术的贡献。最后，我们设置一个实验，以展示SPANN解决方案在分布式搜索场景中的可扩展性。

## 4.1. Experiment setup

我们在一台配置为Ubuntu 16.04.6 LTS的工作站上进行所有实验，该机器配备了两颗Intel Xeon 8171 M CPU（2600 MHz频率，52个CPU核心）、128GB内存和2.6TB的RAID-0 SSD。我们使用的数据集如下：

1. **SIFT1M数据集**[3]：这是评估基于内存的ANNS算法性能时最常用的数据集，包含一百万个128维的SIFT描述符作为基础集，以及10,000个查询描述符作为测试集。
2. **SIFT1B数据集**[3]：这是评估支持大规模向量搜索的ANNS算法性能的经典数据集，包含十亿个128维的字节向量作为基础集，以及10,000个查询向量作为测试集。
3. **DEEP1B数据集**[8]：这是一个从深度图像分类模型学习得来的数据集，包含十亿个96维的浮点向量作为基础集，以及10,000个查询向量作为测试集。
4. **SPACEV1B数据集**[6]（O-UDA许可证）：这是来自商业搜索引擎的数据集，源于生产数据。它代表了另一种不同的内容编码——深度自然语言编码。该数据集包含十亿个100维的字节向量作为基础集，以及29,316个查询向量作为测试集。

---

用于展示性能的比较指标包括：

1. **召回率**：我们将ANNS返回的$R$个向量ID与$R$个真实向量ID进行比较，以计算召回率@ $R$。由于多个数据向量与查询向量具有相同的距离，我们还会在召回计算中将一些真实向量ID替换为与查询向量共享相同距离的向量ID。
   
2. **延迟**：我们使用查询响应时间（以毫秒为单位）作为查询延迟。

3. **VQ（向量-查询）**：这是一个机器每秒能够处理的向量数量与查询数量的乘积（在GRIP [50]中引入）。它展示了搜索引擎的服务能力，同时考虑了查询延迟和内存成本。系统的VQ越高，消耗的资源成本越少。这里我们使用每$\mathrm{KB}$的向量数量乘以每秒的查询数量作为VQ容量。

## 4.2. SPANN on single machine

在本节中，我们展示了基于倒排索引的SPANN解决方案在召回率、延迟和内存成本方面也能达到最先进的性能。我们首先将SPANN与当前最先进的十亿规模基于磁盘的ANNS算法在三个十亿规模的数据集上进行比较。然后，我们在SIFT1M数据集上进行实验，以比较SPANN与最先进的全内存ANNS算法的VQ容量。在本节的所有实验中，我们为SPANN使用以下超参数：1) 在闭包聚类分配中，每个向量最多使用8个闭包副本；2) 将字节向量的最大发布列表大小限制为12 KB，将浮点向量的最大发布列表大小限制为48 KB；3) 将发布列表扩展的$\epsilon_1$设置为10.0，将查询感知动态剪枝的$\epsilon_2$设置为0.6（召回@1）和7.0（召回@10）。我们增加要搜索的最大发布列表数量，以获得不同的召回质量。

### 4.2.1. Comparison with state-of-the-art billion-scale disk-based ANNS algorithms

我们选择了能够支持十亿规模数据集的最先进的基于磁盘的ANNS算法作为比较对象。由于HM-ANN [36]没有开源，并且所需的PMM硬件在某些平台上可能无法获得，因此我们仅将SPANN与最先进的十亿规模基于磁盘的ANNS算法DiskANN进行比较。我们使用DiskANN的默认超参数（对于SIFT1B和SPACEV1B，参考论文[39]，对于DEEP1B，使用[2]提供的预构建索引）。

---

我们通过选择合适数量的发布列表（约占总向量数量的10-12%）来仔细调整SPANN的导航内存索引大小，以确保两个算法消耗相同的内存（对于SIFT1B和SPACEV1B数据集约32GB，对于DEEP1B数据集约60GB）。**图6(a)**展示了SIFT1B数据集的性能结果。结果显示，SPANN在召回率@1和召回率@10方面显著优于DiskANN，特别是在低查询延迟预算（小于4毫秒）下。特别是，SPANN达到95%的召回率@1和召回率@10的速度是DiskANN的两倍多。



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923234932875.png" alt="image-20240923234932875" style="zoom:33%;" /> 

图6：SPANN与DiskANN在（a）SIFT1B，（b）SPACEV1B和（c）DEEP1B上的比较。

---

SPACEV1B和DEEP1B数据集的性能结果分别显示在**图6(b)和6(c)**中。结果同样表明，在查询延迟预算较小（小于4毫秒）时，SPANN在召回率@1和召回率@10上都优于DiskANN。特别是在SPACEV1B数据集中，DiskANN无法在小于4毫秒的时间内达到90%的召回率，而SPANN仅需约1毫秒即可获得90%的召回率。对于DEEP1B数据集，SPANN也能以超过两倍的速度快于DiskANN达到良好的召回质量（90%）。

### 4.2.2. Comparison with state-of-the-art all-in-memory ANNS algorithms

然后，我们在SIFT1M数据集上进行实验，以比较SPANN与最先进的全内存ANNS算法（NSG [19]、HNSW [32]、SCANN [20]、NGT-ONNG [23]、NGT-PANNG [22]和N2 [4]）的VQ容量。这些算法在ann-benchmarks [1]中表现出色。我们选择VQ容量而非延迟作为比较指标，因为这些算法使用更多内存来换取低延迟。然而，内存是一种昂贵的资源，已成为这些算法支持大规模数据集的瓶颈。因此，在性能比较中，我们应该同时考虑内存和延迟。我们选择SIFT1M数据集作为例子，因为我们的测试机器存在内存瓶颈。我们相信这一观察可以推广到十亿规模的数据集。

---

这些算法大多基于图结构。对于NSG，我们从[5]获取了预构建索引，并根据不同的SEARCH_L（从1到256）运行性能测试，以控制搜索结果的质量。对于HNSW（nmslib）、SCANN、NGT-ONNG、NGT-PANNG和N2，我们使用它们在ann-benchmarks [1]中提供的超参数，以实现SIFT1M数据集的最佳性能。

---

**图7和图8**展示了所有算法在召回率@1和召回率@10下的VQ容量和查询延迟。从结果中可以看出，SPANN在几乎所有召回水平上始终实现最佳的VQ容量。这意味着，尽管由于搜索过程中高成本的磁盘访问，SPANN无法达到全内存ANNS算法那样低的延迟，但在大规模向量搜索场景中，它可以获得最佳的服务能力。



<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923235048095.png" alt="image-20240923235048095" style="zoom:33%;" /> 

图7：不同ANNS索引的VQ

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923235136148.png" alt="image-20240923235136148" style="zoom:33%;" /> 

图8：不同ANNS索引的延迟

### 4.2.3. Ablation studies

在本节中，我们进行了一系列实验，对SIFT1M数据集中的每项技术进行消融研究。

#### 4.2.3.1. Hierarchical balanced clustering 

在单台机器上，有三种快速的方法将向量划分为大量发布列表：1）随机选择一组点作为发布列表的质心；2）使用层次KMeans聚类（HC）选择质心；3）使用层次平衡聚类（HBC）生成一组质心。我们通过这三种方法生成16%的点作为质心，以比较索引质量。

------

**图10**展示了这三种质心选择算法的召回率和延迟性能。在召回率@1和召回率@10方面，HBC质心选择优于随机选择和HC选择，这表明平衡的发布长度对基于倒排索引的方法非常重要。此外，HBC非常快速，能够在大约50秒内用64个线程将一百万个点聚类成16万个簇。整个SPANN索引的构建时间约为2分钟。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923235338657.png" alt="image-20240923235338657" style="zoom:33%;" /> 

图10：不同类型的质心选择

------

此外，所需的质心数量是多少？较少的质心可以减少导航内存索引的大小，但较多的质心通常意味着更好的性能。因此，我们需要在内存使用和性能之间做出合理的权衡。**图9**比较了不同质心数量的性能。结果表明，当质心数量较少时，性能会随着质心数量的增加显著提升。然而，当质心数量足够大（16%）时，性能不再增加。因此，我们可以选择16%的点作为质心，以实现良好的搜索性能和较小的内存使用。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923235304165.png" alt="image-20240923235304165" style="zoom:33%;" /> 

图9：不同数量的质心

#### 4.2.3.2. Closure clustering assignment 

为了使用闭合聚类分配，我们需要将一个向量分配到多个闭合簇中，以提高其在搜索中的召回概率。那么，为了确保性能，我们最多需要为一个向量复制多少个闭合副本呢？副本数量太少无法帮助检索那些边界向量，但副本数量过多会大大增加发布列表的大小，从而影响性能。**图11**展示了不同副本数量的闭合聚类分配性能。结果表明，使用超过一个副本显著提高了性能。然而，当副本数量超过8时，性能不再提升。因此，我们在所有实验中选择8个副本

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923235415404.png" alt="image-20240923235415404" style="zoom:33%;" /> 

图11：不同数量的闭包副本

#### 4.2.3.3. Query-aware dynamic pruning 

为了在在线搜索过程中有效处理不同查询，我们引入了查询感知动态剪枝技术，通过修剪最近的$K$个发布列表中的不必要列表，进一步减少要搜索的发布列表数量。我们比较了使用和不使用查询感知动态剪枝的性能，如**图12**所示。结果表明，使用查询感知动态剪枝可以在不降低召回率的情况下进一步减少查询延迟，特别是在延迟预算较小的时候。值得注意的是，这项技术不仅可以减少查询延迟，还能降低查询的资源使用。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923235444950.png" alt=" " style="zoom:33%;" /> 

图12：无查询感知动态修剪

## 4.3. Extension of SPANN to distributed search scenario

与图基方法相比，基于倒排索引的SPANN方法的额外优势在于，最近发布列表的部分搜索思路可以轻松扩展到分布式搜索场景，从而以高效且低服务成本处理超大规模的向量搜索。方法Pyramid [15]展示了在分布式场景中平衡分区和部分搜索方法的强大能力。为了证明SPANN在分布式搜索场景中的可扩展性，我们将数据向量$\mathbf{X}$均匀地划分为$M$个分区$\left\{\mathbf{X}_1, \mathbf{X}_2, \cdots, \mathbf{X}_M\right\}$，使用多约束平衡聚类和闭合聚类分配技术，在分布式索引构建阶段进行划分，其中$M$是机器的数量。在在线搜索阶段，我们还采用查询感知动态剪枝技术来减少分配机器的数量，有效限制查询的总CPU和IO成本。

---

我们面临的唯一挑战是可能出现一些热点机器。因此，我们不仅需要平衡每台机器的数据大小，还需要平衡每台机器的查询访问，以避免热点。为了解决热点问题，我们将向量划分为多个小分区（数量大于机器数量），然后使用最佳适应装箱算法[17]将小分区打包到大箱子中（箱子数量等于机器数量），根据历史查询访问分布进行打包。通过这种方式，我们不仅可以有效平衡数据大小，还可以平衡每台机器处理的查询。

---

我们将优化后的SPANN解决方案与传统的随机分区和全分发解决方案进行比较，以证明SPANN在分布式搜索场景中的工作负载减少和可扩展性。我们在SPACEV1B数据集上进行以下实验，并使用来自生产环境的约100,000个查询访问历史作为测试工作负载。工作负载均匀地分为三个集合：训练集、验证集和测试集。训练集用于离线分布式索引构建，测试集用于在线搜索评估。

### 4.3.1. Workload reduction and scalability

**图13**展示了将所有基向量划分为8、16和32个分区时，每台机器上的向量数量和测试查询访问数量。从结果来看，SPANN将所有数据和查询访问均匀地分配到不同的机器上。尽管由于闭合分配，导致每台机器上的向量数量增加了20%，但与随机分区方案相比，每台机器上的查询访问数量显著减少。此外，SPANN能够通过增加更多机器来持续减少每台机器的查询访问，而随机分区则无法做到。这意味着我们始终可以增加更多机器以支持每秒更多的查询，这证明了我们系统的良好可扩展性。我们能够实现良好可扩展性的原因在于，我们有效限制了每个查询的搜索机器数量。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923235526008.png" alt="image-20240923235526008" style="zoom:33%;" /> 

图13：不同机器上的数据大小和查询访问分布

### 4.3.2. Analysis

接下来，我们分析每个技术对性能的影响。我们使用32个分区的情况进行消融研究，为每个分区构建SPANN单机索引，并使用29,316个带有真实值的查询向量作为测试工作负载。**图14**展示了在端到端分布式搜索场景中，召回率、延迟和最终调度的机器平均数量。左侧的图表显示，SPANN在每个延迟预算下几乎都能达到最佳召回率。右侧的图表则显示，随机分区方案需要将查询调度到所有32台机器进行搜索。使用多约束平衡聚类技术可以显著减少调度的机器数量至9台。通过增加闭合分配，我们进一步将调度的机器数量减少至8台。当所有技术应用（包括在线搜索中的查询感知动态修剪）时，最终我们将调度的机器数量减少至6.3台。这意味着我们可以为每个查询节省约80.3%的计算和I/O成本。同时，通过减少用于查询搜索的机器数量，我们可以进一步降低查询延迟，因为我们减少了最终聚合的候选数量。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240923235612327.png" alt="image-20240923235612327" style="zoom:33%;" /> 

图14：端到端测试中召回率、延迟和分派机器数量的比较

# 5. Conclusion

在本文中，我们介绍了SPANN，这是一种简单但高效的基于倒排索引的ANNS系统，在召回率、延迟和内存成本方面达到了大规模数据集的最先进性能。与之前使用有损数据压缩来解决内存瓶颈的倒排索引方法不同，SPANN采用了一种简单的内存-磁盘混合解决方案，仅将发布列表的质心存储在内存中。我们通过大幅减少磁盘访问次数和提高发布列表的质量，保证了低延迟和高召回率。实验结果表明，SPANN不仅在十亿级规模数据集上建立了新的最先进性能，而且在扩展到分布式搜索场景时也实现了良好的可扩展性。这证明了分层SPANN以高效和低服务成本支持超大规模向量搜索的能力。

















































