## Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph  



# 0. Abstract

近似最近邻搜索(ANNS)是数据库和数据挖掘中的一个基本问题。一个可扩展的ANNS算法应具备高效的内存利用率和快速的搜索速度。一些早期的基于图的方法在搜索时间复杂度上展示了有吸引力的理论保证，但它们都面临高索引时间复杂度的问题。最近，一些基于图的方法通过近似传统图来降低索引复杂性，这些方法在百万规模的数据集上取得了革命性的性能。然而，它们仍无法扩展到十亿节点的数据库。本文为进一步提高基于图的方法的搜索效率和可扩展性，首先介绍了四个方面：(1) 确保图的连通性；(2) 降低图的平均出度以加快遍历；(3) 缩短搜索路径；(4) 减小索引大小。然后，我们提出了一种新的图结构，称为单调相对邻居图(MRNG)，它保证了非常低的搜索复杂度(接近对数时间)。为了进一步降低索引复杂性并使其适用于十亿节点的ANNS问题，我们通过近似MRNG提出了一种新图结构，称为导航扩展图(NSG)，该图同时考虑了上述四个方面。大量实验表明，NSG显著优于所有现有算法。此外，NSG在淘宝(阿里巴巴集团)的电子商务搜索场景中表现出色，并已集成到其十亿节点规模的搜索引擎中。



# 1. INTRODUCTION

近似最近邻搜索(ANNS)几十年来一直是一个热门话题，为数据挖掘、数据库和信息检索等多个应用提供了基础支持 $[2,10,12,23,37,42]$。对于稀疏离散数据(如文档)，可以在高级索引结构(例如倒排索引 [35])上高效地进行最近邻搜索。对于密集连续向量，已提出多种解决方案，例如基于树结构的方法 $[2,6,8,17,24,36]$，基于哈希的方法 $[18,20,23,32,40]$，基于量化的方法 $[1,19,26,39]$，以及基于图的方法 $[3,21,33,41]$。其中，基于图的方法最近显示出巨大的潜力。有些实验结果表明，基于图的方法在常用的欧几里得空间中，比其他类型的一些流行算法表现得更好 $[2,7,15,27,33,34]$。其原因可能在于，这些方法无法像基于图的方法那样良好地表达邻居关系，且它们往往需要在邻域子空间中检查更多的点，以达到相同的准确性 $[39]$。因此，它们的搜索时间复杂度涉及到以维度为底数的巨大指数因子，导致性能较差 $[22]$。

---

通过图进行最近邻搜索的研究已经持续了几十年 $[3,13,25]$。给定一组在 $d$ 维欧几里得空间 $E^d$ 中的点 $S$，图 $G$ 被定义为连接这些点(节点)的边的集合。边 $pq$ 定义了节点 $p$ 和 $q$ 之间的邻居关系。为了使图适用于近似最近邻搜索(ANNS)问题，提出了各种约束。这些图现在被称为接近图(Proximity Graphs) $[25]$。一些接近图，如德劳内图(Delaunay Graphs，或德劳内三角剖分) $[4]$ 和单调搜索网络(Monotonic Search Networks，MSNET) $[13]$，确保从任意节点 $p$ 到另一个节点 $q$，存在一条路径，其中中间节点与 $q$ 的距离逐渐变近 $[13]$。然而，寻找这样一条路径所需的计算复杂度并没有给出。其他工作如随机邻域图(Randomized Neighborhood Graphs) $[3]$ 保证了多对数时间复杂度。根据经验，贪婪路由路径的平均长度随着数据规模的增长呈多对数增长，尤其是在可导航的小世界网络(Navigable Small-World Networks，NSWN)上 $[9, 29]$。然而，构建这些图的时间复杂度非常高(至少为 $O\left(n^2\right)$)，对于大规模问题来说是不切实际的。

---

一些近期的基于图的方法试图通过为图设计近似来解决这一问题。例如，GNNS $[21]$、IEH $[27]$ 和 Efanna $[15]$ 基于 $k$ 最近邻图，这是德劳内图的一个近似。NSW $[33]$ 近似了可导航的小世界网络(NSWN)，FANNG $[7]$ 近似了相对邻居图(RNG) $[38]$，而分层NSW(HNSW) $[34]$ 则旨在利用德劳内图、NSWN 和 RNG 的特性。此外，HNSW 的分层结构使得可以在不同层之间进行多尺度跳跃。

---

这些近似主要基于直觉，通常缺乏严格的理论支持。在我们的实验研究中，我们发现它们对于当前需求量很大的十亿节点应用仍然不够强大。为了进一步提高基于图的方法的搜索效率和可扩展性，我们首先关注ANNS在图上的执行方式。尽管图索引的多样性，几乎所有基于图的方法 $[3, 7,13,21,27,33]$ 都共享相同的贪婪最佳优先搜索算法(在算法1中给出)，我们在下面称之为“图上的搜索算法”。

---

算法1尝试通过以下贪婪过程来达到查询点。对于给定的查询 $q$，我们需要从数据集中检索其最近邻。给定起始节点 $p$，我们沿着出边到达 $p$ 的邻居，并将其与 $q$ 进行比较，以选择一个继续前进。选择的原则是最小化与 $q$ 的距离，新的迭代从所选节点开始。我们可以看到，改善基于图的搜索的关键在于缩短算法形成的搜索路径，并降低图的出度(即，减少每个节点的选择数量)。直观地说，为了改善基于图的搜索，我们需要：(1) 确保图的连通性，以确保查询(或查询的最近邻)可以到达；(2) 降低图的平均出度；(3) 缩短搜索路径以降低搜索时间复杂度；(4) 减小索引大小(内存使用)以提高可扩展性。IEH $[27]$、Efanna $[15]$ 和 HNSW $[34]$ 等方法使用哈希、随机KD树和多层图来加速搜索。然而，这可能会导致大规模数据库的巨大的内存使用。我们的目标是在减少索引大小的同时，保持搜索效率。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240924135328057.png" alt="image-20240924135328057" style="zoom: 25%;" /> 

---

在本文中，我们提出了一种新图，称为单调相对邻居图(MRNG)，它保证了较低的平均搜索时间(非常接近对数复杂度)。为了进一步降低索引复杂性，我们提出了导航扩展图(NSG)，它是MRNG的良好近似，继承了低搜索复杂度，并考虑了四个方面。值得强调我们的贡献如下：

1. 我们首先对一种名为MSNET的图家族的吸引人ANNS特性进行了全面的理论分析。在此基础上，我们提出了一种新图MRNG，它确保了期望的接近对数搜索复杂度。
2. 为了进一步提高基于图的ANNS方法的效率和可扩展性，我们考虑了图的四个方面：确保连通性、降低平均出度、缩短搜索路径和减少索引大小。受到这些因素的激励，我们设计了MRNG的紧密近似，称为导航扩展图(NSG)，以同时解决这四个方面。与MRNG相比，索引复杂性显著降低，适用于大规模问题。大量实验表明，我们的方法在搜索性能上优于现有最先进的方法，并且在基于图的方法中具有最小的内存使用。
3. NSG算法还在淘宝(阿里巴巴集团)的电子商务搜索场景中进行了测试。该算法已集成到他们的十亿节点搜索引擎中。



# 2. PRELIMINARIES

我们用$E^d$表示在$l_2$范数下的欧几里得空间。任何两个点$p$和$q$的接近程度定义为它们之间的$l_2$距离，记作$\delta(p, q)$ 

## 2.1. Problem Setting

在高维数据的信息检索和数据库管理中，各种应用可以抽象为高维空间中的最近邻搜索问题。最近邻搜索(NNS)问题的定义如下：

**定义 1(最近邻搜索)**。给定一个包含$n$个点的有限点集$S$，预处理$S$以高效返回一个点$p \in S$，使其与给定查询点$q$的距离最小。

---

当我们要求算法返回与查询点最接近的$K$个点($K>1$)时，这自然推广为$K$最近邻搜索。最近邻搜索问题的近似版本(ANNS)可以定义如下：

**定义 2($\epsilon$-最近邻搜索)**。给定一个包含$n$个点的有限点集$S$，预处理$S$以高效回答查询，返回一个点$p \in S$，使得$\delta(p, q) \leq(1+\epsilon) \delta(r, q)$，其中$r$是$S$中与$q$最近的邻居。

类似地，当我们要求算法返回$K$个点($K>1$)时，使得对于所有$i=1, \ldots, K$，都有$\delta\left(p_i, q\right) \leq(1+\epsilon) \delta(r, q)$，该问题可推广为**近似$K$最近邻搜索(AKNNS)**。由于精确最近邻搜索的内在难度，大多数研究人员转向AKNNS。主要动机是为了在准确性上牺牲一些损失，以换取更短的搜索时间。

---

为了方便建模和评估，我们通常不计算$\epsilon$的确切值。相反，我们使用另一个指标来表示近似的程度：精度。假设由给定查询$q$的AKNNS算法返回的点集为$R^{\prime}$，而$q$的正确$k$最近邻集为$R$，则精度(准确性)定义如下：

$\displaystyle{}\operatorname{precision}\left(R^{\prime}\right)=\frac{\left|R^{\prime} \cap R\right|}{\left|R^{\prime}\right|}=\frac{\left|R^{\prime} \cap R\right|}{K} \text {. }$

较高的精度对应较小的$\epsilon$，因此，近似程度更高。本文中，我们使用精度作为评估指标。

## 2.2. Non-Graph-Based ANNS Methods

非图基的近似最近邻搜索(ANNS)方法大致包括基于树的方法、基于哈希的方法和基于量化的方法。一些知名且广泛使用的算法，如KD树 [36]、$R^*$树 [6]、VA-file [39]、局部敏感哈希(LSH) [20] 和产品量化(PQ) [26] 都属于上述类别。一些研究致力于改进这些算法(例如 $[2,18,19,23,32]$)，而其他研究则侧重于根据不同的平台和场景优化现有方法(例如 $[10,12,37,42]$)。

---

在一些最近的研究中，图基方法在实验研究中显著优于一些知名的非图基方法(例如KD树、LSH、PQ) $[2,7,15,27,33,34]$。这可能是因为非图基方法都试图通过对空间进行划分并索引结果子空间来快速检索最近邻。然而，索引这些子空间以便高效扫描邻域区域以找到给定查询的最近邻并不容易。**图1**就是一个例子(该图不包括PQ算法，因为从某些角度来看，它可以被视为一种哈希方法)。非图基方法需要检查许多附近的单元，以达到较高的准确性。大量的远离点被检查，而随着维度的增加，这个问题变得更加严重(被称为维度诅咒)。图基方法可能从离查询较远的位置开始，但由于它们都是基于邻接图，通常能更好地表达“邻居”关系，因此能够快速接近查询。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240924135410218.png" alt="image-20240924135410218" style="zoom:30%;" /> 

图1：(a)是树索引，(b)是哈希索引，(c)是图索引。红色星星是查询(不包含在基础数据中)。四个红色圆环是它的最近邻。树索引和哈希索引将空间划分为几个单元，每个单元最多包含三个点。图索引中每个节点的出度也不超过三个。为了检索查询的最近邻，我们需要回溯并检查许多叶节点以进行树索引。对于哈希索引，我们需要检查汉明半径为2的附近桶。至于图索引，算法1形成了一条搜索路径，如红线所示。橙色圆圈是搜索过程中检查的点。基于图的算法需要进行的距离计算次数最少。

---

总之，为了达到相同的准确性，非图基方法往往需要检查比图基方法更多的点。我们在后面的实验中将展示这一点。

## 2.3. Graph-Based ANNS Methods

给定在 $E^d$ 中的有限点集 $S$，图是一种由一组节点(表示点)和连接某些节点对的边构成的结构。如果节点 $p$ 和 $q$ 之间有一条边，则称 $p$ 为 $q$ 的邻居。基于图的近似最近邻搜索(ANNS)通过图索引解决上述的 ANNS 问题。算法 1 在大多数基于图的方法中被广泛使用。在过去的几十年中，许多图的设计旨在实现高效的 ANNS。这里我们将介绍几种具有吸引人理论属性的图结构。

---

**德劳内图**(或德劳内三角剖分)被定义为沃罗诺伊图的对偶图 [4]。研究表明，它是一个单调搜索网络 [30]，但在高维度上，德劳内图的 ANNS 时间复杂度较高。根据 Harwood 等人的研究 [7]，在高维度下，德劳内图几乎会变得完全连接。因此，搜索的效率显著降低。GNNS[21] 基于(近似)$k$ 最近邻图，这是德劳内图的一个近似。IEH [27] 和 Efanna [15] 也基于(近似)$k$ 最近邻图。它们使用哈希和随机化 KD 树为算法 1 在 $k$ 最近邻图上提供更好的起始位置。尽管它们提高了性能，但仍面临大型和复杂索引的挑战。

---

Wen 等人 [31] 提出了一个称为 $DPG$ 的图结构，该结构建立在近似 $k$ 最近邻图之上。他们提出了一种边选择策略，从预构建的 $k$ 最近邻图中剪除一半的边，并最大化剩余边之间的平均角度。最后，他们对图进行补偿，以生成一个无向图。他们的直觉是使边之间的角度在每个节点周围均匀分布，但缺乏理论支持。根据我们的实证研究，DPG 面临着较大的索引和较差的搜索性能。

---

**相对邻域图**(RNG) [38] 最初并不是为 ANNS 问题而设计的。然而，RNG 在 ANNS 中展现出了巨大的潜力。RNG 采用了一种有趣的边选择策略，以消除 $S$ 中所有可能三角形中的最长边。通过这一策略，RNG 将其平均出度减少到一个常数 $C_d + o(1)$，这个常数仅与维度 $d$ 相关，通常非常小 [25]。然而，根据 Dearholt 等人的研究 [13]，由于严格的边选择策略，RNG 并没有足够的边来成为一个单调搜索网络。因此，无法对路径长度提供理论保证。Dearholt 等人提出了一种方法，向 RNG 添加边，并将其转变为最小边数的单调搜索网络(MSNET)，称为最小 MSNET [13]。该算法基于一个预构建的 RNG，其索引复杂度为 $O\left(n^{2-\frac{2}{1+d}+\epsilon}\right)$，在一般位置假设下成立 [25]。构建最小 MSNET 的预处理复杂度为 $O\left(n^2 \log n + n^3\right)$。对于高维和大规模数据库，最小 MSNET 的总索引复杂度非常庞大。最近的一些实用图方法，如 FANNG [7] 和 HNSW [34]，采用了 RNG 的边选择策略，以减少其图的出度并提高搜索性能。然而，它们没有提供理论分析。

---

**可导航小世界网络** [9,29] 天生适合 ANNS 问题。节点的度数和每个节点的邻居都是根据特定的概率分布分配的。该图上的搜索路径长度随网络规模的增长而多对数增长，表示为 $O\left(A[\log N]^\nu\right)$，其中 $A$ 和 $\nu$ 是一些常数。这是一个经验估计，尚未得到证明。因此，总的经验搜索复杂度为 $O\left(A D[\log N]^\nu\right)$，其中 $D$ 是图的平均度数。图的度数需要仔细选择，因为这对搜索效率有很大影响。与其他传统图一样，构建这样一个图的时间复杂度在简单情况下约为 $O\left(n^2\right)$，对于大规模问题而言并不实用。Yury 等人 [33] 提出了 NSW 图，以同时近似可导航小世界网络和 Delaunay 图。但不久他们发现，图的度数过高，并且他们的方法中也存在连通性问题。后来，他们提出了 HNSW [34] 来解决这个问题。具体来说，他们将多个 NSW 堆叠成一个层次结构，以解决连通性问题。上层的节点通过概率分布进行采样，NSW 的大小从底层逐层缩小。他们的直觉是，上层提供了长距离的捷径，以便快速定位目标邻域。然后，他们使用 RNG 的边选择策略来降低图的度数。根据一些在 GitHub 上的开源基准测试(https://github.com/erikbern/ann-benchmarks)，HNSW 迄今为止是最有效的 ANNS 算法。

---

**随机邻域图** [3] 是专为高维空间中的 ANNS 问题而设计的。它以随机方式构建。首先，他们在每个节点周围用一组凸锥来划分空间，然后在每个锥内选择 $O(\log n)$ 个最近的节点作为其邻居。他们证明了该图上的搜索时间复杂度为 $O\left((\log n)^3\right)$，这一点非常吸引人。然而，其索引复杂度太高。为了降低索引复杂度，他们提出了一种变体，称为 $\mathrm{RNG}^*$。$\mathrm{RNG}^*$ 也采用了 RNG 的边选择策略，并使用额外的结构(KD 树)来提高搜索性能。然而，其索引的时间复杂度仍高达 $O\left(n^2\right)$ [3]。



# 3. ALGORITHMS AND ANALYSIS

## 3.1. Motivation

启发式搜索算法(算法 1)在过去几十年中已广泛应用于各种图索引上。该算法在图中遍历，并试图贪婪地到达查询点。因此，影响搜索效率的两个最关键因素是起始节点与目标节点之间的贪婪跳数和每一步选择下一个节点的计算成本。换句话说，图上的搜索时间复杂度可以写为 $O(o l)$，其中 $o$ 是图的平均出度，$l$ 是搜索路径的长度。

---

在最近的图基算法中 $[7,15,21,27,31,33,34]$，图的出度是一个可调参数。在我们的实验研究中，给定一个数据集和期望的搜索精度，我们发现存在最优的出度可以导致最佳的搜索性能。一种可能的解释是，给定期望的精度，$o l$ 是 $o$ 的一个凸函数，其最小值决定了给定图的搜索性能。在高精度范围内，一些算法(例如，GNNS [21]、NSW [33]、DPG [31])的最优出度非常大，这导致图的大小也非常大。它们的 $o l$ 最小值也很大，导致性能较差。其他算法 $[15,27,34]$ 则使用额外的索引结构来改善算法 1 中的起始位置，以直接最小化 $l$，但这也导致了较大的索引。

---

从我们的角度来看，我们可以通过同时最小化 $o$ 和 $l$ 来提高基于图的方法的 ANNS 性能。此外，我们需要尽可能减小索引的大小，以处理大规模数据。一个常被忽视的方面是，首先需要确保从起始节点到查询点存在一条路径。否则，目标将永远无法达到。总之，我们的目标是从以下四个方面设计一个具有高 ANNS 性能的图索引：(1) 确保图的连通性，(2) 降低图的平均出度，(3) 缩短搜索路径，以及 (4) 减少索引大小。第一点容易实现。如果起始节点随查询而变化，则应确保图是强连通的；如果起始节点是固定的，则应确保所有其他节点都可以通过从起始节点的深度优先搜索(DFS)到达。至于第二点到第四点，我们通过设计一个更好的稀疏图来同时解决这些问题。下面，我们将提出一种新的图结构，称为单调相对邻域图(MRNG)，并对其重要性质进行理论分析，这将导致更好的 ANNS 性能。

## 3.2. Graph Monotonicity And Path Length

ANNS 在图上的速度主要由两个因素决定：搜索路径的长度和图的平均出度。我们的目标是找到一个具有低出度和短搜索路径的图。我们将从如何设计一个具有非常短搜索路径的图开始讨论。在介绍我们的提议之前，我们将首先对一种称为单调搜索网络(MSNET)的图类进行详细分析，该类图在 [13] 中首次讨论，并在 ANNS 中显示出巨大的潜力。接下来，我们将呈现 MSNET 的定义。

### 3.2.1. Definition And Notation

给定一个在 $E^d$ 空间中的点集 $S$，$p$ 和 $q$ 是 $S$ 中的任意两点。设 $B(p, r)$ 表示一个开球，满足 $B(p, r) = \{x \mid \delta(x, p) < r\}$。令 $\overrightarrow{pq}$ 表示从 $p$ 指向 $q$ 的有向边。我们首先定义图中的单调路径如下：

**定义 3**(单调路径)。给定在空间 $E^d$ 中的 $n$ 个点的有限点集 $S$，$p$ 和 $q$ 是 $S$ 中的任意两点，$G$ 表示在 $S$ 上定义的图。设 $v_1, v_2, \ldots, v_k$($v_1 = p, v_k = q$)表示从 $p$ 到 $q$ 在 $G$ 中的一条路径，即对于所有 $i = 1, \ldots, k-1$，边 $v_i \overrightarrow{v_{i+1}} \in G$。当且仅当对于所有 $i = 1, \ldots, k-1$，满足 $\delta(v_i, q) > \delta(v_{i+1}, q)$ 时，该路径为单调路径。

然后，单调搜索网络的定义如下：

**定义 4**(单调搜索网络)。给定在空间 $E^d$ 中的 $n$ 个点的有限点集 $S$，在 $S$ 上定义的图称为单调搜索网络，当且仅当对于任意两个节点 $p, q \in S$，存在至少一条从 $p$ 到 $q$ 的单调路径。

### 3.2.2. Analysis On Monotonic Search Networks

单调搜索网络(MSNET)[13] 是一类图，可以保证图中任意两个节点之间存在单调路径。MSNET 本质上是强连通的，这确保了图的连通性。在单调路径上移动时，我们在每一步都朝着目标前进。在 MSNET 中，Dearholt 等人假设可以使用算法 1(常用于基于图的搜索)来检测到目标节点的单调路径，即不需要回溯 [13]，这是一种非常吸引人的特性。回溯意味着，当算法无法找到离查询更近的邻居(即局部最优解)时，我们需要回到已访问的节点，寻找其他方向继续前进。MSNET 的单调性使得算法 1 在图上的搜索行为几乎是确定和可分析的。然而，Dearholt 等人 [13] 未能提供这一特性的证明。在本节中，我们将对此特性给出一个具体的证明。

---

**定理 1**。给定一个包含 $n$ 个点的有限点集 $S$，随机分布在 $E^d$ 空间中，以及在 $S$ 上构建的 MSNET $G$，可以通过算法 1 在 $G$ 中找到任意两个节点 $p, q$ 之间的单调路径，而无需回溯。

证明。由于篇幅限制，请参见我们的技术报告 [16] 中的详细证明。

根据定理 1，我们知道可以在给定的 MSNET 上使用算法 1 达到查询点 $q \in S$，而无需回溯。因此，迭代期望与 MSNET 中单调路径的长度期望是相同的。在讨论给定 MSNET 中单调路径的长度期望之前，我们首先从不同的角度定义 MSNET，这将有助于分析。

---

**引理 1**。给定一个在 $E^d$ 中的点集 $S$ 上的图 $G$，当且仅当对于任意两个节点 $p, q$，存在至少一条边 $\overrightarrow{p r}$，使得 $r \in B(q, \delta(p, q))$，$G$ 是一个 MSNET。

证明。由于篇幅限制，请参见我们的技术报告 [16] 中的详细证明。

根据引理 1，我们可以计算 MSNET 中单调路径的长度期望，如下所示。

---

**定理 2**。设 $S$ 为一个有限点集，包含 $n$ 个均匀分布在 $E^d$ 的有限子空间中的点。假设包含 $S$ 的最小凸包的体积为 $V_S$。$S$ 中任意两点之间的最大距离为 $R$。我们对 $V_S$ 施加一个约束，当 $d$ 固定时，存在常数 $\kappa$，使得 $\kappa V_S \geq V_B(R)$，其中 $\kappa$ 是一个与 $n$ 无关的常数，$V_B(R)$ 是半径为 $R$ 的球体的体积。我们定义 $\Delta r$ 为所有可能的非等腰三角形 $abc$ 上的 $\Delta r=\min \{|\delta(a, b)-\delta(a, c)|,|\delta(a, b)-\delta(b, c)|,|\delta(a, c)-\delta(b, c)|\}$。$\Delta r$ 是 $n$ 的一个递减函数。

对于在这样的 $S$ 上定义的 MSNET，从 $p$ 到 $q$ 的单调路径的长度期望为 $O\left(\frac{n^{1/d} \log(n^{1/d})}{\Delta r}\right)$。

证明。由于篇幅限制，请参见我们的技术报告 [16] 中的详细证明。

---

定理 2 是对所有类型 MSNET 的一般性质。函数 $\Delta r$ 关于 $n$ 并没有明确的表达式，因为它涉及随机性。我们观察到，在实际中，随着 $n$ 的增加，$\Delta r$ 的下降速度非常缓慢。在实验中，我们基于本文提出的图对不同公共数据集估计了 $\Delta r$ 的函数，发现 $\Delta r$ 主要受数据分布和数据密度的影响。结果在实验部分中展示。

---

因为 $O\left(n^{\frac{1}{d}}\right)$ 在高维空间中随着 $n$ 的增加增长非常缓慢，所以在 MSNET 中单调路径的长度期望 $O\left(\frac{n^{1 / d} \log \left(n^{1 / d}\right)}{\Delta r}\right)$ 的增长速率将非常接近于 $O(\log n)$。我们的实验也验证了这一点。同样，我们可以看到，当 $d$ 增加时，单调路径的长度期望的增长速率较低。

---

在定理 2 中，包含数据点的最小凸包的体积的假设实际上是对数据分布的一个约束。我们尝试避免数据分布的特殊形状(例如，所有点形成一条直线)，因为这可能会影响结论。例如，如果数据点均匀分布在一条直线上，那么在这样的数据集上单调路径的长度期望将几乎线性地随 $n$ 增长。

---

此外，尽管我们假设数据点均匀分布，但该性质在其他各种分布上在实践中仍在一定程度上成立。除了某些极特殊形状的数据分布外，我们通常可以预期，随着搜索路径每一步搜索球体的缩小，剩余在球体中的节点数量按一定比例减少。这个比例主要由数据分布决定，如图 2 所示。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240924135524410.png" alt="image-20240924135524410" style="zoom:25%;" /> 

图2：MSNET中的搜索示意图。查询点为 $q$，搜索从点 $p$ 开始。在每一步中，算法1将从当前节点的邻居中选择一个与 $q$ 最近的节点。假设 $p, r, s$ 是算法1选择的单调路径上的节点。搜索区域从球体 $B(q, \delta(p, q))$ 缩小到 $B(q, \delta(r, q))$，然后到 $B(q, \delta(s, q))$。每个球体中的节点数量(可能被检查)在每一步减少一个比例，直到最终球体中只剩下 $q$。 

---

除了搜索路径的长度期望，另一个影响搜索复杂度的重要因素是图的平均出度。一些 MSNET，例如 Delaunay 图，当 $n$ 增加时其出度会增长 [4]。由于 MSNET 没有统一的几何描述，因此对于 MSNET 的出度如何随之变化也没有统一的结论。

---

Dearholt 等人 [13] 声称他们找到了一种构建具有最小出度的 MSNET 的方法。然而，他们的方法存在两个问题。首先，他们没有提供关于他们的 MSNET 的出度如何随 $n$ 变化的分析。这主要是因为他们提出的 MSNET 是通过向 RNG 添加边构建的，缺乏几何描述。其次，所提出的 MSNET 构建方法的时间复杂度非常高(至少为 $O\left(n^{2-\frac{2}{1+d}+\epsilon}+n^2 \log n+n^3\right)$)，在实际的大规模场景中并不实用。下面我们将提出一种新型的 MSNET，其索引复杂度较低且出度期望是常数(与 $n$ 无关)。简单来说，在这个图上的搜索复杂度与单调路径的长度期望以相同的速度随 $n$ 增长。

## 3.3. Monotonic Relative Neighborhood Graph

在这一部分，我们描述了一种新的图结构，用于近似最近邻搜索(ANNS)，称为单调相对邻域图(MRNG)，它属于 MSNET 家族。为了使图稀疏，HNSW 和 FANNG 转向了 RNG [38]，但已经证明 RNG 的边缘不足以成为 MSNET [13]。因此，RNG 中搜索路径的长度没有理论保证，搜索过程可能会遭遇长时间的绕行。

---

考虑以下示例。设 lune ${ }_{p q}$ 表示一个区域，使得 $lune_{p q}=B(p, \delta(p, q)) \cap B(q, \delta(p, q))$ [25]。给定一个在空间 $E^d$ 中的有限点集 $S$，对于任何两个节点 $p, q \in S$，边 $p q \in \mathrm{RNG}$ 当且仅当 lune $_{p q} \cap S=\emptyset$。在图 3 中，(a) 是一个非单调路径的示例。同样，$t, u, q$ 也与 $p$ 不相连。当搜索从 $p$ 到 $q$ 时，路径是非单调的(例如，$r q<p q$)。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240924135701779.png" alt="image-20240924135701779" style="zoom:25%;" /> 

图3：RNG(a)和MRNG(b)边选择策略的比较。RNG是无向图，而MRNG是有向图。在(a)中，$p$ 和 $r$ 之间有边连接，因为 $t \in$ lune $_{p u}$，且 $u \in$ lune $_{p q}$，而 $p$ 和 $s, t, u, q$ 之间没有边。在(b)中，$p$ 和 $r$ 之间有边连接，因为 lune $e_{p r}$ 中没有节点。$p$ 和 $s$ 之间没有边连接，因为 $r \in$ lune $_{p s}$，且 $p r, s r \in MRNG$。有向边 $\overrightarrow{p t} \in MRNG$ 因为 $\overrightarrow{p s} \notin MRNG$。然而，$\overrightarrow{t p} \notin MRNG$ 因为 $\overrightarrow{t s} \in MRNG$。我们可以看到，MRNG是以递归方式定义的，而RNG的边选择策略比MRNG更严格。在RNG(a)中，从 $q$ 到 $p$ 存在一条单调路径，但从 $p$ 到 $q$ 没有单调路径。在MRNG(b)中，任意节点之间至少存在一条单调路径。

---

我们发现这个问题主要是由于 RNG 的边选择策略造成的。Dearholt 等人尝试向 RNG 添加边 [13] 以产生具有最少边的 MSNET，但这种方法非常耗时。相反，受到 RNG 的启发，我们提出了一种新的边选择策略来构建单调图。得到的图可能不是最小的 MSNET，但它非常稀疏。基于这一新策略，我们提出了一种新颖的图结构，称为单调相对邻域图(MRNG)。正式定义如下：

**定义 5** (MRNG)。给定一个在空间 $E^d$ 中的有限点集 $S$，一个 MRNG 是一个有向图，其边集满足以下性质：对于任何边 $\overrightarrow{p q}$，当且仅当 ${ lune}_{p q} \cap S=\emptyset$ 或者 $\forall r \in\left(\right.$ lune $\left._{p q} \cap S\right)，\overrightarrow{p r} \notin M R N G$ 时，$\overrightarrow{p q} \in M R N G$。

---

当出现等腰三角形时，我们通过以下方式避免歧义。如果 $\delta(p, q)=\delta(p, r)$ 且 $qr$ 是三角形 $pqr$ 中的最短边，我们根据预定义的索引选择边，即如果 $\operatorname{index}(q)<\operatorname{index}(r)$，则选择 $\overrightarrow{pq}$。我们可以看到，MRNG 的定义是递归的。换句话说，定义 5 意味着对于任何节点 $p$，我们应该从最近到最远选择其邻居。MRNG 的边选择策略与 RNG 的区别在于，对于任何边 $pq \in MRNG$，$lune_{pq} \cap S$ 不一定是空集。这一差异在图 3 中得到了清晰的展示。这里我们展示 MRNG 是一个 MSNET。

---

**定理 3**。给定一个有限点集 $S$，包含 $n$ 个点。在 $S$ 上定义的 MRNG 是一个 MSNET。

证明。由于空间限制，请参见我们技术报告 [16] 中的详细证明。

虽然结构不同，MRNG 和 RNG 共享一些公共边。我们将首先定义最近邻图(NNG)，如下所示：

**定义 6**(NNG)。给定一个在空间 $E^d$ 中的有限点集 $S$，NNG 是边的集合，对于任何边 $\overrightarrow{pq}$，当且仅当 $q$ 是 $S$ 中 $p$ 的最近邻时，$\overrightarrow{pq} \in NNG$。

---

同样地，我们可以通过为每个节点分配一个唯一的索引来消除 NNG 中的歧义，并将节点链接到其最近邻中索引最小的节点。显然，我们有 $MRNG \cap RNG \supset NNG$(如果节点 $q$ 是 $p$ 的最近邻，则有 $lune_{pq} \cap S = \emptyset$)。这是 MRNG 单调性的必要条件。图 4 显示了如果我们在某个图 $G$ 上应用 MRNG 的边选择策略但不保证 $NNG \subset G$ 时的非单调路径示例。图 4 中的边满足 MRNG 的选择策略，除了 $q$ 被强制不与其最近邻 $t$ 连接。因为 $t$ 是 $q$ 的最近邻，我们有 $\delta(q, r) > \delta(q, t)$。由于 $qt$ 是三角形 $qtr$ 中的最短边，并且 $q$ 和 $r$ 是连接的，那么根据 MRNG 的边选择策略，$rt$ 必须是三角形 $qtr$ 中的最长边。因此，$r$ 和 $t$ 不能连接，我们只能通过其他节点(如 $s$)到达 $t$。类似地，只有当 $rt$ 是三角形 $rst$ 中的最长边时，边 $rs$ 和 $st$ 才能在这个图中共存。因此，当我们从 $p$ 到 $t$ 时，我们需要至少经过节点 $r, s$ 的绕行。由于 $\delta(q, r) > \delta(q, t)$，这是从 $p$ 到 $t$ 的非单调路径。如果我们不保证 $NNG \subset MRNG$，则绕行是不可避免的，这在实际中可能更糟。很容易验证，如果我们在 $G$ 上执行 RNG 的边选择策略但不保证 $NNG \subset G$，类似的绕行问题也会出现。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240924140100004.png" alt="image-20240924140100004" style="zoom:25%;" /> 

图4：说明 $N N G$ $\subset M R N G$ 的必要性。如果不是这样，图就无法成为MSNET。路径 $p, q, r, s, t$ 是从 $p$ 到 $t$ 的一个非单调路径。在这个图中，$t$ 是 $q$ 的最近邻，但与 $q$ 没有边连接。我们在这个图上应用MRNG的边选择策略。根据该策略的定义，$t$ 和 $r$ 之间永远不会有边连接。当搜索从 $p$ 到 $t$ 时，必须至少经过 $s$ 进行一次绕行。在实际中，这个问题会更严重。

---

在这里，我们将讨论 MRNG 的平均出度。MRNG 的边数比 RNG 多，但它仍然非常稀疏，因为共享同一节点的任意两条边之间的夹角至少为 $60^{\circ}$(根据 MRNG 的定义，对于任意两条边 $pq, pr \in MRNG$，$qr$ 必须是三角形 $pqr$ 中的最长边，并且 $qr \notin MRNG$)。

**引理 2**。给定一个 $MRNG$ 在 $E^d$ 中，$MRNG$ 的最大出度是一个常数，并且与 $n$ 无关。

证明。由于空间限制，请参见我们技术报告中的详细证明 [16]。

根据引理 2、定理 1、定理 2 和定理 3，我们可以得出，在 MRNG 上的期望搜索复杂度为 $O\left(c n^{\frac{1}{d}} \log n^{\frac{1}{d}} / \Delta r\right)$，其中 $c$ 是 MRNG 的平均出度，与 $n$ 无关，$\Delta r$ 是一个关于 $n$ 的函数，随着 $n$ 的增加而缓慢减小。

## 3.4. MRNG Construction

MRNG 可以通过在每个节点上应用我们的边选择策略简单构建。具体来说，对于每个节点 $p$，我们将 $S$ 中的其余节点集合记为 $R = S \backslash \{p\}$。我们计算 $R$ 中每个节点与 $p$ 的距离，然后根据距离升序排列。我们将选定的节点集合记为 $L$。我们将 $R$ 中与 $p$ 最近的节点添加到 $L$，以确保 $NNG \subset MRNG$。接下来，我们从 $R$ 中选择一个节点 $q$，从 $L$ 中选择一个节点 $r$，以检查 $pq$ 是否是三角形 $pqr$ 中的最长边。如果 $pq$ 不是三角形 $pqr$ 中的最长边(对于所有 $r \in L$)，我们将 $q$ 添加到 $L$。我们重复这个过程，直到检查完 $R$ 中的所有节点。这个简单的构建过程的时间复杂度为 $O\left(n^2 \log n+n^2 c\right)$，其中 $c$ 是 MRNG 的平均出度，远小于文献 [13] 中提出的 MSNET 索引方法的出度，该方法在一般位置假设下的复杂度至少为 $O\left(n^{2-\frac{2}{1+d}+\epsilon}+n^2 \log n+n^3\right)$。

## 3.5. NSG: Practical Approximation For MRNG

尽管 MRNG 可以保证快速的搜索时间，但其高索引时间对于大规模问题仍然不够实用。在本节中，我们将通过近似 MRNG 提出一种实用的方法，并从四个标准出发设计一个良好的图用于近似最近邻搜索(ANNS)。我们将其命名为 **导航扩展图(NSG)**。我们首先介绍 NSG 构建算法(算法 2)，具体步骤如下： 

1. 使用当前最先进的方法(例如 $[14,28]$)构建一个近似的 $k$ 最近邻图。
2. 找到数据集的近似中值(medoid)。这可以通过以下步骤实现：(1)计算数据集的重心；(2)将重心视为查询，在 $k$ 最近邻图上使用算法 1 进行搜索，将返回的最近邻作为近似中值。该节点被称为导航节点，因为所有搜索将从这个固定节点开始。
3. 对于每个节点，生成一个候选邻居集合，并从候选集合中选择邻居。这可以通过以下步骤实现：对于给定节点 $p$，(1)将其视为查询，从导航节点出发在预构建的 $k$ 最近邻图上执行算法 1；(2)在搜索过程中，每访问一个节点 $q$(即计算节点 $p$ 和 $q$ 之间的距离)时，将其添加到候选集合中(并记录距离)；(3)根据 MRNG 的边选择策略，从候选集合中最多选择 $m$ 个邻居。
4. 在前面步骤生成的图上展开深度优先搜索(DFS)树，将导航节点作为根。当 DFS 终止时，如果有节点未链接到树中，则将其链接到它们的近似最近邻(通过算法 1 找到)并继续 DFS。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240924140346096.png" alt="image-20240924140346096" style="zoom: 60%;" /> 

---

以下是 NSG 构建算法的动机。最终目标是构建一个低索引时间复杂度的 MRNG 近似。

1. MRNG 确保在任意两个节点之间至少存在一条单调路径，但这并不是一项容易的任务。相反，我们只需挑选一个节点，并尝试确保从这个节点到其他所有节点存在单调路径。我们将这个节点称为导航节点。在进行搜索时，我们始终从导航节点开始，这使得在 NSG 上的搜索几乎与在 MRNG 上一样高效。

2. MRNG 的边选择策略将所有其他节点视为当前节点的候选邻居，这导致了高时间复杂度。为了加快这一过程，我们希望为每个节点生成一个小的候选子集。这些候选包含两个部分：(1)如上所述，NNG 对于单调性至关重要。因为获取精确的 NNG 非常耗时，我们转向近似的 kNN 图。一个高质量的近似 kNN 图通常包含高质量的近似 NNG。当只有少数节点未链接到它们的最近邻时，这是可以接受的。(2)由于 NSG 上的搜索始终从导航节点 $p_n$ 开始，对于给定节点 $p$，我们只需要考虑那些在从 $p_n$ 到 $p$ 的搜索路径上的节点。因此，我们将 $p$ 视为查询，并在预构建的 $k$ 最近邻图上执行算法 1。搜索中访问的节点以及 $p$ 在近似 NNG 中的最近邻被记录为候选节点。从导航节点到 $p$ 的单调路径上的节点很可能包含在候选中。当我们对这些候选执行 MRNG 的边选择策略时，NSG 很可能继承了从导航节点到 $p$ 的 MRNG 中的单调路径。

3. 上述方法中可能出现的问题是某些节点的度数爆炸问题。尤其是，导航节点和稠密区域中的节点会作为“交通枢纽”，并具有高出度。这个问题在 HNSW [34] 中也有讨论。他们引入了多层图结构来解决这个问题，但他们的解决方案显著增加了内存使用。我们的解决方案是通过舍弃较长的边，将所有节点的出度限制在一个小值 $m \ll n$。这样做的结果是，图的连通性不再得到保证，因为边的消除。

---

为了解决连通性问题，我们引入了一种基于前面描述的 DFS 跨度树的新方法。经过这个过程，所有节点都保证至少有一条路径从导航节点扩展出去。尽管所提出的方法在最坏情况下会牺牲一些性能，但如果我们构建高质量的近似 $k$ 最近邻图并选择适当的出度限制 $m$，NSG 中的绕路将被最小化。

---

通过近似 MRNG，NSG 可以继承与 MRNG 类似的低搜索复杂度。同时，出度上限使得图非常稀疏，而树跨度操作确保了 NSG 的连通性。NSG 的索引仅包含一个稀疏图，而没有辅助结构。与之前的工作相比，我们的方法在这四个方面都取得了进展。这些改进在我们的实验中得到了验证，详细结果将在后面的部分中呈现。

### 3.5.1. Indexing Complexity of NSG

NSG 的总索引复杂度包含两个部分：$k$ 最近邻图构建的复杂度和 NSG 的预处理步骤。在百万规模的实验中，我们使用 nn-descent 算法在 CPU 上构建近似的 $k$ 最近邻图。在 DEEP100M 实验中，由于 nn-descent 在大数据集上的内存消耗会爆炸，我们使用 Faiss 在 GPU 上构建。我们在这一部分专注于算法 2 的复杂度。

---

NSG 的预处理步骤包括搜索-收集-选择操作和树跨度。由于 $k$ 最近邻图是 Delaunay 图(一个 MSNET)的近似，基于它的搜索复杂度大约为 $O\left(k n^{\frac{1}{d}}\log n^{\frac{1}{d}} / \Delta r\right)$。我们搜索所有节点，因此总复杂度大约为 $O\left(k n^{\frac{1+d}{d}} \log n^{\frac{1}{d}} / \Delta r\right)$。边选择的复杂度为 $O(n l c)$，其中 $l$ 是搜索生成的候选数量，$c$ 是我们为图设置的最大出度。由于 $c$ 和 $l$ 在实践中通常非常小(即 $c \ll n, l \ll n$)，这个过程非常快速。最后的树跨度过程也很快，因为强连通分量的数量通常远小于 $n$，我们只需向图中添加少量边。因此，这些过程的总复杂度大约为 $O\left(k n^{\frac{1+d}{d}} \log n^{\frac{1}{d}} / \Delta r\right)$，这一点在后续的实验评估中得到了验证。我们还发现 $\Delta r$ 几乎是一个常数，不会影响我们实验中的复杂度。

---

在本文的实现中，NSG 的整体经验索引复杂度为 $O\left(k n^{\frac{1+d}{d}} \log n^{\frac{1}{d}}+f(n)\right)$，其中 $f(n)=n^{1.16}$(使用 nn-descent)和 $f(n)=n \log n$(使用 Faiss)，这远低于 MRNG 的 $O\left(n^2 \log n+c n^2\right)$。

## 3.6. Search On NSG

我们在 NSG 上使用算法 1 进行搜索，并始终从导航节点开始搜索。由于 NSG 是对 MRNG 的精心设计的近似，NSG 上的搜索复杂度平均约为 $O\left(c n^{\frac{1}{d}} \log n^{\frac{1}{d}} / \Delta r\right)$，其中 $c$ 是 NSG 的最大出度，$d$ 是维度。在我们的实验中，$\Delta r$ 约为 $O\left(n^{-\frac{\epsilon}{d}}\right)$，其中 $0<\epsilon \ll d$。因此，经验平均搜索复杂度为 $O\left(c n^{\frac{1+\epsilon}{d}} \log n^{\frac{1}{d}}\right)$。由于 $1+\epsilon \ll d$，这个复杂度非常接近于 $O(\log n)$，这一点在我们后续的实验评估中得到了验证。我们的代码已在 GitHub 上发布(https://github.com/ZJULearning/nsg)。



# 4. EXPERIMENTS

在这一部分，我们提供了对公共数据集和合成数据集的详细实验分析，以证明我们方法的有效性。

## 4.1. Million-Scale ANNS

### 4.1.1. Datasets

由于并非所有最新的最先进算法都能扩展到十亿点数据集，因此本实验在四个百万规模的数据集上进行。SIFT1M和GIST1M属于BIGANN数据集，这些数据集在相关文献中被广泛使用。RAND4M和GAUSS5M是两个合成数据集，RAND4M由均匀分布$U(0,1)$生成，而GAUSS5M则由高斯分布$N(0,3)$生成。考虑到数据可能位于低维流形上，我们测量了局部内在维度(LID)，以更好地反映数据集的难度。具体信息见表1。

$\small\begin{array}{|c|c|c|c|c|}
\hline \text { dataset } & \text { D } & \text { LID } & \text { No. of base } & \text { No. of query } \\
\hline \text { SIFT1M } & 128 & 12.9 & 1,000,000 & 10,000 \\
\text { GIST1M } & 960 & 29.1 & 1,000,000 & 1,000 \\
\text { RAND4M } & 128 & 49.5 & 4,000,000 & 10,000 \\
\text { GAUSS5M } & 128 & 48.1 & 5,000,000 & 10,000 \\
\hline
\end{array}$   

---

为了防止索引过拟合查询数据，我们通过随机抽样每个训练集中的百分之一数据点作为验证集来重新划分数据集。由于在实际场景中，在高精度区域(超过90%)快速响应至关重要，我们专注于所有算法在高精度区域的表现，并在验证集上调整它们的索引，以获得最佳性能。

### 4.1.2. Compared Algorithms

我们选择的比较算法涵盖了树基、哈希基、量化基和图基等多种类型。大多数算法的代码在GitHub上可用，并经过良好的优化。对于那些没有发布代码的算法，我们根据其论文实现了这些算法。它们用C++实现，并由g++4.9以“O3”选项编译。SIFT1M和GIST1M的数据集实验在一台配备i7-4790K CPU和32GB内存的机器上进行，而RAND4M和GAUSS5M的数据集实验则在一台配备Xeon E5-2630 CPU和96GB内存的机器上进行。NSG的索引包含两个步骤，即$k \mathrm{NN}$图构建和算法2的执行。我们使用nn-descent算法来构建$k \mathrm{NN}$图。

---

由于并非所有算法都支持内部查询并行化，在所有搜索实验中，我们仅评估单线程算法。考虑到所有比较的算法都有并行版本用于索引构建，为了节省时间，我们用八个线程构建所有索引。

1. **串行扫描** 我们对基础数据进行串行扫描，以获取测试点的准确最近邻。
2. **基于树的方法**。Flann(https://github.com/mariusmuja/flann)是一个基于随机化KD树、K均值树和复合树算法的知名ANNS库。我们使用其随机KD树算法进行比较。Annoy(https://github.com/spotify/annoy)基于二叉搜索森林。
3. **基于哈希的方法**。FALCONN(https://github.com/FALCONN-LIB/FALCONN)是一个基于多探测局部敏感哈希的知名ANNS库。
4. **基于量化的方法**。Faiss(https://github.com/facebookresearch/faiss)最近由Facebook发布。它包含对最新产品量化方法的良好实现代码，支持CPU和GPU。这里使用CPU版本进行公平比较。
5. **基于图的方法**。KGraph(https://github.com/aaalgo/kgraph)基于$k$ NN图。Efanna(https://github.com/ZJULearning/efanna)基于随机KD树和$k \mathrm{NN}$图的复合索引。FANNG基于[7]中提出的一种图结构。他们没有发布代码，因此我们根据他们的论文实现了算法。HNSW(https://github.com/searchivarius/nmslib)基于分层图结构，该结构在[34]中提出。$\mathbf{D P G}$(https://github.com/DBWangGroupUNSW/nns_benchmark)基于从$k \mathrm{NN}$图中选择边的无向图。根据开源基准(https://github.com/erikbern/ann-benchmarks)，HNSW迄今为止是CPU上最快的ANNS算法。
6. **NSG** 是本文提出的方法。它只包含一个带有导航节点的图，搜索始终从该节点开始。
7. **NSG-Naive** 是一个设计的基准，用于证明NSG的搜索-收集-选择操作的必要性以及图连接性的保障。我们直接在近似$k \mathrm{NN}$图的边上执行MRNG的边选择策略以获得NSG-Naive。没有导航节点，因此我们在NSG-Naive上使用随机初始化的算法1。

### 4.1.3. Results

#### 4.1.3.1. A. Non-Graph-Based v.s. Graph-Based

我们记录了在达到一定搜索精度时，Flann(随机化KD树)、FALCONN(LSH)、Faiss(IVFPQ)和NSG在SIFT1M和GIST1M上的距离计算次数。在我们的实验中，在相同精度下，其他方法检查的点数是NSG的十倍以上(详见我们技术报告中的图表 [16])。这也是图基方法与非图基方法之间性能差距的主要原因。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240924140214238.png" alt="image-20240924140214238" style="zoom:25%;" /> 

图5：NSG中边选择的候选节点示意图。节点 $p$ 是待处理的节点，$m$ 是导航节点。红色节点是节点 $p$ 的 $k$ 个最近邻。大黑节点和实线构成从 $m$ 到 $p$ 的可能单调路径，由搜索和收集例程生成。小黑节点是搜索和收集例程访问的节点。图中所有节点都将被添加到 $p$ 的候选集合中。 

#### 4.1.3.2. Check Motivation

在本文中，我们旨在从以下四个方面设计一个高性能的图索引以支持近似最近邻搜索(ANNS)：(1)确保图的连通性，(2)降低图的平均出度，(3)缩短搜索路径，以及(4)减少索引大小。

---

**图的连通性。** 在图基方法中，NSG和HNSW的搜索都是从固定节点开始。为了确保连通性，NSG和HNSW需要保证其他点能够从这个固定起始节点到达。而其他方法需要保证它们的图是强连通的，因为搜索可以从任何节点开始。在我们的实验中，我们发现，除了NSG和HNSW之外，其余方法在某些数据集上存在多个强连通分量。只有NSG和HNSW在不同数据集上保证了连通性(详见我们技术报告中的表格 [16])。

---

$\scriptsize\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|} 
\hline 
\text{数据} & \text{算法} & \text{内存(MB)} & \text{AOD} & \text{MOD} & \text{NN (\%)} & \text{数据} & \text{算法} & \text{内存(MB)} & \text{AOD} & \text{MOD} & \text{NN (\%)} \\ 
\hline 
{\text{SIFT1M}} & \text{NSG} & 153 & 25.9 & 50 & 99.3 & {\text{RAND4M}} & \text{NSG} & 2.7 \text{×} 10^3 & 174.0 & 220 & 96.4 \\ 
& \mathrm{HNSW}_0 & 451 & 32.1 & 50 & 66.3 & & \mathrm{HNSW}_0 & 6.7 \text{×} 10^3 & 161.0 & 220 & 76.5 \\ 
& \text{FANNG} & 374 & 30.2 & 98 & 60.4 & & \text{FANNG} & 5.0 \text{×} 10^3 & 181.2 & 327 & 66.7 \\ 
& \text{Efanna} & 1403 & 300 & 300 & 99.4 & & \text{Efanna} & 6.3 \text{×} 10^3 & 400 & 400 & 96.6 \\ 
& \text{KGraph} & 1144 & 300 & 300 & 99.4 & & \text{KGraph} & 6.1 \text{×} 10^3 & 400 & 400 & 96.6 \\ 
& \text{DPG} & 632 & 165.1 & 1260 & 99.4 & & \text{DPG} & 4.7 \text{×} 10^3 & 246.4 & 5309 & 96.6 \\ 
\hline 
{\text{GIST1M}} & \text{NSG} & 267 & 26.3 & 70 & 98.1 & {\text{GAUSS5M}} & \text{NSG} & 2.6 \text{×} 10^3 & 146.2 & 220 & 94.3 \\ 
& \mathrm{HNSW}_0 & 667 & 23.9 & 70 & 47.5 & & \mathrm{HNSW}_0 & 6.7 \text{×} 10^3 & 131.9 & 220 & 57.6 \\ 
& \text{FANNG} & 1526 & 29.2 & 400 & 39.9 & & \text{FANNG} & 5.2 \text{×} 10^3 & 152.2 & 433 & 53.4 \\ 
& \text{Efanna} & 2154 & 400 & 400 & 98.1 & & \text{Efanna} & 7.8 \text{×} 10^3 & 400 & 400 & 94.3 \\ 
& \text{KGraph} & 1526 & 400 & 400 & 98.1 & & \text{KGraph} & 7.6 \text{×} 10^3 & 400 & 400 & 94.3 \\ 
& \text{DPG} & 741 & 194.3 & 20899 & 98.1 & & \text{DPG} & 3.7 \text{×} 10^3 & 194.0 & 15504 & 94.3 \\ 
\hline 
\end{array}$    

**降低出度和缩短搜索路径。** 在表2中，我们可以看到NSG相比其他图基方法是一个非常稀疏的图。尽管HNSW的底层是稀疏的，但如果考虑其他层，它比NSG密集得多。由于查询点不在基数据中，因此不可能统计每种方法的搜索路径长度。考虑到所有图基方法使用相同的搜索算法，而大部分时间都花费在距离计算上，搜索性能可以粗略指示出$ol$的情况，其中$o$是平均出度，$l$是搜索路径长度。在图6中，NSG在四个数据集上的表现优于其他图基方法。NSG的$ol$值在经验上低于其他图基方法。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240924142033889.png" alt="image-20240924142033889" style="zoom:28%;" /> 

图6：在四个数据集上，基于图的算法在高精度区域的 ANNS 性能与其最优索引比较(右上角越好)。一些非图基方法的性能远不如基于图的方法。因此，我们对 SIFT1M 和 GIST1M 图的 y 轴进行了断裂，并对 RAND4M 图的 x 轴(RAND4M ${ }_1$ 和 RAND4M ${ }_2$)进行了断裂，以提供更好的曲线视图。由于结果是准确的，Serial-Scan 的 x 轴不适用。

---

**减少索引大小**。在表2中，NSG在四个数据集上具有最小的索引大小。特别是，NSG的索引大小约为HNSW的$1 / 2-1 / 3$，后者是之前表现最好的算法(https://github.com/erikbern/ann-benchmarks)。值得注意的是，NSG、HNSW、FANNG、Efanna的图和KGraph的内存占用均由最大出度决定。尽管不同节点的出度各不相同，但每个节点都根据图的最大出度分配相同的内存，以实现连续内存访问(以提高搜索性能)。DPG无法使用此技术，因为其最大出度过大。

$\scriptsize\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|} 
\hline 
\text{数据} & \text{算法} & \text{内存(MB)} & \text{AOD} & \text{MOD} & \text{NN (\%)} & \text{数据} & \text{算法} & \text{内存(MB)} & \text{AOD} & \text{MOD} & \text{NN (\%)} \\ 
\hline 
{\text{SIFT1M}} & \text{NSG} & 153 & 25.9 & 50 & 99.3 & {\text{RAND4M}} & \text{NSG} & 2.7 \text{×} 10^3 & 174.0 & 220 & 96.4 \\ 
& \mathrm{HNSW}_0 & 451 & 32.1 & 50 & 66.3 & & \mathrm{HNSW}_0 & 6.7 \text{×} 10^3 & 161.0 & 220 & 76.5 \\ 
& \text{FANNG} & 374 & 30.2 & 98 & 60.4 & & \text{FANNG} & 5.0 \text{×} 10^3 & 181.2 & 327 & 66.7 \\ 
& \text{Efanna} & 1403 & 300 & 300 & 99.4 & & \text{Efanna} & 6.3 \text{×} 10^3 & 400 & 400 & 96.6 \\ 
& \text{KGraph} & 1144 & 300 & 300 & 99.4 & & \text{KGraph} & 6.1 \text{×} 10^3 & 400 & 400 & 96.6 \\ 
& \text{DPG} & 632 & 165.1 & 1260 & 99.4 & & \text{DPG} & 4.7 \text{×} 10^3 & 246.4 & 5309 & 96.6 \\ 
\hline 
{\text{GIST1M}} & \text{NSG} & 267 & 26.3 & 70 & 98.1 & {\text{GAUSS5M}} & \text{NSG} & 2.6 \text{×} 10^3 & 146.2 & 220 & 94.3 \\ 
& \mathrm{HNSW}_0 & 667 & 23.9 & 70 & 47.5 & & \mathrm{HNSW}_0 & 6.7 \text{×} 10^3 & 131.9 & 220 & 57.6 \\ 
& \text{FANNG} & 1526 & 29.2 & 400 & 39.9 & & \text{FANNG} & 5.2 \text{×} 10^3 & 152.2 & 433 & 53.4 \\ 
& \text{Efanna} & 2154 & 400 & 400 & 98.1 & & \text{Efanna} & 7.8 \text{×} 10^3 & 400 & 400 & 94.3 \\ 
& \text{KGraph} & 1526 & 400 & 400 & 98.1 & & \text{KGraph} & 7.6 \text{×} 10^3 & 400 & 400 & 94.3 \\ 
& \text{DPG} & 741 & 194.3 & 20899 & 98.1 & & \text{DPG} & 3.7 \text{×} 10^3 & 194.0 & 15504 & 94.3 \\ 
\hline 
\end{array}$     

---

NSG小的索引归功于对我们MRNG的近似，并将最大出度限制为小值。MRNG为NSG提供了优越的搜索复杂度上界。我们尝试了不同的辅助结构来替代导航节点或使用随机起始节点，但性能没有改善，有时甚至变得更差。这表明NSG很好地近似了MRNG，并且不需要辅助结构以获得更高的性能。

#### 4.1.3.3. Some Interesting Points

1. 在具有较高局部内在维度的数据集上进行搜索通常更困难，这主要是由于“维度的诅咒”。在图6中，随着局部内在维度的增加，NSG与其他算法之间的性能差距在不断扩大。

---

2. 当所需的精度变得很高时，许多方法的性能甚至比串行扫描还要差。NSG在SIFT1M和GIST1M数据集上以99%的精度比串行扫描快十倍。在RAND4M和GAUSS5M上，所有算法对比串行扫描的加速效果都较低，但NSG在99%的精度下仍然比串行扫描更快。

---

3. NSG的索引速度在图基方法中几乎是最快的，但与非图基方法相比仍然慢得多。由于空间限制，我们仅在**表3**中列出了所有图基方法的预处理时间。

$\small\begin{array}{|c|c|c|c|c|}
\hline
\text{dataset}&\text{algorithms}&\text{time(s)}&\text{algorithm}&\text{time(s)}\\
\hline
\text{SIFT1M}&\text{NSG}&\textbf{140+134}&\text{HNSW}&376\\
\text{SIFT1M}&\text{FANNG}&1860&\text{DPG}&1120\\
\text{SIFT1M}&\text{KGraph}&824&\text{Efanna}&355\\
\hline
\text{GIST1M}&\text{NSG}&\textbf{1982+2078}&\text{HNSW}&\textbf{4010}\\
\text{GIST1M}&\text{FANNG}&34530&\text{DPG}&6700\\
\text{GIST1M}&\text{KGraph}&4300&\text{Efanna}&4335\\
\hline
\text{dataset}&\text{algorithm}&\text{time(h)}&\text{algorithm}&\text{time(h)}\\
\hline
\text{RAND4M}&\text{NSG}&\textbf{2.1+2.5}&\text{HNSW}&5.6\\
\text{RAND4M}&\text{FANNG}&38.3&\text{DPG}&6.0\\
\text{RAND4M}&\text{KGraph}&4.9&\text{Efanna}&5.1\\
\hline
\text{GAUSS5M}&\text{NSG}&\textbf{2.3+2.5}&\text{HNSW}&6.7\\
\text{GAUSS5M}&\text{FANNG}&46.1&\text{DPG}&6.4\\
\text{GAUSS5M}&\text{KGraph}&5.1&\text{Efanna}&5.3\\
\hline
\end{array}$  

---

4. 我们统计了在给定图中包含的最近邻图(NNG)的边数(NN百分比)，结果显示在**表2**中。可以看到，HNSW和FANNG都面临同样的问题：它们之间的最近邻边的缺失比例很大(见**表2**)。这是因为它们以随机边初始化图，然后迭代地优化图。理想情况下，当它们的索引算法收敛到最优时，应该能够连接所有最近邻，但没有任何收敛的保证，这会导致我们在第3.3节中讨论的绕路问题。这是FANNG的搜索性能远逊于NSG的原因之一。另一个原因是FANNG基于随机生成图(RNG)，而RNG并不是单调的。HNSW是第二好表现的算法，因为HNSW通过多层图实现了快速的捷径。然而，这导致了非常大的索引大小。

$\scriptsize\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|} 
\hline 
\text{数据} & \text{算法} & \text{内存(MB)} & \text{AOD} & \text{MOD} & \text{NN (\%)} & \text{数据} & \text{算法} & \text{内存(MB)} & \text{AOD} & \text{MOD} & \text{NN (\%)} \\ 
\hline 
{\text{SIFT1M}} & \text{NSG} & 153 & 25.9 & 50 & 99.3 & {\text{RAND4M}} & \text{NSG} & 2.7 \text{×} 10^3 & 174.0 & 220 & 96.4 \\ 
& \mathrm{HNSW}_0 & 451 & 32.1 & 50 & 66.3 & & \mathrm{HNSW}_0 & 6.7 \text{×} 10^3 & 161.0 & 220 & 76.5 \\ 
& \text{FANNG} & 374 & 30.2 & 98 & 60.4 & & \text{FANNG} & 5.0 \text{×} 10^3 & 181.2 & 327 & 66.7 \\ 
& \text{Efanna} & 1403 & 300 & 300 & 99.4 & & \text{Efanna} & 6.3 \text{×} 10^3 & 400 & 400 & 96.6 \\ 
& \text{KGraph} & 1144 & 300 & 300 & 99.4 & & \text{KGraph} & 6.1 \text{×} 10^3 & 400 & 400 & 96.6 \\ 
& \text{DPG} & 632 & 165.1 & 1260 & 99.4 & & \text{DPG} & 4.7 \text{×} 10^3 & 246.4 & 5309 & 96.6 \\ 
\hline 
{\text{GIST1M}} & \text{NSG} & 267 & 26.3 & 70 & 98.1 & {\text{GAUSS5M}} & \text{NSG} & 2.6 \text{×} 10^3 & 146.2 & 220 & 94.3 \\ 
& \mathrm{HNSW}_0 & 667 & 23.9 & 70 & 47.5 & & \mathrm{HNSW}_0 & 6.7 \text{×} 10^3 & 131.9 & 220 & 57.6 \\ 
& \text{FANNG} & 1526 & 29.2 & 400 & 39.9 & & \text{FANNG} & 5.2 \text{×} 10^3 & 152.2 & 433 & 53.4 \\ 
& \text{Efanna} & 2154 & 400 & 400 & 98.1 & & \text{Efanna} & 7.8 \text{×} 10^3 & 400 & 400 & 94.3 \\ 
& \text{KGraph} & 1526 & 400 & 400 & 98.1 & & \text{KGraph} & 7.6 \text{×} 10^3 & 400 & 400 & 94.3 \\ 
& \text{DPG} & 741 & 194.3 & 20899 & 98.1 & & \text{DPG} & 3.7 \text{×} 10^3 & 194.0 & 15504 & 94.3 \\ 
\hline 
\end{array}$    

---

5. NSG-Naive与NSG的区别在于，NSG-Naive不选择导航节点，也不确保图的连通性。此外，由于其剪枝候选只覆盖小范围的邻域，保留单调路径的概率较小。尽管NSG-Naive使用相同的边选择策略，但其近似程度低于NSG，从而导致性能较差。

---

6. 在KGraph和Efanna的最优索引中，出度远大于NSG。这是因为KGraph和Efanna中使用的$k$NN图是德劳内图的近似。如前所述，德劳内图是单调的，在高维数据集中几乎完全连通。当$k$NN图的$k$足够大时，单调性可能得到最佳近似。然而，高出度显著损害了KGraph和Efanna的性能。

### 4.1.4. Complexity And Parameters

在NSG索引算法中，有三个参数，$k$用于$k$NN图；$l$和$m$用于算法2。在我们的实验中，我们发现最优参数不会随着数据规模的变化而改变。因此，我们通过从基础数据中抽取一个小子集进行参数调优，并进行网格搜索以找到最优参数。

---

我们估计了NSG在SIFT1M和GIST1M上的搜索和索引复杂度。由于空间限制，请参见我们的技术报告[16]以获取图表和详细分析。搜索复杂度约为$O\left(n^{\frac{1}{d}} \log n^{\frac{1}{d}}\right)$。算法2的复杂度约为$O\left(n^{1+\frac{1}{d}} \log n^{\frac{1}{d}}\right)$，其中$d$大约等于内在维度。这与我们的理论分析一致。我们估计搜索复杂度如何随着所需邻居数量$K$的变化而扩展，约为$O\left(K^{0.46}\right)$或$O\left((\log K)^{2.7}\right)$。

## 4.2. Search On DEEP100M

DEEP1B是一个包含十亿个96维浮点向量的数据集，由Artem等人发布[5]。我们从中抽取了1亿个向量，并在一台配备i9-7980 CPU和96GB内存的机器上进行实验。该数据集占用37GB内存，这是NSG在此机器上可以处理的最大数据集。我们在四个1080Ti GPU上使用Faiss [28]构建$k$NN图。构建$k$NN图的时间为6.75小时，算法2的时间为9.7小时。NSG在索引阶段的峰值内存使用为92GB，搜索阶段为55GB。我们尝试运行HNSW，但无论如何设置参数，总是触发内存溢出错误。因此，我们仅将NSG与Faiss进行比较。结果见图7。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240924142805233.png" alt="image-20240924142805233" style="zoom:33%;" /> 

图7：NSG 和 Faiss 在 DEEP1B 的 100 M 子集上的 ANNS 性能比较。右上角越好。

---

NSG-1core表示我们在数据集上构建一个NSG，并使用一个CPU核心评估其性能。NSG-16core表示我们将数据集随机分成16个子集(每个子集625万个向量)，并分别在这些子集上构建16个NSG。通过同时在16个NSG上搜索并合并结果，我们可以实现NSG的内部查询并行搜索。我们为1亿个向量构建一个Faiss索引(IVFPQ)，并分别使用一个CPU核心(Faiss-1core)、16个CPU核心(Faiss-16core)和1080 Ti GPU(Faiss-GPU)评估其性能。Faiss支持内部查询并行搜索。Serial-16core表示我们在16个CPU核心上并行执行串行扫描。

---

在高精度区域，NSG明显优于Faiss。NSG-16core的性能超越Faiss-GPU，并在99%的精度下比Serial-16core快约430倍。同时，在625万个向量上构建NSG耗时794秒。顺序构建16个NSG的总时间仅为3.53小时，这比在整个DEEP100M上构建一个NSG快得多。原因可能如下：算法2的复杂度约为$O\left(n^{1+\frac{1}{d}} \log n^{\frac{1}{d}}\right)$。假设我们有一个包含$n$个点的数据集$D$，可以将$D$均匀地划分为$r$个子集。构建一个NSG在$D$上的时间为$t_1$，在一个子集上的时间为$t_2$。如果选择合适的$r$，很容易验证$t_1 > r t_2$。因此，在子集上进行顺序索引可以比在完整数据集上更快。

## 4.3. Search In E-commercial Scenario

我们与淘宝合作解决亿级高维近似最近邻搜索(ANNS)问题，特别是在电子商务场景下。亿级数据、每日更新以及响应时间限制是主要挑战。我们在不同规模的电子商务数据(用户和商品的128维向量)上评估了NSG，以寻找解决方案。

---

我们将NSG与基线方法(经过良好优化的IVFPQ实现[26])在电子商务数据库上进行比较。我们使用一个10M的数据集测试单线程性能，使用一个45M的数据集测试分布式搜索性能，模拟环境为基于MPI的在线场景压力测试系统。我们将数据集拆分，并将子集放置在不同的机器上。在搜索阶段，我们并行搜索每个子集并合并结果返回。在实验中，我们将数据集随机均匀划分为12个子集，并构建12个NSG。NSG在相同精度下比基线方法快$5-10$倍(详细信息见我们的技术报告[16])，并满足响应时间要求。

---

在完整数据集(约20亿个向量)上，我们发现一天内无法构建一个NSG。因此，我们采用32个分区的分布式搜索方案。平均响应时间约为5毫秒，精度为98%，每个分区的索引时间约为12小时。基线方法(IVFPQ)在完整数据集上无法达到响应时间要求(在98%精度下响应时间需在10毫秒以内)。



# 5. DISCUSSIONS

NSG在高精度下能够实现非常高的搜索性能，但相较于许多流行的基于量化和哈希的方法(例如IVFPQ和LSH)，它需要更多的内存空间和数据预处理时间。考虑到足够的内存，NSG非常适合于高精度和快速响应的场景。在频繁更新的场景中，索引时间也非常重要。在大型数据集上构建一个NSG是不可行的，因此像我们的实验那样的分布式搜索方案是一个不错的选择。

NSG也有可能支持增量索引，我们将把这留给未来的工作。



# 6. CONCLUSIONS

在本文中，我们提出了一种新的单调搜索网络MRNG，它确保了近似对数的搜索复杂度。我们从四个方面(确保连接性、降低平均出度、缩短搜索路径和减少索引大小)来设计更好的图结构以解决大规模问题。基于这四个方面，我们提出了NSG，它是MRNG的一个实用近似，同时考虑了这四个方面。大量实验表明，NSG在不同方面显著优于其他最先进的算法。此外，NSG的性能超过了淘宝(阿里巴巴集团)的基线方法，并已被集成到他们的亿级搜索引擎中。









