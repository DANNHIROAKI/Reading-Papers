# Navigating nets: Simple algorithms for proximity search  



## 0. Abstract

我们提出了一种简单的确定性数据结构，用于在一般度量空间中维护点集 $S$，同时支持邻近搜索(最近邻和范围查询)以及对 $S$ 的更新(插入和删除)。我们的数据结构由一系列逐步精细化的 $S$ 的 $\epsilon$-网组成，并且这些网之间通过指针相互连接，使我们能够轻松地从一个尺度导航到下一个尺度。

---

我们从“抽象维度”的角度分析了该数据结构的最坏情况下的复杂性。对于有界维度的度量空间，该数据结构非常高效，并且在某种距离计算模型中基本达到了最优。最后，作为一个特例，我们的方法优于最近由 Karger 和 Ruhl 提出的一个方法 [KR02]。



## 1. Introduction  

最近邻搜索(NNS)问题是对位于巨大的(可能是无限的)度量空间 $(M, d)$ 中的一组 $n$ 个点集 $S$ 进行预处理，以便在给定查询点 $q \in M$ 时，能够有效地找到 $S$ 中距离 $q$ 最近的点(假设距离函数 $d$ 是非负且对称的，并满足三角不等式)。高效地计算这样的最近邻是一个经典且基本的问题，具有众多实际应用。这些应用包括数据压缩、数据库查询、机器学习、计算生物学、数据挖掘、模式识别和自组织网络。许多这些例子中的一个共同特点是，比较两个元素的代价很高，因此应尽可能减少距离计算的次数。

---

一般的最近邻搜索(NNS)问题有多种变体。例如，所处的度量空间 $(M, d)$ 可能是特定应用相关的(例如，在拼写检查器或基因组数据中，$d$ 可能是加权编辑距离)，它可能是无限的(例如，欧几里得空间)，甚至可能结构非常松散，以至于几乎未知(如在对等网络中，表示节点间的延迟)。预处理阶段的空间和时间可能会受到限制(例如，限制为多项式或线性于 $n$)。当前的应用程序可能要求数据结构是动态的，即它应能有效支持从 $S$ 中插入和删除点。此外，可能希望数据结构在数据点之间分布，在这种情况下，诸如通信和负载等其他成本指标可能变得非常重要。最后，可能只需要近似解。设 $q$ 为查询点，$a \in S$ 为 $S$ 中离 $q$ 最近的点。那么 $(1+\epsilon)$-NNS 算法是指，给定 $q$，返回某个 $s \in S$ 使得 $d(q, s) \leq (1+\epsilon) d(q, a)$。

---

以往的研究大多集中在一个重要的特殊情况，即 $M=\mathbb{R}^d$，并且距离根据某种 $\ell_p$ 范数计算。虽然许多类型的数据可以自然地以这种形式表示，但对许多应用来说，这显然并不成立，因此在一般度量空间中处理最近邻搜索(NNS)是非常必要的。一方面，针对一般度量的数据结构可能会以 $\Omega(n)$ 的时间执行最近邻查询，而这种性能在实际应用中是不可接受的。即使仅需要近似解，这种依赖性也是固有的。一个众所周知的例子是，当 $S$ 形成均匀度量时，$S$ 中的点间距离都是相等的，基本上不提供任何有用的信息。(模型的具体说明和额外的下界请见第3节。)另一方面，实际应用中出现的度量往往具有更好的结构，可以被算法所利用。

---

鉴于这种情况，越来越多的近期研究关注如何通过度量的隐式结构来理解NNS的复杂性。在欧几里得空间中，度量复杂性的一个自然且常见的度量标准是欧几里得宿主空间的维度。事实上，大多数算法的效率都依赖于该维度。实际上，许多算法的运行时间随维度呈指数增长，这种现象被称为“维度灾难”(著名的例外包括 [IM98, KOR98])。因此，尝试为抽象度量空间定义一个类似的维度概念是很自然的想法。

---

我们的第一个贡献是概念性的——我们提出了一种抽象维度的自然概念(在最近邻搜索的背景下)，它基于Assouad [Ass83] 的工作(来自度量空间分析理论)。这一贡献得到了技术上的支持——我们为一般低维度度量空间中的最近邻搜索提供了一个高效且动态的数据结构。在一般度量空间的最近邻搜索上下文中，也有其他维度性概念被提出并研究过 [Cla99, KR02]，我们的方案与它们相比表现优越。此外，尽管我们的方法是通用的，特别是不依赖于欧几里得几何，但它与已知的最优低维欧几里得空间方案(具有线性预处理空间)[AMN+98] 相当！更详细的说明请参见第1.3节。

### 1.1. Abstract dimension

度量空间 $(X, d)$ 的倍增维度，在本文中记为 $\operatorname{dim}(X)$，是使得 $X$ 中的每个集合都可以被 $2^\rho$ 个直径为原集合一半的集合覆盖的最小值 $\rho$。(集合 $S \subseteq X$ 的直径为 $\sup \{d(x, y): x, y \in S\}$。)按照惯例，定义在 $S \subseteq X$ 中以 $x$ 为中心、半径为 $r$ 的(闭)球为 $B_S(x, r) = \{y \in S: d(x, y) \leq r\}$；当上下文清晰时，我们可以省略下标 $S$。不难看出，使得 $X$ 中的每个球可以被 $2^\rho$ 个半径为原球一半的球覆盖的最小值 $\rho$ 与上述定义相差不超过一个因子 2——这也是我们在整个讨论中使用的值。

---

如果度量空间的维度是 $O(1)$，则称该度量空间为倍增空间。例如，$k$ 个点上的均匀度量的维度是 $\log k$。一个近似的逆命题也成立(见下面的引理 1.2)，这表明小的倍增维度量化了缺乏大的近似均匀度量。换句话说，这种维度概念衡量的是 $X$ 的“体积增长”。我们注意到 Clarkson [Cla99] 在最近邻搜索的背景下使用了类似的概念(但称谓不同)，不过他的结果似乎没有充分发挥这个概念的全部潜力。(详见第 1.3 节。)

---

倍增维度具有几个自然且吸引人的特性。设 $(X, d)$ 为任意度量空间，以下是一些比较简单的性质(见例如 [Hei01])：

1. 对于带有任意范数的 $X = \mathbb{R}^k$，$\operatorname{dim}(X) = \Theta(k)$。
2. 如果 $X$ 是 $Y$ 的子空间，则 $\operatorname{dim}(X) \leq \operatorname{dim}(Y)$。
3. $\operatorname{dim}\left(X_1 \cup \cdots \cup X_m\right) \leq \max_i \operatorname{dim}\left(X_i\right) + \log m$。

---

在 [KR02] 中指出，倍增度量(那些具有统一有界维度的度量)在实际应用中自然出现，如对等网络。在特征识别问题中，数据集 $S$ 通常包含在一些非常高维空间 $\mathbb{R}^L$ 中的 $m$ 个低维流形的并集中，所使用的距离函数是 $\mathbb{R}^L$ 的某种范数(通常是 $\ell_1$ 或 $\ell_2$ 范数)。例如，一个流形可以表示对应于同一物体不同视角的特征向量，因此 $m$ 个物体会产生 $m$ 个流形的并集。在这种情况下，仅使用高维宿主空间的结构来执行最近邻搜索会非常昂贵。实际情况更加复杂，因为测量中产生的噪声和误差实际上导致的是 $m$ 个集合的并集，每个集合都仅接近一个流形。幸运的是，倍增维度对这种小的扰动相当不敏感。

### 1.2. Our results  

我们为一般度量空间提供了一个 $(1+\epsilon)$-NNS 的简单方案。该数据结构在第2节中进行了描述，它是确定性的，因此可以保证正确回答 $(1+\epsilon)$-NNS 查询。我们的数据结构是动态的，支持对 $S$ 进行插入和删除操作。它还支持范围查询：给定查询点 $q \in M$ 和范围 $t$，目标是返回所有满足 $d(q, s) \leq t$ 的点 $s \in S$。

---

这些操作的复杂性取决于 $S$ 的维度和 $(S, d)$ 的纵横比，记作 $\Delta=\Delta_S$，它定义为 $S$ 中最大和最小点间距离的比值。在大多数应用中，$\Delta=\operatorname{poly}(|S|)$，因此 $\log \Delta=O(\log n)$。

---

如果 $S$ 是倍增空间，则该数据结构使用 $O(n)$ 空间；它在 $O(\log \Delta) + (1 / \epsilon)^{O(1)}$ 的时间内回答 $(1+\epsilon)$-NNS 查询；并且在 $O(\log \Delta \log \log \Delta)$ 时间内实现插入和删除操作。运行时间指数依赖于 $\operatorname{dim}(S)$(见第2节)，但在距离预言机模型中，我们只能通过查询距离函数 $d$ 来访问 $M$，因此这种依赖是必要的，正如我们在第3节所展示的那样。

---

在 [KR02] 中定义了一个不同的维度概念(隐式地)，我们记作 $\operatorname{dim}_{\mathrm{KR}}(X)$。这是使得对每个点 $x \in X$ 和任意 $r > 0$，都有 $\left|B_X(x, 2r)\right| \leq 2^k\left|B_X(x, r)\right|$ 的最小值 $k$。在 [GKL03] 中证明了以下简单的引理；为了完整性，我们在附录中重复了该证明。

---

**引理 1.1** ([GKL03]) 每个有限度量空间 $(X, d)$ 都满足 $\operatorname{dim}(X) \leq 4 \operatorname{dim}_{\mathrm{KR}}(X)$。

---

反过来的方向(即 $\operatorname{dim}_{\mathrm{KR}}(X)$ 关于 $\operatorname{dim}(X)$ 的界限)并不成立。换句话说，倍增度量(具有有界倍增维度的度量)构成了一个严格更大的类，超出了那些具有有界 KR-维度的度量。接下来的部分对这两种概念进行了进一步比较。

---

当由数据集 $S$ 和查询点 $q$(即 $S \cup \{q\}$)形成的度量属于具有有界 KR 维度的度量类时，通过对我们的查询过程进行轻微修改(但不是数据结构)，我们能够返回精确的最近邻，并匹配或改善 [KR02] 的界限；详见第 1.3 节。

### 1.3. Related work  

许多工作(例如 [Bri95])表明，度量空间中点之间距离的直方图(集中度)可以(凭经验)指示其维度。Chávez 等人 [CNBYM01] 建议将度量的维度定义为 $\rho=\mu^2 / 2 \sigma^2$，其中 $\mu$ 和 $\sigma^2$ 分别是直方图的均值和方差，并且他们展示了在随机情况下，这个维度影响某些最近邻搜索算法的效率。Faloutsos 和 Kamel [FK97] 建议测量数据集的分形维度，但这个概念仅适用于欧几里得空间。

---

Clarkson [Cla99] 设计了两个用于满足某种球堆积性质(等价于具有 $O(1)$ 倍增维度)的度量空间的 NNS 数据结构。(见表 1。)然而，他的数据结构是随机的，非动态的，查询时间是超对数的，并且假设数据集 $S$ 和查询点 $q$ 是从相同的(未知的)概率分布中选取的。我们的算法在这些方面相对于 [Cla99] 有所改进，尽管它们只保证 $(1+\epsilon)$-NNS。

---

最近，Karger 和 Ruhl [KR02] 提出了上一节讨论的维度概念。该概念的一个理由是，$l_1$ 度量下的 $k$ 维网格的 KR 维度为 $\Theta(k)$。Karger 和 Ruhl [KR02] 展示了一个针对具有有界 KR 维度的度量空间的高效 NNS 数据结构。(见表 1)

---

引理 1.1 表明，每个度量空间的倍增维度(本质上)不大于其 KR 维度。因此，我们所有关于倍增度量的结果可以立即适用于具有有界 KR 维度的度量空间。此外，我们可以证明，当在这类度量空间上运行时，我们的算法可以(稍微)修改，从而以类似于 [KR02] 的运行时间找到精确的 NNS，但使用线性(而非近线性)空间。我们的结果在多个方面优于 [KR02]。除了扩展到更大的度量空间家族之外，我们的算法是确定性的且不需要任何参数，而 [KR02] 的算法是随机的(Las Vegas 算法)，其性能依赖于正确设置维度参数。

---

我们稍作讨论 KR 维度概念的脆弱性。特别是，正如 [KR02] 中指出的，具有有界 KR 维度的度量空间的子集并不总是具有有界维度。特别地，实线的某些子集具有无界的 KR 维度(尽管显然 $\mathbb{R}$ 的每个子集都具有有界的倍增维度)。其原因很简单：[KR02] 的算法使用随机采样来寻找最近邻，从而在度量空间中的点布局上强加了某种均匀性。因此，不清楚他们的方法是否可以扩展到更大的倍增度量类。要看这个不稳定性的一个简单例子，考虑离散圆环 $S=\{x \in \mathbb{Z}: 2r > |x| > r\}$。不难看出 $\operatorname{dim}_{\mathrm{KR}}(S)=O(1)$(对于任意 $r>0$，都是一致的)。然而，$\operatorname{dim}_{\mathrm{KR}}(S \cup \{0\})=\Omega(\log r)$，因此向 $S$ 中添加一个点就可以使 KR 维度任意增长。

---

或许最有趣的是，我们的结果与 $\left[\mathrm{AMN}^{+} 98\right]$ 对于有界维度欧几里得度量的结果相当(当然，它们也具有有界的倍增维度)；见表 1。令人惊讶的是，仅通过对点集的体积增长进行约束，而不依赖于欧几里得空间的几何结构，也能够实现类似的运行时间。

|          |                 Dimension                  |               Space               |          NNS or $(1+\epsilon)$-NNS          |           Insert/Delete           |
| :------: | :----------------------------------------: | :-------------------------------: | :-----------------------------------------: | :-------------------------------: |
| [Cla99]* |   $\operatorname{dim}(S)=O(1)^{\dagger}$   |        $O(n \log \Delta)$         | $O\left(\log ^4 n \cdot \log \Delta\right)$ |                 -                 |
| [Cla99]* |  $\operatorname{dim}(S)=O(1)^{\ddagger}$   | $n(\log n)^{O(\log \log \Delta)}$ |      $(\log n)^{O(\log \log \Delta)}$       |                 -                 |
|   本文   |        $\operatorname{dim}(S)=O(1)$        |              $O(n)$               |   $O(\log \Delta)+(1 / \epsilon)^{O(1)}$    | $O(\log \Delta \log \log \Delta)$ |
| [KR02]*  | $\operatorname{dim}_{\mathrm{KR}}(S)=O(1)$ |           $O(n \log n)$           |                 $O(\log n)$                 |      $O(\log n \log \log n)$      |
|   本文   | $\operatorname{dim}_{\mathrm{KR}}(S)=O(1)$ |              $O(n)$               |                 $O(\log n)$                 |      $O(\log n \log \log n)$      |
| [AMN98]  |              $O(1)$ Euclidean              |              $O(n)$               |       $(1 / \epsilon)^{O(1)} \log n$        |            $O(\log n)$            |

$\star$ 随机化(Las Vegas)数据结构。

$\dagger$ 假设有用于查询的训练集。

$\ddagger$ 假设查询来自与数据集相同的(未知)分布。

Table 1: 一般度量空间中 NNS 方案的比较。

### 1.4. Techniques  

设 $(X, d)$ 为一个度量空间。我们称子集 $Y \subseteq X$ 为 $X$ 的一个 $\epsilon$-网，如果它满足以下两个条件：(1) 对于每个 $x, y \in Y$，$d(x, y) \geq \epsilon$，(2) $X \subseteq \bigcup_{y \in Y} B(y, \epsilon)$。对于任意 $\epsilon > 0$，这样的网总是存在的。对于有限度量空间，它们可以通过贪心算法构造。对于任意度量空间，它们的存在性证明是佐恩引理的一个简单应用。这种经典的 $\epsilon$-网概念，是度量空间研究中的一个标准工具(见例如 [Hei01])，不应与计算几何中最近引入的(同名的)概念混淆。

---

以下引理是我们分析的关键。

**引理 1.2.** 设 $(S, d)$ 为一个度量空间，且 $Y \subseteq S$。如果 $Y$ 上诱导的度量的纵横比至多为 $\alpha$ 且 $\alpha \geq 2$，那么 $|Y| \leq \alpha^{O(\operatorname{dim}(S))}$。

证明。设 $d_{\min} = \inf \{d(x, y): x, y \in Y\}$ 和 $d_{\max} = \sup \{d(x, y): x, y \in Y\}$ 分别为 $Y$ 中的最小和最大点间距离，且假设 $\alpha = \cfrac{d_{\max}}{d_{\min}} < \infty$。注意，$Y$ 包含在 $S$ 中一个半径为 $2 d_{\max} \leq 2 \alpha d_{\min}$ 的球内(以 $Y$ 的任意一点为中心)。通过多次迭代应用倍增维度的定义，我们得到这个球，特别是 $Y$，可以被 $2^{\operatorname{dim}(S) \cdot O(\log \alpha)}$ 个半径为 $d_{\min} / 3$ 的球覆盖。根据 $d_{\min}$ 的定义，每个球至多覆盖 $Y$ 的一个点，因此 $|Y| \leq 2^{\operatorname{dim}(S) \cdot O(\log \alpha)} \leq \alpha^{O(\operatorname{dim}(S))}$

#### 1.4.1. A simplified 3-NNS algorithm  

简化的 $3$-NNS 算法。这里是我们数据结构的一个简化版本。设 $(S, d)$ 为待查询的度量空间，为了方便讨论，假设 $S$ 中的最小点间距离为 $\min \{d(x, y): x, y \in S\} = 1$。在这种情况下，纵横比 $\Delta$ 就是 $S$ 的直径。接下来，对于任意子集 $R \subseteq S$ 和点 $x \in M$，我们定义 $d(x, R) = \inf _{y \in R} d(x, y)$。

---

设 $k = \log \Delta$，对于 $i=0,1, \ldots, k$，令 $Y_i$ 为 $S$ 的一个 $2^i$-网。现在，对于每个点 $y \in Y_i$，假设我们有集合 $L_{y, i} = \left\{z \in Y_{i-1}: d(y, z) \leq \gamma 2^i\right\}$，其中 $\gamma$ 是待确定的常数。

---

注意到集合 $L_{y, i}$ 的最小距离为 $2^{i-1}$(因为这是 $Y_{i-1}$ 中的最小距离)，最大距离为 $\gamma 2^{i+1}$，因此它的纵横比是有界的。当 $S$ 是一个倍增度量空间时，引理 1.2 表明 $\left|L_{y, i}\right| = O(1)$，其中常数取决于 $\gamma$ 的选择。此外，$Y_k$ 只包含一个(任意的)点，记作 $y_{\text{top}}$。

---

现在，给定一个查询点 $q \in M$，我们首先设定 $y = y_{\text{top}}$，然后从 $i = k, k-1, \ldots$ 逐步迭代，找到 $L_{y, i} \subseteq Y_{i-1}$ 中距离 $q$ 最近的点，并将 $y$ 设置为该点(用于下一次迭代)。如果在某个阶段，我们达到了 $d(q, L_{y, i}) > 3 \cdot 2^{i-1}$，则停止并输出 $y$。否则，输出最后的值 $y \in Y_0$。

---

首先，注意到该算法的运行时间最多为 $O(\log \Delta)$，因为在列表 $L_{y, i}$ 中找到距离 $q$ 最近的点只需要常数时间(这些列表的大小是 $O(1)$)。因此，我们只需要证明输出点 $y$ 是最近邻 $a \in S$ 的 3-近似解，即 $d(q, y) \leq 3 d(q, a)$。

---

为此，设 $j$ 满足 $d(q, y) \leq 3 \cdot 2^j$，但 $d(q, L_{y, j}) > 3 \cdot 2^{j-1}$，即在该步中距离没有按2的因子减少。首先，我们要证明 $d(a, L_{y, j}) \leq 2^{j-1}$。换句话说，距离 $a$ 最近的 $2^{j-1}$ 网格点包含在 $L_{y, j}$ 中。设 $y^* \in Y_{j-1}$ 使得 $d(a, y^*) \leq 2^{j-1}$。我们需要证明 $d(y^*, y) \leq \gamma 2^j$；在这种情况下，$y^* \in L_{y, j}$。由于 $d(q, y) \leq 3 \cdot 2^j$，我们有

$d(y^*, y) \leq d(y^*, a) + d(a, y) \leq 2^{j-1} + d(a, q) + d(q, y) \leq 2^{j-1} + 2 \cdot d(q, y) \leq 7 \cdot 2^j$ 

因此，选择 $\gamma = 7$ 即可满足条件。

---

这表明下降过程“追踪”了距离 $a$ 最近的网格点。现在很容易看到 

$3 \cdot 2^{j-1} < d(q, L_{y, j}) \leq d(q, a) + d(a, L_{y, j}) \leq d(q, a) + 2^{j-1}$

因此 $d(q, a) > 2^j$。由于 $d(q, y) \leq 3 \cdot 2^j$，因此 $y$ 是 $q$ 的 3-近似最近邻。类似的论证表明，如果最终得到的是 $y \in Y_0$，那么 $y$ 实际上是距离 $q$ 最近的点。稍后我们会看到，在获得 $O(1)$ 近似最近邻之后，$(1+\epsilon)$-NNS 问题可以在时间 $O(1/\epsilon)^{O(1)}$ 内解决。

---

上述简单算法展示了使用不同尺度的网格来在度量空间中导航的强大功能，基本上在每一步都将 $q$ 与最近点之间的距离减半。我们数据结构中的所有操作都在这个简单框架内实现。

---

为了克服上述方案的一些技术限制，第 2 节中展示的实际数据结构更加复杂。首先，上述算法必须扩展为适用于任意 $\epsilon > 0$ 的 $(1+\epsilon)$-NNS。这可以通过大约 $\log(1/\epsilon)$ 次更多的迭代决策过程来实现，但在每次迭代中，我们现在必须处理多个点 $y$，而不仅仅是一个点。其次，在存在插入和删除的情况下，我们不能简化假设 $S$ 中的最小点间距离为 1，因此我们必须找到一种方法来跟踪“相关”的尺度。最后，由于技术原因，上述数据结构不支持高效的删除操作，且可能需要空间 $\Omega(n \log \Delta)$。解决这些问题的主要方法是选择 $Y_i$ 为 $Y_{i-1}$ 中的 $2^i$-网格，而不是 $S$ 中的。



## 2. Navigating nets  

在本节中，我们展示了一种数据结构，用于在度量空间中维护点集 $S$，以支持邻近查询和对 $S$ 的更新。该数据结构的性能保证(在时间和空间方面)取决于第 1 节中定义的数据集 $S$ 的维度。然而，这个数据结构是确定性的，并且保证对任何度量空间都是无条件正确的。不需要预先知道 $S$ 的维度或 $(1+\epsilon)$-NNS 中的 $\epsilon$。

---

我们首先在第 2.1 节描述数据结构，并在第 2.2 节分析其空间需求。接下来，在第 2.3 和 2.4 节中介绍计算邻近查询的过程，并在第 2.5 和 2.6 节中介绍更新 $S$ 的操作。最后，在第 2.7 节中我们给出了 KR 度量的改进界限。

---

我们将使用第 1 节中的符号，设 $(M, d)$ 为宿主度量空间，$S$ 为数据结构需要维护的点集，$n=|S|$。令 $d_{\text{max}} := \sup \{d(x, y): x, y \in S\}$ 和 $d_{\min} := \inf \{d(x, y): x, y \in S\}$ 分别表示 $S$ 中的最大和最小点间距离，因此 $\Delta := d_{\max} / d_{\min}$ 为 $S$ 的纵横比。

### 2.1. The data structure  

设 $\Gamma = \left\{2^i: i \in \mathbb{Z}\right\}$，我们将每个值 $r \in \Gamma$ 称为一个尺度。为了简化描述，我们考虑无穷多个尺度，但显然只有 $O(\log \Delta)$ 个是“相关”的，其余的尺度将是无关紧要的。

---

对于每个 $r \in \Gamma$，令 $Y_r$ 为 $Y_{r / 2}$ 的一个 $r$-网。作为基础情况，我们定义 $Y_r := S$ 对于所有 $r \leq d_{\min }$ 的尺度。对于每个 $r \in \Gamma$ 和每个 $y \in Y_r$，数据结构存储了一个 $r / 2$-网 $Y_{r / 2}$ 中靠近 $y$ 的点列表。尺度 $r$ 下 $y$ 的导航列表定义为：

$L_{y, r} := \left\{z \in Y_{r / 2}: d(z, y) \leq \gamma \cdot r\right\}$ 

其中 $\gamma > 0$ 是一个通用常数。我们稍后将看到，$\gamma \geq 4$ 即可满足所有操作的要求。虽然 $Y_r$ 不一定是 $S$ 的 $r$-网，但以下引理表明 $Y_r$ 是 $S$ 的一种放宽版的 $r$-网。

---

**引理 2.1** 对于每个尺度 $r$，我们有：

1. 覆盖性：对于每个 $z \in S$，$d(z, Y_r) < 2r$。
2. 打包性：对于每个 $x, y \in Y_r$，$d(x, y) \geq r$。

证明。覆盖性可以通过归纳法证明。基础情况是 $r < d_{\min}$，此时 $Y_r = S$，所需的性质显然成立。对于归纳步骤，假设性质对尺度 $r$ 成立，即存在 $y \in Y_r$ 使得 $d(z, y) < 2r$。由于 $Y_{2r}$ 是 $Y_r$ 的 $2r$-网，我们得到 $d(z, Y_{2r}) \leq d(z, y) + d(y, Y_{2r}) < 4r$。

---

打包性直接来源于 $Y_r$ 是 $Y_{r / 2}$ 的 $r$-网这一事实。

---

接下来的引理对任何导航列表的大小给出了上界。其证明基于观察到列表 $L_{y, r}$ 中的所有点都是 $Y_{r / 2} \subseteq S$ 的点，因此它们之间的距离至少为 $r / 2$，而它们都位于半径为 $\gamma r$ 的球内。应用引理 1.2 得出以下结果。

---

**引理 2.2** 每个导航列表的大小至多为 $2^{O(\operatorname{dim}(S))}$。

#### 2.1.1. Implementation  

我们现在讨论该数据结构的实现。首先，网格 $Y_r$ 并不是显式维护的，而是隐含地从列表 $L_{y, r}$ 推导出来，即 $Y_r$ 是所有存在 $L_{y, r}$ 的点 $y \in S$ 的集合。其次，如果 $S$ 非空，依据引理 2.1，对于每个尺度 $r > d_{\max}$，网格 $Y_r$ 由相同的单个点组成，我们记作 $y_{\text{top}}$。数据结构维护此点 $y_{\text{top}}$ 以及截断尺度 $r_{\max}:=\min \left\{r \in \Gamma: \forall r^{\prime} \geq r,\left|Y_{r^{\prime}}\right|=1\right\}$，以便简化大多数操作。第三，对于所有尺度 $r \leq d_{\min}$，网格 $Y_r$ 等于 $S$，因此对于尺度 $r \leq d_{\min}/2$，每个点 $x \in S$ 都有一个简单列表 $L_{x, r}=\{x\}$。这些简单列表可以简洁地表示，方法是为每个 $x \in S$ 存储一个尺度 $r_x \in \Gamma$，在此尺度以下，所有 $x$ 的列表都是简单的。为了分析，定义 $r_{\min}:=\min \left\{r_x: x \in S\right\}$。

---

现在我们可以对任意点 $x \in S$ 需要存储的导航列表数量进行上界估计。

**引理 2.3** $r_{\max}=\Theta(d_{\max})$ 且 $r_{\min}=\Theta(d_{\min})$，因此每个点有 $O(\log \Delta)$ 个非简单的导航列表。

**证明** 使用引理 2.1 很容易看出 $r_{\max}=\Theta(d_{\max})$。根据定义 (2.1) 可直接得出，对于每个 $x \in S$，$r_x \geq \Omega(d_{\min})$。此外，由引理 2.1 得出 $Y_{r_{\min}/2}=S$，因此 $d_{\min} \geq r_{\min}/2$。我们得出，对于尺度 $r$，列表只有在 $\Omega(d_{\min}) \leq r_x \leq r \leq r_{\max} \leq O(d_{\max})$ 时需要存储。引理得证。

---

结合引理 2.2 和 2.3，得出该数据结构所需总空间的上界为 $n \cdot 2^{O(\operatorname{dim}(X))} \log \Delta$。我们将在第 2.2 节中通过更仔细的分析消除 $\log \Delta$ 因子，从而改进这一结果。

---

在分析数据结构的性能时，我们假设导航列表是这样存储的：对于每个点 $x \in S$，$x$ 的非简单导航列表使用例如平衡搜索树存储，该树需要线性空间并在对查询、插入和删除操作中实现对数时间。根据引理 2.3，插入一个新导航列表的操作可以在 $O(\log \log \Delta)$ 时间内完成。

---

**注**：通过让每个导航列表 $L_{x, r}$ 不仅包含指向 $Y_{r / 2}$ 中点 $y$ 的指针，还包含指向它们相应的导航列表 $L_{y, r / 2}$ 的指针，可以加速 $(1+\epsilon)$-NNS 过程。由于后者的导航列表可能是简单的(且未显式存储)，我们实际存储的是指向导航列表 $L_{y, r^{\prime}}$ 的指针，其中 $r^{\prime}$ 是最大的满足 $r^{\prime} \leq r / 2$ 且 $L_{y, r^{\prime}}$ 非简单的尺度。为了在插入和删除操作中更新这些指针，它们必须实现为双向指针。

### 2.2. Space requirement  

我们现在证明，对于倍增度量空间 $S$，我们数据结构使用的总空间是 $O(n)$。

**定理 2.1** 该数据结构的大小是 $2^{O(\operatorname{dim}(S))} \cdot n$ 个字。

**证明** 回忆一下，简单的导航列表 $L_{x, r} = \{x\}$ 是隐式表示的。因此，我们数据结构实现所使用的总空间与表示所有非简单导航列表所需的空间成线性关系。根据引理 2.2，每个这样的列表的大小至多为 $2^{O(\operatorname{dim}(S))}$，因此只需证明非简单导航列表的数量为 $O(n)$。为了简化接下来的讨论，我们假设 $\gamma$ 是 2 的幂。

---

我们通过一个针对 $S$ 中点的费用分配策略来限制非简单导航列表的数量。每个这样的列表分配给 $S$ 中的一个点，具体分配方式如下：一个非简单列表 $L_{x, r}$ 必须包含至少一个与 $x$ 不同的点 $y$。根据定义，$x, y \in Y_{r / 2}$，因此 $d(x, y) \geq r / 2$。此外，$d(x, y) \leq \gamma r$，因此 $Y_{2 \gamma r}$ 不可能同时包含 $x$ 和 $y$。我们将列表 $L_{x, r}$ 分配给集合 $\{x, y\}$ 中不包含在 $Y_{2 \gamma r}$ 的点 $z$。

---

接下来，我们需要限制分配给任意单个点 $z \in S$ 的导航列表数量。考虑一个分配给 $z$ 的非简单尺度 $r$ 的列表。这个列表必须包含 $z$ 和另一个点 $z^{\prime}$，该列表要么是 $L_{z, r}$，要么是 $L_{z^{\prime}, r}$。设 $r^* = r^*(z)$ 为 $z \in Y_{r^*}$ 的最大尺度。分配方案规定 $r^* \in \{r / 2, r, \ldots, \gamma r\}$。因此，一旦 $z$ 被固定，$r$ 只能有 $O(1)$ 个不同的取值。此外，$z, z^{\prime}$ 属于同一个尺度 $r$ 列表，因此 $r / 2 \leq d(z, z^{\prime}) \leq \gamma r$。由此可见，一旦 $z$ 和 $r$ 被固定，根据引理 1.2，可能的 $z^{\prime}$ 只有 $2^{O(\operatorname{dim}(S))}$ 个。如前所述，一旦三元组 $z, z^{\prime}, r$ 被固定，只有两个导航列表 $\left(L_{z, r}\right.$ 和 $\left.L_{z^{\prime}, r}\right)$。我们得出，分配给任意单个点 $z$ 的列表数量为 $2^{O(\operatorname{dim}(S))}$，因此非简单列表的总数量确实是 $2^{O(\operatorname{dim}(S))} n$。

### 2.3. Approximate nearest neighbor query  

我们现在介绍一个程序，利用上述数据结构找到查询点的$(1+\epsilon)$近似最近邻。也就是说，给定一个点$q \in M$和一个值$\epsilon>0$，该程序找到一个点$p \in S$，使得$d(q, p)<(1+\epsilon) d(q, S)$。我们强调，$\epsilon$是任意的，这既意味着数据结构与$\epsilon$无关，也意味着$\epsilon$不必很小；例如，设置$\epsilon=1$时，我们可以非常快速地找到一个2-近似最近邻。选择更小的$\epsilon$在速度和准确性之间进行权衡；具体结果如下。

**定理 2.2**：可以使用该数据结构在时间$2^{O(\operatorname{dim}(S))} \log \Delta+(1 / \epsilon)^{O(\operatorname{dim}(S))}$内计算出$S$中的$(1+\epsilon)$近似最近邻。这尤其限制了距离计算的数量。

---

为证明该定理，我们接下来将介绍程序Approx-NNS。我们将在引理2.5中证明其正确性，并在引理2.6中分析其运行时间。我们将利用以下定义。

定义：$Z_r \subseteq Y_r$称为非正规(non-proper)，如果它只包含一个点$x$且$r_x>r$。否则，$Z_r$是正规(proper)。

#### 2.3.1. The algorithm  

我们现在概述程序ApproxNNS；完整描述见图2.3。该程序从最大的(非平凡)尺度$r=r_{\max}$开始(第1行)，并迭代地将$r$减小一半(第2-4行)。在每次迭代中，目标是构建一个子集$Z_{r / 2} \subseteq Y_{r / 2}$，该子集包含与$q$相近的$Y_{r / 2}$中的点，如引理2.4所形式化。通过利用上一次迭代的$Z_r$和对应点的导航列表，这一过程高效完成(第3行)。关键在于，$Z_r$中包含一个点，其距离$q$的距离至多为$d(q, S)+r$。迭代大致持续到$r \leq \epsilon \cdot d(q, S)$(第2行)，然后我们简单地报告$Z_r$中距离$q$最近的点(第5行)。

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20240922005112985.png" alt="image-20240922005112985" style="zoom:60%;" /> 

Figure 1: Approximate nearest neighbor procedure  

### 2.3.2. The analysis  

分析。通过归纳法，对于该程序计算的每个集合$Z_r$，都有$Z_r \subseteq Y_r$，因此第3行中的$L_{z, r}$是明确定义的。

**引理 2.4**：设$a$为$S$中与$q$距离最近的点。那么，程序ApproxNNS计算的每个集合$Z_r$中都包含一个点$z_r$，使得$d(a, z_r) \leq 2r$。

证明：对$r$进行归纳。基础情况是$r=r_{\max}$，第1行设置$Z_{r_{\max}}=\{y_{\mathrm{top}}\}=Y_{r_{\max}}$，因此对于$S$中与$q$距离最近的点$a$，根据引理2.1，我们确实有$d(a, Z_{r_{\max}}) \leq 2r_{\max}$。

---

对于归纳步骤，考虑在第3行构建$Z_{r / 2}$，假设$Z_r$满足归纳假设。由此可知，$Z_r$中包含一个点$z$，使得$d(z, a) \leq 2r$。根据引理2.1，$Y_{r / 2}$中包含一个点$y$，使得$d(a, y) \leq r$，因此有：

$d(z, y) \leq d(z, a) + d(a, y) \leq 2r + r = 3r$

因此$y \in L_{z, r}$($y$出现在$z$的导航列表中，因为$\gamma \geq 6$)。为了完成证明，只需证明$y$包含在$Z_{r / 2}$中。确实，当在第3行构建$Z_{r / 2}$时：

$d(q, y) \leq d(q, a) + d(a, y) \leq d(q, Z_r) + r$

所以$Z_{r / 2}$将包括$y$。

---

**引理 2.5**：程序Approx-NNS输出的点与$q$的距离至多为$(1+\epsilon) d(q, S)$。

证明：设$r^*$为程序结束时的$r$值。只需证明$d(q, Z_{r^*}) \leq (1+\epsilon) d(q, S)$，因为这样引理的证明就可以从第5行的选择中得出。第2行中有两个条件可能导致循环停止；首先考虑情况$2 r^*(1+1/\epsilon) \leq d(q, Z_{r^*})$。根据引理2.4，我们知道$d(q, Z_{r^*}) \leq d(q, S) + 2 r^*$。结合这两个不等式，我们得到$2 r^* / \epsilon \leq d(q, S)$。将这个结果代入第二个不等式，我们得到$d(q, Z_{r^*}) \leq d(q, S) + 2 r^* \leq (1+\epsilon) d(q, S)$，如所希望的。

---

接下来考虑 $Z_{r^*}$ 非规范的情况。我们可以假设 $d\left(q, Z_{r^*}\right) > 0$，否则我们已经完成证明。为了进行分析，假设我们继续迭代(即忽略第 2 行中对 $Z_r$ 为规范的要求)。注意，这个过程不可能无限进行，因为 $d\left(q, Z_r\right) \geq d(q, S) > 0$，而 $2 r(1+1/\epsilon) \rightarrow 0$。此外，经过修改的过程返回的点正是实际过程返回的点，因为进一步构造的集合 $Z_r$(即对于 $r < r^*$)都将包含与 $Z_{r^*}$ 相同的单个点。最后，很容易看出，我们之前对 $d\left(q, Z_{r^*}\right) \leq (1+\epsilon) d(q, S)$ 的分析(包括引理 2.4)同样适用于修改后的过程，因此我们可以得到在修改后的过程和实际过程中的 $d\left(q, Z_{r^*}\right) \leq (1+\epsilon) d(q, S)$，这正是我们所希望的。实际上，如果迭代以 $Z_r$ 非规范的方式停止，则该过程输出的是 $S$ 中离 $q$ 最近的最佳(唯一)点，因为我们可以用任意小的 $\epsilon > 0$ 应用相同的“修改后的过程”论证。

---

**引理 2.6**。程序 Approx-NNS 的运行时间为 $2^{O(\operatorname{dim}(S))} \log \Delta + (1 / \epsilon)^{O(\operatorname{dim}(S))}$。

证明。我们首先对程序 Approx-NNS 计算的集合 $Z_r$ 的大小进行界定。第 1 行的集合 $Z_r$ 是平凡的，因此考虑第 3 行构造的集合 $Z_{r / 2}$。对于所有 $y \in Z_{r / 2}$，第 2 行和第 3 行的条件意味着 $d(q, y) \leq d\left(q, Z_r\right) + r < 2 r(1 + 1 / \epsilon) + r = r(3 + 2 / \epsilon)$。由于 $Z_{r / 2} \subseteq Y_{r / 2}$ 中任意两个点之间的距离至少为 $r / 2$，根据引理 1.2，我们有 $\left|Z_{r / 2}\right| \leq (2 + 1 / \epsilon)^{O(\operatorname{dim}(S))}$。

---

一个更粗略的时间界限为 $(2 + 1 / \epsilon)^{O(\operatorname{dim}(S))} \log \Delta$，这是通过分别界定程序执行的迭代次数和单次迭代的运行时间得到的。对于第一个界限(迭代次数)，观察到一旦 $r$ 小于 $d_{\min } /(6 + 6 / \epsilon)$，$Z_{r / 2}$ 的直径(根据第 2 行和第 3 行)最多为 $2\left[d\left(q, Z_r\right) + r\right] < 2[2 r(1 + 1 / \epsilon) + r] < d_{\min }$，因此 $Z_r \subseteq S$ 至多只包含一个点。如果 $r < d_{\min } / \gamma$，则 $Z_r$ 必须是非规范的，且第 2 行中的第二个条件不成立。因此，迭代的尺度 $r$ 在 $r_{\max } = O\left(d_{\max }\right)$ 和 $\Omega\left(d_{\min } / [\gamma + 1 + 1 / \epsilon]\right)$ 之间，因此迭代次数最多为 $\log \Delta + \log(1 + 1 / \epsilon) + O(1)$。现在我们展示第二个界限(单次迭代的运行时间)。一次迭代扫描 $\left|Z_r\right|$ 个长度为 $\left|L_{z, r}\right| \leq 2^{O(\operatorname{dim}(S))}$ 的列表 $L_{z, r}$，并计算它们的并集(这需要去重，可以通过排序实现)。因此，单次迭代的运行时间最多为 $O\left(\left|Z_r\right| \cdot \left|L_{z, r}\right| \log \left(\left|Z_r\right| \cdot \left|L_{z, r}\right|\right)\right) \leq (2 + 1 / \epsilon)^{O(\operatorname{dim}(S))}$。第 5 行也执行类似的扫描，运行时间为 $O\left(\left|Z_r\right|\right)$。总的来说，该过程的总运行时间确实最多为 $(2 + 1 / \epsilon)^{O(\operatorname{dim}(S))} \log \Delta$。

---

通过更仔细的分析，我们将改进上界至引理中所述的。我们可以假设 $\epsilon < 1$，否则结果与之前的粗略界限相同。对于 $r \geq d(q, S)$ 的迭代，我们可以使用第 3 行和引理 2.4 估计 $Z_r$ 的直径为 $2\left[d\left(q, Z_{2 r}\right) + 2 r\right] \leq 2[d(q, S) + 4 r + 2 r] \leq 14 r$，根据引理 1.2，我们得到 $\left|Z_r\right| \leq 2^{O(\operatorname{dim}(S))}$，与 $\epsilon$ 无关。这类迭代的次数上界为 $\log \Delta + \log (1 / \epsilon) + O(1)$。对于 $r < d(q, S)$ 的迭代，我们使用上述较弱的界限 $\left|Z_r\right| \leq (2 + 1 / \epsilon)^{O(\operatorname{dim}(S))}$。然而，这类迭代的次数至多为 $\log (1 / \epsilon) + O(1)$，因为在每次迭代中，第 2 行的条件保证 $d\left(q, Z_r\right) < 4 r / \epsilon$，即 $r > \epsilon d\left(q, Z_r\right) / 4 \geq \epsilon d(q, S) / 4$。我们得出结论，总运行时间最多为 $2^{O(\operatorname{dim}(S))} \log \Delta + (1 / \epsilon)^{O(\operatorname{dim}(S))}$ 。

---

请注意，我们在上面仅分析了程序 Approx-NNS 明确执行的操作次数。一般来说，为给定点定位特定的导航列表需要时间 $O(\log \log \Delta)$。然而，正如第 2.1 节中的说明，我们可以降低这个成本。由于所有对导航列表的访问都是通过其他导航列表进行的，我们可以通过为每个尺度 $r$ 的导航列表维护指向尺度 $r / 2$ 列表的直接指针，以 $O(1)$ 的时间访问每个列表

### 2.4. Range queries and exact nearest neighbor  

我们可以利用该数据结构来实现范围查询操作：给定一个点 $q \in X$ 和一个距离 $t > 0$，该操作返回点集 $B(q, t)$。我们对该操作的运行时间的上界与 $|B(q, O(t))|$ 线性相关。在许多情况下，这个数量与输出长度 $|B(q, t)|$ 之间的差距不会太大，尽管在最坏情况下，两者之间的比例可以是任意大的，即使在倍增度度量中。然后，将范围查询操作与第 2.3 节的 2-NNS 算法结合，得到一个精确的最近邻搜索过程。

---

**定理 2.3**：围绕点 $q$ 在距离 $t$ 内的范围查询可以在时间 $2^{O(\operatorname{dim}(S))}(\log \Delta + |B(q, O(t))|)$ 内使用该数据结构计算。

证明(概述)：通过取整，我们可以假设 $t \in \Gamma$。范围查询过程对尺度 $r = r_{\max}, \ldots, r_{\min}$ 进行迭代，并为每个这样的 $r$ 值构造一个集合 $Z_r \subseteq Y_r$。第一阶段对应于尺度 $r = r_{\max}, \ldots, 2t$，与第 2.3 节相似，集合 $Z_r \subseteq Y_r$ 包含了接近 $q$ 的 $Y_r$ 中的点。在第二阶段，尺度为 $r = t, \ldots, r_{\min}$，集合 $Z_r$ 包含了通过扫描 $Z_{2r}$ 中所有点的尺度 $2r$ 导航列表可以找到的所有点。该过程报告在第二阶段找到的所有距离 $q$ 至多为 $t$ 的点。

---

让我们证明该过程确实报告了任何点 $s \in B(q, t)$。根据引理 2.1，存在一个点 $y \in Y_t$，使得 $d(s, y) \leq 2t$。为了进行分析，设 $a$ 为在 $S$ 中离 $q$ 最近的点。那么 $d(q, a) \leq d(q, s) \leq t$。根据引理 2.4，存在一个点 $z \in Z_{2t}$，使得 $d(a, z) \leq 4t$。综上所述，我们有 $d(z, y) \leq d(z, a) + d(a, q) + d(q, s) + d(s, y) \leq 4t + t + t + 2t \leq \gamma \cdot 2t$，因此 $y \in Y_t$ 必须出现在列表 $L_{z, 2t}$ 中。现在很容易看出，算法的第二阶段通过递归扫描所有从 $Z_{2t}$ 可达的导航列表，必定会沿途找到 $s$，这一点与引理 2.1 的论证相同。

---

为了分析运行时间，注意到由于构造及引理 2.4 的原因，$Z_{2t}$ 被包含在一个围绕 $q$ 的半径为 $d(q, Z_{4t}) + 2t = O(t)$ 的球内。因此，在第二阶段找到的任何点与 $Z_{2t}$ 的距离不超过 $\gamma(2t + t + t/2 + \cdots) \leq 4\gamma t$，因此与 $q$ 的距离为 $O(t)$。接着，我们可以利用定理 2.1 的改编来限制第二阶段的运行时间为 $2^{O(\operatorname{dim}(S))}|B(q, O(t))|$。为了限制第一阶段的运行时间，我们需要修改它，使得当 $d(q, Z_r) > 3r$ 时停止迭代并报告空集。这意味着 $Z_r$ 的直径至多为 $2d(q, Z_r) + r = O(r)$，因此 $\left|Z_r\right| \leq 2^{O(\operatorname{dim}(S))}$(根据引理 1.2)。因此，第一阶段的运行时间为 $2^{O(\operatorname{dim}(S))} \log \Delta$。注意，如果存在一个点 $s \in B(q, t)$，则第一阶段在达到尺度 $2t$ 之前不会停止，因为根据引理 2.4，对于每个 $r \geq 2t$，都有 $d(q, Z_r) \leq d(q, s) + 2r \leq 3r$。

---

**定理 2.4**：查询 $q$ 的精确最近邻搜索可以在时间 $2^{O(\operatorname{dim}(S))}(\log \Delta + |B(q, O(d(q, S)))|)$ 内使用该数据结构计算。

证明：给定查询 $q$，我们首先应用定理 2.2 计算 $q$ 的 2-NNS，即找到一个点 $s_q \in S$，使得 $d(q, s_q) \leq 2d(q, S)$。然后，我们应用定理 2.3 找到所有在距离 $t = d(q, s_q)$ 内的点，并报告其中与 $q$ 最近的一个或多个点。显然，该算法确实找到了所有与 $q$ 最近的点，并且运行时间的上界如所声明的那样。

---

### 2.5. Inserting a point  

我们现在展示一个过程，当一个新点 $q \in M$ 被插入到 $S$ 时，如何更新数据结构。

定理 2.5：可以在时间 $2^{O(\operatorname{dim}(S))} \log \Delta \log \log \Delta$ 内更新数据结构以插入一个点到 $S$。这包括 $2^{O(\operatorname{dim}(S))} \log \Delta$ 次距离计算。

证明(略述)：主要思路是，无论网络是如何构造的，都可以通过将 $q$ 添加到每个 $r$-网 $Y_r$ 或保持其不变来更新它。实际上，这可以通过对 $r$ 的归纳来证明。基础情况是对于足够小的尺度 $r$，根据定义，$Y_r$ 包含所有点，因此必须将 $q$ 添加到这个网中。对于归纳步骤，首先假设更新 $Y_{r/2}$ 后它保持不变；那么显然 $Y_r$ 不需要更改。接下来假设通过将 $q$ 添加到 $Y_{r/2}$ 来更新该网；那么我们仅当 $d(q, Y_r) \geq r$ 时才将 $q$ 添加到 $Y_r$。这确保了 $Y_r$ 仍然是 $Y_{r/2}$ 的一个 $r$-网(尽管可能还有其他更新 $Y_r$ 的方式)。决定 $d(q, Y_r) \geq r$ 将通过使用一个集合 $Z_r$ 来完成，该集合包含所有接近 $q$ 的 $Y_r$ 的点(类似于第 2.3 节)。请记住，$Y_r$ 仅通过其导航列表维护，因此将 $q$ 添加到 $Y_r$ 实际上需要为 $q$ 构建一个尺度为 $r$ 的导航列表，并更新附近点的尺度为 $2r$ 的列表。同样，这一任务是通过前述的集合 $Z_r$ 来完成的。

### 2.6. Deleting a point  

我们的数据结构还支持删除点。删除点的过程通常与插入点(第2.5节)相似。主要的技术差异发生在从网 $Y_r$ 中删除点 $q$ 时。在这种情况下，$Y_r \backslash \{q\}$ 不一定是 $Y_{r/2} \backslash \{q\}$ 的一个 $r$-网，因此需要将一个或多个 $Y_{r/2}$ 的点提升(即添加)到 $Y_r$ 中。然而，这个决定(以及相应的列表更新)可以通过包含所有接近 $q$ 的 $Y_r$ 的点的相对较小的集合 $Z_r$ 来完成。具体细节在本文版本中省略。

### 2.7. Improved bounds for KR metrics  

显然，根据引理 1.1，所有上述复杂度界限中的 $\operatorname{dim}(S)$ 可以被 $\operatorname{dim}_{\mathrm{KR}}(S)$ 替换。我们现在展示，使用基本相同的数据结构，在额外假设 $\operatorname{dim}_{\mathrm{KR}}(S \cup \{q\})$ 较小的情况下，我们可以实现更好的性能。首先，在各种操作的时间复杂度中，我们能够将 $\Delta$ 替换为 $n=|S|$。其次，我们证明可以找到精确的最近邻(而不仅仅是 $(1+\epsilon)$-近似值)。实际上，$k$ 个最近点可以在时间 $O(\log n + k)$ 内找到。这些界限与 Karger 和 Ruhl [KR02] 的保证相匹配，但我们的数据结构是确定性的，与维度无关，并且需要更小的空间。(当然，它还扩展到具有有界倍增维度的度量)

---

**定理 2.6**. 从定理 2.2 得到的 $(1+\epsilon)$-近似最近邻算法的运行时间为 $2^{O\left(\operatorname{dim}*{\mathrm{KR}}(S \cup{q})\right)} \log n+(1 / \epsilon)^{O\left(\operatorname{dim}*{\mathrm{KR}}(S \cup{q})\right)}$。类似地，定理 2.5 的插入和删除过程的运行时间为 $2^{O\left(\operatorname{dim}_{\mathrm{KR}}(S \cup{q})\right)} \log n \log \log n$。

定理 2.6 的证明源于下面的引理 2.8；详细内容在本版本中省略。

---

**引理 2.7.** 设 $x, y$ 是度量空间 $(X, d)$ 中的点，设 $r=\cfrac{1}{3} d(x, y)$；则 $|B(x, r)| \geq 2^{-2 \operatorname{dim}_{\mathrm{KR}}(X)}|B(y, r)|$。

证明. 观察到 $B(y, r) \subseteq B(x, 3 r + r)$，因此 $|B(y, r)| \leq |B(x, 4 r)| \leq 2^{2 \operatorname{dim}_{\mathrm{KR}}(X)}|B(x, r)|$。

---

**引理 2.8.** 每个点 $x \in S$ 至多有 $2^{O\left(\operatorname{dim}*{\mathrm{KR}}(S)\right)} \log n$ 个非平凡的导航列表 $L*{x, r}$。

证明. 假设 $x$ 对于尺度 $r_0<r_1<\ldots<r_t$ 具有非平凡的导航列表。根据定义，每个导航列表 $L_{x, r_i}$ 包含一个点 $y_i \in Y_{r_i / 2}$，且 $y_i$ 不同于 $x$，并且满足 $d\left(x, y_i\right) \leq \gamma r_i$。根据定义，$x \in Y_{r_i} \subseteq Y_{r_i / 2}$，因此 $d\left(x, y_i\right) \geq r_i / 2$。

---

我们现在声称，如果 $c=c(\gamma)$ 是一个合适的常数，则对于每个 $i \geq 0$，都有 $\left|B\left(x, r_{i+c}\right)\right| \geq\left(1+2^{-2 \operatorname{dim}*{K R}(X)}\right)\left|B\left(x, r_i\right)\right|$。基于这个声称，引理很容易得出；通过归纳法，我们得到 $\left|B\left(x, r*{4 i}\right)\right| \geq\left(1+2^{-2 \operatorname{dim}*{\mathrm{KR}}(X)}\right)^i$(基础情况 $i=0$ 是显然的)，并且由于 $|S|=n$，我们得到不同尺度 $r_i$ 的数量为 $t+1 \leq 2^{O\left(\operatorname{dim}*{\mathrm{KR}}(X)\right)} \log n$。

---

剩下的任务是证明上述声称。通过对点 $x$ 和 $y_{i+3}$ 应用引理 2.7，并取半径 $r=d\left(x, y_{i+3}\right) / 3$，我们得到 $\left|B\left(y_{i+3}, r\right)\right| \geq 2^{-2 \operatorname{dim}*{\mathrm{KR}}(X)}|B(x, r)|$。根据定义，这两个球是互不相交的，并且都包含在一个以 $x$ 为中心的半径 $4 r$ 的球内。因此，$|B(x, 4 r)| \geq \left|B\left(y*{i+3}, r\right)\right| + |B(x, r)| \geq \left(1+2^{-2 \operatorname{dim}*{\mathrm{KR}}(X)}\right)|B(x, r)|$。现在，通过代入上界 $4 r=\cfrac{4}{3} d\left(x, y*{i+3}\right) \leq \cfrac{4}{3} \gamma r_{i+3} \leq r_{i+c}$(假设 $2^{c-3} \geq \cfrac{4}{3} \gamma$)和下界 $r=d\left(x, y_{i+3}\right) / 3 \geq r_{i+3} / 6 > r_i$，就可以得到所声称的不等式。

---

**定理 2.7.** 查询点 $q$ 的 $k$ 个最近邻可以在时间 $2^{O\left(\operatorname{dim}*{\mathrm{Kr}}(S \cup{q})\right)}(k+\log n)$ 内使用数据结构计算。特别地，当 $\operatorname{dim}*{\mathrm{KR}}(S \cup {q})=O(1)$ 时，可以在时间 $O(\log n)$ 内计算精确的最近邻搜索。

该定理的证明基于应用定理 2.3，并利用类似于引理 2.8 的论证来界定运行时间。详细内容在本版本中省略。



## 3. Lower bounds  

本节通过下界所需的距离计算数量(从信息理论的角度)来证明我们的数据结构的复杂性几乎是最优的，以回答最近邻搜索(NNS)和 $(1+\epsilon)$ NNS 查询。以下提出的两个下界假设计算两点之间的距离是唯一的耗时操作，并且不能通过其他方式(例如哈希点的标识符)推断出距离信息(这在精神上类似于排序中比较次数的下界)。

---

正式地，假设 $S$ 中每两个点之间的距离是已知的，而查询点 $q$ 与 $S$ 中任意点之间的距离需要访问一个oracle。我们考虑一个对抗性的oracle，即我们检查回答 $(1+\epsilon)$-NNS 查询的最坏情况复杂性(针对所有可能的oracle)。为了简化，我们将下界表述为 $\epsilon \leq 2$，尽管证明可以立即扩展到更大的 $\epsilon$。这些结果同样适用于随机算法(包括 Las-Vegas 和 Monte-Carlo 方法)。

---

**引理 3.1.** 存在一个输入数据集 $S$，对于距离 oracle 模型，尽管 $S$ 是双倍的且 $\operatorname{dim}_{\mathrm{KR}}(S)=O(1)$，任何精确的最近邻搜索(NNS)算法在最坏查询情况下必须访问 oracle 至少 $\Omega(n)$ 次。

证明(概述)。设 $(S, d)$ 是实数线上整数点 $1, \ldots, n$ 的度量。显然，$\operatorname{dim}(S)=O(1)$。令查询点 $q$ 距离数据集 $S$ 中一个点 $i$ 的距离为 $n-1$，而与 $S$ 中其他所有点的距离为 $n$。显然，任何确定性的 NNS 算法必须报告点 $i$。因此，如果 $i$ 的值是对抗性选择的，则在最坏情况下，NNS 算法必须计算 $n$ 个距离以找到 $i$。随机算法的证明类似，使用 Yao 的极小极大原则。

---

**引理 3.2.** 存在一个输入数据集 $S$，使得任何 $(1+\epsilon)$-NNS 算法(对于固定的 $0 \leq \epsilon \leq 1$)在距离 oracle 模型下，必须至少访问 oracle $2^{\Omega(\operatorname{dim}(S))} \log n$ 次(对于最坏查询)。

我们省略此版本的证明。该证明基于一个数据集 $S$，该数据集是一个完整的 $\lambda$-叉树度量中叶子之间的最短路径度量，其中深度为 $i$ 的边长为 $1 / 2^i$。它表明，任何算法本质上都必须在根节点的子节点之间执行线性搜索，然后在深度为 1 的节点的子节点之间执行线性搜索，依此类推。



## References  

[AMN+98] S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu. An optimal algorithm for approximate nearest neighbor searching in fixed dimensions. J. ACM, 45(6):891–923, 1998  

[Ass83] P. Assouad. Plongements lipschitziens dans Rn. Bull. Soc. Math. France, 111(4):429–448, 1983.  

[Bri95] S. Brin. Near neighbor search in large metric spaces. In 21st International Conference on Very Large Data Bases, pages 574–584, 1995.  

[Cla99] K. L. Clarkson. Nearest neighbor queries in metric spaces. Discrete Comput. Geom., 22(1):63–93, 1999.  

[CNBYM01] E. Ch‘avez, G. Navarro, R. Baeza-Yates, and J. L. Marroqu´ın. Proximity searching in metric spaces. ACM Computing Surveys, 33(3):273–321, September 2001.  

[FK97] C. Faloutsos and I. Kamel. Relaxing the uniformity and independence assumptions using the concept of fractal dimension. J. Comput. System Sci., 55(2):229– 240, 1997.  

[GKL03] A. Gupta, R. Krauthgamer, and J. R. Lee. Bounded geometries, fractals, and low–distortion embeddings. Accepted to 43rd Symposium on Foundations of Computer Science, 2003.  

[Hei01] J. Heinonen. Lectures on analysis on metric spaces. Universitext. Springer-Verlag, New York, 2001.  

[IM98] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In 30th Annual ACM Symposium on Theory of Computing, pages 604–613, May 1998.  

[KOR98] E. Kushilevitz, R. Ostrovsky, and Y. Rabani. Efficient search for approximate nearest neighbor in highdimensional spaces. In 30th Annual ACM Symposium on Theory of Computing, pages 614–623. ACM, 1998.  

[KR02] D. Karger and M. Ruhl. Finding nearest neighbors in growth-restricted metrics. In 34th Annual ACM Symposium on the Theory of Computing, pages 63–66, 2002.  

## Appendix  A

证明。[引理 1.1] 设 $K$ 是 $X$ 的 KR-常数，并固定某个球 $B(x, 2r)$。我们将证明 $B(x, 2r)$ 可以被 $K^4$ 个半径为 $r$ 的球覆盖。这将表明 $\operatorname{dim}(X) \leq 4 \log_2 K = 4 \cdot \operatorname{dim}_{\mathrm{KR}}(X)$。

设 $Y$ 是 $B(x, 2r)$ 的一个 $r$-网，则$\displaystyle{}B(x, 2r) \subset \bigcup_{y \in Y} B(y, r) \subset B(x, 4r)$  

此外，对于每个 $y \in Y$，有 $|B(x, 4r)| \leq |B(y, 8r)| \leq K^4\left|B\left(y, \cfrac{r}{2}\right)\right|$。由于当 $y \neq y' \in Y$ 时，$B\left(y, \cfrac{r}{2}\right)$ 和 $B\left(y', \cfrac{r}{2}\right)$ 是不相交的，因此 $|Y| \leq K^4$。由此我们得出，$K^4$ 个球 $\{B(y, r): y \in Y\}$ 覆盖了 $B(x, 2r)$。