# 0. Abstract  

在本文中，我们提出了一种新的嵌入模型——M3-Embedding，该模型在多语言性（Multi-Linguality）、多功能性（Multi-Functionality）和多粒度性（Multi-Granularity）方面具有显著的优势。它为超过100种工作语言的语义检索提供了统一的支持。该模型能够同时实现三种常见的检索功能：==密集检索、 多向量检索和稀疏检索==。此外，M3-Embedding 还能够处理不同粒度的输入，从短句到最长可处理8,192个Token的长文档。M3-Embedding的有效训练带来了一系列技术创新，特别是我们提出了一种新的自知识蒸馏方法，其中来自不同检索功能的相关性得分可以作为教师信号进行集成，从而提升训练质量。我们还优化了批处理策略，使得能够使用较大的批次大小和高效的训练吞吐量，以提高嵌入的判别性。M3-Embedding在我们的实验中表现优异，在多语言、跨语言和长文档检索基准上取得了新的最先进结果。

# 1. Introduction  

嵌入模型是深度神经网络（DNN）在自然语言处理中的关键应用形式。它们==将文本数据编码到潜在空间中==，在该空间中，数据的==潜在语义可以通过输出嵌入来表达==（Reimers 和 Gurevych，2019；Ni 等，2022）。随着预训练语言模型的出现，文本嵌入的质量得到了显著提升，使其成为信息检索（IR）系统中不可或缺的组件。基于嵌入的IR应用中一种常见的形式是==密集检索==，在这种方式中，可以通过==嵌入相似度==来检索与查询相关的答案（Karpukhin 等，2020；Xiong 等，2020；Neelakantan 等，2022；Wang 等，2022；Xiao 等，2023）。此外，嵌入模型还可以应用于其他IR任务，例如多向量检索，其中基于多个嵌入的交互得分计算查询与文档之间的细粒度相关性（Khattab 和 Zaharia，2020），以及稀疏或词汇检索，其中通过输出嵌入来估算每个术语的重要性（Gao 等，2021a；Lin 和 Ma，2021；Dai 和 Callan，2020）。

------

尽管文本嵌入广泛应用，但现有方法在==通用性==方面仍然存在局限。首先，大多数嵌入模型==仅为英语==量身定制，其他语言的选择很少。其次，现有的嵌入模型通常只针对==单一检索功能==进行训练。然而，典型的IR系统需要多个检索方法的复合工作流程。第三，由于训练成本过于庞大，训练一个具有竞争力的长文档检索器是具有挑战性的，==大多数嵌入模型只能支持短输入==。

- 长文档检索器：从大量或长篇文档中检索出相关信息的系统
- 短输入：模型通常处理的较短文本（如查询、单个句子等）

------

为了解决上述挑战，我们提出了M3-Embedding，其突破性地解决了工作==语言==、==检索功能==和==输入粒度==的通用性问题。特别地，M3-Embedding在==多语言性方面表现出色===，能够支持超过100种世界语言。通过学习不同语言的共同语义空间，它不仅支持每种语言内部的多语言检索，还支持不同语言之间的==跨语言检索==。此外，M3-Embedding能够生成多功能嵌入，以==支持不同的检索功能==，不仅限于密集检索，还包括稀疏检索和多向量检索。最后，M3-Embedding能够处理不同粒度的输入，从短输入（如句子和段落）到最长可处理8,192个输入Token的长文档。

- 多语言：支持100多种语言，学习不同语言的共同语义空间→语言内部检索+跨语言检索
- 多功能：支持密集检索，稀疏检索，多向量检索
  - 密集检索：将查询/文档表示为单个嵌入向量，计算查询嵌入与文档嵌入之间的相似度，如BERT
  - 稀疏检索：基于词袋模型，将文档/查询表示为TF-IDF等稀疏向量，再计算稀疏向量的相似度
  - 多向量检索：将查询/文档表示为多个嵌入向量，让查询/文档的多个嵌入互相交互来计算相似度，如ColBERT
- 多颗粒度：处理不同粒度的输入，从短输入→最多8192个Token

---

M3-Embedding的==训练面临着重大挑战==。在我们的研究中，提出了以下技术贡献，以优化嵌入质量。首先，我们提出了一种新颖的==自我知识蒸馏框架==，其中可以共同学习和相互增强多种检索功能。在M3-Embedding中，[CLS]嵌入用于密集检索，而其他Token的嵌入用于稀疏检索和多向量检索。基于集成学习的原理（Bühlmann，2012），这些异构预测器可以结合为一个更强大的预测器。因此，我们==将来自不同检索功能的相关性分数组合为教师信号==，用于通过知识蒸馏增强学习过程。其次，我们==优化了批处理策略==，以实现大批量大小和高训练吞吐量，这在很大程度上提高了嵌入的区分性。最后但同样重要的是，我们进行了广泛且高质量的数据策划。我们的数据集包括三个来源：1）从大规模多语言语料库中提取的无监督数据；2）整合紧密相关的有监督数据；3）合成稀缺的训练数据。这三个数据来源互为补充，并应用于不同的训练阶段，为多功能文本嵌入奠定了坚实的基础。

- 解决M3-Embedding训练的问题
  - 自我知识蒸馏框架：共同学习+相互增强多种检索功能
  - 优化了批处理策略：实现大批量大小和高训练吞吐量
  - 广泛且高质量的数据策划：数据源于——语料库中提取的无监督数据，整合紧密相关的有监督数据，合成稀缺的训练数据

- 知识蒸馏：模型压缩和优化技术，
  - 将一个大型、复杂的“教师”模型的知识传递给一个较小、简单的“学生”模型
  - 学生模型能够借鉴教师模型学到的深层次特征，从而在计算资源有限的情况下提高性能
- 自我知识蒸馏框架
  - 核心思想：不同的检索功能，通过相互学习和增强来提升彼此的性能
  - 集成学习：将多个异构预测器（不同检索功能）结合起来，可以得到一个更强大的预测器

---

M3-Embedding在我们的实验中展示了==卓越的多功能性==。它在多种语言的检索质量上表现出色，在流行的多语言和跨语言基准测试中，如MIRACL（Zhang 等，2023c）和MKQA（Longpre 等，2021），取得了最新的性能。它有效地==学习了三种检索功能==，这些功能不仅可以单独工作，==还能结合在一起==提供更强的检索质量。它还在==不同输入粒度==（最多8192个Token）中保持了优越的能力，显著超越了现有方法。

------

我们的贡献总结如下：1）我们提出了M3-Embedding，实现了在多语言性、多功能性和多粒度上的前所未有的通用性。2）我们提出了一种==新颖的自我知识蒸馏训练框架==，并优化了批处理策略以实现高效训练。我们还通过全面的数据策划创建了高质量的训练资源。3）我们的模型、代码和数据是公开的，为文本嵌入的直接应用和未来开发提供了关键资源。

# 2. Related Work

相关工作从三个方面进行了回顾：==一般==文本嵌入、==神经检索==的嵌入模型、==多语言==嵌入。

------

在过去几年中，文本嵌入领域取得了显著进展。一个主要的推动力是预训练语言模型的普及，通过这些强大的文本编码器，==数据的潜在语义可以被有效地编码==（Reimers 和 Gurevych，2019；Karpukhin 等，2020；Ni 等，2022）。此外，==对比学习==的进展也是另一个关键因素，特别是==负采样==的改进（Xiong 等，2020；Qu 等，2021）和==知识蒸馏==的应用（Hofstatter 等，2021；Ren 等，2021；Zhang 等，2021a）。在这些成熟技术的基础上，学习多功能的嵌入模型变得越来越流行，这些模型能够统一支持多种应用场景。目前，已有许多有影响力的方法，如Contriever（Izacard 等，2022）、LLM-Embedder（Zhang 等，2023a）、E5（Wang 等，2022）、BGE（Xiao 等，2023）、SGPT（Muennighoff，2022）和Open Text Embedding（Neelakantan 等，2022），这些方法显著推动了文本嵌入在一般任务中的应用。

------

嵌入模型的一个主要应用是==神经检索==（Lin 等，2022）。通过测量与文本嵌入的语义关系，可以根据嵌入相似度检索与输入查询相关的答案。最常见的嵌入检索方法是==密集检索==（Karpukhin 等，2020），其中文本编码器的输出会被聚合（例如，通过[CLS]或均值池化）来计算嵌入相似度。另一种常见的替代方法是多==向量检索(BERT)==（Khattab 和 Zaharia，2020；Humeau 等，2020），它应用细粒度的交互来计算文本编码器输出的嵌入相似度。最后，文本嵌入也可以转化为==词项权重==，从而促进==稀疏或词汇检索==（Luan 等，2021；Dai 和 Callan，2020；Lin 和 Ma，2021）。通常，以上检索方法是通过不同的嵌入模型实现的。据我们所知，目前没有现有方法能够统一这些功能。

- 老三样，密集检索，向量检索，稀疏/词汇检索

------

尽管技术上取得了显著进展，但大多数现有的文本嵌入模型==仅针对英语开发==，其他语言仍处于滞后状态。为了解决这个问题，多个方向提出了持续的努力。一个方向是开发预训练的==多语言文本编码器==，如mBERT（Pires 等，2019）、mT5（Xue 等，2021）、XLM-R（Conneau 等，2020）。另一个方向是策划==多语言文本嵌入==的训练和评估数据，例如MIRACL（Zhang 等，2023c）、mMARCO（Bonifacio 等，2021）、Mr. TyDi（Zhang 等，2021b）、MKQA（Longpre 等，2021）。与此同时，多语言文本嵌入模型也在社区中不断发展，例如mDPR（Zhang 等，2023b）、mContriever（Izacard 等，2022）、mE5（Wang 等，2022）等。然而，当前的进展仍远远不够，考虑到与英语模型之间的显著差距以及不同语言之间的巨大不平衡，仍有很长的路要走。

- 多语言的嵌入
  - 开发预训练的==多语言文本编码器==
  - 策划==多语言文本嵌入==的训练和评估数据
- 困难：与英语模型之间的显著差距，不同语言之间的巨大不平衡

# 3. M3-Embedding  

M3-Embedding 实现了==三重多功能性==。它支持==多种语言==，并处理==不同粒度==的输入数据。此外，它统一了文本嵌入的常见检索功能。形式上，给定一个任意语言 $x$ 的查询 $q$，它能够从语料库 $D^y$ 中检索语言 $y$ 的文档 $d$：$d^y \leftarrow \mathrm{fn}^*\left(q^x, D^y\right)$。在这里，$\mathrm{fn}^*(\cdot)$ 可以是密集检索、词汇检索或多向量检索中的任何一种函数；$y$ 可以是与 $x$ 相同的语言，也可以是另一种语言。

- $d^y \leftarrow \mathrm{fn}^*\left(q^x, D^y\right)$
  - $q^x$：由语言$x$给出的查询$q$
  - $D^y$：由语言$y$组成的语料库$D$
  - $\text{fn}^*$：可以是密集检索、词汇检索，多向量检索中的任何一种函数
  - $d^y$：根据由语言$x$给出的查询$q$，在由语言$y$组成的语料库$D$中，采取$\text{fn}^*$方式检索，所得的结果

## 3.1. Data Curation(策划)

M3-Embedding需要一个大规模且多样化的多语言数据集。在本研究中，我们从三个来源进行了全面的数据收集：来自无标签语料库的无监督数据、来自有标签语料库的微调数据以及通过合成生成的微调数据（如表8所示）。这三种数据源相互补充，并在训练过程中的不同阶段应用。特别地，来自无监督数据的部分是通过从多种多语言语料库中提取丰富的语义结构来整理的，例如标题-正文、标题-摘要、指令-输出等，这些语料库包括维基百科、S2ORC（Lo et al., 2020）、xP3（Muennighoff et al., 2023）、mC4（Raffel et al., 2020）、CC-News（Hamborg et al., 2017）以及MTP（Xiao et al., 2023）中的优质数据。为了学习跨语言语义匹配的统一嵌入空间，我们引入了来自两个翻译数据集的平行句子，即NLLB（NLLB Team et al., 2022）和CCMatrix（Schwenk et al., 2021）。原始数据经过筛选，去除了潜在的低质量内容和低相关性样本。最终，这些数据带来了12亿对文本，涵盖194种语言以及2655个跨语言对应关系。

- 多样化/多语言数据集的来源
  - 无标签语料库→无监督数据
  - 有标签语料库→微调数据
  - 合成生成→微调数据
- 无标签语料库/有标签语料库
  - 无标签语料库：主要用于无监督学习/预训练模型，如维基百科/网络文章
  - 有标签语料库：用于有监督学习，如情感分析数据集(每段文本被标为正负面)/NER数据集(句中实体都被标出)
- 无监督数据/微调数据
  - 无监督数据：就是单纯的原始的文本，通常的结构为“标题-正文”/"标题-摘要"/"指令-输出"，一般需要清洗
  - 微调数据：也就是有监督数据，包含明确标签，一般用于适应特定任务时的微调训练(比如NER微调就用NER监督数据)
- 对跨语言的语义匹配：引入来自两个翻译数据集的平行句子，包含双语或多语言对照数据

------

此外，我们还从有标签的语料库中收集了==相对较小但多样且高质量的微调数据==。对于英语，我们整合了8个数据集，包括HotpotQA（Yang et al., 2018）、TriviaQA（Joshi et al., 2017）、NQ（Kwiatkowski et al., 2019）、MS MARCO（Nguyen et al., 2016）、COLIEE（Kim et al., 2023）、PubMedQA（Jin et al., 2019）、SQuAD（Rajpurkar et al., 2016）以及来自SimCSE（Gao et al., 2021b）的NLI数据。对于中文，我们整合了7个数据集，包括DuReader（He et al., 2018）、mMARCO-ZH（Bonifacio et al., 2021）、T2-Ranking（Xie et al., 2023）、LawGPT（Liu et al., 2023）、CMedQAv2（Zhang et al., 2018）、NLIzh和LeCaRDv2（Li et al., 2023）。对于其他语言，我们利用了来自Mr. TyDi（Zhang et al., 2021b）和MIRACL（Zhang et al., 2023c）的训练数据。

------

最后，我们生成合成数据来缓解长文档检索任务的不足，并引入额外的多语言微调数据（称为MultiLongDoc）。具体而言，我们从维基百科、Wudao（Yuan et al., 2021）和mC4数据集中抽取长篇文章，并随机选择其中的段落。然后，我们使用GPT-3.5根据这些段落生成问题。生成的问题与所选文章一起构成了一个新的文本对，加入到微调数据中。详细规格见附录A.2。

- 生成合成数据：抽取长篇文章→随机选择段落→用GPT3.5生成段落对应的问题→整合问题+段落构成文本对

## 3.2. Hybrid Retrieval  

M3-Embedding统一了嵌入模型的常见检索功能，即稠密检索、词汇（稀疏）检索和多向量检索。其公式如下所示：

------

__稠密检索。__ 输入查询$q$通过文本编码器转换为隐藏状态$\mathbf{H}_{\mathbf{q}}$。我们使用特殊Token"[CLS]"的归一化隐藏状态作为查询的表示：$e_q=\operatorname{norm}\left(\mathbf{H}_{\mathbf{q}}[0]\right)$。类似地，我们可以得到段落$p$的嵌入表示为$e_p=\operatorname{norm}\left(\mathbf{H}_{\mathbf{p}}[0]\right)$。因此，查询和段落之间的相关性得分通过两者嵌入$e_q$和$e_p$的内积来衡量：$s_{\text {dense }} \leftarrow\left\langle e_p, e_q\right\rangle$。

- $q$=`<cls> <Token-1> <Token-2> ....` 
- 嵌入：$q\xrightarrow{\text{Encoder}}\mathbf{H}_{\mathbf{q}}$，其中
  - `<cls>`$\xrightarrow{\text{Encoder}}\mathbf{H}_{\mathbf{q}}[0]$
  - `<Token1>`$\xrightarrow{\text{Encoder}}\mathbf{H}_{\mathbf{q}}[1]$ 
  - `<Token2>`$\xrightarrow{\text{Encoder}}\mathbf{H}_{\mathbf{q}}[2]$ 
- 归一化：选取`<cls>`$\xrightarrow{\text{Encoder}}\mathbf{H}_{\mathbf{q}}[0]$作为$q$的嵌入表示，对于文档/段落也同理，并且都归一化
  - $\operatorname{norm}\left(\mathbf{H}_q[0]\right)$表示查询$q$
  - $\operatorname{norm}\left(\mathbf{H}_p[0]\right)$表示段落$p$
  - 归一化的目的在于，归一化后内积就直接等于余弦相似度
- 相似性计算(得分)：$s_{\text{dense}} \leftarrow \langle e_p, e_q \rangle$

------

__词汇检索。__ 输出的嵌入也用于估算每个术语的重要性，从而促进词汇检索。对于查询中的每个术语$t$（==在我们的工作中，术语对应于一个Token==），术语权重通过以下公式计算：$\left.w_{q_t} \leftarrow \operatorname{Relu}\left(\mathbf{W}_{l e x}^T \mathbf{H}_{\mathbf{q}}[i]\right)\right)$，其中$\mathbf{W}_{\text {lex }} \in \mathcal{R}^{d \times 1}$是将隐藏状态映射到浮动数值的矩阵。如果某个术语$t$在查询中出现多次，我们仅保留其最大权重。我们使用相同的方法来计算段落中每个术语的权重。基于术语权重的估算，查询和段落之间的相关性得分通过查询和段落中共同存在的术语（记作$q \cap p$）的重要性联合计算：$s_{\text {lex }} \leftarrow \sum_{t \in q \cap p}\left(w_{q_t} * w_{p_t}\right)$。

- 对于查询$q$=`<Token1> <Token2> ....` 
- 为每个Token生成嵌入向量
  - `<Token-1>`$\xrightarrow{\text{Encoder}}\mathbf{H}_{\mathbf{q}}[1]$ 
  - `<Token-2>`$\xrightarrow{\text{Encoder}}\mathbf{H}_{\mathbf{q}}[2]$ 
  - ........
- 将每个Token的嵌入，映射到一个权值
  - `<Token-i>`$\xrightarrow{\text{Encoder}}\mathbf{H}_{\mathbf{q}}[i]\xrightarrow[\text{ReLU}激活函数]{\mathbf{W}_{\text{lex}}权重矩阵}w_{q_i}\text{=}\text{ReLU}\left(\mathbf{W}_{\mathrm{lex}}^T \mathbf{H}_q[i]\right)$
    - 其中$\mathbf{W}_{\mathrm{lex}}^T \mathbf{H}_q[i]$为标量
  - 当$\mathbf{H}_q[i]$和$\mathbf{H}_q[j]$对应的Token相同(皆为`<Token-t>`)时，认为二者权值的最大值才是`<Token-t>`权值
- 对于段落$p$，按照同样的方式处理，得到$w_{p_i}\text{=}\text{ReLU}\left(\mathbf{W}_{\mathrm{lex}}^T \mathbf{H}_p[i]\right)$
- 最终得分：计算查询和段落中，共同存在的Token的权重乘积，并求和
  - $s_{\text {lex }} \leftarrow \sum_{t \in q \cap p}\left(w_{q_t} * w_{p_t}\right)$ 

------

__多向量检索。__ 作为稠密检索的扩展，多向量方法利用整个输出嵌入来表示查询和段落：$E_q=\operatorname{norm}\left(\mathbf{W}_{m u l}^T \mathbf{H}_{\mathbf{q}}\right)$，$E_p=\operatorname{norm}\left(\mathbf{W}_{m u l}^T \mathbf{H}_{\mathbf{p}}\right)$，其中$\mathbf{W}_{m u l} \in \mathbb{R}^{d \times d}$是可学习的投影矩阵。继Col Bert（Khattab和Zaharia，2020）之后，我们使用后交互来计算细粒度的相关性得分：$s_{m u l} \leftarrow \frac{1}{N} \sum_{i=1}^N \max _{j=1}^M E_q[i] \cdot E_p^T[j]$；其中$N$和$M$分别是查询和段落的长度。

- 嵌入操作
  - 查询$q\xrightarrow{\text{Encoder}}\mathbf{H}_{\mathbf{q}}$ 
  - 段落$p\xrightarrow{\text{Encoder}}\mathbf{H}_{\mathbf{p}}$ 
- 嵌入后：投影+归一化
  - 查询$q\xrightarrow{\text{Encoder}}\mathbf{H}_{\mathbf{q}}\text{→}E_q\text{=}\operatorname{norm}\left(\mathbf{W}_{\mathrm{mul}}^T \mathbf{H}_q\right)$ 
  - 段落$p\xrightarrow{\text{Encoder}}\mathbf{H}_{\mathbf{p}}\text{→}E_p\text{=}\operatorname{norm}\left(\mathbf{W}_{\mathrm{mul}}^T \mathbf{H}_p\right)$ 
- 后期交互的相似度计算(BERT)
  - $\text{MaxSim}_i\text{=}\max _{j=1}^M E_q[i] \cdot E_p^T[j]$
  - 求平均$s_{\text{mul}}\text{=}\sum_{i=1}^{N}\text{MaxSim}_i$

------

得益于嵌入模型的多功能性，检索过程可以以混合方式进行。首先，可以通过每种方法单独检索候选结果（由于多向量方法的计算成本较高，这一步可以省略）。然后，基于综合的相关性得分对最终的检索结果进行重排序：

==集成方法==

$s_{\text {rank }} \leftarrow w_1 \cdot s_{\text {dense }}+w_2 \cdot s_{\text {lex }}+w_3 \cdot s_{\text {mul }}$其中$w_1, w_2$和$w_3$的值取决于下游的应用场景。

## 3.3. Self-Knowledge Distillation(蒸馏)

嵌入模型的训练旨在区分正样本和负样本。对于每种检索方法，期望为查询的正样本分配更高的得分，而非负样本。因此，训练过程旨在最小化InfoNCE损失，其一般形式由以下损失函数表示：

$\mathcal{L}_{s(\cdot)}=-\log \cfrac{\exp \left(s\left(q, p^*\right) / \tau\right)}{\sum_{p \in\left\{p^*, P^{\prime}\right\}} \exp (s(q, p) / \tau)}$

其中，$p^*$和$P^{\prime}$分别代表查询$q$的正样本和负样本；$s(\cdot)$是$\left\{s_{\text {dense }}(\cdot), s_{\text {lex }}(\cdot), s_{\text {mul }}(\cdot)\right\}$中的任何函数。

- 关于正负样本
  - **正样本**：对查询来说是相关的样本，例如查询与其匹配的段落、文档等
  - **负样本**：对查询来说是不相关的样本
- 训练的目标：将查询与正样本之间的相似度（得分）最大化，同时将查询与负样本之间的相似度最小化
- InfoNCE损失
  - **$q$**：查询的表示（如通过编码器得到的查询嵌入）。
  - **$p^*$**：查询 $q$ 的正样本，即与查询最相关的段落或文档。
  - **$P'$**：查询 $q$ 的负样本集，包含与查询不相关的段落或文档。
  - **$s(q, p)$**：查询 $q$ 和段落或文档 $p$ 之间的相似度得分，可以是稠密检索、词汇检索或多向量检索的得分（即 $s_{\text{dense}}$, $s_{\text{lex}}$, $s_{\text{mul}}$ 中的任何一个函数）
  - **$\tau$**：温度参数，控制得分的平滑度。温度值越小，得分差异越大，模型会更偏向于区分正负样本

------

不同检索方法的训练目标可能相互冲突。因此，本地的多目标训练可能不利于嵌入质量。为了促进多个检索函数的优化，我们提出在自知识蒸馏的基础上统一训练过程。特别地，基于集成学习的原理（Buhlmann，2012），不同检索方法的预测可以作为更精确的相关性得分进行整合，鉴于它们具有异质性。在最简单的形式下，整合可以是不同预测得分的加权和：

$s_{\text {inter }} \leftarrow w_1 \cdot s_{\text {dense }}+w_2 \cdot s_{\text {lex }}+w_3 \cdot s_{\text {mul }}$

然后，我们计算$\mathcal{L}_{\text {dense }}$、$\mathcal{L}_{\text {lex }}$、$\mathcal{L}*{\text {mul }}$和$\mathcal{L}_{\text {inter }}$的加权和，作为==没有自知识蒸馏的损失==：

$\mathcal{L} \leftarrow\left(\lambda_1 \cdot \mathcal{L}_{\text {dense }}+\lambda_2 \cdot \mathcal{L}_{\text {lex }}+\lambda_3 \cdot \mathcal{L}_{\text {mul }}+\mathcal{L}_{\text {inter }}\right) / 4$ 

- 动机：不同的检索方法(稠密/词汇/多向量)的目标和优化方向不一样(甚至冲突)，对每个目标进行单独优化然后集成是不行的

- 自知识蒸馏：

  - 知识蒸馏：是让一个复杂的教师模型，将其知识(输出)传递给一个较小的学生模型
  - 自知识蒸馏：学生和老师都是一个模型，让模型学习自己的输出，自己教(训练)自己以优化参数提升性能

- 集成学习：将不同模型的预测结果进行加权合并，如此处最简单的线性加权

  - 得分的线性加权：$s_{\text {inter }} \leftarrow w_1 \cdot s_{\text {dense }}+w_2 \cdot s_{\text {lex }}+w_3 \cdot s_{\text {mul }}$ 
  - 损失的线性加权：$\mathcal{L} \leftarrow\left(\lambda_1 \cdot \mathcal{L}_{\text {dense }}+\lambda_2 \cdot \mathcal{L}_{\text {lex }}+\lambda_3 \cdot \mathcal{L}_{\text {mul }}+\mathcal{L}_{\text {inter }}\right) / 4$ 

  注意没有自知识蒸馏的损失，==就是指的InfoNCE损失==

------

在以往的研究中，嵌入模型的训练质量可以通过知识蒸馏获益，知识蒸馏利用来自另一个排序模型的细粒度软标签（Hofstatter et al.，2021）。在这里，我们简单地将整合得分$s_{\text{inter}}$作为教师模型，其中每个检索方法的损失函数修改为：

$\mathcal{L}_*^{\prime} \leftarrow-p\left(s_{\text {inter }}\right) * \log p\left(s_*\right)$ 

这里，$p(\cdot)$是softmax激活函数；$s_*$是${s_{\text {dense }}, s_{\text {lex }}, s_{\text {mul }}}$中的任何一个成员。我们进一步整合并归一化修改后的损失函数：

$\mathcal{L}^{\prime} \leftarrow\left(\lambda_1 \cdot \mathcal{L}_{\text {dense }}^{\prime}+\lambda_2 \cdot \mathcal{L}_{\text {lex }}^{\prime}+\lambda_3 \cdot \mathcal{L}_{\text {mul }}^{\prime}\right) / 3$

最后，我们得出自知识蒸馏的最终损失函数，通过将$\mathcal{L}$和$\mathcal{L}^{\prime}$进行线性组合得到：$\mathcal{L}_{\text {final }} \leftarrow\left(\mathcal{L}+\mathcal{L}^{\prime}\right) / 2$。

- 教师-学生模型
  - 整合得分$s_{\text{inter}}$作为教师模型，指导学生模型的学习
  - 不同检索方法中的每个$s_*$作为学生模型
- ==知识蒸馏损失函数==：
  - 每个模型的损失函数：$\mathcal{L}_*^{\prime} \leftarrow-p\left(s_{\text {inter }}\right) * \log p\left(s_*\right)$ ，其中$p$是Softmax
  - 归一化：$\mathcal{L}^{\prime} \leftarrow\left(\lambda_1 \cdot \mathcal{L}_{\text {dense }}^{\prime}+\lambda_2 \cdot \mathcal{L}_{\text {lex }}^{\prime}+\lambda_3 \cdot \mathcal{L}_{\text {mul }}^{\prime}\right) / 3$ 
- 最终损失函数：$\mathcal{L}_{\text {final }} \leftarrow\left(\mathcal{L}+\mathcal{L}^{\prime}\right) / 2$。

------

![image-20241201223114266](https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241201223114266.png)

训练过程构成了一个多阶段的工作流程（图2）。首先，文本编码器（一个通过RetroMAE（Xiao et al.，2022）方法调整的XLM-RoBERTa（Conneau et al.，2020）模型）通过大规模的无监督数据进行预训练，其中只有稠密检索在对比学习的基本形式下进行训练。在第二阶段应用自知识蒸馏，在这一阶段，嵌入模型被微调以建立三种检索功能。由于$\mathbf{W}_{l e x}$的随机初始化，训练开始时$s_{\text{lex}}$的准确性较差，且$\mathcal{L}_{\text{lex}}$较高。为了减少这一影响，我们在训练过程中设置$w_1=1, w_2=0.3, w_3=1, \lambda_1=1, \lambda_2=0.1$和$\lambda_3=1$。在这一阶段，使用标注数据和合成数据，其中引入了硬负样本，以每个查询为基础，按照ANCE方法（Xiong et al.，2020）进行处理。（更多细节见附录B.1。）

- 第一阶段：无监督预训练
  - 预训练模型：使用RetroMAE调整过的XLM-RoBERTa
  - 预训练策略：通过**对比学习**来实现
    - 对比学习策略：
      - 为每个查询都会找到一个正样本对+负样本对
      - 让**正样本对**的嵌入相似度更高，**负样本对**的嵌入相似度更低
  - 预训练任务：**稠密检索任务**
  - 预训练流程：
    - 让模型反复执行**稠密检索任务**，
    - 检测稠密检索得到的正/负样本对的相似度，使用对比学习的策略调整参数
    - 重复以上过程，最终让模型学会如何将文本转化为嵌入表示
- 第二阶段：使用自知识蒸馏进行微调
  - 自知识蒸馏：模型将自己作为“教师”来指导自己的改进
  - 权重超参数：$w_1=1, w_2=0.3, w_3=1, \lambda_1=1, \lambda_2=0.1, \lambda_3=1$ 
    - 优于$\mathbf{W}_{\text{lex}}$是随机初始化的，所以一开始$s_{\text{lex}}$准确率很低，故有意降低其权重
  - 训练数据：
    - 标注数据：为使微调适应特定任务
    - 合成数据：通过数据增强或合成生成的方法得到，是为了数据更多样化，提高模型泛化能力
  - 硬负样本的引入：
    - 硬负样本：对模型来说难以区分的负样本
    - 目的：迫使模型在难以区分的情形下学习更加精准的判别边界
  - 采用ANCE方法进行处理：
    - 对比学习：先要找到正负样本，训练过程中，为每个查询都会找到一个正样本对+负样本对
    - ANCE的优化：通过ANN加速找到正/负样本对

## 3.4. Efficient Batching  

嵌入模型需要从多样化的大规模多语言数据中学习，以充分捕捉不同语言的通用语义。同时，它还需要尽可能保持==较大的批量大小==（引入大量批内负样本），以确保文本嵌入的可区分性。由于GPU的内存和计算能力有限，通常采用==将输入数据截断为较短的序列==的方式，以提高训练的吞吐量和批量大小。然而，这种常见做法对于M3-Embedding来说并不可行，因为它需要从短序列和长序列数据中同时学习，以有效处理不同粒度的输入。在我们的工作中，我们通过优化批处理策略来提高训练效率，从而实现高训练吞吐量和大批量大小。

- 训练的批大小通常更大，利于引入耕读偶批内负样本，提升模型的区分学习能力
- 批过大面临着GPU内存限制问题
  - Old-Fashion的方法：直接截断
    - 显然适合M3嵌入，因为M3需要处理不同颗粒度(输入长度)，阶段后丧失了对于长序列的学习能力
  - 结局方案：优化批处理策略:point_down:

------

<img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241202001333874.png" alt="image-2024874" style="zoom:50%;" /> 

具体来说，训练数据通过按序列长度分组进行预处理。在生成小批量时，训练实例会从相同的组中进行抽样。由于序列长度相似，这显著减少了序列填充（图3中标红部分），并有助于更有效地利用GPU资源。此外，在为不同的GPU采样训练数据时，==随机种子始终保持固定==，这==确保了负载平衡==并最小化每个训练步骤中的等待时间。另外，在处理长序列训练数据时，小批量进一步被划分为子批量，从而减少内存占用。我们通过梯度检查点技术（Chen et al., 2016）迭代地对每个子批量进行编码，并收集所有生成的嵌入。这种方法可以显著增加批量大小。例如，在处理长度为8192的文本时，批量大小可以增加超过20倍。（详细信息见附录B.3。）最后，来自不同GPU的嵌入会被广播，使得每个设备都能在分布式环境中获得所有的嵌入，这显著扩展了批内负样本的规模。

==讲了些啥，咋看不懂==

- 分组预处理：将长度相似的序列分到相同小组中

- 小批次的采样生成：

  - 从相同的组抽取一定数量的样本，用`<PAD>`填充每个文档与当前组中最长文档对齐

  - 将抽取+对齐后的样本，作为一个小批送进GPU训练

- 随机种子与负载平衡

  - 操作：每个GPU在每次训练时都能获得一致的训练数据
  - 目的：确保每个GPU处理的数据是**平衡的**，从而负载都差不多

- 长序列的划分

  - 将长序列进一步划分为**子批量**，并且通过**梯度检查点技术**迭代地对每个子批量进行编码
    - **梯度检查点技术**：将一些中间计算结果存储在硬盘(而非内存)上，从而在较低内存的设备上处理更大的批量

- 广播嵌入到不同GPU

  - 目的：在多GPU环境中，每个GPU的**嵌入**(模型的输出)会被**广播**，确保每个GPU都能获得来自**所有GPU**的嵌入
  - 好处：每个GPU不仅处理自己的数据，还能接收到其他GPU的嵌入
    - 极大地**扩展批内负样本的规模**
    - 增强模型的学习能力

------

对于计算或数据资源极其有限的用户，我们提出了一种更简单的方法，称为MCLS（Multi-CLS），即在推理过程中将多个CLSToken插入到长文档中，并取所有CLS嵌入的平均值作为文档的最终嵌入。尽管方法简单，但在实践中效果出乎意料地好。（更多细节见附录B.2。）

# 4. Experiment  

在本节中，我们研究了M3-Embedding在多语言检索、跨语言检索和长文档检索方面的表现，并探讨了其技术因素的影响。

## 4.1. Multi-Lingual Retrieval  

我们使用MIRACL（Zhang et al., 2023c）评估多语言检索性能，MIRACL包含18种语言的临时检索任务。每个任务由查询和相应语言的段落组成。根据官方基准，我们使用Pyserini（Lin et al., 2021）进行评估，并以==nDCG@10==作为主要评估指标（Recall@100也被测量，并在附录C.1中报告）。具体来说，==对于密集检索方法==（Token为Dense），我们首先使用该方法生成语料库的嵌入表示，然后使用==Faiss==建立密集索引，检索前1000个候选项。==对于稀疏检索方法==（Token为Sparse），我们首先使用该方法生成语料库的权重，然后使用==Lucene==建立稀疏索引，检索前1000个候选项。对于==多向量方法==（Token为Multi-vec），考虑到其高计算成本，我们将其用作重排序器，对密集方法检索出的前200个候选项进行重排序。对于密集方法和稀疏方法的混合检索（Token为Dense + Sparse），我们在公式(1)中设置$w_1=1, w_2=0.3$，$w_3=0$，对来自Dense方法的前1000个候选项和来自Sparse方法的前1000个候选项的并集进行重排序。对于三种方法的混合检索（Token为$\underline{All}$），我们在公式(1)中设置$w_1=1$，$w_2=0.3$，$w_3=1$，对来自Dense方法的前200个候选项进行重排序。

------

在实验中，我们纳入了以下基准方法：词汇检索方法：BM25（Robertson和Zaragoza, 2009）；密集检索方法：$\mathrm{mDPR}^3$（Zhang et al., 2023b）、mContriever ${ }^4$（Izacard et al., 2022）、$\mathrm{mE5}*{\text{large}}$（Wang et al., 2022）和$\mathrm{E} 5*{\text{mistral-7b}}$（Wang et al., 2023）。为了使BM25与M3更具可比性，在实验中我们使用与M3相同的分词器（即XLM-RoBERTa的分词器）来处理BM25。使用与XLM-RoBERTa相同的词汇表也确保了两种方法具有相同的检索延迟。不同分词器下的BM25结果请参见附录C.2。我们还与OpenAI最近发布的Text-Embedding-3-Large（简称OpenAI3）进行了对比。

------

根据表1中的实验结果，我们可以得出以下观察结论。首先，M3-Embedding仅凭其密集检索功能（Dense）就已实现了优越的检索性能。它不仅在平均性能上超越了其他基准方法，而且在大多数单独语言的表现上也保持了一致的优势。即便与使用更大Mistral-7B模型作为文本编码器，并专门用英语数据训练的$\mathrm{E} 5_{\text{mistral-7b}}$相比，我们的方法在英语的表现与其相当，但在其他语言中取得了明显更高的成绩。此外，M3-Embedding的稀疏检索功能（Sparse）也得到了有效训练，因为它在所有语言中均优于典型的BM25方法。我们还观察到多向量检索带来的额外改进，它依赖于查询和段落嵌入之间的细粒度交互来计算相关性得分。最后，密集方法和稀疏方法的结合（Dense+Sparse）进一步提升了每种方法的表现，而三种方法的协同（All）则带来了最佳性能。

## 4.2. Cross-Lingual Retrieval  

我们使用MKQA基准（Longpre et al., 2021）评估跨语言检索性能，该基准包含25种非英语语言的查询。对于每个查询，系统需要从英文维基百科语料库中检索包含答案的段落。在我们的实验中，我们使用BEIR提供的经过良好处理的语料库（Thakur et al., 2021）。根据之前的研究（Izacard et al., 2022），我们报告Recall@100作为主要评估指标（Recall@20作为辅助指标在附录C.1中报告）。对于Dense+Sparse方法和All方法，我们在MIRACL数据集中设置相同的权重。

------

实验结果如表2所示。与在多语言检索中的观察类似，M3-Embedding继续展现出卓越的性能，尤其是仅凭其密集检索功能（Dense）就显著超越了其他基准方法。不同检索方法的结合进一步提升了性能，达到了跨语言检索的最佳经验表现。此外，我们还可以观察到一些在此基准中独特的有趣结果。首先，性能差距不如MIRACL中那么显著，其中一些竞争性基准方法，如$\mathrm{E} 5_{\text{mistral-7b}}$，在某些测试语言上能够产生相似甚至更好的结果。然而，这些基准方法在许多其他语言，特别是低资源语言（如阿拉伯语、柬埔寨语、希伯来语等）中表现较差。相比之下，M3-Embedding在所有语言中保持了相对稳定的表现，这在很大程度上归功于其在广泛无监督数据上的预训练。其次，尽管M3-Embedding（Sparse）仍然优于BM25，但与其他方法相比，它的表现较差。这是因为在跨语言检索中，由于查询和段落分别以不同语言呈现，共存的术语非常有限。

## 4.3. Multilingual Long-Doc Retrieval  

我们使用两个基准评估长序列的检索性能：MLDR（多语言长文档检索），该基准由来自维基百科的多语言文章、Wudao和mC4（见表7）组成；以及NarrativeQA（Kočiský et al., 2018；Günther et al., 2023），该基准仅限于英语。除了之前的基准外，我们还引入了JinaEmbeddingv2（Günther et al., 2023）、OpenAI的text-embedding-ada-002和text-embedding-3-large，考虑到它们在长文档检索中的出色表现。对于Dense+Sparse方法，我们在公式（1）中设置$w_1=0.2$，$w_2=0.8$和$w_3=0$。对于All方法，我们在公式（1）中设置$w_1=0.15$，$w_2=0.5$和$w_3=0.35$。

------

MLDR的评估结果如表3所示。有趣的是，M3（Sparse）在长文档检索中表现出更高的效果，相比于密集方法，提升了约10个点。此外，多向量检索也令人印象深刻，相比于M3（Dense），提升了5.1+个点。最终，不同检索方法的结合带来了65.0的显著平均性能。

------

为了探讨M3-Embedding在长文档检索中的竞争力，我们进行了一项消融研究，通过去除长文档数据来进行微调（Token为w.o. long）。在这一修改后，密集方法（即Dense-w.o.long）仍然能够超越大多数基准方法，这表明它的经验优势在预训练阶段已得到充分建立。我们还提出了一种简单的策略——MCLS，来解决这种情况（无数据或无GPU资源进行文档检索微调）。实验结果表明，MCLS可以显著提升文档检索性能（41.2 → 45.0）。

------

我们在NarrativeQA上进行了进一步分析（见表4），可以观察到与MLDR类似的结果。此外，随着序列长度的增长，我们的方法逐渐扩展了相对于基准方法的优势（见图5），这反映了其在处理长输入方面的高效性。

## 4.4. Ablation study  

**自我知识蒸馏（Self-knowledge distillation）**。为了分析自我知识蒸馏（skd）的影响，我们进行了消融研究。具体来说，我们禁用了蒸馏处理，并让每种检索方法独立训练（Token为M3-w.o.skd）。根据我们在MIRACL上的评估（见表5），原始方法，即M3-w.skd，在所有设置中（如Dense、Sparse、Multivec）都能够比消融方法取得更好的表现。特别地，这种影响在稀疏检索中更加明显。这样的结果也反映了密集检索和稀疏检索方法之间的不兼容性。通过skd，这种不兼容性在很大程度上得以克服。（更多详细结果见附录C.1。）

------

**多阶段训练的影响**。我们还探讨了不同训练阶段的影响。Fine-tuning指的是直接对XLM-RoBERTA（Conneau et al., 2020）进行微调；RetroMAE+Fine-tuning指的是在从RetroMAE（Xiao et al., 2022）预训练的模型上进行微调。与此同时，RetroMAE+Unsup+Finetuning指的是在RetroMAE训练后，先进行无监督数据预训练再进行微调。结果如表6所示。我们可以观察到，RetroMAE能够显著提高检索性能，而在无监督数据上进行预训练进一步增强了嵌入模型的检索质量。（更多详细结果见附录C.1。）

# 5. Conclusion  

首先，虽然我们提出的M3-Embedding模型在流行的多语言和跨语言基准测试（如MIRACL和MKQA）上取得了领先的性能，但需要认识到，我们的方法在不同数据集和实际场景中的泛化能力仍需进一步研究。不同的数据集可能具有不同的特点和挑战，这些因素可能会影响我们模型的表现。其次，尽管M3-Embedding旨在处理不同粒度的输入，包括最多8192个Token的长文档，我们也意识到，处理极长的文档可能会在计算资源和模型效率方面带来挑战。我们模型在非常长的文档或超过规定Token限制的文档上的表现需要进一步研究。此外，我们声称M3-Embedding支持超过100种工作语言，但不同语言之间的性能变化没有得到充分讨论。需要对更广泛的语言进行进一步分析和评估，以了解我们模型在不同语言家族和语言特征中的鲁棒性和有效性。

# A. Details of Datasets  

## A.1. 收集的数据

无监督数据的语言和长度分布（Token数量）如图4所示。

------

我们观察到，对于长文本（例如cc-news中的新闻），初始句子往往是总结性陈述，模型可以仅依赖这些初始句子中提供的信息来建立相关关系。为了防止模型仅专注于这些开头的句子，我们实现了一种策略，即随机打乱整个文本中各段落的顺序。具体来说，我们将文本分为三个段落，随机打乱它们的顺序，并重新组合。这种方法允许相关文本段落随机出现在长序列中的任何位置。在训练过程中，我们以0.2%的概率对段落应用这一操作。

## A.2. 合成数据

GPT3.5的提示是：“你是一个好奇的AI助手，请根据以下文本生成一个具体且有价值的问题。生成的问题应围绕文本的核心内容展开，并避免使用代词（例如‘this’）。请注意，你只需生成一个问题，不要包含额外的内容。” 生成数据集的详细信息如表7所示。

# B. Implementation Details  

## B.1. 实验超参数

我们采用进一步预训练的XLM-RoBERTa8作为基础模型。我们将最大位置长度扩展到8192，并通过RetroMAE方法（Xiao等，2022）更新模型。数据集包括Pile（Gao等，2020）、Wudao（Yuan等，2021）和mC4（Raffel等，2020）数据集。我们从这些来源中采样了总计1.84亿个文本样本，覆盖105种语言。最大序列长度为8192，学习率为7×10^-5。批次大小设置为32，并且我们在16个步骤上累积梯度。预训练在32个A100（40GB）GPU上进行，训练步数为20,000步。

------

对于使用大规模无监督数据的预训练，查询和段落的最大长度分别设置为512和8192。学习率为5×10^-5，warmup比例为0.1，权重衰减为0.01。这个训练过程持续了25,000步。对于具有不同序列长度范围（例如0-500、500-1000等）的训练数据，我们使用不同的批次大小。详细信息见表9。第二阶段在96个A800（80GB）GPU上进行。

------

在微调阶段，我们为每个查询采样7个负例。批次大小请参考表9。在初始阶段，我们使用约6000步对密集嵌入、稀疏嵌入和多向量进行热身。随后，我们进行了统一训练并应用了自我知识蒸馏。这些实验在24个A800（80GB）GPU上进行。

## B.2. MCLS 方法

由于缺乏长文本数据或计算资源，长文本的微调可能受到限制。在这种情况下，我们提出了一种简单但有效的方法：MCLS（Multiple CLS）来增强模型的能力，而无需在长文本上进行微调。MCLS方法旨在利用多个CLSToken共同捕捉长文本的语义信息。具体而言，我们为每256个Token插入一个CLSToken，在我们的实验中，每256个Token插入一个"[CLS]"Token，每个CLSToken可以捕捉其相邻Token的语义信息。最终，文本嵌入通过对所有CLSToken的最后隐藏状态进行平均得到。

## B.3. Split-batch 方法

算法1：Split-batch伪代码.

```python
# enable gradient-checkpointing
M3.gradient_checkpointing_enable()

embs = []
for batch_data in loader:
   # split the large batch into multiple sub-batch
   for sub_batch_data in batch_data:
      sub_emb = M3(sub_batch_data)
      # only collect the embs
      embs.append(sub_emb)
      
# concatenate the outputs to get final embeddings
embs = cat(embs)
```

---

算法1提供了split-batch策略的伪代码。在当前批次中，我们将其划分为多个更小的子批次。对于每个子批次，我们利用模型生成嵌入，并在前向传播过程中通过梯度检查点丢弃所有中间激活。最后，我们将所有子批次的编码结果汇总，获得当前批次的嵌入。启用梯度检查点策略至关重要，否则每个子批次的中间激活会不断累积，最终占用与传统方法相同的GPU内存。

------

在表10中，我们研究了split-batch对批次大小的影响。可以观察到，在启用split-batch后，批次大小显著增加。同时，随着文本长度的增加，这种增长变得更加明显，在文本长度为8192时，启用split-batch使得批次大小增长超过20倍。

# C. 更多结果

## C.1. 额外结果

在本节中，我们展示了在MIRACL和MKQA基准上的额外评估结果。如表12和表13所示，M3-Embedding在所有基准测试中平均表现优于其他方法。

------

关于自知识蒸馏和多阶段训练的消融实验结果，分别列在表14和表15中，均基于MIRACL开发集。

## C.2. BM25使用不同分词器的影响

我们研究了不同分词器对BM25方法的影响，结果如表11所示。我们可以观察到：

------

- 使用Lucene的Analyzer可以显著提高BM25的效果。Lucene分析器包括多个步骤，通常包括分词、词干提取、停用词去除等，比直接使用XLM-RoBERTa的分词器取得更好的效果。此外，值得注意的是，XLM-RoBERTa的分词器词汇表有限，导致编码文档后生成的唯一词汇数量较少（例如，在MLDR数据集上，XLM-RoBERTa的分词器每篇文章生成1056个唯一词汇，而Lucene的分析器生成1451个唯一词汇，增加了37%以上，且可能会增加检索延迟）。

------

- M3在所有数据集上都优于使用相同分词器的BM25模型，表明学习到的权重明显优于BM25计算的权重。

------

- M3的稀疏检索在MIRACL和MKQA数据集上优于BM25。在长文档检索（MLDR）中，M3的稀疏方法未能超越BM25，但表现仍具有竞争力。这表明BM25依然是一个高度竞争的基准模型。探索对稀疏表示更有效的分词器，是未来研究的一个有意义的方向。