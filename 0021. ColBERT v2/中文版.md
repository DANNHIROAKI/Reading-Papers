# 0. Abstract

神经信息检索（IR）极大地推动了搜索和其他知识密集型语言任务的发展。虽然许多神经IR方法将查询和文档编码为单一向量表示，但后期交互模型在每个Token的粒度上生成多向量表示，并将相关性建模分解为可扩展的Token级计算。研究表明，这种分解使得后期交互更加有效，但也使这些模型的空间占用增加了一个数量级。在本研究中，我们引入了ColBERTv2，一种结合了激进的残差压缩机制和降噪监督策略的检索模型，旨在同时提高后期交互的质量和空间占用。我们在广泛的基准测试中评估了ColBERTv2，证明其在训练域内外均达到了最新的质量水平，同时将后期交互模型的空间占用减少了6至10倍。

# 1. Intro

神经信息检索（IR）在过去的$2\text{–}3$年间迅速主导了搜索领域，不仅极大地推动了段落和文档搜索的发展（Nogueira 和 Cho，2019），还在许多知识密集型的自然语言处理任务中取得了显著进展，例如开放域问答（Guu等，2020）、多跳声明验证(multi-hop claim verification  )（Khattab等，2021a）以及开放式生成（Paranjape等，2022）。

- 信息检索：
  - 含义：理解用户查询$/$文档之间的语义关系
  - 种类：传统信息检索(基于关键词捕捉意义匹配)，神经信息检索(基于深度学习捕捉意义匹配)

---

许多神经IR方法遵循单向量相似性范式：使用预训练的语言模型将每个查询和文档编码为一个高维向量，并通过两者之间的简单点积来建模相关性。另一种方法是ColBERT（Khattab和Zaharia，2020）中提出的后期交互，其中查询和文档被编码为细粒度的多向量表示，并通过这两组向量之间丰富而可扩展的交互来估计相关性。ColBERT为查询（和文档）中的每个Token生成一个嵌入，并将相关性建模为每个查询向量与文档中所有向量之间最大相似度的总和。

- 单向量神经IR：(查询$+$文档)$\xrightarrow[文档级]{预训练模型}$(查询高维向量$+$文档高维向量)$\xrightarrow{互相点积}$相似度
- 后期交互神经IR：(查询$+$文档)$\xrightarrow[细颗粒度/\text{Token级}]{预训练模型}$(查询多向量集$+$文档多向量集)$\xrightarrow{丰富/可扩展的后期交互}$相似度

---

通过将相关性建模分解为Token级计算，后期交互旨在减轻编码器的负担：单向量模型必须通过一个点积捕捉复杂的查询\text{–}文档关系，而后期交互则在Token级编码含义，并将查询\text{–}文档匹配交给交互机制。这种增加的表达能力是有代价的：现有的后期交互系统的空间占用比单向量模型大一个数量级，因为它们必须为网络规模的集合存储数十亿个小向量。考虑到这一挑战，似乎更有成效的方式是通过引入新的负样本挖掘（Xiong等人，2020）、预训练（Gao和Callan，2021）和蒸馏（Qu等人，2021）监督范式，来解决单向量模型的脆弱性（Menon等人，2022）。确实，最近的单向量模型在高度调优的监督策略下（Ren等人，2021b；Formal等人，2021a）有时表现与“原生”后期交互模型相当，甚至更好，而且后期交互架构——由于其固有的Token级归纳偏差——是否能从改进的监督中获得类似的大幅提升，也并不明确。

- 后期交互的代价：需存储的嵌入量很大，空间占比较单向量模型大一个数量级
- 单向量模型的优化：
  - 正负样本挖掘
    - 正/负样本：与用户查询相关/无关的文档
    - 挖掘方式：在训练时，让模型学会区分与查询相关和不相关的文档
  - 蒸馏：
    - 含义：训练一个小型的“学生”模型，来模仿一个大型的“教师”模型的行为
    - 目的：减小模型规模，但同时保持其表达能力
- 单向量模型的优化效果：在高度调优的监督策略下，单向量模型在许多任务中与后期交互模型匹敌
- 后期交互模型的缺陷：存在固有的Token级归纳偏差
  - 含义：即更关注Token级相似性而非上下文(全局)相似性
  - 影响：适用于单向量模型的调优/强化监督策略，可能不会给后期交互模型带来太多性能提升

---

在本研究中，我们展示了后期交互检索器能够自然地生成==轻量级的Token表示==，这些表示可以直接高效存储，并且能够极大地受益于==降噪监督==。我们在ColBERTv2中结合了这些特点。ColBERTv2 ${ }^1$ 是一种新的后期交互检索器，采用了来自==交叉编码器==的简单==蒸馏==方法和==强负样本挖掘==(§3.2)，在提升质量上超越了任何现有方法，然后利用==残差压缩机制== (§3.3) 将后期交互的空间占用减少了6\text{–}10倍，同时保持了质量。因此，ColBERTv2在训练域内外都建立了最先进的检索质量，并在空间占用方面与典型的单向量模型相当。

- 存储空间方面：降到了与单向量模型相当
  - 轻量级的Token表示，大幅减少存储需求
  - 残差压缩：具体是压缩个啥看后面的吧
- 训练策略方面：建立了最先进的检索质量
  - 降噪监督：训练过程中加入噪声样本，使模型能过滤出有效的信息，增强其鲁棒性
  - 交叉编码器的蒸馏：通过蒸馏，从一个强大的交叉编码器中提取出有价值的信息，*并将其传递给ColBERTv2* (?)
  - 强负样本挖掘：挑选出困难的负样本(与查询高度相似但实际无关的文档)，ColBERTv2能够更好地学习到正负样本之间的差异

---

在MS MARCO Passage Ranking数据集上训练时，ColBERTv2实现了所有独立检索器中最高的MRR@10。除了在域内的高质量表现外，我们还寻求一种能够“零样本”泛化到特定领域语料库和长尾主题的检索器，这些主题往往在大型公共训练集中代表性不足。为此，我们在大量域外基准测试上评估了ColBERTv2，这些基准包括三个Wikipedia开放问答检索测试以及来自BEIR (Thakur et al., 2021) 的13个多样化的检索和语义相似性任务。

- 域内检索：
  - 含义：在模型训练数据集/语料库上，执行检索
  - 此处：模型在MS MARCO Passage Ranking上训练与检索，取得了极高的检索质量
  
- 零样本泛化：
  
  - 零样本：模型没有接触过的全新的测试集/任务
  
  - 泛化含义：模型在没有特定领域训练数据的情况下，能在零样本上表现良好
  - 泛化目标：在特定领域语料库/长尾主题检索效果良好
    - 长尾主题：某个领域内数量庞大但极为低频(非主流)的话题，类似于Zipf定律
    - 特定领域/长尾主题特点：在公共训练集中，这类话题的出现较少
  
- 泛化测试：
  - 测试内容：在域外基准数据上测试ColBERTv2
  - 评估标准：
    - Wikipedia开放问答：一种信息检索数据库，旨在整个开放域(wiki总库)找到与查询相关的段落
    - BEIR基准测试：广泛用于域外评估，包含13个多样化任务(如自然查询/语义相似性检索)
    - LoTTE基准：本文引入的新库，旨在更关注自然搜索查询&长尾主题

---

此外，我们引入了一个新的基准，称为LoTTE，用于长尾主题分层评估，包含12个特定领域的搜索测试，覆盖了StackExchange社区并使用了GooAQ (Khashabi et al., 2021)中的查询。与开放问答测试和许多BEIR任务不同，LoTTE在其段落中专注于相对长尾的主题，并评估模型回答具有实际意图的自然搜索查询的能力，而非BEIR的许多语义相似性任务。在28个域外测试中的22个上，ColBERTv2达到了最高质量表现，其相对增益超过了第二最佳检索器达8%，并使用了压缩后的表示。

- 关于LoTTE测试：
  - 是啥：一个新的基准测试，专门评估模型在长尾主题上的检索能力
  - 测试内容：12个特定领域的检索测试
    - 领域数据：来自StackExchange社区(包含诸如Stack Overflow/Ask Ubuntu等)
    - 查询数据：来自GooAQ(包含了多样的问题和答案)
  - 测试目标：面对自然查询(用户带有实际意图)时，能否高效地从长尾主题中抽取出相关答案
- ColBERTv2的表现：
  - 即使压缩了表示，在**28个域外测试**中的22个测试取得了最高质量表现→零样本泛化能力强

---

本研究的主要贡献如下：

1. 我们提出了ColBERTv2，这是一种结合了==降噪监督==和==残差压缩==的检索模型，利用后期交互的Token级分解，在减少空间占用的同时实现了高鲁棒性。

2. 我们引入了LoTTE，一种用于域外检索器评估的新资源。LoTTE专注于围绕长尾主题的自然信息查询，这是一个==重要但尚未被充分研究的应用领域==。

3. 我们在广泛的环境中评估了ColBERTv2，确立了其在训练域内外的最先进质量水平。

# 2. Background & Related Work  

## 2.1. Token-Decomposed Scoring in Neural IR  

许多神经信息检索（IR）方法将段落编码为一个高维向量，以提升效率和可扩展性，权衡了交叉编码器的较高质量（Karpukhin等人，2020；Xiong等人，2020；Qu等人，2021）。ColBERT（Khattab和Zaharia，2020）的后期交互模式通过计算多向量嵌入并使用可扩展的“MaxSim”操作符进行检索，解决了这一权衡问题。其他一些系统也利用了多向量表示，包括Poly-encoders（Humeau等人，2020）、PreTTR（MacAvaney等人，2020）和MORES（Gao等人，2020），但这些方法主要用于基于注意力的重新排序，而非ColBERT的可扩展MaxSim端到端检索。

- 不同IR方法的权衡
  - 将查询/文档编码为单一高维向量：成本低，但会倾向于简化语义表示
  - 交叉编码器(如BERT)：成本高，但能够通过更细粒度的交互精确捕捉语义
- 多向量方法的改良：基于ColBERT式的后期交互
  - 意义：同时兼顾了编码成本/检索质量
  - 操作：
    - 嵌入：在查询前，将文档/段落编码为多向量
    - 查询：临时再去编码查询成查询嵌入集，然后再计算每个查询嵌入与所有段落嵌入的MaxSim，再相加
  - 优势：保留了细粒度的交互信息，同时减轻了嵌入成本
- 多向量方法：基于注意力重排
  - 注意力重排是啥：
    - 先用计算量小的方法(词袋模型)，对文档大规模排序
    - 再用计算量大的方法(注意力/BERT/交叉编码)，对初步排序结果进行小规模细颗粒度重排
  - 模型实例：Poly\text{–}encoders/PreTTR/MORES

---

ME\text{–}BERT（Luan等人，2021）生成类似于ColBERT的词级文档嵌入，但对查询仅保留一个嵌入向量。COIL（Gao等人，2021）也生成词级文档嵌入，但词的交互限制在查询和文档词项之间的词汇匹配。uniCOIL（Lin和Ma，2021）将COIL的词嵌入向量限制为单一维度，将其简化为扩展了DeepCT（Dai和Callan，2020）和DeepImpact（Mallia等人，2021）等模型的标量权重。为了生成标量权重，SPLADE（Formal等人，2021b）和SPLADEv2（Formal等人，2021a）生成一个稀疏的词汇级向量，保留了后期交互的词项级分解，同时将存储简化为每个词一维。SPLADE系列还依托BERT在预训练期间获得的语言建模能力。SPLADEv2已被证明在不同领域内外均具有很高的有效性，并且是本文实验中对比的核心点。

==几种基于BERT的文档嵌入方法== 

- ME-BERT：为每个文档生成词级多向量表示，但只为查询生成单向量表示
- COIL：为文档/查询都生成词级嵌入，但是交互仅限于文档\text{–}查询匹配的词汇
- uniCOIL：交互原理同COIL，区别在于将COIL的词级嵌入压缩到一维(权重标量)
- SPLADE：为查询/文档生成词汇级的稀疏向量，查询和文档进行词级的后期交互

## 2.2. Vector Compression for Neural IR  

最近，对信息检索（IR）表示的压缩研究兴趣激增。Izacard等人（2020）研究了单向量检索器的维度缩减、积量化（PQ）和段落过滤。BPR（Yamada等人，2021a）通过可微的tanh函数将嵌入直接哈希为二进制代码。JPQ（Zhan等人，2021a）及其扩展RepCONC（Zhan等人，2022）使用PQ压缩嵌入，并通过排序导向的损失联合训练查询编码器与PQ产生的质心。

- PQ：先训练查询编码器和文档编码器，再对嵌入应用PQ压缩
- JPQ：将训练\text{–}压缩过程合在一起，在训练的时候就同时优化编码器和质心分布
- BPR：通过tanh函数将嵌入(浮点型)哈希映射为二进制编码，以高效存储与计算距离

---

SDR（Cohen等人，2021）使用自编码器降低用于注意力重排序的上下文化嵌入的维度，并应用量化方案进一步压缩。DensePhrases（Lee等人，2021a）用于开放问答，依赖段落的多向量编码，但搜索基于单个向量且未使用后期交互。Lee等人（2021b）提出基于PQ的量化感知微调，减少DensePhrases的空间占用。尽管DensePhrases在开放问答中有效，但在NaturalQuestions和TriviaQA上的前20检索准确率上与DPR相当，远低于ColBERT。 

- SDR：使用自编码器将原始嵌入映射到更低语义空间，再对低维语义空间向量应用PQ压缩
- DensePhrases：为每个段落表示为多向量，将每个查询表示为单向量，无后期交互
  - 压缩优化：基于PQ的量化感知微调，==直接忽略这个模型吧挺垃圾的== 

---

在本研究中，我们聚焦于后期交互检索，并使用可==直接应用于后期交互模型的残差压缩方法==。附录A显示，ColBERT的表示自然适用于残差压缩。残差压缩技术应用广泛（Barnes等人，1996），已用于近似最近邻搜索（Wei等人，2014）、神经网络量化（Li等人，2021a，2021b）以及分布式深度学习（Chen等人，2018；Liu等人，2020）。据我们所知，ColBERTv2是第一个在可扩展的神经IR中使用残差压缩的方法。

- 残差压缩是啥
  - 将原始数据压缩为低维(占据空间更少)的表示，捕获大部分的信息
  - 计算压缩前后的残差，用于表示剩下的信息
  - 用低维近似表示+残差表示原有数据

- ==ColBERT的表示为什么自然适用于残差压缩==(附录A)

## 2.3. Improving the Quality of Single-Vector Representations

与我们进行多向量表示的压缩不同，许多最新研究致力于提高单向量模型的质量，这些模型通常==对监督细节非常敏感==。这类研究可分为三个方向：(1) 更具表现力的架构的蒸馏（Hofstätter等人，2020；Lin等人，2020），包括显式去噪（Qu等人，2021；Ren等人，2021b），(2) 硬负采样（Xiong等人，2020；Zhan等人，2020a，2021b），以及 (3) 改进的预训练（Gao和Callan，2021；Oguz等人，2021）。我们在ColBERTv2的多向量表示中采用了类似于(1)和(2)的技术（参见第3.2节）。

- 对单向量表示的优化

  - 优化特点：高度依赖监督的细节

  - 优化途径：

    - 强大模型的蒸馏：将强大的多向量教师模型中的知识，蒸馏到单向量模型中
    - 硬负采样：训练过程中，引入与查询==语义相似==但与文档==实际不相关==的样本(硬负样本)

    - 改进预训练：比如针对特定任务训练/对抗训练/多任务学习

- 硬负样本举例：

  - 查询：机器学习的优点
    - 简单负样本：完全不相关的文本，比如关于 "旅游" 的文章
    - 硬负样本：一段介绍深度学习但是没有阐明其优点的文章

## 2.4. Out-of-Domain Evaluation in IR  

检索的最新进展主要集中在大数据评估上，其中数万条带有标注的训练查询与测试领域相关，例如MS MARCO或Natural Questions（Kwiatkowski等人，2019）。在这些基准中，查询往往反映维基百科中高人气的主题，如电影和运动员。实际上，面向用户的IR和QA应用通常涉及特定领域的语料库，而这些语料库几乎没有可用的训练数据，且在大型公共集合中相关主题的代表性不足。

- 背景：IR领域高度依赖于大规模数据集
- 困境：领域外评估
  - 查询分布局限：数据集的查询主题集中在热门话题，以至于无法代表实际用户检索需求
  - 领域差异：用户的IR/RA请求往往涉及特定领域，某些特定领域几乎没有标记数据库

---

这种域外情形最近受到了BEIR（Thakur等人，2021）基准的关注。BEIR将若干现有数据集结合成一个异质套件，用于“零样本IR”任务，涵盖生物医学、金融和科学等领域。虽然BEIR数据集提供了有用的测试平台，但其中许多任务主要涉及广泛的语义相关性任务，如引文、反驳或重复问题，而非自然搜索任务，或是关注像维基百科中那样的高人气实体。在第4节中，我们引入了LoTTE，一个用于域外检索的新数据集，展示了长尾主题的自然搜索查询。

- 关于BEIR数据集
  - 是啥：针对零样本检索任务的评估基准，包含多个领域的数据集
  - 局限：BEIR的检索任务更集中于广义的语义相关性检索，而非自然搜索任务
    - 广义的语义相关性检索：找到与查询在语义上相关的文档、引文或概念
    - 自然搜索任务：模拟用户在真实情况下的查询

# 3. ColBERTv2  

我们现在介绍ColBERTv2，它在提高多向量检索模型质量的同时（第3.2节），减少了它们的空间占用（第3.3节）。

## 3.1. Modeling  

ColBERTv2采用了ColBERT的后期交互架构，如图1所示。查询和段落独立地用BERT（Devlin等人，2019）编码，每个词的输出嵌入被投影到较低维度。在离线索引期间，语料库中的每个段落 $d$ 被编码为一组向量，这些向量随后被存储。在搜索时，查询 $q$ 被编码为多向量表示，其与段落 $d$ 的相似度计算为查询端“MaxSim”操作的求和，即每个查询词嵌入与所有段落词嵌入之间的最大余弦相似度的总和：

其中，$Q$ 是一个矩阵，用 $N$ 个向量编码查询，而 $D$ 用 $M$ 个向量编码段落。该架构的直观理解是将每个查询词与最具上下文相关性的段落词对齐，量化这些匹配，然后在查询上合并部分得分。关于后期交互的详细讨论，我们参考Khattab和Zaharia（2020）。

- ColBERT后期交互结构概述
  - 编码结构：查询/段落分别独立地输入BERT以得到上下文嵌入，并都投影到较低维度
  - 编码流程：
    - 离线时：将数据库中所有段落$d$都编码成词级嵌入$D_{M\text{×}k}$并存储，其中$M$为段落词数$k$为嵌入维度
    - 在线时：在给出查询$q$后再将其编码为次级嵌入$Q_{N\text{×}k}$，其中$N$为查询词数$k$为嵌入维度
  - 查询流程：$\displaystyle{}S_{q, d}=\sum_{i=1}^N \max _{j=1}^M Q_i \cdot D_j^T$ 
    - 求$\text{MaxSim}$：让$d$的每个嵌入和$q$的所有嵌入计算相似度并取最大值，最终得到$N$个$\text{MaxSim}$
    - 求和：将所有$\text{MaxSim}$求和，即得到查询对一个文档的最终相似度

## 3.2. Supervision: ColBERT的改进训练技术

训练神经检索模型通常需要训练集中每个查询的正负段落。Khattab和Zaharia（2020）使用MS MARCO的官方三元组$\left\langle\mathrm{q}, \mathrm{d}^{+}, \mathrm{d}^{\text{–}}\right\rangle$来训练ColBERT。对于每个查询，一个正样本 $d^{+}$ 由人工标注，每个负样本 $d^{\text{–}}$ 从BM25检索的未标注段落中采样。

后续研究发现了这种标准监督方法的几个缺陷（见第2.3节）。我们的目标是采用一个简单且一致的监督方案，选择具有挑战性的负样本，避免奖励假阳性或惩罚假阴性。为此，我们首先使用Khattab等人（2021b）提出的三元组训练ColBERT模型，并使用ColBERTv2的压缩方式对训练段落进行索引。

- 传统的ColBERT监督训练方法
  - 训练数据结构：来自MS MARCO的三元组$\langle q, d^+, d^\text{–} \rangle$
    - **$q$**：查询
    - **$d^+$**：正样本，即与查询相关的段落，来源于人工标注
    - **$d^\text{–}$**：负样本，与查询无关的段落，来源于通过BM25检索算法得到的未标注段落
  - 训练目标：曾强$\langle q, d^+ \rangle$相似性，降低$\langle q, d^\text{–} \rangle$相似性
  - 存在的问题：
    - BM25采样得到的负样本不够有挑战性
    - 三元组的模式倾向于奖励false positive/false negative

---

==ColBERTv2改进的监督训练方法==

对于每个训练查询，我们检索前k个段落。我们将每个查询\text{–}段落对输入到一个交叉编码器重排序模型中。我们使用由Thakur等人（2021）通过蒸馏训练的22M参数MiniLM（Wang等人，2020）交叉编码器。该小模型表现非常出色，同时推理效率较高，适合用于蒸馏。

接下来，我们收集由查询、一个高排名段落（或标注的正样本）和一个或多个低排名段落组成的$w$\text{–}路元组。在本研究中，我们为每个样本使用 $w=64$ 个段落。

- 训练数据：硬负样本的生成
  - 段落初选：对于待训练的查询，先检索出$k$个可能相关/无关的段落，构成$k$个查询\text{–}段落对
  - 段落重排：使用经过蒸馏的小型交叉编码模型MiniLM，对所有查询\text{–}段落对进行相关性评分
  - $w$-路元组：将一个训练样本构建为查询$q$以及$w=64$个段落$d$
    - 其中一个段落：交叉编码器评分较高的段落，或者标注的正样本
    - 余下的段落：交叉编码器评分较低的段落

---

类似于RocketQAv2（Ren等人，2021b），我们使用KL散度损失将交叉编码器的评分蒸馏到ColBERT架构中。我们选择KL散度，因为ColBERT生成的分数（即余弦相似度之和）尺度受限，可能无法直接与交叉编码器的输出分数对齐。// 我们还在每个GPU上使用批内负样本，并对每个查询的正样本分数与同批次其他查询对应段落的分数应用==交叉熵==损失。我们重复此过程一次以刷新索引并重新采样负样本。

- 去噪训练过程：蒸馏
  - 目的：将交叉编码器的评分分布蒸馏到ColBERT架构中去
  - 难题：
    - 交叉编码器分数是任意实数，ColBERT分数源于余弦相似度$\text{∈}[\text{–}1,1]$
    - 二者尺度不同无法对齐，不可直接用绝对分数蒸馏
  - 解决：使用KL散度
    - KL散度是啥：一种衡量两个概率分布之间差异的损失函数
    - 应用的原理：
      - 将交叉编码器/ColBERT的分数都归一化(SoftMax)为概率分布
      - 再开始蒸馏，即让ColBERT的分布尽量接近交叉编码器的分布
- 去噪训练方式
  - 批内负样本：在GPU的一批待训练查询中，一个查询会将其它查询\text{–}段落对的段落也视作自己的负样本
  - 损失函数：计算并比较每个查询的正样本/批内负样本交叉熵，并以扩大二者交叉熵差距为训练目标
  - 刷新操作：
    - 若干训练轮后，用初步训练后的模型重新计算段落嵌入
    - 同时也重新进行负样本采样
    - 继续训练

---

使用硬负样本的去噪训练在最近的研究中被认为是缩小单向量模型与交互式模型之间差距的有效方法，包括像ColBERT这样的后期交互架构。我们的结果（见第5节）表明，这种监督方式可以显著提高多向量模型的效果，达到最先进的检索质量。

- 总结：本文证明硬负样本/去噪训练，显著提高了多向量模型的性能

## 3.3. Representation: ColBERT的改进向量压缩技术

我们假设ColBERT的向量聚集在能够捕捉高度特定词语语义的区域。我们在附录A中测试了这一假设，证据表明每个词义对应的向量聚集在一起，因上下文变化仅存在轻微差异。我们利用这一规律，通过残差表示显著减少了后期交互模型的空间占用，完全无需架构或训练的改动。给定一组质心 $C$，ColBERTv2将每个向量 $v$ 编码为其最近质心 $C_t$ 的索引以及一个量化向量 $\tilde{r}$，用于近似残差 $r=v\text{–}C_t$。在检索时，我们使用质心索引 $t$ 和残差 $\tilde{r}$ 来恢复一个近似的 $\tilde{v}=C_t+\tilde{r}$。

- 背景假设：向量的语义聚集性
  - ColBERT对同一文档/查询的不同Token的嵌入，倾向于聚集在特定区域，以此也反应了特定的语义
  - 同一词在不同上下文(文档/查询)的嵌入表示，仅存在微小的差别
- 质心$+$残差编码：对PQ压缩的自然扩展
  - 运行聚类：将文档的每个嵌入$t$聚类到其最近的质心$C_t$
  - 压缩编码：$t$的嵌入$\xrightarrow{编码为}$(离$t$最近质心$C_t$的索引)$+$(残差向量$r\text{=}t\text{–}C_t$的量化近似$\tilde{r}\text{≈}t\text{–}C_t$)
  - 压缩表示：执行检索时，$t$被近似还原为为$\tilde{t}\text{=}C_t\text{+}\tilde{r}$
- ==注意事项==：
  - 所谓质心，是对于语料库中所有文档的所有嵌入而言的，必不是一个文档有一组质心
  - 得益于向量的语义聚集性的假设，这样做并不会导致每个Token的残差过大

---

为了编码 $\tilde{r}$，我们将 $r$ 的每个维度量化为一或两位。原则上，$n$ 维向量的 $b$ 位编码需要每个向量 $\lceil\log |C|\rceil+b n$ 位。在实际操作中，当 $n=128$ 时，我们使用四字节表示最多 $2^{32}$ 个质心，并使用16或32字节（对于 $b=1$ 或 $b=2$）来编码残差。每个向量总共需要20或36字节，与ColBERT使用16位精度的256字节向量编码相比具有显著优势。尽管存在多种压缩替代方案，我们发现这种简单的编码在大幅降低存储成本的同时，基本保持了模型质量，相比典型的32位或16位精度的后期交互系统存储更为高效。

- 残差向量的量化：可1/2bit量化，以下以2bit为例
  - 操作：($r$每个维度的连续分布$\xrightarrow{拆分为}$4个离散状态)$\xLeftrightarrow{对应}$2个bit所表示的四种状态
  - 原理：由于残差每个维度的绝对大小远小于原始嵌入，所以这并不会导致太多信息丢失
  - 示例：$[\text{–}\Delta, \Delta]\xrightarrow{拆分}00\text{⇔}[\text{–}\Delta, \text{–}\Delta/2), 01\text{⇔}[\text{–}\Delta/2, 0), 10\text{⇔}[0, \Delta/2), 11\text{⇔}[\Delta/2, \Delta]$ 
- 编码的空间占用
  - 质心索引编码：$\lceil\log |C|\rceil+b n$
    - 当共有$C$个质心是索引占据$\lceil\log |C|\rceil$个bit
    - 实际操作上支持至多$2^{32}$个质心，故占用空间为32bit
  - 残差量化编码：
    - 每个维度划分为$2^b\text{=}2/4$种状态用$b\text{=}1/2\text{ bit}$大小表示
    - 实际操作中残差一般为$n\text{=}128$维，故占用空间为128/256bit，
  - 每个嵌入压缩后总占用空间为：160/288bit，即20/36Byte
- 相比于直接存储原始嵌入，其一般128维/每维精度为16/32位，每个嵌入压缩前占据256/512字节

---

这种基于质心的编码可以看作是积量化（PQ）在多向量表示中的自然扩展。积量化（Gray, 1984；Jegou等人，2010）通过将单一向量分割成小子向量并使用代码本中的ID编码每个子向量来实现压缩。在我们的方法中，每个表示本身已是一个矩阵，自然分为若干小向量（每个词一个）。我们通过最近的质心加残差来编码每个向量。有关压缩对检索质量的影响测试以及与类似于BPR（Yamada等人，2021b）的ColBERT基线压缩方法的对比，参见附录B。

- 残差压缩的效果，==见附录B==

## 3.4. Indexing  

给定一个段落语料库，索引阶段预先计算所有段落嵌入，并组织其表示以支持快速最近邻搜索。ColBERTv2将索引划分为三个阶段，如下所述。

- 索引含义：其实就是预计算文档(段落)的嵌入，并一定形式组织与存储

---

**Centroid Selection.**  在第一阶段，ColBERTv2选择一组聚类质心 $C$。这些质心用于支持残差编码（第3.3节）以及最近邻搜索（第3.5节）。通常，我们发现将 $|C|$ 设置为语料库中嵌入数量 $n_{\text {embeddings }}$ 的平方根的比例在实践中效果良好。Khattab和Zaharia（2020）仅在计算所有段落的表示后进行向量聚类，但这需要未压缩地存储它们。为了减少内存消耗，我们将 $k$ 均值聚类应用于通过BERT编码器处理的部分段落嵌入，这些段落的样本量与集合大小的平方根成比例。我们发现这一方法在实践中表现良好。

- 质心的作用：是残差编码的基础，在最邻近查询时快速定位到查询的相似向量
- 质心的数量：实验证明，如果嵌入数量为 $n_{\text{embeddings}}$，质心数量为 $\sqrt{n_{\text{embeddings}}}$效果良好
- 质心的选择：
  - ColBERT的做法：预先用BERT计算并存储数据库中的所有原始嵌入后，再执行$k\text{-Means}$聚类得到质心
  - 本文的做法：抽样以快速选择质心
    - 聚类的对象必不是全体嵌入，而是随机抽取一部分，$抽样大小\text{∝}\sqrt{语料库总大小}$
    - 先对抽样部分用BERT完成嵌入，再执行$\sqrt{n_{\text{embeddings}}}\text{-Means}$，得到的质心近似为整体质心
    - 实践证明效果良好

---

**Passage Encoding.**  选定质心后，我们对语料库中的每个段落进行编码。这需要调用BERT编码器并按照第3.3节所述压缩输出嵌入，将每个嵌入分配到最近的质心并计算量化的残差。一批段落编码完成后，将压缩后的表示保存到磁盘。

- 本文流程
  - 质心选择：用BERT计算并==存储==部分嵌入，用部分嵌入得到近似质心
  - 文章编码：按以下方式并行批处理一组段落
    - 用BERT计算完整嵌入==不存储==
    - 用近似质心压缩嵌入，==存储压缩后的嵌入== 
- 对比做法：
  - 用BERT计算所有的嵌入并==存储==，用完整嵌入得到精确质心
  - 用精确质心压缩嵌入，==存储压缩后的嵌入==
- 对比：
  - 传统做法的好处：计算量更少(少了部分嵌入的计算)，质心还更准确
  - 传统做法的弊端：
    - 计算质心时所用嵌入必须被存储，传统方法无法避免对全体嵌入的存储(这是本文极力避免的)
    - 嵌入计算和压缩操作被质心计算过程分隔，二者无法实现不同批间的并行运算

---

**Index Inversion.**  为支持快速最近邻搜索，我们将对应于每个质心的嵌入ID分组，并将此倒排列表保存到磁盘。在搜索时，这使我们能够快速找到与查询中的嵌入相似的词级嵌入。

- 列表构建：构建质心$\text{→}$嵌入的嵌入ID列表，存储到磁盘中

  ```txt
  质心a -> 属于质心a的嵌入ID=a1, a2, a3, ...
  质心b -> 属于质心b的嵌入ID=b1, b2, b3, ...
  质心c -> 属于质心c的嵌入ID=c1, c2, c3, ...
  ......
  ```

- 应用方式：查询时先找到离查询向量最近的质心$\text{→}$直接从质心的列表中快速找到相关的嵌入

## 3.5. Retrieval  

给定查询表示 $Q$，检索从候选生成开始。对于查询中的每个向量 $Q_i$，找到最近的 $n_{\text {probe }} \geq 1$ 个质心。利用倒排列表，ColBERTv2识别出接近这些质心的段落嵌入，解压它们，并计算它们与每个查询向量的余弦相似度。然后将得分按查询向量的段落ID分组，并对相同段落的得分进行最大化归约。这使ColBERTv2能够对每个查询向量执行近似“MaxSim”操作。此操作使用通过倒排列表识别的嵌入计算出真实MaxSim（第3.1节）的下界，类似于Macdonald和Tonellotto（2021）为评分探索的近似方法，但在此用于候选生成。

- 查询输入：原始查询$Q\xrightarrow[预处理(嵌入)]{\text{BERT}}$多向量表示$\{Q_1, Q_2, \dots, Q_N\}$ 

- 候选生成：将所有嵌入解压不现实，所以需要缩小解压的范围，缩小方法就是倒排索引

  - 质心查找：对于每个$Q_i\xrightarrow{质心查找}$离$Q_i$最近的$n_{\text{probe}}\text{≥}1$个质心

  - 倒排查找：对于每个$Q_i\xrightarrow{质心查找}$离$Q_i$最近的$n_{\text{probe}}\text{≥}1$个质心$\xrightarrow{嵌入查找}$每个质心的所有嵌入(候选集)
  - 候选还原：将候选集中所有嵌入用(索引质心$+$残差)解压

- 相似度计算：

  - 嵌入得分：计算$Q_i\xleftrightarrow{余弦相似度}$候选集中每个解压后的嵌入，当作每个嵌入的得分

  - MaxSim下界：

    - 嵌入分组：将所有嵌入$+$得分，按照嵌入所属的文档/段落分组

    - 最大规约：记录每组中的最大得分，当作相应文档/段落的MaxSim的近似下界

  - 段落得分：

    - 循环：对查询的每个$Q_i$执行以上操作以计算所有MaxSim近似下界
    - 加合：将每个文档/段落的所有MaxSim近似下界相加，即得到该文档/段落得分的近似下界

- 文档排序

  - 初排：根据每个文档得分的近似下界排序，选取前面若干个文档/段落
  - 重排：
    - 加载选定文档/段落的全部完整嵌入
    - 按照ColBERT的模式再进行精确的相似度计算与排序，并返回最终的结果

---

这些下界在查询词上求和，并根据这些近似得分选择得分最高的 $n_{\text {candidate }}$ 个候选段落进行排序，加载每个段落的完整嵌入集，并使用文档中所有嵌入执行与公式1一致的评分函数。结果段落按得分排序并返回。

# 4. LoTTE: Long-Tail, Cross-Domain Retrieval Evaluation

我们引入了LoTTE（发音为“latte”），这是一个用于长尾主题分层评估的IR新数据集。为了补充BEIR（Thakur等人，2021）的域外测试（如第2.4节中所述），LoTTE专注于涉及长尾主题的自然用户查询，这些主题可能未被维基百科这样的以实体为中心的知识库所覆盖。

LoTTE是什么：

- 目标：用于对长尾主题进行分层评估的数据集，强调用户自然查询
  - 长尾主题：某个领域内数量庞大但极为低频(非主流)的话题，类似于Zipf定律
  - 分层评估：将数据集按主题显式划分，针对每个主题(每层)单独评估IR系统的性能
  - 自然查询：模拟用户在真实情况下的提问
- 原理：为何LoTTE能针对长尾主题进行评估
  - 一般的数据集：不会(难以)收录某一领域的长尾主题内容
  - LoTTE数据集：收录内容大多是不同领域的长尾主题
- 角色：充当BEIR数据集的补充
  - BIER：专注于域外测试的数据集，但是话题往往集中于热门主题(长尾主题极少)
  - LoTTE+BEIR：进行域外评估时，可以覆盖更多的长尾话题

---

LoTTE由12个测试集组成，每个测试集包含500-2000个查询和10万-200万个段落。测试集按主题显式划分，每个测试集附带一个包含相关但不重叠的查询和段落的验证集。我们选择使段落文本不重叠，以促进更现实的域外迁移测试，允许在相关但不同的主题上进行最小的开发。测试集（和开发集）包括一个“汇总”设置。在汇总设置中，段落和查询在所有测试（或开发）主题中进行聚合，以评估在更大且更多样化的语料库上的域外检索。

- LoTTE的数据组成
  - 测试集：包含12个主题不同互不重叠的测试集，每个测试集包含500-2000个查询$+$10-200w个段落
    - 汇总操作：将测所有主题的测试集数据合并，以测试模型在跨领域语料库上的域外检索性能
  - 验证集：每个测试集都有一验证集，其查询/段落与测试集的查询/段落相关但不重叠(以验证域外迁移)

---

表1概述了LoTTE的组成。我们从各种StackExchange论坛的答案帖子中提取主题和段落语料库。StackExchange是一个围绕特定主题（如“物理”或“自行车”）的问答社区集合。我们从五个主要领域中收集论坛：写作、娱乐、科学、技术和生活方式。为了评估检索器，我们收集了“搜索”和“论坛”查询，每个查询都与其语料库中的一个或多个目标答案帖子相关。表2展示了示例查询以及语料库中回答这些查询的帖子中的简短片段。

## 4.1. Search Queries.  

我们从GooAQ（Khashabi等人，2021）中收集搜索查询，这是一个包含Google搜索自动完成查询及其答案框的最新数据集，我们筛选出答案链接到特定StackExchange帖子的查询。正如Khashabi等人（2021）所假设的，Google搜索可能通过依赖各种相关性信号（包括专家注释、用户点击、超链接以及各种问题类型的专门QA组件）来将这些自然查询映射到答案。使用这些注释作为真实标签，我们评估模型在仅使用答案帖子的自由文本（即，无超链接、用户点击、问题标题或正文等）的情况下进行检索的能力，这对仅在公共数据集上训练的IR和NLP系统构成了显著挑战。

- StackExchange：围绕特定主题的问答社区集合，类似于早期还没那么抽象的知乎

- GooAQ：由最新的[Google自动完成$\xleftrightarrow{对应}$答案框]对，构成的[查询$\xleftrightarrow{对应}$结果]对数据库

  - 自动完成：根据历史/语义相关性/热门搜索，生成的对用户输入查询的自动补全

    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241209231007382.png" alt="image-20241209231007382" width=250 /> 

    ```txt
    用户输入查询: 今天天气
    候选自动完成: 今天天气怎么样, 今天天气温度, 今天天气好不好, ....
    ```

  - 答案框：从搜索结果的网页中，提取的简短答案摘要

    <img src="https://raw.githubusercontent.com/DANNHIROAKI/New-Picture-Bed/main/img/image-20241209231055508.png" alt="image-20241209231055508" width=450 /> 

    ```txt
    用户输入查询: 今天天气
    选定自动完成: 今天天气怎么样
    检索得答案框: 新加坡天气,目前在新加坡,15:43-9十二月,周一,+25°C.....
    ```

- LoTTE文档/段落语料库：

  - 预料来源：从StackE.抽取的答案帖，内容涵盖并分为写作/娱乐/科学/技术/生活五个主题
  - 注意事项：仅使用答案帖子的自由文本，即去除超链接/HTML标签/QA组件等

- LoTTE查询语料库：

  - 搜索查询：

    - 筛选法则：若一个GooAQ查询的回答能映射到特定StackE.的答案帖，就将此查询加入LoTTE
      - 此外还对相应帖子加以谷歌搜索的排名作为注释

    - 换言之：Google一个GooAQ查询，能搜到特定StackE.答案帖，就将此查询加入LoTTE
    - 特点：查询较短，回答具体且直接(快速直白的信息呈现)，**超出了维基百科等传统知识库范围**

  - 论坛查询：

    - 抽取法则：对StackE.中的标题(好比知乎问题)进行收集，并将其与相应回答对应
    - 频率倾向：优先选择热门的问题(标题)，并确保每个主题抽取占比均衡
    - 特点：查询较长，回答宽泛且复杂(往往带有讨论性质)，**超出了维基百科等传统知识库范围**

---

## 4.2. Forum Queries.  

我们通过提取StackExchange社区的帖子标题作为查询并收集其对应的答案帖子作为目标来收集论坛查询。我们按受欢迎程度选择问题，并根据每个主题中各社区的比例贡献对问题进行采样。

---

这些查询的种类往往比“搜索”查询更为广泛，而搜索查询可能表现出更自然的模式。表3对比了随机抽取的搜索和论坛查询。可以看出，搜索查询倾向于简短、基于知识的问题并有直接答案，而论坛查询往往反映出更开放式的问题。两组查询的主题均超出了一般知识库（如维基百科）的范围。

----

对于搜索和论坛查询，最终的评估集由一个查询和一个StackExchange答案帖子的目标集合组成（特别是目标StackExchange页面的答案帖子）。类似于开放问答文献中的评估（Karpukhin等人，2020；Khattab等人，2021b），我们通过计算成功率@5（S@5）来评估检索质量。具体来说，对于每个查询，如果系统在前5个结果中找到了来自目标页面的已接受或得票（得分≥1）的答案，则给予系统一个得分。

- 对LoTTE查询的评估
  - 评估集：一个查询$+$StackE.目标答案贴子集(查询的标签)
  - 评估方法：计算$\text{Scucess@5}$，即查询返回的前5个结果中有位于目标集的帖子，则系统得$1$分

---

附录D报告了每个主题的组成社区的细分，LoTTE的构建过程以及许可考虑和相关统计数据。图5和图6定量比较了搜索和论坛查询。

# 5. Evaluation  

我们现在对ColBERTv2在段落检索任务上的表现进行评估，测试其在训练域内（第5.1节）以及零样本情境下的训练域外（第5.2节）的质量。除非另有说明，我们在评估中将ColBERTv2的嵌入压缩为每维 $b=2$ 位。

## 5.1. In-Domain Retrieval Quality  

与相关研究类似，我们在MS MARCO Passage Ranking（Nguyen等人，2016）上进行信息检索任务的训练。表4展示了我们的开发集结果，比较了ColBERTv2、原始ColBERT以及最先进的单向量系统。

---

虽然ColBERT优于单向量系统，如RepBERT、ANCE，甚至TAS-B，但跨编码器的蒸馏等监督改进使得SPLADEv2、PAIR和RocketQAv2等系统的质量超过了原始ColBERT。这些监督增益对细粒度后期交互的价值提出了挑战，且不确定类似ColBERT模型的更强归纳偏置是否允许其在蒸馏下获得类似的增益，尤其是在使用压缩表示时。尽管如此，我们发现，通过去噪监督和残差压缩，ColBERTv2在所有系统中达到了最高质量。正如我们在第5.3节中讨论的那样，它的空间占用与这些单向量模型竞争，且远低于原始ColBERT。

---

除了官方开发集外，我们还在Khattab和Zaharia（2020）为MS MARCO描述的“本地评估”测试集上评估了ColBERTv2、SPLADEv2和RocketQAv2，该测试集包含5000个与训练集和官方开发集不重叠的查询。这些查询取自MS MARCO Passage Ranking任务中提供的标注的5万查询，作为额外验证数据${ }^4$。在该测试集上，ColBERTv2获得了40.8%的MRR@10，显著超越了基线系统，包括RocketQAv2（该系统在段落文本之外还使用了文档标题，其他系统则未使用）。

- 官方评估：

  - 数据集：官方预定义好的MS MARCO Passage Ranking上，与训练集高度相似

  - 评估对象：ColBERT，ColBERTv2，单向量系统

  - 评估结果：
    - ColBERT优于普通单向量系统(RepBERT/ANCE/TAS-B)
      - 体现了细颗粒度后期交互的价值
    - ColBERT不及高度监督调优的单向量系统(SPLADEv2/PAIR/RocketQAv2)，
      - 改进监督能让单向量模型性能大幅提升
    - ColBERTv2性能超越了所有系统，且空间占用只和单向量模型相当
      - ColBERT也能从改进监督(去噪监督/残差压缩)中获得大幅性能提升

- 本地评估：

  - 数据集：从MS MARCO Passage Ranking中抽取的若干查询集，与训练集严格不重叠以评估泛化能力
  - 评估对象：ColBERTv2/SPLADEv2/RocketQAv2
  - 评估结果：ColBERTv2远超Baseline

## 5.2. Out-of-Domain Retrieval Quality  

接下来，我们使用BEIR（Thakur等人，2021）、Wikipedia开放问答检索（Khattab等人，2021b）和LoTTE对ColBERTv2在训练域外进行评估。我们与文献中的广泛最新和最先进的检索系统进行了比较。

### 5.2.1. BEIR.  

我们首先评估BEIR，报告不包含跨编码器蒸馏的模型的质量，包括ColBERT（Khattab和Zaharia，2020）、DPR-MARCO（Xin等人，2021）、ANCE（Xiong等人，2020）和MoDIR（Xin等人，2021），以及包含蒸馏的模型，包括TAS-B（Hofstätter等人，2021）、SPLADEv2（Formal等人，2021a）和我们使用MS MARCO训练的官方检查点测试的RocketQAv2。我们将表格按“搜索”（即自然查询和问题）和“语义相关性”（如引文相关性和声明验证）任务划分，以反映每个数据集的查询性质${ }^5$。

- 评估模型
  - 无蒸馏模型：ColBERT/DPR-MARCO/ANCE/MoDIR
  - 有蒸馏模型：TAS-B/SPLADEv2/RocketQAv2
- 评估任务：分别在搜索(自然查询)/语义相关性二者上进行评估

---

表5a报告了官方的nDCG@10指标结果。在不使用蒸馏的模型中，我们看到原始ColBERT模型在除了三个任务外的所有任务上都优于单向量系统DPR、ANCE和MoDIR。ColBERT通常远超这三个系统，事实上在大多数数据集上甚至超过了使用蒸馏的TAS-B模型。转向蒸馏模型，我们看到类似的模式：虽然基于蒸馏的模型通常比其原始模型更强，但分解评分为词级交互的模型，如ColBERTv2和SPLADEv2，几乎总是最强的。

- 评估结果
  - 无蒸馏组：原始ColBERT的表现优于单向量系统(DPR/ANCE/MoDIR)
  - 有蒸馏组：蒸馏后的模型都比原始模型更强，词级蒸馏模型(ColBERTv2/SPLADEv2)强于其它模型

---

进一步比较SPLADEv2和ColBERTv2，我们发现ColBERTv2在六个基准上具有优势，并在两个基准上与SPLADEv2持平，其中在NQ、TREC-COVID和FiQA-2018上的提升最大，这些数据集均包含自然搜索查询。另一方面，SPLADEv2在五个基准上领先，特别是在Climate-FEVER（C-FEVER）和HotPotQA上表现出显著提升。在C-FEVER中，输入查询为气候相关声明句子，因此不具备典型的搜索查询特征。而在HotPotQA中，查询由能够访问目标段落的众包工人编写，已知这会导致人为的词汇偏差（Lee等人，2019），众包工人会在问题中复制段落中的词汇，这与Open-SQuAD基准相似。

- 对比分析：ColBERTv2/SPLADEv2
  - ColBERTv2的优势：在自然查询有关的基准上表现更佳
  - SPLADEv2的优势：在语义相关性更复杂/查询偏离自然语言的数据集上表现更优

---

### 5.2.2. Wikipedia Open QA.  

作为域外泛化的进一步测试，我们评估了MS MARCO训练的ColBERTv2、SPLADEv2和原始ColBERT在开放域问答检索中的表现，类似于Khattab等人（2021b）的域外设置。我们报告成功率@5（有时称为Recall@5），即短答案字符串与前5个段落之一重叠的问题百分比。查询使用开放域版本的开发集问题（Lee等人，2019；Karpukhin等人，2020），包括Natural Questions（NQ；Kwiatkowski等人，2019）、TriviaQA（TQ；Joshi等人，2017）和SQuAD（Rajpurkar等人，2016）数据集的开发集问题，见表5b。作为基线，我们包含了使用Anserini（Yang等人，2018a）工具包的BM25（Robertson等人，1995）结果。我们观察到，ColBERTv2在三个查询集上均优于BM25、原始ColBERT和SPLADEv2，比SPLADEv2的最高提升达到4.6个百分点。

- 语料库
  - 查询来源：Natural Questions(维基百科中的短文本)，TriviaQA，SQuAD
  - 段落来源：维基百科所有内容聚合的总库
- 评估模型：ColBERTv2/ColBERT/SPLADEv2/BM25
- 评估结果：以$\text{Scucess@5}$为指标，ColBERTv2优于所有模型，SPLADEv略逊之

### 5.2.3. LoTTE.  

接下来，我们分析LoTTE测试基准上的表现，该基准聚焦于长尾主题的自然查询，并展示了不同于先前域外评估数据集的注释模式。特别是，LoTTE使用自动Google排名（针对“搜索”查询）和自然生成的StackExchange问答对（针对“论坛”查询），补充了如TREC-COVID（在BEIR中）等数据集的基于汇总的注释和开放问答检索中的答案重叠指标。我们报告了每个语料库在搜索查询和论坛查询上的成功率@5。

---

总体来看，我们发现ANCE和原始ColBERT在所有主题上均优于BM25，并且使用蒸馏的三种方法总体表现最强。与Wikipedia-OpenQA结果类似，我们发现ColBERTv2在所有主题和查询类型上均优于基线系统，比SPLADEv2和RocketQAv2分别提升了最多3.7和8.1个百分点。考虑到基线系统，我们观察到RocketQAv2在“搜索”查询上相对SPLADEv2具有些许优势，而SPLADEv2在“论坛”测试中则显著更有效。我们推测，来自Google的搜索查询（通过GooAQ获取）比论坛查询更接近MS MARCO，因此后者更强调泛化，利于像SPLADEv2和ColBERTv2这样的词项分解模型。  

- 评估结果：
  - 传统模型：BM25特别烂，ANCE/ColBERT显著好很多
  - 蒸馏模型：SPLADEv2/RocketQAv2/ColBERTv2整体都很强，但ColBERTv2略微胜出

## 5.3. Efficiency  

ColBERTv2的残差压缩方法显著减少了索引大小，相比原始ColBERT，存储MS MARCO的索引时，ColBERT需要154 GiB，而ColBERTv2在将嵌入压缩为每维1或2位时，仅需16 GiB或25 GiB，压缩比达到6-10倍。此存储数据包括存储倒排列表的4.5 GiB。

- 残差压缩的效果：在MS MARCO上的编码索引大小
  - ColBERT要154GB，ColBERTv2在b=1/b=2时只需16/25GB(缩小6-10倍)
  - 甚至只与采取激进压缩策略的单向量模型齐平

---

这与MS MARCO上的典型单向量模型存储相当，后者使用4字节无损浮点存储，每个9百万段落存储一个768维向量，存储总量略超25 GiB。实际上，单向量模型在使用像HNSW这样的最近邻索引进行快速搜索时，存储需求可能更大。相反，单向量表示本身可以被非常激进地压缩（Zhan等人，2021a, 2022），但通常相对于ColBERTv2等后期交互方法在质量上会有更大损失。

---

我们在附录B中讨论了压缩方法对搜索质量的影响，并在附录C中展示了每次查询延迟约为50-250毫秒的结果。

# 6. Conclusion  

我们介绍了ColBERTv2，这是一种提升多向量表示质量和空间效率的检索模型。我们假设聚类质心捕捉了词级表示的上下文语义，并提出了残差表示，利用这些模式显著减少多向量系统的存储占用。然后我们探索了多向量检索的改进监督，发现通过跨编码器系统的蒸馏可以显著提高其质量。所提出的ColBERTv2在我们对28个数据集的广泛评估中，无论是在域内还是域外，都显著优于现有检索器，达到了最先进的质量，同时展示出具有竞争力的存储占用。

- 废话一堆

## 6.1. Broader Impact & Ethical Considerations  

本研究主要致力于开发具有更好泛化能力且在空间消耗方面具有合理效率的检索模型。对小型领域应用的开箱即用的强泛化性能可以在实践中为许多用户服务，尤其是在没有训练数据的情况下。此外，检索对许多下游NLP任务具有重要意义，因为它可以帮助缩小语言模型的规模，从而提高效率（即，通过将知识与计算解耦），提升透明性（即，允许用户查看模型在进行陈述或预测时依赖的来源），并更容易更新（即，允许开发人员通过替换或添加语料库中的文档而无需重新训练模型）（Guu等人，2020；Borgeaud等人，2021；Khattab等人，2021a）。尽管如此，这项工作在滥用方面存在风险，尤其是在错误信息方面，因为检索可能会展示出相关但不准确的结果，具体取决于语料库的内容。此外，在大规模数据集上训练的泛化能力可能会将该数据集的偏见传播到新的领域和应用中。 

- 还是一堆废话

---

尽管我们的贡献使ColBERT的后期交互在存储成本上更高效，但相比于原始ColBERT模型的简单训练范式，大规模蒸馏和硬负样本增加了系统复杂性，进而增加了训练成本。虽然ColBERTv2在推理时的延迟和存储方面效率较高，但在极端资源受限的情况下，我们怀疑像SPLADEv2或RocketQAv2这样的更简单模型设计可能更适合优化环境。我们将所有系统的低层系统优化留待未来研究。另一值得探索的权衡方向是基于交叉编码器的重新排序架构，尽管交叉编码器由于其高表达能力而代价昂贵，但通常表现出较高精确度。

- 增加了训练的成本

## 6.2. Research Limitations  

尽管我们在广泛的测试上评估了ColBERTv2，但所有基准都是英文的，并且按照相关工作，我们的域外测试使用了在MS MARCO上训练的模型。我们预计该方法在其他语言和使用其他更小的训练集（例如NaturalQuestions）训练时也能有效，但将这类测试留待未来研究。

---

我们在多种不同环境下观察到ColBERTv2相对于现有最先进系统的稳定增益。尽管如此，几乎所有IR数据集都包含假阴性（即，相关但未标注的段落），因此在解读任何单个结果时需要谨慎。然而，我们特意选择了带有不同注释偏差的基准数据集，例如，TREC-COVID（在BEIR中）注释了提交系统在竞赛时检索的文档池，LoTTE使用了自动Google排名（针对“搜索”查询）和StackExchange问答对（针对“论坛”查询），而开放问答测试依赖于事实性问题的段落-答案重叠。ColBERTv2在这些设置中表现良好。我们在附录§D中讨论了与LoTTE相关的其他问题。

---

我们与广泛的强基线进行了比较——包括稀疏检索和单向量模型——并在测试中发现了可靠的模式。然而，我们提醒读者，随着创新引入到这些模型系列中，实证趋势可能会发生变化，并且在模型系列之间确保完全一致的对比（apple-to-apple comparison）是很难的，因为每个系列都需要不同的复杂调优策略。因此，我们主要使用了这些问题的丰富最新文献中的结果和模型，如RocketQAv2和SPLADEv2。

---

在表示方面，我们专注于通过残差压缩来减少存储成本，在减少占用空间方面取得了显著进展，同时在很大程度上保留了质量。然而，我们尚未穷尽可能的更复杂的优化空间，并且我们预计更复杂的残差压缩形式和与令牌丢弃（Zhou和Devlin，2021）相结合的方法将为进一步减少空间占用开辟更多可能性。

- 其它限制：
  - 仅在英文基准上评估
  - 评估的数据集上假阴性标注普遍存在
  - 难以与经过了高度调优的基线一致性对比