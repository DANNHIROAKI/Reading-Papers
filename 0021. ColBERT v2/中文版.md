# 0. Abstract

神经信息检索（IR）极大地推动了搜索和其他知识密集型语言任务的发展。虽然许多神经IR方法将查询和文档编码为单一向量表示，但后期交互模型在每个Token的粒度上生成多向量表示，并将相关性建模分解为可扩展的Token级计算。研究表明，这种分解使得后期交互更加有效，但也使这些模型的空间占用增加了一个数量级。在本研究中，我们引入了ColBERTv2，一种结合了激进的残差压缩机制和降噪监督策略的检索模型，旨在同时提高后期交互的质量和空间占用。我们在广泛的基准测试中评估了ColBERTv2，证明其在训练域内外均达到了最新的质量水平，同时将后期交互模型的空间占用减少了6至10倍。

# 1. Intro

神经信息检索（IR）在过去的$2-3$年间迅速主导了搜索领域，不仅极大地推动了段落和文档搜索的发展（Nogueira 和 Cho，2019），还在许多知识密集型的自然语言处理任务中取得了显著进展，例如开放域问答（Guu等，2020）、多跳声明验证(multi-hop claim verification  )（Khattab等，2021a）以及开放式生成（Paranjape等，2022）。

- 信息检索：
  - 含义：理解用户查询$/$文档之间的语义关系
  - 种类：传统信息检索(基于关键词捕捉意义匹配)，神经信息检索(基于深度学习捕捉意义匹配)

---

许多神经IR方法遵循单向量相似性范式：使用预训练的语言模型将每个查询和文档编码为一个高维向量，并通过两者之间的简单点积来建模相关性。另一种方法是ColBERT（Khattab和Zaharia，2020）中提出的后期交互，其中查询和文档被编码为细粒度的多向量表示，并通过这两组向量之间丰富而可扩展的交互来估计相关性。ColBERT为查询（和文档）中的每个Token生成一个嵌入，并将相关性建模为每个查询向量与文档中所有向量之间最大相似度的总和。

- 单向量神经IR：(查询$+$文档)$\xrightarrow[文档级]{预训练模型}$(查询高维向量$+$文档高维向量)$\xrightarrow{互相点积}$相似度
- 后期交互神经IR：(查询$+$文档)$\xrightarrow[细颗粒度/\text{Token级}]{预训练模型}$(查询多向量集$+$文档多向量集)$\xrightarrow{丰富/可扩展的后期交互}$相似度

---

通过将相关性建模分解为Token级计算，后期交互旨在减轻编码器的负担：单向量模型必须通过一个点积捕捉复杂的查询-文档关系，而后期交互则在Token级编码含义，并将查询-文档匹配交给交互机制。这种增加的表达能力是有代价的：现有的后期交互系统的空间占用比单向量模型大一个数量级，因为它们必须为网络规模的集合存储数十亿个小向量。考虑到这一挑战，似乎更有成效的方式是通过引入新的负样本挖掘（Xiong等人，2020）、预训练（Gao和Callan，2021）和蒸馏（Qu等人，2021）监督范式，来解决单向量模型的脆弱性（Menon等人，2022）。确实，最近的单向量模型在高度调优的监督策略下（Ren等人，2021b；Formal等人，2021a）有时表现与“原生”后期交互模型相当，甚至更好，而且后期交互架构——由于其固有的Token级归纳偏差——是否能从改进的监督中获得类似的大幅提升，也并不明确。

- 后期交互的代价：需存储的嵌入量很大，空间占比较单向量模型大一个数量级
- 单向量模型的优化：
  - 正负样本挖掘
    - 正/负样本：与用户查询相关/无关的文档
    - 挖掘方式：在训练时，让模型学会区分与查询相关和不相关的文档
  - 蒸馏：
    - 含义：训练一个小型的“学生”模型，来模仿一个大型的“教师”模型的行为
    - 目的：减小模型规模，但同时保持其表达能力
- 单向量模型的优化效果：在高度调优的监督策略下，单向量模型在许多任务中与后期交互模型匹敌
- 后期交互模型的缺陷：存在固有的Token级归纳偏差
  - 含义：即更关注Token级相似性而非上下文(全局)相似性
  - 影响：适用于单向量模型的调优/强化监督策略，可能不会给后期交互模型带来太多性能提升

---

在本研究中，我们展示了后期交互检索器能够自然地生成==轻量级的Token表示==，这些表示可以直接高效存储，并且能够极大地受益于==降噪监督==。我们在ColBERTv2中结合了这些特点。ColBERTv2 ${ }^1$ 是一种新的后期交互检索器，采用了来自==交叉编码器==的简单==蒸馏==方法和==强负样本挖掘==(§3.2)，在提升质量上超越了任何现有方法，然后利用==残差压缩机制== (§3.3) 将后期交互的空间占用减少了6-10倍，同时保持了质量。因此，ColBERTv2在训练域内外都建立了最先进的检索质量，并在空间占用方面与典型的单向量模型相当。

- 存储空间方面：降到了与单向量模型相当
  - 轻量级的Token表示，大幅减少存储需求
  - 残差压缩：具体是压缩个啥看后面的吧
- 训练策略方面：建立了最先进的检索质量
  - 降噪监督：训练过程中加入噪声样本，使模型能过滤出有效的信息，增强其鲁棒性
  - 交叉编码器的蒸馏：通过蒸馏，从一个强大的交叉编码器中提取出有价值的信息，*并将其传递给ColBERTv2* (?)
  - 强负样本挖掘：挑选出困难的负样本(与查询高度相似但实际无关的文档)，ColBERTv2能够更好地学习到正负样本之间的差异

---

在MS MARCO Passage Ranking数据集上训练时，ColBERTv2实现了所有独立检索器中最高的MRR@10。除了在域内的高质量表现外，我们还寻求一种能够“零样本”泛化到特定领域语料库和长尾主题的检索器，这些主题往往在大型公共训练集中代表性不足。为此，我们在大量域外基准测试上评估了ColBERTv2，这些基准包括三个Wikipedia开放问答检索测试以及来自BEIR (Thakur et al., 2021) 的13个多样化的检索和语义相似性任务。

- 域内检索：
  - 含义：在模型训练数据集/语料库上，执行检索
  - 此处：模型在MS MARCO Passage Ranking上训练与检索，取得了极高的检索质量
- 零样本泛化：
  - 泛化含义：模型在没有特定领域训练数据的情况下，能够在全新的测试集/任务上(即零样本上)表现良好
  - 泛化目标：在特定领域语料库/长尾主题检索效果良好
    - 长尾主题：某个领域内数量庞大但极为低频的话题，类似于Zipf定律
    - 特定领域/长尾主题特点：在公共训练集中，这类话题的出现较少
- 泛化测试：
  - 测试内容：在域外基准数据上测试ColBERTv2
  - 评估标准：
    - Wikipedia开放问答：模型在Wikipedia语料上回答开放问题的能力
    - BEIR基准测试：包含13个多样化任务，许多是语义相似性任务
    - LoTTE基准：更关注自然搜索查询

---

此外，我们引入了一个新的基准，称为LoTTE，用于长尾主题分层评估，包含12个特定领域的搜索测试，覆盖了StackExchange社区并使用了GooAQ (Khashabi et al., 2021)中的查询。与开放问答测试和许多BEIR任务不同，LoTTE在其段落中专注于相对长尾的主题，并评估模型回答具有实际意图的自然搜索查询的能力，而非BEIR的许多语义相似性任务。在28个域外测试中的22个上，ColBERTv2达到了最高质量表现，其相对增益超过了第二最佳检索器达8%，并使用了压缩后的表示。

- 关于LoTTE测试：
  - 是啥：一个新的基准测试，专门评估模型在长尾主题上的检索能力
  - 测试内容：12个特定领域的检索测试
    - 领域数据：来自StackExchange社区(包含诸如Stack Overflow/Ask Ubuntu等)
    - 查询数据：来自GooAQ(包含了多样的问题和答案)
  - 测试目标：面对自然查询(用户带有实际意图)时，能否高效地从长尾主题中抽取出相关答案
- ColBERTv2的表现：
  - 即使压缩了表示，在**28个域外测试**中的22个测试取得了最高质量表现→零样本泛化能力强

---

本研究的主要贡献如下：

1. 我们提出了ColBERTv2，这是一种结合了==降噪监督==和==残差压缩==的检索模型，利用后期交互的Token级分解，在减少空间占用的同时实现了高鲁棒性。

2. 我们引入了LoTTE，一种用于域外检索器评估的新资源。LoTTE专注于围绕长尾主题的自然信息查询，这是一个==重要但尚未被充分研究的应用领域==。

3. 我们在广泛的环境中评估了ColBERTv2，确立了其在训练域内外的最先进质量水平。

# 2. Background & Related Work  

## 2.1. Token-Decomposed Scoring in Neural IR  

许多神经信息检索（IR）方法将段落编码为一个高维向量，以提升效率和可扩展性，权衡了交叉编码器的较高质量（Karpukhin等人，2020；Xiong等人，2020；Qu等人，2021）。ColBERT（Khattab和Zaharia，2020）的后期交互模式通过计算多向量嵌入并使用可扩展的“MaxSim”操作符进行检索，解决了这一权衡问题。其他一些系统也利用了多向量表示，包括Poly-encoders（Humeau等人，2020）、PreTTR（MacAvaney等人，2020）和MORES（Gao等人，2020），但这些方法主要用于基于注意力的重新排序，而非ColBERT的可扩展MaxSim端到端检索。

- 不同IR方法的权衡
  - 将查询/文档编码为单一高维向量：成本低，但会倾向于简化语义表示
  - 交叉编码器(如BERT)：成本高，但能够通过更细粒度的交互精确捕捉语义
- 多向量方法的改良：基于ColBERT式的后期交互
  - 意义：同时兼顾了编码成本/检索质量
  - 操作：
    - 嵌入：在查询前，将文档/段落编码为多向量
    - 查询：临时再去编码查询成查询嵌入集，然后再计算每个查询嵌入与所有段落嵌入的MaxSim，再相加
  - 优势：保留了细粒度的交互信息，同时减轻了嵌入成本
- 多向量方法：基于注意力重排
  - 注意力重排是啥：
    - 先用计算量小的方法(词袋模型)，对文档大规模排序
    - 再用计算量大的方法(注意力/BERT/交叉编码)，对初步排序结果进行小规模细颗粒度重排
  - 模型实例：Poly-encoders/PreTTR/MORES

---

ME-BERT（Luan等人，2021）生成类似于ColBERT的词级文档嵌入，但对查询仅保留一个嵌入向量。COIL（Gao等人，2021）也生成词级文档嵌入，但词的交互限制在查询和文档词项之间的词汇匹配。uniCOIL（Lin和Ma，2021）将COIL的词嵌入向量限制为单一维度，将其简化为扩展了DeepCT（Dai和Callan，2020）和DeepImpact（Mallia等人，2021）等模型的标量权重。为了生成标量权重，SPLADE（Formal等人，2021b）和SPLADEv2（Formal等人，2021a）生成一个稀疏的词汇级向量，保留了后期交互的词项级分解，同时将存储简化为每个词一维。SPLADE系列还依托BERT在预训练期间获得的语言建模能力。SPLADEv2已被证明在不同领域内外均具有很高的有效性，并且是本文实验中对比的核心点。

==几种基于BERT的文档嵌入方法== 

- ME-BERT：为每个文档生成词级多向量表示，但只为查询生成单向量表示
- COIL：为文档/查询都生成词级嵌入，但是交互仅限于文档-查询匹配的词汇
- uniCOIL：交互原理同COIL，区别在于将COIL的词级嵌入压缩到一维(权重标量)
- SPLADE：为查询/文档生成词汇级的稀疏向量，查询和文档进行词级的后期交互

## 2.2. Vector Compression for Neural IR  

最近，对信息检索（IR）表示的压缩研究兴趣激增。Izacard等人（2020）研究了单向量检索器的维度缩减、积量化（PQ）和段落过滤。BPR（Yamada等人，2021a）通过可微的tanh函数将嵌入直接哈希为二进制代码。JPQ（Zhan等人，2021a）及其扩展RepCONC（Zhan等人，2022）使用PQ压缩嵌入，并通过排序导向的损失联合训练查询编码器与PQ产生的质心。

- PQ：先训练查询编码器和文档编码器，再对嵌入应用PQ压缩
- JPQ：将训练-压缩过程合在一起，在训练的时候就同时优化编码器和质心分布
- BPR：通过tanh函数将嵌入(浮点型)哈希映射为二进制编码，以高效存储与计算距离

---

SDR（Cohen等人，2021）使用自编码器降低用于注意力重排序的上下文化嵌入的维度，并应用量化方案进一步压缩。DensePhrases（Lee等人，2021a）用于开放问答，依赖段落的多向量编码，但搜索基于单个向量且未使用后期交互。Lee等人（2021b）提出基于PQ的量化感知微调，减少DensePhrases的空间占用。尽管DensePhrases在开放问答中有效，但在NaturalQuestions和TriviaQA上的前20检索准确率上与DPR相当，远低于ColBERT。 

- SDR：使用自编码器将原始嵌入映射到更低语义空间，再对低维语义空间向量应用PQ压缩
- DensePhrases：为每个段落表示为多向量，将每个查询表示为单向量，无后期交互
  - 压缩优化：基于PQ的量化感知微调，==直接忽略这个模型吧挺垃圾的== 

---

在本研究中，我们聚焦于后期交互检索，并使用可直接应用于后期交互模型的残差压缩方法。附录A显示，ColBERT的表示自然适用于残差压缩。残差压缩技术应用广泛（Barnes等人，1996），已用于近似最近邻搜索（Wei等人，2014）、神经网络量化（Li等人，2021a，2021b）以及分布式深度学习（Chen等人，2018；Liu等人，2020）。据我们所知，ColBERTv2是第一个在可扩展的神经IR中使用残差压缩的方法。

- 残差压缩是啥
- ==ColBERT的表示自然适用于残差压缩==

## 2.3. Improving the Quality of Single-Vector Representations

与我们进行多向量表示的压缩不同，许多最新研究致力于提高单向量模型的质量，这些模型通常对监督细节非常敏感。这类研究可分为三个方向：(1) 更具表现力的架构的蒸馏（Hofstätter等人，2020；Lin等人，2020），包括显式去噪（Qu等人，2021；Ren等人，2021b），(2) 硬负采样（Xiong等人，2020；Zhan等人，2020a，2021b），以及 (3) 改进的预训练（Gao和Callan，2021；Oguz等人，2021）。我们在ColBERTv2的多向量表示中采用了类似于(1)和(2)的技术（参见第3.2节）。

## 2.4. Out-of-Domain Evaluation in IR  

检索的最新进展主要集中在大数据评估上，其中数万条带有标注的训练查询与测试领域相关，例如MS MARCO或Natural Questions（Kwiatkowski等人，2019）。在这些基准中，查询往往反映维基百科中高人气的主题，如电影和运动员。实际上，面向用户的IR和QA应用通常涉及特定领域的语料库，而这些语料库几乎没有可用的训练数据，且在大型公共集合中相关主题的代表性不足。

---

这种域外情形最近受到了BEIR（Thakur等人，2021）基准的关注。BEIR将若干现有数据集结合成一个异质套件，用于“零样本IR”任务，涵盖生物医学、金融和科学等领域。虽然BEIR数据集提供了有用的测试平台，但其中许多任务主要涉及广泛的语义相关性任务，如引文、反驳或重复问题，而非自然搜索任务，或是关注像维基百科中那样的高人气实体。在第4节中，我们引入了LoTTE，一个用于域外检索的新数据集，展示了长尾主题的自然搜索查询。

# 3. ColBERTv2  

我们现在介绍ColBERTv2，它在提高多向量检索模型质量的同时（第3.2节），减少了它们的空间占用（第3.3节）。

## 3.1. Modeling  

ColBERTv2采用了ColBERT的后期交互架构，如图1所示。查询和段落独立地用BERT（Devlin等人，2019）编码，每个词的输出嵌入被投影到较低维度。在离线索引期间，语料库中的每个段落 $d$ 被编码为一组向量，这些向量随后被存储。在搜索时，查询 $q$ 被编码为多向量表示，其与段落 $d$ 的相似度计算为查询端“MaxSim”操作的求和，即每个查询词嵌入与所有段落词嵌入之间的最大余弦相似度的总和：

- $\displaystyle{}S_{q, d}=\sum_{i=1}^N \max _{j=1}^M Q_i \cdot D_j^T$ 

其中，$Q$ 是一个矩阵，用 $N$ 个向量编码查询，而 $D$ 用 $M$ 个向量编码段落。该架构的直观理解是将每个查询词与最具上下文相关性的段落词对齐，量化这些匹配，然后在查询上合并部分得分。关于后期交互的详细讨论，我们参考Khattab和Zaharia（2020）。

## 3.2. Supervision  

训练神经检索模型通常需要训练集中每个查询的正负段落。Khattab和Zaharia（2020）使用MS MARCO的官方三元组$\left\langle\mathrm{q}, \mathrm{d}^{+}, \mathrm{d}^{-}\right\rangle$来训练ColBERT。对于每个查询，一个正样本 $d^{+}$ 由人工标注，每个负样本 $d^{-}$ 从BM25检索的未标注段落中采样。

---

后续研究发现了这种标准监督方法的几个缺陷（见第2.3节）。我们的目标是采用一个简单且一致的监督方案，选择具有挑战性的负样本，避免奖励假阳性或惩罚假阴性。为此，我们首先使用Khattab等人（2021b）提出的三元组训练ColBERT模型，并使用ColBERTv2的压缩方式对训练段落进行索引。

---

对于每个训练查询，我们检索前k个段落。我们将每个查询-段落对输入到一个交叉编码器重排序模型中。我们使用由Thakur等人（2021）通过蒸馏训练的22M参数MiniLM（Wang等人，2020）交叉编码器。该小模型表现非常出色，同时推理效率较高，适合用于蒸馏。

---

接下来，我们收集由查询、一个高排名段落（或标注的正样本）和一个或多个低排名段落组成的$w$-路元组。在本研究中，我们为每个样本使用 $w=64$ 个段落。类似于RocketQAv2（Ren等人，2021b），我们使用KL散度损失将交叉编码器的评分蒸馏到ColBERT架构中。我们选择KL散度，因为ColBERT生成的分数（即余弦相似度之和）尺度受限，可能无法直接与交叉编码器的输出分数对齐。我们还在每个GPU上使用批内负样本，并对每个查询的正样本分数与同批次其他查询对应段落的分数应用交叉熵损失。我们重复此过程一次以刷新索引并重新采样负样本。

---

使用硬负样本的去噪训练在最近的研究中被认为是缩小单向量模型与交互式模型之间差距的有效方法，包括像ColBERT这样的后期交互架构。我们的结果（见第5节）表明，这种监督方式可以显著提高多向量模型的效果，达到最先进的检索质量。

## 3.3. Representation  

我们假设ColBERT的向量聚集在能够捕捉高度特定词语语义的区域。我们在附录A中测试了这一假设，证据表明每个词义对应的向量聚集在一起，因上下文变化仅存在轻微差异。我们利用这一规律，通过残差表示显著减少了后期交互模型的空间占用，完全无需架构或训练的改动。给定一组质心 $C$，ColBERTv2将每个向量 $v$ 编码为其最近质心 $C_t$ 的索引以及一个量化向量 $\tilde{r}$，用于近似残差 $r=v-C_t$。在检索时，我们使用质心索引 $t$ 和残差 $\tilde{r}$ 来恢复一个近似的 $\tilde{v}=C_t+\tilde{r}$。

---

为了编码 $\tilde{r}$，我们将 $r$ 的每个维度量化为一或两位。原则上，$n$ 维向量的 $b$ 位编码需要每个向量 $\lceil\log |C|\rceil+b n$ 位。在实际操作中，当 $n=128$ 时，我们使用四字节表示最多 $2^{32}$ 个质心，并使用16或32字节（对于 $b=1$ 或 $b=2$）来编码残差。每个向量总共需要20或36字节，与ColBERT使用16位精度的256字节向量编码相比具有显著优势。尽管存在多种压缩替代方案，我们发现这种简单的编码在大幅降低存储成本的同时，基本保持了模型质量，相比典型的32位或16位精度的后期交互系统存储更为高效。

---

这种基于质心的编码可以看作是积量化（PQ）在多向量表示中的自然扩展。积量化（Gray, 1984；Jegou等人，2010）通过将单一向量分割成小子向量并使用代码本中的ID编码每个子向量来实现压缩。在我们的方法中，每个表示本身已是一个矩阵，自然分为若干小向量（每个词一个）。我们通过最近的质心加残差来编码每个向量。有关压缩对检索质量的影响测试以及与类似于BPR（Yamada等人，2021b）的ColBERT基线压缩方法的对比，参见附录B。

## 3.4. Indexing  

给定一个段落语料库，索引阶段预先计算所有段落嵌入，并组织其表示以支持快速最近邻搜索。ColBERTv2将索引划分为三个阶段，如下所述。

### 3.4.1. Centroid Selection.  

在第一阶段，ColBERTv2选择一组聚类质心 $C$。这些质心用于支持残差编码（第3.3节）以及最近邻搜索（第3.5节）。通常，我们发现将 $|C|$ 设置为语料库中嵌入数量 $n_{\text {embeddings }}$ 的平方根的比例在实践中效果良好${ }^3$。Khattab和Zaharia（2020）仅在计算所有段落的表示后进行向量聚类，但这需要未压缩地存储它们。为了减少内存消耗，我们将 $k$ 均值聚类应用于通过BERT编码器处理的部分段落嵌入，这些段落的样本量与集合大小的平方根成比例。我们发现这一方法在实践中表现良好。

### 3.4.2. Passage Encoding.  

选定质心后，我们对语料库中的每个段落进行编码。这需要调用BERT编码器并按照第3.3节所述压缩输出嵌入，将每个嵌入分配到最近的质心并计算量化的残差。一批段落编码完成后，将压缩后的表示保存到磁盘。

### 3.4.3. Index Inversion  

为支持快速最近邻搜索，我们将对应于每个质心的嵌入ID分组，并将此倒排列表保存到磁盘。在搜索时，这使我们能够快速找到与查询中的嵌入相似的词级嵌入。

## 3.5. Retrieval  

给定查询表示 $Q$，检索从候选生成开始。对于查询中的每个向量 $Q_i$，找到最近的 $n_{\text {probe }} \geq 1$ 个质心。利用倒排列表，ColBERTv2识别出接近这些质心的段落嵌入，解压它们，并计算它们与每个查询向量的余弦相似度。然后将得分按查询向量的段落ID分组，并对相同段落的得分进行最大化归约。这使ColBERTv2能够对每个查询向量执行近似“MaxSim”操作。此操作使用通过倒排列表识别的嵌入计算出真实MaxSim（第3.1节）的下界，类似于Macdonald和Tonellotto（2021）为评分探索的近似方法，但在此用于候选生成。

---

这些下界在查询词上求和，并根据这些近似得分选择得分最高的 $n_{\text {candidate }}$ 个候选段落进行排序，加载每个段落的完整嵌入集，并使用文档中所有嵌入执行与公式1一致的评分函数。结果段落按得分排序并返回。

# 4. LoTTE: Long-Tail, Cross-Domain Retrieval Evaluation

我们引入了LoTTE（发音为“latte”），这是一个用于长尾主题分层评估的IR新数据集。为了补充BEIR（Thakur等人，2021）的域外测试（如第2.4节中所述），LoTTE专注于涉及长尾主题的自然用户查询，这些主题可能未被维基百科这样的以实体为中心的知识库所覆盖。LoTTE由12个测试集组成，每个测试集包含500-2000个查询和10万-200万个段落。

---

测试集按主题显式划分，每个测试集附带一个包含相关但不重叠的查询和段落的验证集。我们选择使段落文本不重叠，以促进更现实的域外迁移测试，允许在相关但不同的主题上进行最小的开发。测试集（和开发集）包括一个“汇总”设置。在汇总设置中，段落和查询在所有测试（或开发）主题中进行聚合，以评估在更大且更多样化的语料库上的域外检索。

---

表1概述了LoTTE的组成。我们从各种StackExchange论坛的答案帖子中提取主题和段落语料库。StackExchange是一个围绕特定主题（如“物理”或“自行车”）的问答社区集合。我们从五个主要领域中收集论坛：写作、娱乐、科学、技术和生活方式。为了评估检索器，我们收集了“搜索”和“论坛”查询，每个查询都与其语料库中的一个或多个目标答案帖子相关。表2展示了示例查询以及语料库中回答这些查询的帖子中的简短片段。

## 4.1. Search Queries.  

我们从GooAQ（Khashabi等人，2021）中收集搜索查询，这是一个包含Google搜索自动完成查询及其答案框的最新数据集，我们筛选出答案链接到特定StackExchange帖子的查询。正如Khashabi等人（2021）所假设的，Google搜索可能通过依赖各种相关性信号（包括专家注释、用户点击、超链接以及各种问题类型的专门QA组件）来将这些自然查询映射到答案。使用这些注释作为真实标签，我们评估模型在仅使用答案帖子的自由文本（即，无超链接、用户点击、问题标题或正文等）的情况下进行检索的能力，这对仅在公共数据集上训练的IR和NLP系统构成了显著挑战。

## 4.2. Forum Queries.  

我们通过提取StackExchange社区的帖子标题作为查询并收集其对应的答案帖子作为目标来收集论坛查询。我们按受欢迎程度选择问题，并根据每个主题中各社区的比例贡献对问题进行采样。

---

这些查询的种类往往比“搜索”查询更为广泛，而搜索查询可能表现出更自然的模式。表3对比了随机抽取的搜索和论坛查询。可以看出，搜索查询倾向于简短、基于知识的问题并有直接答案，而论坛查询往往反映出更开放式的问题。两组查询的主题均超出了一般知识库（如维基百科）的范围。

----

对于搜索和论坛查询，最终的评估集由一个查询和一个StackExchange答案帖子的目标集合组成（特别是目标StackExchange页面的答案帖子）。类似于开放问答文献中的评估（Karpukhin等人，2020；Khattab等人，2021b），我们通过计算成功率@5（S@5）来评估检索质量。具体来说，对于每个查询，如果系统在前5个结果中找到了来自目标页面的已接受或得票（得分≥1）的答案，则给予系统一个得分。

---

附录D报告了每个主题的组成社区的细分，LoTTE的构建过程以及许可考虑和相关统计数据。图5和图6定量比较了搜索和论坛查询。

# 5. Evaluation  

我们现在对ColBERTv2在段落检索任务上的表现进行评估，测试其在训练域内（第5.1节）以及零样本情境下的训练域外（第5.2节）的质量。除非另有说明，我们在评估中将ColBERTv2的嵌入压缩为每维 $b=2$ 位。

## 5.1. In-Domain Retrieval Quality  

与相关研究类似，我们在MS MARCO Passage Ranking（Nguyen等人，2016）上进行信息检索任务的训练。表4展示了我们的开发集结果，比较了ColBERTv2、原始ColBERT以及最先进的单向量系统。

---

虽然ColBERT优于单向量系统，如RepBERT、ANCE，甚至TAS-B，但跨编码器的蒸馏等监督改进使得SPLADEv2、PAIR和RocketQAv2等系统的质量超过了原始ColBERT。这些监督增益对细粒度后期交互的价值提出了挑战，且不确定类似ColBERT模型的更强归纳偏置是否允许其在蒸馏下获得类似的增益，尤其是在使用压缩表示时。尽管如此，我们发现，通过去噪监督和残差压缩，ColBERTv2在所有系统中达到了最高质量。正如我们在第5.3节中讨论的那样，它的空间占用与这些单向量模型竞争，且远低于原始ColBERT。

---

除了官方开发集外，我们还在Khattab和Zaharia（2020）为MS MARCO描述的“本地评估”测试集上评估了ColBERTv2、SPLADEv2和RocketQAv2，该测试集包含5000个与训练集和官方开发集不重叠的查询。这些查询取自MS MARCO Passage Ranking任务中提供的标注的5万查询，作为额外验证数据${ }^4$。在该测试集上，ColBERTv2获得了40.8%的MRR@10，显著超越了基线系统，包括RocketQAv2（该系统在段落文本之外还使用了文档标题，其他系统则未使用）。

## 5.2. Out-of-Domain Retrieval Quality  

接下来，我们使用BEIR（Thakur等人，2021）、Wikipedia开放问答检索（Khattab等人，2021b）和LoTTE对ColBERTv2在训练域外进行评估。我们与文献中的广泛最新和最先进的检索系统进行了比较。

### 5.2.1. BEIR.  

我们首先评估BEIR，报告不包含跨编码器蒸馏的模型的质量，包括ColBERT（Khattab和Zaharia，2020）、DPR-MARCO（Xin等人，2021）、ANCE（Xiong等人，2020）和MoDIR（Xin等人，2021），以及包含蒸馏的模型，包括TAS-B（Hofstätter等人，2021）、SPLADEv2（Formal等人，2021a）和我们使用MS MARCO训练的官方检查点测试的RocketQAv2。我们将表格按“搜索”（即自然查询和问题）和“语义相关性”（如引文相关性和声明验证）任务划分，以反映每个数据集的查询性质${ }^5$。

---

表5a报告了官方的nDCG@10指标结果。在不使用蒸馏的模型中，我们看到原始ColBERT模型在除了三个任务外的所有任务上都优于单向量系统DPR、ANCE和MoDIR。ColBERT通常远超这三个系统，事实上在大多数数据集上甚至超过了使用蒸馏的TAS-B模型。转向蒸馏模型，我们看到类似的模式：虽然基于蒸馏的模型通常比其原始模型更强，但分解评分为词级交互的模型，如ColBERTv2和SPLADEv2，几乎总是最强的。

---

进一步比较SPLADEv2和ColBERTv2，我们发现ColBERTv2在六个基准上具有优势，并在两个基准上与SPLADEv2持平，其中在NQ、TREC-COVID和FiQA-2018上的提升最大，这些数据集均包含自然搜索查询。另一方面，SPLADEv2在五个基准上领先，特别是在Climate-FEVER（C-FEVER）和HotPotQA上表现出显著提升。在C-FEVER中，输入查询为气候相关声明句子，因此不具备典型的搜索查询特征。而在HotPotQA中，查询由能够访问目标段落的众包工人编写，已知这会导致人为的词汇偏差（Lee等人，2019），众包工人会在问题中复制段落中的词汇，这与Open-SQuAD基准相似。

---

### 5.2.2. Wikipedia Open QA.  

作为域外泛化的进一步测试，我们评估了MS MARCO训练的ColBERTv2、SPLADEv2和原始ColBERT在开放域问答检索中的表现，类似于Khattab等人（2021b）的域外设置。我们报告成功率@5（有时称为Recall@5），即短答案字符串与前5个段落之一重叠的问题百分比。查询使用开放域版本的开发集问题（Lee等人，2019；Karpukhin等人，2020），包括Natural Questions（NQ；Kwiatkowski等人，2019）、TriviaQA（TQ；Joshi等人，2017）和SQuAD（Rajpurkar等人，2016）数据集的开发集问题，见表5b。作为基线，我们包含了使用Anserini（Yang等人，2018a）工具包的BM25（Robertson等人，1995）结果。我们观察到，ColBERTv2在三个查询集上均优于BM25、原始ColBERT和SPLADEv2，比SPLADEv2的最高提升达到4.6个百分点。

### 5.2.3. LoTTE.  

接下来，我们分析LoTTE测试基准上的表现，该基准聚焦于长尾主题的自然查询，并展示了不同于先前域外评估数据集的注释模式。特别是，LoTTE使用自动Google排名（针对“搜索”查询）和自然生成的StackExchange问答对（针对“论坛”查询），补充了如TREC-COVID（在BEIR中）等数据集的基于汇总的注释和开放问答检索中的答案重叠指标。我们报告了每个语料库在搜索查询和论坛查询上的成功率@5。

---

总体来看，我们发现ANCE和原始ColBERT在所有主题上均优于BM25，并且使用蒸馏的三种方法总体表现最强。与Wikipedia-OpenQA结果类似，我们发现ColBERTv2在所有主题和查询类型上均优于基线系统，比SPLADEv2和RocketQAv2分别提升了最多3.7和8.1个百分点。考虑到基线系统，我们观察到RocketQAv2在“搜索”查询上相对SPLADEv2具有些许优势，而SPLADEv2在“论坛”测试中则显著更有效。我们推测，来自Google的搜索查询（通过GooAQ获取）比论坛查询更接近MS MARCO，因此后者更强调泛化，利于像SPLADEv2和ColBERTv2这样的词项分解模型。  

## 5.3. Efficiency  

ColBERTv2的残差压缩方法显著减少了索引大小，相比原始ColBERT，存储MS MARCO的索引时，ColBERT需要154 GiB，而ColBERTv2在将嵌入压缩为每维1或2位时，仅需16 GiB或25 GiB，压缩比达到6-10倍。此存储数据包括存储倒排列表的4.5 GiB。

---

这与MS MARCO上的典型单向量模型存储相当，后者使用4字节无损浮点存储，每个9百万段落存储一个768维向量，存储总量略超25 GiB。实际上，单向量模型在使用像HNSW这样的最近邻索引进行快速搜索时，存储需求可能更大。相反，单向量表示本身可以被非常激进地压缩（Zhan等人，2021a, 2022），但通常相对于ColBERTv2等后期交互方法在质量上会有更大损失。

---

我们在附录B中讨论了压缩方法对搜索质量的影响，并在附录C中展示了每次查询延迟约为50-250毫秒的结果。

# 6. Conclusion  

我们介绍了ColBERTv2，这是一种提升多向量表示质量和空间效率的检索模型。我们假设聚类质心捕捉了词级表示的上下文语义，并提出了残差表示，利用这些模式显著减少多向量系统的存储占用。然后我们探索了多向量检索的改进监督，发现通过跨编码器系统的蒸馏可以显著提高其质量。所提出的ColBERTv2在我们对28个数据集的广泛评估中，无论是在域内还是域外，都显著优于现有检索器，达到了最先进的质量，同时展示出具有竞争力的存储占用。

## 6.1. Broader Impact & Ethical Considerations  

本研究主要致力于开发具有更好泛化能力且在空间消耗方面具有合理效率的检索模型。对小型领域应用的开箱即用的强泛化性能可以在实践中为许多用户服务，尤其是在没有训练数据的情况下。此外，检索对许多下游NLP任务具有重要意义，因为它可以帮助缩小语言模型的规模，从而提高效率（即，通过将知识与计算解耦），提升透明性（即，允许用户查看模型在进行陈述或预测时依赖的来源），并更容易更新（即，允许开发人员通过替换或添加语料库中的文档而无需重新训练模型）（Guu等人，2020；Borgeaud等人，2021；Khattab等人，2021a）。尽管如此，这项工作在滥用方面存在风险，尤其是在错误信息方面，因为检索可能会展示出相关但不准确的结果，具体取决于语料库的内容。此外，在大规模数据集上训练的泛化能力可能会将该数据集的偏见传播到新的领域和应用中。 

---

尽管我们的贡献使ColBERT的后期交互在存储成本上更高效，但相比于原始ColBERT模型的简单训练范式，大规模蒸馏和硬负样本增加了系统复杂性，进而增加了训练成本。虽然ColBERTv2在推理时的延迟和存储方面效率较高，但在极端资源受限的情况下，我们怀疑像SPLADEv2或RocketQAv2这样的更简单模型设计可能更适合优化环境。我们将所有系统的低层系统优化留待未来研究。另一值得探索的权衡方向是基于交叉编码器的重新排序架构，尽管交叉编码器由于其高表达能力而代价昂贵，但通常表现出较高精确度。

## 6.2. Research Limitations  

尽管我们在广泛的测试上评估了ColBERTv2，但所有基准都是英文的，并且按照相关工作，我们的域外测试使用了在MS MARCO上训练的模型。我们预计该方法在其他语言和使用其他更小的训练集（例如NaturalQuestions）训练时也能有效，但将这类测试留待未来研究。

---

我们在多种不同环境下观察到ColBERTv2相对于现有最先进系统的稳定增益。尽管如此，几乎所有IR数据集都包含假阴性（即，相关但未标注的段落），因此在解读任何单个结果时需要谨慎。然而，我们特意选择了带有不同注释偏差的基准数据集，例如，TREC-COVID（在BEIR中）注释了提交系统在竞赛时检索的文档池，LoTTE使用了自动Google排名（针对“搜索”查询）和StackExchange问答对（针对“论坛”查询），而开放问答测试依赖于事实性问题的段落-答案重叠。ColBERTv2在这些设置中表现良好。我们在附录§D中讨论了与LoTTE相关的其他问题。

---

我们与广泛的强基线进行了比较——包括稀疏检索和单向量模型——并在测试中发现了可靠的模式。然而，我们提醒读者，随着创新引入到这些模型系列中，实证趋势可能会发生变化，并且在模型系列之间确保完全一致的对比（apple-to-apple comparison）是很难的，因为每个系列都需要不同的复杂调优策略。因此，我们主要使用了这些问题的丰富最新文献中的结果和模型，如RocketQAv2和SPLADEv2。

---

在表示方面，我们专注于通过残差压缩来减少存储成本，在减少占用空间方面取得了显著进展，同时在很大程度上保留了质量。然而，我们尚未穷尽可能的更复杂的优化空间，并且我们预计更复杂的残差压缩形式和与令牌丢弃（Zhou和Devlin，2021）相结合的方法将为进一步减少空间占用开辟更多可能性。

# A. Analysis of ColBERT’s Semantic Space

ColBERT（Khattab和Zaharia，2020）在词级别分解表示和相似度计算。基于这种组合架构，我们假设ColBERT展示了一个“轻量级”语义空间：即使不进行特别的重新训练，每个词义对应的向量会非常接近，仅因上下文而产生轻微变化。

---

如果该假设成立，我们预计词汇中每个词的嵌入将局限于嵌入空间中的少数区域，对应于词的上下文“词义”。为验证该假设，我们分析了MS MARCO Passage Ranking（Nguyen等人，2016）集合中对应词的ColBERT嵌入：我们对近6亿个嵌入（对应27,000个唯一词）进行了 $k$-均值聚类，分为 $k=2^{18}$ 个簇。作为对比，我们使用随机嵌入重复该聚类过程，但保持词的真实分布。图2展示了每个簇中出现的非停用词数（图2a）和每个词出现在的不同簇数（图2b）的经验累积分布函数（eCDF）图${ }^6$。大多数词出现在非常少的质心数量中：特别是，我们观察到约90%的簇包含 $\leq 16$ 个不同的词，ColBERT嵌入，而随机嵌入的情况下，少于50%的簇包含 $\leq 16$ 个不同词。这表明质心有效地映射了ColBERT的语义空间。

---

表6展示了一些质心捕捉的语义空间示例。簇#917中最常见的词与摄影相关，例如“photos”和“photographs”。若进一步检查这些词出现的其他簇，会发现这些新簇之间有显著的语义重叠（例如，Photos-Photo、Photo-Image-Picture）和簇#917。我们在簇#216932中出现的龙卷风相关词也观察到类似的效果。

---

该分析表明质心能够高精度地概括ColBERT的表示。在第3.3节中，我们提出了一种利用这些质心并在维度级别进行细微调整的残差压缩机制，以有效编码后期交互向量。

# B. Impact of Compression  

我们的残差压缩方法（第3.3节）大致保留了未压缩嵌入的质量。具体而言，应用于MS MARCO上的原始ColBERT模型时，其MRR@10为36.2%，Recall@50为82.1%；采用2位压缩后，模型的MRR@10为36.2%，Recall@50为82.3%。在1位压缩时，模型达到35.5%的MRR@10和81.6%的Recall@50。

- 我们与ColBERT的早期压缩实现对比，该实现使用类似BPR（Yamada等人，2021a）的二进制表示，没有使用残差质心，1位（二进制）和2位（二进制）压缩分别实现了34.8%（35.7%）的MRR@10和80.5%（81.8%）的Recall@50。与原始ColBERT一样，这种压缩形式依赖于一个单独的FAISS索引来生成候选项。

---

我们还在执行下游任务的后期交互检索器上测试了残差压缩方法，即ColBERT-QA（Khattab等人，2021b）用于开放域问答任务NaturalQuestions和Baleen（Khattab等人，2021a）用于HoVer上的多跳推理以验证声明。在NQ开发集上，ColBERT-QA的成功率@5（成功率@20）仅略微下降，从75.3%（84.3%）降至74.3%（84.2%），而使用2位压缩检索时，开放问答答案精确匹配从47.9%降至47.7%。

---

类似地，在HoVer（Jiang等人，2020）开发集上，Baleen的检索R@100从92.2%下降到90.6%，但其句子级别精确匹配几乎保持不变，从39.2%升至39.4%。我们假设ColBERTv2中应用的监督方法（第3.2节）也可以用于提升下游任务的质量，通过提高这些任务的检索召回率。我们将此类探索留待未来研究。

# C. Retrieval Latency  

图3评估了ColBERTv2在不同规模的三个集合上的延迟表现，即MS MARCO、LoTTE Pooled（dev）和LoTTE Lifestyle（dev），分别包含约900万段落、240万答案帖子和27万答案帖子。我们在MS MARCO开发集和LoTTE“搜索”查询的三次运行中平均计算延迟。搜索在具有两颗Intel Xeon Gold 6132 CPU的服务器上执行，每颗CPU有28个硬件执行上下文，使用Titan V GPU。

---

图中展示了ColBERTv2的三种设置，特别是我们评估了1位和2位编码（第3.4节）的索引，以及查询向量探测最近1、2或4个质心（第3.5节）时的搜索情况。在每个向量探测probe个质心时，我们为每个查询评分probe $\times 2^{12}$或probe $\times 2^{14}$个候选项${ }^8$。

---

首先，我们注意到$x$轴上报告的质量范围相对较窄。例如，MS MARCO的质量范围从38.50到39.75，除了两个成本最低的设置外，所有设置的得分都在39.00以上。同样，$y$轴的范围在每次查询大约50毫秒到250毫秒之间（大多数低于150毫秒），这使用了我们相对简单的基于Python的实现。

---

进一步深入分析，我们看到在这三个数据集中，最优质量或接近最佳质量的结果均可在大约100毫秒的延迟内实现，尽管它们的规模和特性各异。2位索引稳定地优于1位索引，但更激进的压缩导致的质量损失很小。

# D. LoTTE  

## D.1. Domain coverage  

表9展示了LoTTE开发数据集中社区的完整分布。LoTTE所覆盖的主题涵盖了广泛的语言现象，反映了多样化的主题和社区。然而，由于所有帖子都是匿名用户提交的，我们没有关于贡献者身份的具体人口统计信息。所有帖子均以英语书写。

## D.2. Passages  

如第4节所述，我们通过从StackExchange存档中选择评分为正的段落来构建LoTTE集合。我们从段落中移除HTML标签并过滤掉空段落。对于每个段落，我们记录其对应的查询并保存查询到段落的映射，以跟踪与每个查询对应的已发布答案。

## D.3. Search queries  

我们从GooAQ中抽取出现在StackExchange帖子存档中的查询列表，构建LoTTE搜索查询。首先对GooAQ查询列表进行随机排序，这样在同一答案段落对应多个查询的情况下，我们可以随机选择要包含在LoTTE中的查询，而不是总是选择第一个出现的查询。我们确认每个查询至少有一个对应的答案段落。

## D.4. Forum queries  

对于每个LoTTE主题及其组成社区，首先计算每个社区占总查询量的比例。然后我们根据此分布构建一个截断的查询集，从每个社区中选择得分最高的查询，这些查询按1）查询评分和2）查询浏览量排序。我们仅使用具有被接受答案的查询。尽可能确保每个社区至少为截断集贡献50个查询。我们将截断集的总体大小设置为2000个查询，但由于四舍五入和/或每个社区的最低查询数要求，总数可能会超过2000。我们移除所有引号和HTML标签。

## D.5. Statistics   

图4绘制了每个LoTTE开发语料库中每个段落的单词数量。图5和图6分别绘制了每个查询的单词数量和对应答案段落数量，分为搜索查询和论坛查询。 

## D.6. Dev Results  

表7展示了LoTTE开发查询的域外评估结果。延续我们在第5节观察到的趋势，ColBERTv2在所有测试的模型中表现出色，持续优于其他模型。

## D.7. Licensing and Anonymity  

原始StackExchange帖子存档受知识共享署名-相同方式共享4.0许可协议（CC BY-SA 4.0）许可（sta）。在上传前，存档中的个人数据已被移除，尽管所有帖子均为公开的；当我们公开发布LoTTE时，将包含指向原始帖子的URL，以满足许可要求的适当归属。GooAQ数据集受Apache 2.0许可（Khashabi等人，2021）。我们也将以CC BY-SA 4.0许可发布LoTTE。根据GooAQ许可，搜索查询仅限用于非商业研究目的。

# E. BEIR数据集

表8列出了我们在评估中使用的BEIR数据集，包括其相应的许可信息、文档数量和测试集查询数量。关于每个数据集的详细描述，请参考Thakur等人（2021）。

---

我们的Touché评估使用了BEIR中的更新数据版本，用于评估我们运行的模型（即ColBERTv2和RocketQAv2）以及SPLADEv2。

---

我们还在开放问答基准NQ、TQ和SQuAD上进行了测试，每个基准大约有9000个开发集问题，以及多跳HoVer，其开发集中包含4000个声明。在压缩评估第B节中，我们使用了在NQ和HoVer域内训练的模型，这些训练集分别包含79,000个和18,000个查询。

# F. 实现与超参数

我们使用Python 3.7、PyTorch 1.9和HuggingFace Transformers 4.10（Wolf等人，2020）实现了ColBERTv2，基于Khattab和Zaharia（2020）最初的ColBERT实现进行扩展。我们使用FAISS 1.7（Johnson等人，2019）进行 $k$-均值聚类，但与ColBERT不同，我们未使用其进行最近邻搜索。相反，我们使用Python中的PyTorch原语实现了候选生成机制（第3.5节）。

---

我们在内部集群上进行了实验，通常在每个推理任务（如索引、计算蒸馏得分和检索）上使用多达四个12GB的Titan V GPU，训练时则使用四个80GB的A100 GPU。对于内存较小的GPU，可通过梯度累积实现。借助此基础设施，计算蒸馏得分的时间不到一天，在MS MARCO上以64路模型进行40万步训练约需五天，而索引约需两小时。我们大致估算在几个月的工作期内，实验、开发和评估的总耗时上限为20个GPU月。

---

与ColBERT一样，我们的编码器是一个共享的bert-base-uncased模型，用于查询和段落编码器，包含1.1亿个参数。我们保留Khattab和Zaharia（2020）建议的默认向量维度以及后续工作中使用的维度，即 $d=128$。本文报告的实验在MS MARCO训练集上进行。学习率 $\left(10^{-5}\right)$、批量大小（32个样本）和线性衰减的暖启动（20,000步）采用简单默认值，仅进行了有限的手动探索。

---

检索相关的超参数探索详见第C节。默认情况下，探测数设为probe $=2$，但在MS MARCO和Wikipedia等大型数据集上使用probe $=4$。默认设置下候选数为probe $* 2^{12}$，但在Wikipedia上设置为probe $* 2^{13}$，在MS MARCO上设置为probe $* 2^{14}$。我们将更广泛的超参数调优留待未来工作。

---

我们在MS MARCO上进行64路蒸馏训练，从每个查询的前500个检索段落中采样。MS MARCO的训练集包含约80万个查询，但只有约50万有标签。我们对全部80万查询应用蒸馏，每个训练样本包含一个“正样本”，定义为Token为正的段落或由跨编码器教师得分最高的段落，无论其标签如何。

---

我们进行了40万步训练，从经过预微调的检查点开始，使用32路训练样本和15万步。为生成每个训练查询的top-k段落，我们进行了两轮训练，遵循Khattab等人（2021b）。我们从硬三元组训练模型开始（类似于Khattab等人，2021b），然后进行蒸馏训练，最后使用蒸馏模型进行第二轮训练。初步实验表明，该初始化和两轮训练对质量影响较小，这表明可以避免它们以降低训练成本。

---

除非另有说明，展示的结果均代表单次运行。第3节中的延迟结果为三次运行的平均值。为了开放问答检索评估，我们使用了Khattab等人（2021b）的评估脚本，该脚本检查短答案字符串是否出现在（有标题的）Wikipedia段落中。这适配了DPR（Karpukhin等人，2020）的评估代码。我们使用了Karpukhin等人（2020）发布的预处理后的2018年12月的Wikipedia数据集。

---

对于域外评估，我们选择遵循Thakur等人（2021），在BEIR和LoTTE上将ColBERT、RocketQAv2和ColBERTv2的最大文档长度设置为300个词元。Formal等人（2021a）在SPLADEv2的MS MARCO和BEIR上为查询和文档都选择了最大序列长度256，我们在LoTTE上测试其系统时保留了这一默认设置。除非另有说明，我们在ColBERTv2和RocketQAv2中保持默认查询最大序列长度为32个词元。在BEIR的ArguAna测试中，由于查询本身是长文档，我们将ColBERTv2和RocketQAv2的最大查询长度设置为300。在Climate-FEVER中，由于查询是较长的声明句，我们将ColBERTv2的最大查询长度设置为64。

---

我们使用了BEIR的开源实现${ }^{11}$和SPLADEv2的评估代码${ }^{12}$，用于评估SPLADEv2和ANCE以及LoTTE上的BM25。我们使用Anserini（Yang等人，2018a）工具包在Wikipedia开放问答检索测试中执行BM25评估，如Khattab等人（2021b）。我们使用RocketQAv2作者开发的实现评估RocketQAv2。${ }^{13}$